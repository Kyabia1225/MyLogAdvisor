[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "setupJob",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setupJob() throws Exception\n{\r\n    super.setupJob();\r\n    getWrapperFS().setLogEvents(MockS3AFileSystem.LOG_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "newJobCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PartitionedStagingCommitter newJobCommitter() throws IOException\n{\r\n    return new PartitionedStagingCommitterForTesting(createTaskAttemptForJob());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testDefaultFailAndAppend",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testDefaultFailAndAppend() throws Exception\n{\r\n    FileSystem mockS3 = getMockS3A();\r\n    for (String mode : Arrays.asList(null, CONFLICT_MODE_FAIL, CONFLICT_MODE_APPEND)) {\r\n        if (mode != null) {\r\n            getJob().getConfiguration().set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, mode);\r\n        } else {\r\n            getJob().getConfiguration().unset(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE);\r\n        }\r\n        PartitionedStagingCommitter committer = newJobCommitter();\r\n        committer.commitJob(getJob());\r\n        reset(mockS3);\r\n        pathsExist(mockS3, \"dateint=20161116\", \"dateint=20161116/hour=10\");\r\n        committer.commitJob(getJob());\r\n        verifyCompletion(mockS3);\r\n        reset(mockS3);\r\n        pathsExist(mockS3, \"dateint=20161115/hour=14\");\r\n        committer.commitJob(getJob());\r\n        verifyCompletion(mockS3);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testBadConflictMode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBadConflictMode() throws Throwable\n{\r\n    getJob().getConfiguration().set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, \"merge\");\r\n    intercept(IllegalArgumentException.class, \"MERGE\", \"committer conflict\", this::newJobCommitter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testReplace",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testReplace() throws Exception\n{\r\n    S3AFileSystem mockS3 = getMockS3A();\r\n    getJob().getConfiguration().set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, CONFLICT_MODE_REPLACE);\r\n    PartitionedStagingCommitter committer = newJobCommitter();\r\n    committer.commitJob(getJob());\r\n    verifyReplaceCommitActions(mockS3);\r\n    verifyCompletion(mockS3);\r\n    reset(mockS3);\r\n    pathsExist(mockS3, \"dateint=20161115\", \"dateint=20161115/hour=12\");\r\n    committer.commitJob(getJob());\r\n    verifyReplaceCommitActions(mockS3);\r\n    verifyCompletion(mockS3);\r\n    reset(mockS3);\r\n    pathsExist(mockS3, \"dateint=20161115/hour=12\", \"dateint=20161115/hour=13\");\r\n    canDelete(mockS3, \"dateint=20161115/hour=13\");\r\n    committer.commitJob(getJob());\r\n    verifyDeleted(mockS3, \"dateint=20161115/hour=13\");\r\n    verifyReplaceCommitActions(mockS3);\r\n    verifyCompletion(mockS3);\r\n    reset(mockS3);\r\n    pathsExist(mockS3, \"dateint=20161116/hour=13\", \"dateint=20161116/hour=14\");\r\n    canDelete(mockS3, \"dateint=20161116/hour=13\", \"dateint=20161116/hour=14\");\r\n    committer.commitJob(getJob());\r\n    verifyReplaceCommitActions(mockS3);\r\n    verifyDeleted(mockS3, \"dateint=20161116/hour=13\");\r\n    verifyDeleted(mockS3, \"dateint=20161116/hour=14\");\r\n    verifyCompletion(mockS3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "verifyReplaceCommitActions",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyReplaceCommitActions(FileSystem mockS3) throws IOException\n{\r\n    verifyDeleted(mockS3, \"dateint=20161115/hour=13\");\r\n    verifyDeleted(mockS3, \"dateint=20161115/hour=14\");\r\n    verifyDeleted(mockS3, \"dateint=20161116/hour=13\");\r\n    verifyDeleted(mockS3, \"dateint=20161116/hour=14\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testReplaceWithDeleteFailure",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testReplaceWithDeleteFailure() throws Exception\n{\r\n    FileSystem mockS3 = getMockS3A();\r\n    getJob().getConfiguration().set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, CONFLICT_MODE_REPLACE);\r\n    final PartitionedStagingCommitter committer = newJobCommitter();\r\n    pathsExist(mockS3, \"dateint=20161116/hour=14\");\r\n    when(mockS3.delete(new Path(outputPath, \"dateint=20161116/hour=14\"), true)).thenThrow(new PathCommitException(\"fake\", \"Fake IOException for delete\"));\r\n    intercept(PathCommitException.class, \"Fake IOException for delete\", \"Should throw the fake IOException\", () -> committer.commitJob(getJob()));\r\n    verifyReplaceCommitActions(mockS3);\r\n    verifyDeleted(mockS3, \"dateint=20161116/hour=14\");\r\n    assertTrue(\"Should have aborted\", ((PartitionedStagingCommitterForTesting) committer).aborted);\r\n    verifyCompletion(mockS3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "listObjectsAsync",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CompletableFuture<S3ListResult> listObjectsAsync(final S3ListRequest request, final DurationTrackerFactory trackerFactory, AuditSpan span)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "continueListObjectsAsync",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CompletableFuture<S3ListResult> continueListObjectsAsync(final S3ListRequest request, final S3ListResult prevResult, final DurationTrackerFactory trackerFactory, AuditSpan span)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "toLocatedFileStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3ALocatedFileStatus toLocatedFileStatus(S3AFileStatus status) throws IOException\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "createListObjectsRequest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3ListRequest createListObjectsRequest(String key, String delimiter, AuditSpan span)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "getDefaultBlockSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDefaultBlockSize(Path path)\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "getMaxKeys",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxKeys()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testUnbuffer",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testUnbuffer() throws IOException\n{\r\n    Path path = new Path(\"/file\");\r\n    ObjectMetadata meta = mock(ObjectMetadata.class);\r\n    when(meta.getContentLength()).thenReturn(1L);\r\n    when(meta.getLastModified()).thenReturn(new Date(2L));\r\n    when(meta.getETag()).thenReturn(\"mock-etag\");\r\n    when(s3.getObjectMetadata(any())).thenReturn(meta);\r\n    S3ObjectInputStream objectStream = mock(S3ObjectInputStream.class);\r\n    when(objectStream.read()).thenReturn(-1);\r\n    S3Object s3Object = mock(S3Object.class);\r\n    when(s3Object.getObjectContent()).thenReturn(objectStream);\r\n    when(s3Object.getObjectMetadata()).thenReturn(meta);\r\n    when(s3.getObject(any())).thenReturn(s3Object);\r\n    FSDataInputStream stream = fs.open(path);\r\n    assertEquals(0, stream.read(new byte[8]));\r\n    stream.unbuffer();\r\n    verify(objectStream, times(1)).close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { \"keep-markers-auditing\", true, true }, { \"delete-markers-unaudited\", false, false } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    final Configuration conf = super.createConfiguration();\r\n    removeBaseAndBucketOverrides(conf, AUDIT_ENABLED);\r\n    conf.setBoolean(AUDIT_ENABLED, auditing);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "withAuditCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OperationCostValidator.ExpectedProbe withAuditCount(final int expected)\n{\r\n    return probe(AUDIT_SPAN_CREATION, auditing ? expected : 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testMkdirOverDir",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testMkdirOverDir() throws Throwable\n{\r\n    describe(\"create a dir over a dir\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path baseDir = dir(methodPath());\r\n    verifyMetrics(() -> fs.mkdirs(baseDir), withAuditCount(1), with(OBJECT_METADATA_REQUESTS, 0), with(OBJECT_LIST_REQUEST, FILESTATUS_DIR_PROBE_L));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testGetContentSummaryRoot",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGetContentSummaryRoot() throws Throwable\n{\r\n    describe(\"getContentSummary on Root\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path root = new Path(\"/\");\r\n    verifyMetrics(() -> getContentSummary(root), with(INVOCATION_GET_CONTENT_SUMMARY, 1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testGetContentSummaryDir",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetContentSummaryDir() throws Throwable\n{\r\n    describe(\"getContentSummary on test dir with children\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path baseDir = methodPath();\r\n    Path childDir = new Path(baseDir, \"subdir/child\");\r\n    touch(fs, childDir);\r\n    final ContentSummary summary = verifyMetrics(() -> getContentSummary(baseDir), with(INVOCATION_GET_CONTENT_SUMMARY, 1), withAuditCount(1), always(FILE_STATUS_FILE_PROBE.plus(LIST_OPERATION)));\r\n    Assertions.assertThat(summary.getDirectoryCount()).as(\"Summary \" + summary).isEqualTo(2);\r\n    Assertions.assertThat(summary.getFileCount()).as(\"Summary \" + summary).isEqualTo(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testGetContentMissingPath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGetContentMissingPath() throws Throwable\n{\r\n    describe(\"getContentSummary on a missing path\");\r\n    Path baseDir = methodPath();\r\n    verifyMetricsIntercepting(FileNotFoundException.class, \"\", () -> getContentSummary(baseDir), with(INVOCATION_GET_CONTENT_SUMMARY, 1), withAuditCount(1), always(FILE_STATUS_FILE_PROBE.plus(FILE_STATUS_FILE_PROBE).plus(LIST_OPERATION).plus(LIST_OPERATION)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "getContentSummary",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ContentSummary getContentSummary(final Path baseDir) throws IOException\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    return fs.getContentSummary(baseDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "toClose",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T toClose(T tool)\n{\r\n    toolsToClose.add(tool);\r\n    return tool;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "expectResult",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectResult(int expected, String message, S3GuardTool tool, String... args) throws Exception\n{\r\n    assertEquals(message, expected, tool.run(args));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "expectSuccess",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String expectSuccess(String message, S3GuardTool tool, Object... args) throws Exception\n{\r\n    ByteArrayOutputStream buf = new ByteArrayOutputStream();\r\n    exec(SUCCESS, message, tool, buf, args);\r\n    return buf.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int run(Configuration conf, Object... args) throws Exception\n{\r\n    return runS3GuardCommand(conf, args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int run(Object... args) throws Exception\n{\r\n    return runS3GuardCommand(getConfiguration(), args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "runToFailure",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void runToFailure(int status, Object... args) throws Exception\n{\r\n    final Configuration conf = getConfiguration();\r\n    ExitUtil.ExitException ex = intercept(ExitUtil.ExitException.class, () -> runS3GuardCommand(conf, args));\r\n    if (ex.status != status) {\r\n        throw ex;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    super.teardown();\r\n    toolsToClose.forEach(t -> IOUtils.cleanupWithLogger(LOG, t));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testBucketInfoUnguarded",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBucketInfoUnguarded() throws Exception\n{\r\n    final Configuration conf = getConfiguration();\r\n    URI fsUri = getFileSystem().getUri();\r\n    S3GuardTool.BucketInfo infocmd = toClose(new S3GuardTool.BucketInfo(conf));\r\n    String info = exec(infocmd, S3GuardTool.BucketInfo.NAME, \"-\" + S3GuardTool.BucketInfo.UNGUARDED_FLAG, fsUri.toString());\r\n    assertTrue(\"Output should contain information about S3A client \" + info, info.contains(\"S3A Client\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testBucketInfoMarkerAware",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBucketInfoMarkerAware() throws Throwable\n{\r\n    final Configuration conf = getConfiguration();\r\n    URI fsUri = getFileSystem().getUri();\r\n    S3GuardTool.BucketInfo infocmd = toClose(new S3GuardTool.BucketInfo(conf));\r\n    String info = exec(infocmd, S3GuardTool.BucketInfo.NAME, \"-\" + MARKERS, S3GuardTool.BucketInfo.MARKERS_AWARE, fsUri.toString());\r\n    assertTrue(\"Output should contain information about S3A client \" + info, info.contains(IS_MARKER_AWARE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testBucketInfoMarkerPolicyUnknown",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testBucketInfoMarkerPolicyUnknown() throws Throwable\n{\r\n    final Configuration conf = getConfiguration();\r\n    URI fsUri = getFileSystem().getUri();\r\n    S3GuardTool.BucketInfo infocmd = toClose(new S3GuardTool.BucketInfo(conf));\r\n    intercept(ExitUtil.ExitException.class, \"\" + EXIT_NOT_ACCEPTABLE, () -> exec(infocmd, S3GuardTool.BucketInfo.NAME, \"-\" + MARKERS, \"unknown\", fsUri.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "makeBindedTool",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "S3GuardTool makeBindedTool(Class<? extends S3GuardTool> tool) throws Exception\n{\r\n    Configuration conf = getConfiguration();\r\n    return tool.getDeclaredConstructor(Configuration.class).newInstance(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testToolsNoBucket",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testToolsNoBucket() throws Throwable\n{\r\n    List<Class<? extends S3GuardTool>> tools = Arrays.asList(S3GuardTool.BucketInfo.class, S3GuardTool.Uploads.class);\r\n    for (Class<? extends S3GuardTool> tool : tools) {\r\n        S3GuardTool cmdR = makeBindedTool(tool);\r\n        describe(\"Calling \" + cmdR.getName() + \" on a bucket that does not exist.\");\r\n        String[] argsR = new String[] { cmdR.getName(), S3A_THIS_BUCKET_DOES_NOT_EXIST };\r\n        intercept(UnknownStoreException.class, () -> cmdR.run(argsR));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testToolsNoArgsForBucket",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testToolsNoArgsForBucket() throws Throwable\n{\r\n    List<Class<? extends S3GuardTool>> tools = Arrays.asList(S3GuardTool.BucketInfo.class, S3GuardTool.Uploads.class);\r\n    for (Class<? extends S3GuardTool> tool : tools) {\r\n        S3GuardTool cmdR = makeBindedTool(tool);\r\n        describe(\"Calling \" + cmdR.getName() + \" without any arguments.\");\r\n        assertExitCode(INVALID_ARGUMENT, intercept(ExitUtil.ExitException.class, () -> cmdR.run(new String[] { tool.getName() })));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testUnsupported",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testUnsupported() throws Throwable\n{\r\n    describe(\"Verify the unsupported tools are rejected\");\r\n    for (String tool : UNSUPPORTED_COMMANDS) {\r\n        describe(\"Probing %s\", tool);\r\n        runToFailure(E_S3GUARD_UNSUPPORTED, tool);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testProbeForMagic",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testProbeForMagic() throws Throwable\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    String name = fs.getUri().toString();\r\n    S3GuardTool.BucketInfo cmd = new S3GuardTool.BucketInfo(getConfiguration());\r\n    exec(cmd, S3GuardTool.BucketInfo.MAGIC_FLAG, name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "assertExitCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertExitCode(final int expectedErrorCode, final ExitUtil.ExitException e)\n{\r\n    if (e.getExitCode() != expectedErrorCode) {\r\n        throw new AssertionError(\"Expected error code \" + expectedErrorCode + \" in \" + e, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testCleanMarkersLegacyDir",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCleanMarkersLegacyDir() throws Throwable\n{\r\n    describe(\"Clean markers under a deleting FS -expect none\");\r\n    CreatedPaths createdPaths = createPaths(getDeletingFS(), methodPath());\r\n    markerTool(getDeletingFS(), createdPaths.base, false, 0);\r\n    markerTool(getDeletingFS(), createdPaths.base, true, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testCleanMarkersFileLimit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCleanMarkersFileLimit() throws Throwable\n{\r\n    describe(\"Clean markers under a keeping FS -with file limit\");\r\n    CreatedPaths createdPaths = createPaths(getKeepingFS(), methodPath());\r\n    markerTool(EXIT_INTERRUPTED, getDeletingFS(), createdPaths.base, false, 0, 1, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testCleanMarkersKeepingDir",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testCleanMarkersKeepingDir() throws Throwable\n{\r\n    describe(\"Audit then clean markers under a deleting FS \" + \"-expect markers to be found and then cleaned up\");\r\n    CreatedPaths createdPaths = createPaths(getKeepingFS(), methodPath());\r\n    int expectedMarkerCount = createdPaths.dirs.size();\r\n    S3AFileSystem fs = getDeletingFS();\r\n    LOG.info(\"Auditing a directory with retained markers -expect failure\");\r\n    markerTool(EXIT_NOT_ACCEPTABLE, fs, createdPaths.base, false, 0, UNLIMITED_LISTING, false);\r\n    LOG.info(\"Auditing a directory expecting retained markers\");\r\n    markerTool(fs, createdPaths.base, false, expectedMarkerCount);\r\n    LOG.info(\"Auditing a directory expecting retained markers\");\r\n    markerTool(fs, createdPaths.base, false, expectedMarkerCount);\r\n    LOG.info(\"Purging a directory of retained markers\");\r\n    assertMarkersDeleted(expectedMarkerCount, markerTool(fs, createdPaths.base, true, expectedMarkerCount));\r\n    LOG.info(\"Auditing a directory with retained markers -expect success\");\r\n    assertMarkersDeleted(0, markerTool(fs, createdPaths.base, true, 0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testRenameKeepingFS",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRenameKeepingFS() throws Throwable\n{\r\n    describe(\"Rename with the keeping FS -verify that no markers\" + \" exist at far end\");\r\n    Path base = methodPath();\r\n    Path source = new Path(base, \"source\");\r\n    Path dest = new Path(base, \"dest\");\r\n    S3AFileSystem fs = getKeepingFS();\r\n    CreatedPaths createdPaths = createPaths(fs, source);\r\n    int expectedMarkerCount = createdPaths.dirs.size();\r\n    markerTool(fs, source, false, expectedMarkerCount);\r\n    fs.rename(source, dest);\r\n    assertIsDirectory(dest);\r\n    markerTool(fs, dest, false, 0);\r\n    LOG.info(\"Auditing destination paths\");\r\n    verifyRenamed(dest, createdPaths);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testAuthPathIsMixed",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testAuthPathIsMixed() throws Throwable\n{\r\n    describe(\"Create a source tree with mixed semantics\");\r\n    Path base = methodPath();\r\n    Path source = new Path(base, \"source\");\r\n    Path dest = new Path(base, \"dest\");\r\n    Path dir2 = new Path(source, \"dir2\");\r\n    S3AFileSystem mixedFSDir2 = createFS(DIRECTORY_MARKER_POLICY_AUTHORITATIVE, dir2.toUri().toString());\r\n    setMixedFS(mixedFSDir2);\r\n    CreatedPaths createdPaths = createPaths(mixedFSDir2, source);\r\n    markerTool(mixedFSDir2, toPath(source, \"dir1\"), false, 0);\r\n    markerTool(mixedFSDir2, source, false, expectedMarkersUnderDir2);\r\n    markerTool(EXIT_NOT_ACCEPTABLE, mixedFSDir2, source, false, 0, 0, false);\r\n    markerTool(0, mixedFSDir2, source, false, 0, 0, true);\r\n    LOG.info(\"Executing rename\");\r\n    mixedFSDir2.rename(source, dest);\r\n    assertIsDirectory(dest);\r\n    MarkerTool.ScanResult scanResult = markerTool(mixedFSDir2, dest, false, 0);\r\n    Assertions.assertThat(scanResult).describedAs(\"Scan result %s\", scanResult).extracting(s -> s.getTracker().getFilesFound()).isEqualTo(expectedFileCount);\r\n    verifyRenamed(dest, createdPaths);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "assertMarkersDeleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertMarkersDeleted(int expected, MarkerTool.ScanResult result)\n{\r\n    Assertions.assertThat(result.getPurgeSummary()).describedAs(\"Purge result of scan %s\", result).isNotNull().extracting(f -> f.getMarkersDeleted()).isEqualTo(expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testRunNoArgs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRunNoArgs() throws Throwable\n{\r\n    runToFailure(EXIT_USAGE, MARKERS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testRunWrongBucket",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRunWrongBucket() throws Throwable\n{\r\n    runToFailure(EXIT_NOT_FOUND, MARKERS, AUDIT, \"s3a://this-bucket-does-not-exist-hopefully\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testRunUnknownPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRunUnknownPath() throws Throwable\n{\r\n    runToFailure(EXIT_NOT_FOUND, MARKERS, AUDIT, methodPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testRunTooManyActions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRunTooManyActions() throws Throwable\n{\r\n    runToFailure(EXIT_USAGE, MARKERS, AUDIT, CLEAN, methodPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testRunAuditWithExpectedMarkers",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRunAuditWithExpectedMarkers() throws Throwable\n{\r\n    describe(\"Run a verbose audit expecting some markers\");\r\n    CreatedPaths createdPaths = createPaths(getKeepingFS(), methodPath());\r\n    final File audit = tempAuditFile();\r\n    run(MARKERS, V, AUDIT, m(OPT_LIMIT), 0, m(OPT_OUT), audit, m(OPT_MIN), expectedMarkersWithBaseDir - 1, m(OPT_MAX), expectedMarkersWithBaseDir + 1, createdPaths.base);\r\n    expectMarkersInOutput(audit, expectedMarkersWithBaseDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testRunAuditWithExpectedMarkersSwappedMinMax",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRunAuditWithExpectedMarkersSwappedMinMax() throws Throwable\n{\r\n    describe(\"Run a verbose audit with the min/max ranges swapped;\" + \" see HADOOP-17332\");\r\n    CreatedPaths createdPaths = createPaths(getKeepingFS(), methodPath());\r\n    final File audit = tempAuditFile();\r\n    run(MARKERS, V, AUDIT, m(OPT_LIMIT), 0, m(OPT_OUT), audit, m(OPT_MIN), expectedMarkersWithBaseDir + 1, m(OPT_MAX), expectedMarkersWithBaseDir - 1, createdPaths.base);\r\n    expectMarkersInOutput(audit, expectedMarkersWithBaseDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testRunAuditWithExcessMarkers",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRunAuditWithExcessMarkers() throws Throwable\n{\r\n    describe(\"Run a verbose audit failing as surplus markers were found\");\r\n    CreatedPaths createdPaths = createPaths(getKeepingFS(), methodPath());\r\n    final File audit = tempAuditFile();\r\n    runToFailure(EXIT_NOT_ACCEPTABLE, MARKERS, V, AUDIT, m(OPT_OUT), audit, createdPaths.base);\r\n    expectMarkersInOutput(audit, expectedMarkersWithBaseDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testRunLimitedAudit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testRunLimitedAudit() throws Throwable\n{\r\n    describe(\"Audit with a limited number of files (2)\");\r\n    CreatedPaths createdPaths = createPaths(getKeepingFS(), methodPath());\r\n    runToFailure(EXIT_INTERRUPTED, MARKERS, V, m(OPT_LIMIT), 2, CLEAN, createdPaths.base);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testRunLimitedLandsatAudit",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testRunLimitedLandsatAudit() throws Throwable\n{\r\n    describe(\"Audit a few thousand landsat objects\");\r\n    final File audit = tempAuditFile();\r\n    runToFailure(EXIT_INTERRUPTED, MARKERS, AUDIT, m(OPT_LIMIT), 3000, m(OPT_OUT), audit, LANDSAT_BUCKET);\r\n    readOutput(audit);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testBucketInfoKeepingOnDeleting",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBucketInfoKeepingOnDeleting() throws Throwable\n{\r\n    describe(\"Run bucket info with the keeping config on the deleting fs\");\r\n    runS3GuardCommandToFailure(uncachedFSConfig(getDeletingFS()), EXIT_NOT_ACCEPTABLE, BUCKET_INFO, m(MARKERS), DIRECTORY_MARKER_POLICY_KEEP, methodPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testBucketInfoKeepingOnKeeping",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBucketInfoKeepingOnKeeping() throws Throwable\n{\r\n    describe(\"Run bucket info with the keeping config on the keeping fs\");\r\n    runS3GuardCommand(uncachedFSConfig(getKeepingFS()), BUCKET_INFO, m(MARKERS), DIRECTORY_MARKER_POLICY_KEEP, methodPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testBucketInfoDeletingOnDeleting",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBucketInfoDeletingOnDeleting() throws Throwable\n{\r\n    describe(\"Run bucket info with the deleting config on the deleting fs\");\r\n    runS3GuardCommand(uncachedFSConfig(getDeletingFS()), BUCKET_INFO, m(MARKERS), DIRECTORY_MARKER_POLICY_DELETE, methodPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "testBucketInfoAuthOnAuth",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBucketInfoAuthOnAuth() throws Throwable\n{\r\n    describe(\"Run bucket info with the auth FS\");\r\n    Path base = methodPath();\r\n    S3AFileSystem authFS = createFS(DIRECTORY_MARKER_POLICY_AUTHORITATIVE, base.toUri().toString());\r\n    setMixedFS(authFS);\r\n    runS3GuardCommand(uncachedFSConfig(authFS), BUCKET_INFO, m(MARKERS), DIRECTORY_MARKER_POLICY_AUTHORITATIVE, methodPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "createPaths",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "CreatedPaths createPaths(FileSystem fs, Path base) throws IOException\n{\r\n    CreatedPaths r = new CreatedPaths(fs, base);\r\n    r.mkdir(\"\");\r\n    r.emptydir(\"empty\");\r\n    r.mkdir(\"dir1\");\r\n    expectedFileCount = r.files(\"dir1/file1\");\r\n    expectedMarkersUnderDir1 = 1;\r\n    r.dirs(\"dir2\", \"dir2/dir3\");\r\n    r.emptydir(\"dir2/empty2\");\r\n    expectedFileCount += r.files(\"dir2/file2\", \"dir2/dir3/file3\");\r\n    expectedMarkersUnderDir2 = 2;\r\n    expectedMarkers = expectedMarkersUnderDir1 + expectedMarkersUnderDir2;\r\n    expectedMarkersWithBaseDir = expectedMarkers + 1;\r\n    return r;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "verifyRenamed",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyRenamed(final Path dest, final CreatedPaths createdPaths) throws IOException\n{\r\n    for (String p : createdPaths.emptyDirsUnderBase) {\r\n        assertIsDirectory(toPath(dest, p));\r\n    }\r\n    for (String p : createdPaths.dirsUnderBase) {\r\n        assertIsDirectory(toPath(dest, p));\r\n    }\r\n    for (String p : createdPaths.filesUnderBase) {\r\n        assertIsFile(toPath(dest, p));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "createAndBindMockFSInstance",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "S3AFileSystem createAndBindMockFSInstance(Configuration conf, Pair<StagingTestBase.ClientResults, StagingTestBase.ClientErrors> outcome) throws IOException\n{\r\n    S3AFileSystem mockFs = mockS3AFileSystemRobustly();\r\n    MockS3AFileSystem wrapperFS = new MockS3AFileSystem(mockFs, outcome);\r\n    URI uri = RAW_BUCKET_URI;\r\n    wrapperFS.initialize(uri, conf);\r\n    root = wrapperFS.makeQualified(new Path(\"/\"));\r\n    outputPath = new Path(root, OUTPUT_PREFIX);\r\n    outputPathUri = outputPath.toUri();\r\n    FileSystemTestHelper.addFileSystemForTesting(uri, conf, wrapperFS);\r\n    return mockFs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "mockS3AFileSystemRobustly",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "S3AFileSystem mockS3AFileSystemRobustly()\n{\r\n    S3AFileSystem mockFS = mock(S3AFileSystem.class);\r\n    doNothing().when(mockFS).incrementReadOperations();\r\n    doNothing().when(mockFS).incrementWriteOperations();\r\n    doNothing().when(mockFS).incrementWriteOperations();\r\n    doNothing().when(mockFS).incrementWriteOperations();\r\n    return mockFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "lookupWrapperFS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MockS3AFileSystem lookupWrapperFS(Configuration conf) throws IOException\n{\r\n    return (MockS3AFileSystem) FileSystem.get(outputPathUri, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "verifyCompletion",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyCompletion(FileSystem mockS3) throws IOException\n{\r\n    verifyCleanupTempFiles(mockS3);\r\n    verifyNoMoreInteractions(mockS3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "verifyDeleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyDeleted(FileSystem mockS3, Path path) throws IOException\n{\r\n    verify(mockS3).delete(path, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "verifyDeleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyDeleted(FileSystem mockS3, String child) throws IOException\n{\r\n    verifyDeleted(mockS3, new Path(outputPath, child));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "verifyCleanupTempFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyCleanupTempFiles(FileSystem mockS3) throws IOException\n{\r\n    verifyDeleted(mockS3, new Path(outputPath, CommitConstants.TEMPORARY));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "assertConflictResolution",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertConflictResolution(StagingCommitter committer, JobContext job, ConflictResolution mode)\n{\r\n    Assert.assertEquals(\"Conflict resolution mode in \" + committer, mode, committer.getConflictResolutionMode(job, new Configuration()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "pathsExist",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void pathsExist(FileSystem mockS3, String... children) throws IOException\n{\r\n    for (String child : children) {\r\n        pathExists(mockS3, new Path(outputPath, child));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "pathExists",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void pathExists(FileSystem mockS3, Path path) throws IOException\n{\r\n    when(mockS3.exists(path)).thenReturn(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "pathIsDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void pathIsDirectory(FileSystem mockS3, Path path) throws IOException\n{\r\n    hasFileStatus(mockS3, path, new FileStatus(0, true, 0, 0, 0, path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "pathIsFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void pathIsFile(FileSystem mockS3, Path path) throws IOException\n{\r\n    pathExists(mockS3, path);\r\n    hasFileStatus(mockS3, path, new FileStatus(0, false, 0, 0, 0, path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "pathDoesNotExist",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void pathDoesNotExist(FileSystem mockS3, Path path) throws IOException\n{\r\n    when(mockS3.exists(path)).thenReturn(false);\r\n    when(mockS3.getFileStatus(path)).thenThrow(new FileNotFoundException(\"mock fnfe of \" + path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "hasFileStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void hasFileStatus(FileSystem mockS3, Path path, FileStatus status) throws IOException\n{\r\n    when(mockS3.getFileStatus(path)).thenReturn(status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "mkdirsHasOutcome",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void mkdirsHasOutcome(FileSystem mockS3, Path path, boolean outcome) throws IOException\n{\r\n    when(mockS3.mkdirs(path)).thenReturn(outcome);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "canDelete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void canDelete(FileSystem mockS3, String... children) throws IOException\n{\r\n    for (String child : children) {\r\n        canDelete(mockS3, new Path(outputPath, child));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "canDelete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void canDelete(FileSystem mockS3, Path f) throws IOException\n{\r\n    when(mockS3.delete(f, true)).thenReturn(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "verifyExistenceChecked",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyExistenceChecked(FileSystem mockS3, String child) throws IOException\n{\r\n    verifyExistenceChecked(mockS3, new Path(outputPath, child));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "verifyExistenceChecked",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyExistenceChecked(FileSystem mockS3, Path path) throws IOException\n{\r\n    verify(mockS3).getFileStatus(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "verifyMkdirsInvoked",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyMkdirsInvoked(FileSystem mockS3, Path path) throws IOException\n{\r\n    verify(mockS3).mkdirs(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getArgumentAt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T getArgumentAt(InvocationOnMock invocation, int index, Class<T> clazz)\n{\r\n    return (T) invocation.getArguments()[index];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "newMockS3Client",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "AmazonS3 newMockS3Client(final ClientResults results, final ClientErrors errors)\n{\r\n    AmazonS3Client mockClient = mock(AmazonS3Client.class);\r\n    final Object lock = new Object();\r\n    when(mockClient.initiateMultipartUpload(any(InitiateMultipartUploadRequest.class))).thenAnswer(invocation -> {\r\n        LOG.debug(\"initiateMultipartUpload for {}\", mockClient);\r\n        synchronized (lock) {\r\n            if (results.requests.size() == errors.failOnInit) {\r\n                if (errors.recover) {\r\n                    errors.failOnInit(-1);\r\n                }\r\n                throw new AmazonClientException(\"Mock Fail on init \" + results.requests.size());\r\n            }\r\n            String uploadId = UUID.randomUUID().toString();\r\n            InitiateMultipartUploadRequest req = getArgumentAt(invocation, 0, InitiateMultipartUploadRequest.class);\r\n            results.requests.put(uploadId, req);\r\n            results.activeUploads.put(uploadId, req.getKey());\r\n            results.uploads.add(uploadId);\r\n            return newResult(results.requests.get(uploadId), uploadId);\r\n        }\r\n    });\r\n    when(mockClient.uploadPart(any(UploadPartRequest.class))).thenAnswer(invocation -> {\r\n        LOG.debug(\"uploadPart for {}\", mockClient);\r\n        synchronized (lock) {\r\n            if (results.parts.size() == errors.failOnUpload) {\r\n                if (errors.recover) {\r\n                    errors.failOnUpload(-1);\r\n                }\r\n                LOG.info(\"Triggering upload failure\");\r\n                throw new AmazonClientException(\"Mock Fail on upload \" + results.parts.size());\r\n            }\r\n            UploadPartRequest req = getArgumentAt(invocation, 0, UploadPartRequest.class);\r\n            results.parts.add(req);\r\n            String etag = UUID.randomUUID().toString();\r\n            List<String> etags = results.tagsByUpload.get(req.getUploadId());\r\n            if (etags == null) {\r\n                etags = Lists.newArrayList();\r\n                results.tagsByUpload.put(req.getUploadId(), etags);\r\n            }\r\n            etags.add(etag);\r\n            return newResult(req, etag);\r\n        }\r\n    });\r\n    when(mockClient.completeMultipartUpload(any(CompleteMultipartUploadRequest.class))).thenAnswer(invocation -> {\r\n        LOG.debug(\"completeMultipartUpload for {}\", mockClient);\r\n        synchronized (lock) {\r\n            if (results.commits.size() == errors.failOnCommit) {\r\n                if (errors.recover) {\r\n                    errors.failOnCommit(-1);\r\n                }\r\n                throw new AmazonClientException(\"Mock Fail on commit \" + results.commits.size());\r\n            }\r\n            CompleteMultipartUploadRequest req = getArgumentAt(invocation, 0, CompleteMultipartUploadRequest.class);\r\n            String uploadId = req.getUploadId();\r\n            removeUpload(results, uploadId);\r\n            results.commits.add(req);\r\n            return newResult(req);\r\n        }\r\n    });\r\n    doAnswer(invocation -> {\r\n        LOG.debug(\"abortMultipartUpload for {}\", mockClient);\r\n        synchronized (lock) {\r\n            if (results.aborts.size() == errors.failOnAbort) {\r\n                if (errors.recover) {\r\n                    errors.failOnAbort(-1);\r\n                }\r\n                throw new AmazonClientException(\"Mock Fail on abort \" + results.aborts.size());\r\n            }\r\n            AbortMultipartUploadRequest req = getArgumentAt(invocation, 0, AbortMultipartUploadRequest.class);\r\n            String id = req.getUploadId();\r\n            removeUpload(results, id);\r\n            results.aborts.add(req);\r\n            return null;\r\n        }\r\n    }).when(mockClient).abortMultipartUpload(any(AbortMultipartUploadRequest.class));\r\n    doAnswer(invocation -> {\r\n        LOG.debug(\"deleteObject for {}\", mockClient);\r\n        synchronized (lock) {\r\n            results.deletes.add(getArgumentAt(invocation, 0, DeleteObjectRequest.class));\r\n            return null;\r\n        }\r\n    }).when(mockClient).deleteObject(any(DeleteObjectRequest.class));\r\n    doAnswer(invocation -> {\r\n        LOG.debug(\"deleteObject for {}\", mockClient);\r\n        synchronized (lock) {\r\n            results.deletes.add(new DeleteObjectRequest(getArgumentAt(invocation, 0, String.class), getArgumentAt(invocation, 1, String.class)));\r\n            return null;\r\n        }\r\n    }).when(mockClient).deleteObject(any(String.class), any(String.class));\r\n    when(mockClient.toString()).thenAnswer(invocation -> \"Mock3AClient \" + results + \" \" + errors);\r\n    when(mockClient.listMultipartUploads(any(ListMultipartUploadsRequest.class))).thenAnswer(invocation -> {\r\n        synchronized (lock) {\r\n            MultipartUploadListing l = new MultipartUploadListing();\r\n            l.setMultipartUploads(results.activeUploads.entrySet().stream().map(e -> newMPU(e.getKey(), e.getValue())).collect(Collectors.toList()));\r\n            return l;\r\n        }\r\n    });\r\n    return mockClient;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "removeUpload",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void removeUpload(final ClientResults results, final String uploadId)\n{\r\n    String removed = results.activeUploads.remove(uploadId);\r\n    if (removed == null) {\r\n        AmazonS3Exception ex = new AmazonS3Exception(\"not found \" + uploadId);\r\n        ex.setStatusCode(404);\r\n        throw ex;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "newResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CompleteMultipartUploadResult newResult(CompleteMultipartUploadRequest req)\n{\r\n    return new CompleteMultipartUploadResult();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "newMPU",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MultipartUpload newMPU(String id, String path)\n{\r\n    MultipartUpload up = new MultipartUpload();\r\n    up.setUploadId(id);\r\n    up.setKey(path);\r\n    return up;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "newResult",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "UploadPartResult newResult(UploadPartRequest request, String etag)\n{\r\n    UploadPartResult result = new UploadPartResult();\r\n    result.setPartNumber(request.getPartNumber());\r\n    result.setETag(etag);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "newResult",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "InitiateMultipartUploadResult newResult(InitiateMultipartUploadRequest request, String uploadId)\n{\r\n    InitiateMultipartUploadResult result = new InitiateMultipartUploadResult();\r\n    result.setUploadId(uploadId);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "createTestOutputFiles",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createTestOutputFiles(List<String> relativeFiles, Path attemptPath, Configuration conf) throws IOException\n{\r\n    FileSystem attemptFS = attemptPath.getFileSystem(conf);\r\n    attemptFS.delete(attemptPath, true);\r\n    for (String relative : relativeFiles) {\r\n        OutputStream out = attemptFS.create(new Path(attemptPath, relative));\r\n        out.write(34);\r\n        out.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    removeBaseAndBucketOverrides(conf, CANNED_ACL);\r\n    conf.set(CANNED_ACL, LOG_DELIVERY_WRITE);\r\n    conf.setBoolean(S3AAuditConstants.REJECT_OUT_OF_SPAN_OPERATIONS, false);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCreatedObjectsHaveACLs",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCreatedObjectsHaveACLs() throws Throwable\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path dir = methodPath();\r\n    fs.mkdirs(dir);\r\n    assertObjectHasLoggingGrant(dir, false);\r\n    Path path = new Path(dir, \"1\");\r\n    ContractTestUtils.touch(fs, path);\r\n    assertObjectHasLoggingGrant(path, true);\r\n    Path path2 = new Path(dir, \"2\");\r\n    fs.rename(path, path2);\r\n    assertObjectHasLoggingGrant(path2, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertObjectHasLoggingGrant",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void assertObjectHasLoggingGrant(Path path, boolean isFile)\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    StoreContext storeContext = fs.createStoreContext();\r\n    AmazonS3 s3 = fs.getAmazonS3ClientForTesting(\"acls\");\r\n    String key = storeContext.pathToKey(path);\r\n    if (!isFile) {\r\n        key = key + \"/\";\r\n    }\r\n    AccessControlList acl = s3.getObjectAcl(storeContext.getBucket(), key);\r\n    List<Grant> grants = acl.getGrantsAsList();\r\n    for (Grant grant : grants) {\r\n        LOG.info(\"{}\", grant.toString());\r\n    }\r\n    Grant loggingGrant = new Grant(GroupGrantee.LogDelivery, Permission.Write);\r\n    Assertions.assertThat(grants).describedAs(\"ACL grants of object %s\", path).contains(loggingGrant);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\integration",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { new DirectoryCommitterTestBinding() }, { new PartitionCommitterTestBinding() }, { new MagicCommitterTestBinding() } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\integration",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    committerTestBinding.setup(getClusterBinding(), getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\integration",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    disableFilesystemCaching(conf);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\integration",
  "methodName" : "committerName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String committerName()\n{\r\n    return committerTestBinding.getCommitterName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\integration",
  "methodName" : "test_000",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_000() throws Throwable\n{\r\n    committerTestBinding.validate();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\integration",
  "methodName" : "test_100",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_100() throws Throwable\n{\r\n    committerTestBinding.test_100();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\integration",
  "methodName" : "test_200_execute",
  "errType" : null,
  "containingMethodsNum" : 54,
  "sourceCodeText" : "void test_200_execute() throws Exception\n{\r\n    describe(\"Run an MR with committer %s\", committerName());\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path outputPath = path(\"ITestS3ACommitterMRJob-execute-\" + committerName());\r\n    String commitUUID = UUID.randomUUID().toString();\r\n    String suffix = isUniqueFilenames() ? (\"-\" + commitUUID) : \"\";\r\n    int numFiles = getTestFileCount();\r\n    List<String> expectedFiles = new ArrayList<>(numFiles);\r\n    Set<String> expectedKeys = Sets.newHashSet();\r\n    for (int i = 0; i < numFiles; i += 1) {\r\n        File file = localFilesDir.newFile(i + \".text\");\r\n        try (FileOutputStream out = new FileOutputStream(file)) {\r\n            out.write((\"file \" + i).getBytes(StandardCharsets.UTF_8));\r\n        }\r\n        String filename = String.format(\"part-m-%05d%s\", i, suffix);\r\n        Path path = new Path(outputPath, filename);\r\n        expectedFiles.add(path.toString());\r\n        expectedKeys.add(\"/\" + fs.pathToKey(path));\r\n    }\r\n    Collections.sort(expectedFiles);\r\n    Job mrJob = createJob(newJobConf());\r\n    JobConf jobConf = (JobConf) mrJob.getConfiguration();\r\n    mrJob.setOutputFormatClass(LoggingTextOutputFormat.class);\r\n    FileOutputFormat.setOutputPath(mrJob, outputPath);\r\n    File mockResultsFile = localFilesDir.newFile(\"committer.bin\");\r\n    mockResultsFile.delete();\r\n    String committerPath = \"file:\" + mockResultsFile;\r\n    jobConf.set(\"mock-results-file\", committerPath);\r\n    jobConf.set(FS_S3A_COMMITTER_UUID, commitUUID);\r\n    mrJob.setInputFormatClass(TextInputFormat.class);\r\n    FileInputFormat.addInputPath(mrJob, new Path(localFilesDir.getRoot().toURI()));\r\n    mrJob.setMapperClass(MapClass.class);\r\n    mrJob.setNumReduceTasks(0);\r\n    URL log4j = getClass().getClassLoader().getResource(\"log4j.properties\");\r\n    if (log4j != null && \"file\".equals(log4j.getProtocol())) {\r\n        Path log4jPath = new Path(log4j.toURI());\r\n        LOG.debug(\"Using log4j path {}\", log4jPath);\r\n        mrJob.addFileToClassPath(log4jPath);\r\n        String sysprops = String.format(\"-Xmx128m -Dlog4j.configuration=%s\", log4j);\r\n        jobConf.set(JobConf.MAPRED_MAP_TASK_JAVA_OPTS, sysprops);\r\n        jobConf.set(JobConf.MAPRED_REDUCE_TASK_JAVA_OPTS, sysprops);\r\n        jobConf.set(\"yarn.app.mapreduce.am.command-opts\", sysprops);\r\n    }\r\n    applyCustomConfigOptions(jobConf);\r\n    mrJob.setMaxMapAttempts(1);\r\n    try (DurationInfo ignore = new DurationInfo(LOG, \"Job Submit\")) {\r\n        mrJob.submit();\r\n    }\r\n    String jobID = mrJob.getJobID().toString();\r\n    String logLocation = \"logs under \" + getYarn().getTestWorkDir().getAbsolutePath();\r\n    try (DurationInfo ignore = new DurationInfo(LOG, \"Job Execution\")) {\r\n        mrJob.waitForCompletion(true);\r\n    }\r\n    JobStatus status = mrJob.getStatus();\r\n    if (!mrJob.isSuccessful()) {\r\n        String message = String.format(\"Job %s failed in state %s with cause %s.\\n\" + \"Consult %s\", jobID, status.getState(), status.getFailureInfo(), logLocation);\r\n        LOG.error(message);\r\n        fail(message);\r\n    }\r\n    Path successPath = new Path(outputPath, _SUCCESS);\r\n    SuccessData successData = validateSuccessFile(outputPath, committerName(), fs, \"MR job \" + jobID, 1, \"\");\r\n    String commitData = successData.toString();\r\n    FileStatus[] results = fs.listStatus(outputPath, S3AUtils.HIDDEN_FILE_FILTER);\r\n    int fileCount = results.length;\r\n    Assertions.assertThat(fileCount).describedAs(\"No files from job %s in output directory %s; see %s\", jobID, outputPath, logLocation).isNotEqualTo(0);\r\n    List<String> actualFiles = Arrays.stream(results).map(s -> s.getPath().toString()).sorted().collect(Collectors.toList());\r\n    Assertions.assertThat(actualFiles).describedAs(\"Files found in %s\", outputPath).isEqualTo(expectedFiles);\r\n    Assertions.assertThat(successData.getFilenames()).describedAs(\"Success files listed in %s:%s\", successPath, commitData).isNotEmpty().containsExactlyInAnyOrderElementsOf(expectedKeys);\r\n    assertPathDoesNotExist(\"temporary dir should only be from\" + \" classic file committers\", new Path(outputPath, CommitConstants.TEMPORARY));\r\n    customPostExecutionValidation(outputPath, successData);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\integration",
  "methodName" : "applyCustomConfigOptions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void applyCustomConfigOptions(final JobConf jobConf) throws IOException\n{\r\n    committerTestBinding.applyCustomConfigOptions(jobConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\integration",
  "methodName" : "customPostExecutionValidation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void customPostExecutionValidation(final Path destPath, final SuccessData successData) throws Exception\n{\r\n    committerTestBinding.validateResult(destPath, successData);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\integration",
  "methodName" : "test_500",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_500() throws Throwable\n{\r\n    committerTestBinding.test_500();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { \"keep-markers\", true }, { \"delete-markers\", false } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    if (isKeepingMarkers()) {\r\n        deleteTestDirInTeardown();\r\n    }\r\n    super.teardown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testDeleteSingleFileInDir",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testDeleteSingleFileInDir() throws Throwable\n{\r\n    describe(\"delete a file\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path dir = dir(methodPath());\r\n    Path simpleFile = file(new Path(dir, \"simple.txt\"));\r\n    boolean rawAndKeeping = !isDeleting();\r\n    boolean rawAndDeleting = isDeleting();\r\n    verifyMetrics(() -> {\r\n        fs.delete(simpleFile, false);\r\n        return \"after fs.delete(simpleFile) \" + getMetricSummary();\r\n    }, probe(rawAndKeeping, OBJECT_METADATA_REQUESTS, FILESTATUS_FILE_PROBE_H), probe(rawAndDeleting, OBJECT_METADATA_REQUESTS, FILESTATUS_FILE_PROBE_H + FILESTATUS_DIR_PROBE_H), with(OBJECT_LIST_REQUEST, FILESTATUS_FILE_PROBE_L + FILESTATUS_DIR_PROBE_L), with(DIRECTORIES_DELETED, 0), with(FILES_DELETED, 1), with(OBJECT_DELETE_REQUEST, DELETE_OBJECT_REQUEST), withWhenKeeping(DIRECTORIES_CREATED, 0), withWhenKeeping(OBJECT_BULK_DELETE_REQUEST, 0), withWhenDeleting(DIRECTORIES_CREATED, 1), withWhenDeleting(OBJECT_BULK_DELETE_REQUEST, DELETE_MARKER_REQUEST));\r\n    S3AFileStatus status = verifyInnerGetFileStatus(dir, true, StatusProbeEnum.ALL, GET_FILE_STATUS_ON_DIR);\r\n    assertEmptyDirStatus(status, Tristate.TRUE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testDeleteFileInDir",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testDeleteFileInDir() throws Throwable\n{\r\n    describe(\"delete a file in a directory with multiple files\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path dir = dir(methodPath());\r\n    Path file1 = file(new Path(dir, \"file1.txt\"));\r\n    Path file2 = file(new Path(dir, \"file2.txt\"));\r\n    boolean rawAndKeeping = !isDeleting();\r\n    boolean rawAndDeleting = isDeleting();\r\n    verifyMetrics(() -> {\r\n        fs.delete(file1, false);\r\n        return \"after fs.delete(file1) \" + getMetricSummary();\r\n    }, probe(rawAndKeeping, OBJECT_METADATA_REQUESTS, FILESTATUS_FILE_PROBE_H), probe(rawAndDeleting, OBJECT_METADATA_REQUESTS, FILESTATUS_FILE_PROBE_H + FILESTATUS_DIR_PROBE_H), with(OBJECT_LIST_REQUEST, FILESTATUS_FILE_PROBE_L + FILESTATUS_DIR_PROBE_L), with(DIRECTORIES_DELETED, 0), with(FILES_DELETED, 1), with(DIRECTORIES_CREATED, 0), withWhenKeeping(OBJECT_DELETE_REQUEST, DELETE_OBJECT_REQUEST), withWhenDeleting(OBJECT_DELETE_REQUEST, DELETE_OBJECT_REQUEST));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testDirMarkersSubdir",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testDirMarkersSubdir() throws Throwable\n{\r\n    describe(\"verify cost of deep subdir creation\");\r\n    Path methodPath = methodPath();\r\n    Path parent = new Path(methodPath, \"parent\");\r\n    Path subDir = new Path(parent, \"1/2/3/4/5/6\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path sibling = new Path(methodPath, \"sibling\");\r\n    ContractTestUtils.touch(fs, sibling);\r\n    int dirsCreated = 2;\r\n    fs.delete(parent, true);\r\n    LOG.info(\"creating parent dir {}\", parent);\r\n    fs.mkdirs(parent);\r\n    LOG.info(\"creating sub directory {}\", subDir);\r\n    final int fakeDirectoriesToDelete = directoriesInPath(subDir) - 1;\r\n    verifyMetrics(() -> {\r\n        mkdirs(subDir);\r\n        return \"after mkdir(subDir) \" + getMetricSummary();\r\n    }, with(DIRECTORIES_CREATED, 1), with(DIRECTORIES_DELETED, 0), withWhenKeeping(getDeleteMarkerStatistic(), 0), withWhenKeeping(FAKE_DIRECTORIES_DELETED, 0), withWhenDeleting(getDeleteMarkerStatistic(), isBulkDelete() ? DELETE_MARKER_REQUEST : fakeDirectoriesToDelete), withWhenDeleting(FAKE_DIRECTORIES_DELETED, fakeDirectoriesToDelete));\r\n    LOG.info(\"About to delete {}\", parent);\r\n    verifyMetrics(() -> {\r\n        fs.delete(parent, true);\r\n        return \"deleting parent dir \" + parent + \" \" + getMetricSummary();\r\n    }, withWhenKeeping(OBJECT_DELETE_OBJECTS, dirsCreated), withWhenDeleting(OBJECT_DELETE_OBJECTS, 1));\r\n    verifyNoListing(parent);\r\n    verifyNoListing(subDir);\r\n    fs.mkdirs(parent);\r\n    FileStatus[] children = fs.listStatus(parent);\r\n    Assertions.assertThat(children).describedAs(\"Children of %s\", parent).isEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "verifyNoListing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyNoListing(final Path path) throws Exception\n{\r\n    intercept(FileNotFoundException.class, () -> {\r\n        FileStatus[] statuses = getFileSystem().listStatus(path);\r\n        return Arrays.deepToString(statuses);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testDirMarkersFileCreation",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testDirMarkersFileCreation() throws Throwable\n{\r\n    describe(\"verify cost of file creation\");\r\n    Path srcBaseDir = dir(methodPath());\r\n    Path srcDir = dir(new Path(srcBaseDir, \"1/2/3/4/5/6\"));\r\n    final int directories = directoriesInPath(srcDir);\r\n    verifyMetrics(() -> {\r\n        file(new Path(srcDir, \"source.txt\"));\r\n        LOG.info(\"Metrics: {}\\n{}\", getMetricSummary(), getFileSystem());\r\n        return \"after touch(fs, srcFilePath) \" + getMetricSummary();\r\n    }, with(DIRECTORIES_CREATED, 0), with(DIRECTORIES_DELETED, 0), withWhenKeeping(getDeleteMarkerStatistic(), 0), withWhenKeeping(FAKE_DIRECTORIES_DELETED, 0), withWhenDeleting(getDeleteMarkerStatistic(), isBulkDelete() ? 1 : directories), withWhenDeleting(FAKE_DIRECTORIES_DELETED, directories));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testProviderWrongClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testProviderWrongClass() throws Exception\n{\r\n    expectProviderInstantiationFailure(this.getClass(), NOT_AWS_PROVIDER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testProviderAbstractClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testProviderAbstractClass() throws Exception\n{\r\n    expectProviderInstantiationFailure(AbstractProvider.class, ABSTRACT_PROVIDER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testProviderNotAClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testProviderNotAClass() throws Exception\n{\r\n    expectProviderInstantiationFailure(\"NoSuchClass\", \"ClassNotFoundException\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testProviderConstructorError",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testProviderConstructorError() throws Exception\n{\r\n    expectProviderInstantiationFailure(ConstructorSignatureErrorProvider.class, CONSTRUCTOR_EXCEPTION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testProviderFailureError",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testProviderFailureError() throws Exception\n{\r\n    expectProviderInstantiationFailure(ConstructorFailureProvider.class, INSTANTIATION_EXCEPTION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testInstantiationChain",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testInstantiationChain() throws Throwable\n{\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(AWS_CREDENTIALS_PROVIDER, TemporaryAWSCredentialsProvider.NAME + \", \\t\" + SimpleAWSCredentialsProvider.NAME + \" ,\\n \" + AnonymousAWSCredentialsProvider.NAME);\r\n    Path testFile = getCSVTestPath(conf);\r\n    AWSCredentialProviderList list = createAWSCredentialProviderSet(testFile.toUri(), conf);\r\n    List<Class<?>> expectedClasses = Arrays.asList(TemporaryAWSCredentialsProvider.class, SimpleAWSCredentialsProvider.class, AnonymousAWSCredentialsProvider.class);\r\n    assertCredentialProviders(expectedClasses, list);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testDefaultChain",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testDefaultChain() throws Exception\n{\r\n    URI uri1 = new URI(\"s3a://bucket1\"), uri2 = new URI(\"s3a://bucket2\");\r\n    Configuration conf = new Configuration(false);\r\n    conf.unset(AWS_CREDENTIALS_PROVIDER);\r\n    AWSCredentialProviderList list1 = createAWSCredentialProviderSet(uri1, conf);\r\n    AWSCredentialProviderList list2 = createAWSCredentialProviderSet(uri2, conf);\r\n    List<Class<?>> expectedClasses = STANDARD_AWS_PROVIDERS;\r\n    assertCredentialProviders(expectedClasses, list1);\r\n    assertCredentialProviders(expectedClasses, list2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testDefaultChainNoURI",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testDefaultChainNoURI() throws Exception\n{\r\n    Configuration conf = new Configuration(false);\r\n    conf.unset(AWS_CREDENTIALS_PROVIDER);\r\n    assertCredentialProviders(STANDARD_AWS_PROVIDERS, createAWSCredentialProviderSet(null, conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testConfiguredChain",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testConfiguredChain() throws Exception\n{\r\n    URI uri1 = new URI(\"s3a://bucket1\"), uri2 = new URI(\"s3a://bucket2\");\r\n    List<Class<?>> expectedClasses = Arrays.asList(EnvironmentVariableCredentialsProvider.class, InstanceProfileCredentialsProvider.class, AnonymousAWSCredentialsProvider.class);\r\n    Configuration conf = createProviderConfiguration(buildClassListString(expectedClasses));\r\n    AWSCredentialProviderList list1 = createAWSCredentialProviderSet(uri1, conf);\r\n    AWSCredentialProviderList list2 = createAWSCredentialProviderSet(uri2, conf);\r\n    assertCredentialProviders(expectedClasses, list1);\r\n    assertCredentialProviders(expectedClasses, list2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testConfiguredChainUsesSharedInstanceProfile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testConfiguredChainUsesSharedInstanceProfile() throws Exception\n{\r\n    URI uri1 = new URI(\"s3a://bucket1\"), uri2 = new URI(\"s3a://bucket2\");\r\n    Configuration conf = new Configuration(false);\r\n    List<Class<?>> expectedClasses = Arrays.asList(InstanceProfileCredentialsProvider.class);\r\n    conf.set(AWS_CREDENTIALS_PROVIDER, buildClassListString(expectedClasses));\r\n    AWSCredentialProviderList list1 = createAWSCredentialProviderSet(uri1, conf);\r\n    AWSCredentialProviderList list2 = createAWSCredentialProviderSet(uri2, conf);\r\n    assertCredentialProviders(expectedClasses, list1);\r\n    assertCredentialProviders(expectedClasses, list2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testFallbackToDefaults",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testFallbackToDefaults() throws Throwable\n{\r\n    final AWSCredentialProviderList credentials = buildAWSProviderList(new URI(\"s3a://bucket1\"), createProviderConfiguration(\"  \"), ASSUMED_ROLE_CREDENTIALS_PROVIDER, Arrays.asList(EnvironmentVariableCredentialsProvider.class), Sets.newHashSet());\r\n    assertTrue(\"empty credentials\", credentials.size() > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testAWSExceptionTranslation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAWSExceptionTranslation() throws Throwable\n{\r\n    IOException ex = expectProviderInstantiationFailure(AWSExceptionRaisingFactory.class, AWSExceptionRaisingFactory.NO_AUTH);\r\n    if (!(ex instanceof AccessDeniedException)) {\r\n        throw ex;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testFactoryWrongType",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFactoryWrongType() throws Throwable\n{\r\n    expectProviderInstantiationFailure(FactoryOfWrongType.class, CONSTRUCTOR_EXCEPTION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "expectProviderInstantiationFailure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOException expectProviderInstantiationFailure(String option, String expectedErrorText) throws Exception\n{\r\n    return intercept(IOException.class, expectedErrorText, () -> createAWSCredentialProviderSet(TESTFILE_URI, createProviderConfiguration(option)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "expectProviderInstantiationFailure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOException expectProviderInstantiationFailure(Class aClass, String expectedErrorText) throws Exception\n{\r\n    return expectProviderInstantiationFailure(buildClassListString(Collections.singletonList(aClass)), expectedErrorText);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createProviderConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createProviderConfiguration(final String providerOption)\n{\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(AWS_CREDENTIALS_PROVIDER, providerOption);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createProviderConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createProviderConfiguration(final Class<?> aClass)\n{\r\n    return createProviderConfiguration(buildClassListString(Collections.singletonList(aClass)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertCredentialProviders",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void assertCredentialProviders(List<Class<?>> expectedClasses, AWSCredentialProviderList list)\n{\r\n    assertNotNull(list);\r\n    List<AWSCredentialsProvider> providers = list.getProviders();\r\n    assertEquals(expectedClasses.size(), providers.size());\r\n    for (int i = 0; i < expectedClasses.size(); ++i) {\r\n        Class<?> expectedClass = expectedClasses.get(i);\r\n        AWSCredentialsProvider provider = providers.get(i);\r\n        assertNotNull(String.format(\"At position %d, expected class is %s, but found null.\", i, expectedClass), provider);\r\n        assertTrue(String.format(\"At position %d, expected class is %s, but found %s.\", i, expectedClass, provider.getClass()), expectedClass.isAssignableFrom(provider.getClass()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testAuthenticationContainsProbes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testAuthenticationContainsProbes()\n{\r\n    Configuration conf = new Configuration(false);\r\n    assertFalse(\"found AssumedRoleCredentialProvider\", authenticationContains(conf, AssumedRoleCredentialProvider.NAME));\r\n    conf.set(AWS_CREDENTIALS_PROVIDER, AssumedRoleCredentialProvider.NAME);\r\n    assertTrue(\"didn't find AssumedRoleCredentialProvider\", authenticationContains(conf, AssumedRoleCredentialProvider.NAME));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testExceptionLogic",
  "errType" : [ "AccessDeniedException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testExceptionLogic() throws Throwable\n{\r\n    AWSCredentialProviderList providers = new AWSCredentialProviderList();\r\n    NoAuthWithAWSException noAuth = intercept(NoAuthWithAWSException.class, AWSCredentialProviderList.NO_AWS_CREDENTIAL_PROVIDERS, () -> providers.getCredentials());\r\n    providers.close();\r\n    S3ARetryPolicy retryPolicy = new S3ARetryPolicy(new Configuration(false));\r\n    assertEquals(\"Expected no retry on auth failure\", RetryPolicy.RetryAction.FAIL.action, retryPolicy.shouldRetry(noAuth, 0, 0, true).action);\r\n    try {\r\n        throw S3AUtils.translateException(\"login\", \"\", noAuth);\r\n    } catch (AccessDeniedException expected) {\r\n        assertEquals(\"Expected no retry on AccessDeniedException\", RetryPolicy.RetryAction.FAIL.action, retryPolicy.shouldRetry(expected, 0, 0, true).action);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRefCounting",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testRefCounting() throws Throwable\n{\r\n    AWSCredentialProviderList providers = new AWSCredentialProviderList();\r\n    assertEquals(\"Ref count for \" + providers, 1, providers.getRefCount());\r\n    AWSCredentialProviderList replicate = providers.share();\r\n    assertEquals(providers, replicate);\r\n    assertEquals(\"Ref count after replication for \" + providers, 2, providers.getRefCount());\r\n    assertFalse(\"Was closed \" + providers, providers.isClosed());\r\n    providers.close();\r\n    assertFalse(\"Was closed \" + providers, providers.isClosed());\r\n    assertEquals(\"Ref count after close() for \" + providers, 1, providers.getRefCount());\r\n    providers.close();\r\n    assertTrue(\"Was not closed \" + providers, providers.isClosed());\r\n    assertEquals(\"Ref count after close() for \" + providers, 0, providers.getRefCount());\r\n    assertEquals(\"Ref count after second close() for \" + providers, 0, providers.getRefCount());\r\n    intercept(IllegalStateException.class, \"closed\", () -> providers.share());\r\n    providers.close();\r\n    assertEquals(\"Ref count after close() for \" + providers, 0, providers.getRefCount());\r\n    providers.refresh();\r\n    intercept(NoAuthWithAWSException.class, AWSCredentialProviderList.CREDENTIALS_REQUESTED_WHEN_CLOSED, () -> providers.getCredentials());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testIOEInConstructorPropagation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testIOEInConstructorPropagation() throws Throwable\n{\r\n    IOException expected = expectProviderInstantiationFailure(IOERaisingProvider.class.getName(), \"expected\");\r\n    if (!(expected instanceof InterruptedIOException)) {\r\n        throw expected;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "partSizeInBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int partSizeInBytes()\n{\r\n    return partitionSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "getTestPayloadCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestPayloadCount()\n{\r\n    return 3;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileSystem getFileSystem()\n{\r\n    return (S3AFileSystem) super.getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return SCALE_TEST_TIMEOUT_MILLIS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "supportsConcurrentUploadsToSamePath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean supportsConcurrentUploadsToSamePath()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "finalizeConsumesUploadIdImmediately",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean finalizeConsumesUploadIdImmediately()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    Configuration conf = getContract().getConf();\r\n    boolean enabled = getTestPropertyBool(conf, KEY_SCALE_TESTS_ENABLED, DEFAULT_SCALE_TESTS_ENABLED);\r\n    assume(\"Scale test disabled: to enable set property \" + KEY_SCALE_TESTS_ENABLED, enabled);\r\n    partitionSize = (int) getTestPropertyBytes(conf, KEY_HUGE_PARTITION_SIZE, DEFAULT_HUGE_PARTITION_SIZE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "testDirectoryInTheWay",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDirectoryInTheWay() throws Exception\n{\r\n    skip(\"Unsupported\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "testMultipartUploadReverseOrder",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMultipartUploadReverseOrder() throws Exception\n{\r\n    skip(\"skipped for speed\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "getBlockOutputBufferName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockOutputBufferName()\n{\r\n    return Constants.FAST_UPLOAD_BUFFER_DISK;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "getTestSuiteName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTestSuiteName()\n{\r\n    return \"ITestS3AHugeMagicCommits\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    CommitUtils.verifyIsMagicCommitFS(getFileSystem());\r\n    Path finalDirectory = new Path(getScaleTestDir(), \"commit\");\r\n    magicDir = new Path(finalDirectory, MAGIC);\r\n    jobDir = new Path(magicDir, \"job_001\");\r\n    String filename = \"commit.bin\";\r\n    setHugefile(new Path(finalDirectory, filename));\r\n    magicOutputFile = new Path(jobDir, filename);\r\n    pendingDataFile = new Path(jobDir, filename + PENDING_SUFFIX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "getPathOfFileToCreate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getPathOfFileToCreate()\n{\r\n    return magicOutputFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "test_030_postCreationAssertions",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void test_030_postCreationAssertions() throws Throwable\n{\r\n    describe(\"Committing file\");\r\n    assertPathDoesNotExist(\"final file exists\", getHugefile());\r\n    assertPathExists(\"No pending file\", pendingDataFile);\r\n    S3AFileSystem fs = getFileSystem();\r\n    FileStatus status = fs.getFileStatus(magicOutputFile);\r\n    assertEquals(\"Non empty marker file \" + status, 0, status.getLen());\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    CommitOperations operations = new CommitOperations(fs);\r\n    Path destDir = getHugefile().getParent();\r\n    assertPathExists(\"Magic dir\", new Path(destDir, CommitConstants.MAGIC));\r\n    String destDirKey = fs.pathToKey(destDir);\r\n    List<String> uploads = listMultipartUploads(fs, destDirKey);\r\n    assertEquals(\"Pending uploads: \" + uploads.stream().collect(Collectors.joining(\"\\n\")), 1, uploads.size());\r\n    assertNotNull(\"jobDir\", jobDir);\r\n    Pair<PendingSet, List<Pair<LocatedFileStatus, IOException>>> results = operations.loadSinglePendingCommits(jobDir, false);\r\n    try (CommitOperations.CommitContext commitContext = operations.initiateCommitOperation(jobDir)) {\r\n        for (SinglePendingCommit singlePendingCommit : results.getKey().getCommits()) {\r\n            commitContext.commitOrFail(singlePendingCommit);\r\n        }\r\n    }\r\n    timer.end(\"time to commit %s\", pendingDataFile);\r\n    uploads = listMultipartUploads(fs, destDirKey);\r\n    assertEquals(\"Pending uploads\" + uploads.stream().collect(Collectors.joining(\"\\n\")), 0, operations.listPendingUploadsUnderPath(destDir).size());\r\n    super.test_030_postCreationAssertions();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "skipQuietly",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void skipQuietly(String text)\n{\r\n    describe(\"Skipping: %s\", text);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "test_040_PositionedReadHugeFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_040_PositionedReadHugeFile()\n{\r\n    skipQuietly(\"test_040_PositionedReadHugeFile\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "test_050_readHugeFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_050_readHugeFile()\n{\r\n    skipQuietly(\"readHugeFile\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "test_100_renameHugeFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_100_renameHugeFile()\n{\r\n    skipQuietly(\"renameHugeFile\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "test_800_DeleteHugeFiles",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void test_800_DeleteHugeFiles() throws IOException\n{\r\n    if (getFileSystem() != null) {\r\n        try {\r\n            getFileSystem().abortOutstandingMultipartUploads(0);\r\n        } catch (IOException e) {\r\n            LOG.info(\"Exception while purging old uploads\", e);\r\n        }\r\n    }\r\n    try {\r\n        super.test_800_DeleteHugeFiles();\r\n    } finally {\r\n        ContractTestUtils.rm(getFileSystem(), magicDir, true, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testBadConfiguration",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBadConfiguration() throws IOException\n{\r\n    Configuration conf = createConf();\r\n    conf.set(AWS_CREDENTIALS_PROVIDER, \"no.such.class\");\r\n    try {\r\n        createFailingFS(conf);\r\n    } catch (IOException e) {\r\n        if (!(e.getCause() instanceof ClassNotFoundException)) {\r\n            LOG.error(\"Unexpected nested cause: {} in {}\", e.getCause(), e, e);\r\n            throw e;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testBadCredentialsConstructor",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testBadCredentialsConstructor() throws Exception\n{\r\n    Configuration conf = createConf();\r\n    conf.set(AWS_CREDENTIALS_PROVIDER, BadCredentialsProviderConstructor.class.getName());\r\n    try {\r\n        createFailingFS(conf);\r\n    } catch (IOException e) {\r\n        GenericTestUtils.assertExceptionContains(CONSTRUCTOR_EXCEPTION, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConf()\n{\r\n    Configuration conf = new Configuration();\r\n    removeBaseAndBucketOverrides(conf, DELEGATION_TOKEN_BINDING, AWS_CREDENTIALS_PROVIDER);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createFailingFS",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createFailingFS(Configuration conf) throws IOException\n{\r\n    S3AFileSystem fs = S3ATestUtils.createTestFileSystem(conf);\r\n    fs.listStatus(new Path(\"/\"));\r\n    fail(\"Expected exception - got \" + fs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testBadCredentials",
  "errType" : [ "AccessDeniedException", "AWSServiceIOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testBadCredentials() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(AWS_CREDENTIALS_PROVIDER, BadCredentialsProvider.class.getName());\r\n    try {\r\n        createFailingFS(conf);\r\n    } catch (AccessDeniedException e) {\r\n    } catch (AWSServiceIOException e) {\r\n        GenericTestUtils.assertExceptionContains(\"UnrecognizedClientException\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testAnonymousProvider",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testAnonymousProvider() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(AWS_CREDENTIALS_PROVIDER, AnonymousAWSCredentialsProvider.class.getName());\r\n    Path testFile = getCSVTestPath(conf);\r\n    FileSystem fs = FileSystem.newInstance(testFile.toUri(), conf);\r\n    assertNotNull(fs);\r\n    assertTrue(fs instanceof S3AFileSystem);\r\n    FileStatus stat = fs.getFileStatus(testFile);\r\n    assertNotNull(stat);\r\n    assertEquals(testFile, stat.getPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\yarn",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    fc = S3ATestUtils.createTestFileContext(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\yarn",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (fc != null) {\r\n        fc.delete(getTestPath(), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\yarn",
  "methodName" : "getTestPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTestPath()\n{\r\n    return S3ATestUtils.createTestPath(new Path(\"/tests3afc\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\yarn",
  "methodName" : "testS3AStatus",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testS3AStatus() throws Exception\n{\r\n    FsStatus fsStatus = fc.getFsStatus(null);\r\n    assertNotNull(fsStatus);\r\n    assertTrue(\"Used capacity should be positive: \" + fsStatus.getUsed(), fsStatus.getUsed() >= 0);\r\n    assertTrue(\"Remaining capacity should be positive: \" + fsStatus.getRemaining(), fsStatus.getRemaining() >= 0);\r\n    assertTrue(\"Capacity should be positive: \" + fsStatus.getCapacity(), fsStatus.getCapacity() >= 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\yarn",
  "methodName" : "testS3ACreateFileInSubDir",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testS3ACreateFileInSubDir() throws Exception\n{\r\n    Path dirPath = getTestPath();\r\n    fc.mkdir(dirPath, FileContext.DIR_DEFAULT_PERM, true);\r\n    Path filePath = new Path(dirPath, \"file\");\r\n    try (FSDataOutputStream file = fc.create(filePath, EnumSet.of(CreateFlag.CREATE))) {\r\n        file.write(666);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { false, SMALL, SMALL_REQS }, { false, MAXIMUM, MAXIMUM_REQS }, { true, SMALL, SMALL_REQS }, { true, MAXIMUM, MAXIMUM_REQS } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "createScaleConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createScaleConfiguration()\n{\r\n    Configuration conf = super.createScaleConfiguration();\r\n    S3ATestUtils.disableFilesystemCaching(conf);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    final Configuration conf = getConf();\r\n    S3ATestUtils.removeBaseAndBucketOverrides(conf, EXPERIMENTAL_AWS_INTERNAL_THROTTLING, BULK_DELETE_PAGE_SIZE, USER_AGENT_PREFIX);\r\n    conf.setBoolean(EXPERIMENTAL_AWS_INTERNAL_THROTTLING, throttle);\r\n    Assertions.assertThat(pageSize).describedAs(\"page size\").isGreaterThan(0);\r\n    conf.setInt(BULK_DELETE_PAGE_SIZE, pageSize);\r\n    conf.set(USER_AGENT_PREFIX, String.format(\"ILoadTestS3ABulkDeleteThrottling-%s-%04d\", throttle, pageSize));\r\n    super.setup();\r\n    Assume.assumeTrue(\"multipart delete disabled\", conf.getBoolean(ENABLE_MULTI_DELETE, true));\r\n    dataDir = GenericTestUtils.getTestDir(\"throttling\");\r\n    dataDir.mkdirs();\r\n    final String size = getFileSystem().getConf().get(BULK_DELETE_PAGE_SIZE);\r\n    Assertions.assertThat(size).describedAs(\"page size\").isNotEmpty();\r\n    Assertions.assertThat(getFileSystem().getConf().getInt(BULK_DELETE_PAGE_SIZE, -1)).isEqualTo(pageSize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "test_010_Reset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void test_010_Reset() throws Throwable\n{\r\n    testWasThrottled = false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "test_020_DeleteThrottling",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void test_020_DeleteThrottling() throws Throwable\n{\r\n    describe(\"test how S3 reacts to massive multipart deletion requests\");\r\n    final File results = deleteFiles(requests, pageSize);\r\n    LOG.info(\"Test run completed against {}:\\n see {}\", getFileSystem(), results);\r\n    if (testWasThrottled) {\r\n        LOG.warn(\"Test was throttled\");\r\n    } else {\r\n        LOG.info(\"No throttling recorded in filesystem\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "test_030_Sleep",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_030_Sleep() throws Throwable\n{\r\n    maybeSleep();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "maybeSleep",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void maybeSleep() throws InterruptedException, IOException\n{\r\n    if (testWasThrottled) {\r\n        LOG.info(\"Sleeping briefly to let store recover\");\r\n        Thread.sleep(30_000);\r\n        getFileSystem().delete(path(\"recovery\"), true);\r\n        testWasThrottled = false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "deleteFiles",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "File deleteFiles(final int requestCount, final int entries) throws Exception\n{\r\n    File csvFile = new File(dataDir, String.format(\"delete-%03d-%04d-%s.csv\", requestCount, entries, throttle));\r\n    describe(\"Issuing %d requests of size %d, saving log to %s\", requestCount, entries, csvFile);\r\n    Path basePath = path(\"testDeleteObjectThrottling\");\r\n    final S3AFileSystem fs = getFileSystem();\r\n    final String base = fs.pathToKey(basePath);\r\n    final List<DeleteObjectsRequest.KeyVersion> fileList = buildDeleteRequest(base, entries);\r\n    final FileWriter out = new FileWriter(csvFile);\r\n    Csvout csvout = new Csvout(out, \"\\t\", \"\\n\");\r\n    Outcome.writeSchema(csvout);\r\n    final ContractTestUtils.NanoTimer jobTimer = new ContractTestUtils.NanoTimer();\r\n    for (int i = 0; i < requestCount; i++) {\r\n        final int id = i;\r\n        completionService.submit(() -> {\r\n            final long startTime = System.currentTimeMillis();\r\n            Thread.currentThread().setName(\"#\" + id);\r\n            LOG.info(\"Issuing request {}\", id);\r\n            final ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n            Exception ex = null;\r\n            try {\r\n                fs.removeKeys(fileList, false);\r\n            } catch (IOException e) {\r\n                ex = e;\r\n            }\r\n            timer.end(\"Request \" + id);\r\n            return new Outcome(id, startTime, timer, ex);\r\n        });\r\n    }\r\n    NanoTimerStats stats = new NanoTimerStats(\"Overall\");\r\n    NanoTimerStats success = new NanoTimerStats(\"Successful\");\r\n    NanoTimerStats throttled = new NanoTimerStats(\"Throttled\");\r\n    List<Outcome> throttledEvents = new ArrayList<>();\r\n    for (int i = 0; i < requestCount; i++) {\r\n        Outcome outcome = completionService.take().get();\r\n        ContractTestUtils.NanoTimer timer = outcome.timer;\r\n        Exception ex = outcome.exception;\r\n        outcome.writeln(csvout);\r\n        stats.add(timer);\r\n        if (ex != null) {\r\n            LOG.info(\"Throttled at event {}\", i, ex);\r\n            throttled.add(timer);\r\n            throttledEvents.add(outcome);\r\n        } else {\r\n            success.add(timer);\r\n        }\r\n    }\r\n    csvout.close();\r\n    jobTimer.end(\"Execution of operations\");\r\n    LOG.info(\"Summary file is \" + csvFile);\r\n    LOG.info(\"Made {} requests with {} throttle events\\n: {}\\n{}\\n{}\", requestCount, throttled.getCount(), stats, throttled, success);\r\n    double duration = jobTimer.duration();\r\n    double iops = requestCount * entries * 1.0e9 / duration;\r\n    LOG.info(String.format(\"TPS %3f operations/second\", iops));\r\n    if (LOG.isDebugEnabled()) {\r\n        throttledEvents.forEach((outcome -> {\r\n            LOG.debug(\"{}: duration: {}\", outcome.id, outcome.timer.elapsedTimeMs());\r\n        }));\r\n    }\r\n    return csvFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "buildDeleteRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<DeleteObjectsRequest.KeyVersion> buildDeleteRequest(String base, int count)\n{\r\n    List<DeleteObjectsRequest.KeyVersion> request = new ArrayList<>(count);\r\n    for (int i = 0; i < count; i++) {\r\n        request.add(new DeleteObjectsRequest.KeyVersion(String.format(\"%s/file-%04d\", base, i)));\r\n    }\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    selectTool = new SelectTool(getConfiguration());\r\n    selectConf = new Configuration(getConfiguration());\r\n    localFile = getTempFilename();\r\n    landsatSrc = getLandsatGZ().toString();\r\n    ChangeDetectionPolicy changeDetectionPolicy = getLandsatFS().getChangeDetectionPolicy();\r\n    Assume.assumeFalse(\"the standard landsat bucket doesn't have versioning\", changeDetectionPolicy.getSource() == Source.VersionId && changeDetectionPolicy.isRequireVersion());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    super.teardown();\r\n    if (localFile != null) {\r\n        localFile.delete();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "expectSuccess",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String expectSuccess(String message, S3GuardTool tool, String... args) throws Exception\n{\r\n    ByteArrayOutputStream buf = new ByteArrayOutputStream();\r\n    exec(EXIT_SUCCESS, message, tool, buf, args);\r\n    return buf.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int run(Configuration conf, S3GuardTool tool, String... args) throws Exception\n{\r\n    return ToolRunner.run(conf, tool, args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "runToFailure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void runToFailure(int status, Configuration conf, String message, S3GuardTool tool, String... args) throws Exception\n{\r\n    final ExitUtil.ExitException ex = intercept(ExitUtil.ExitException.class, message, () -> ToolRunner.run(conf, tool, args));\r\n    if (ex.status != status) {\r\n        throw ex;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testLandsatToFile",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testLandsatToFile() throws Throwable\n{\r\n    describe(\"select part of the landsat to a file\");\r\n    int lineCount = LINE_COUNT;\r\n    S3AFileSystem landsatFS = (S3AFileSystem) getLandsatGZ().getFileSystem(getConfiguration());\r\n    S3ATestUtils.MetricDiff selectCount = new S3ATestUtils.MetricDiff(landsatFS, Statistic.OBJECT_SELECT_REQUESTS);\r\n    run(selectConf, selectTool, D, v(CSV_OUTPUT_QUOTE_CHARACTER, \"'\"), D, v(CSV_OUTPUT_QUOTE_FIELDS, CSV_OUTPUT_QUOTE_FIELDS_AS_NEEEDED), \"select\", o(OPT_HEADER), CSV_HEADER_OPT_USE, o(OPT_COMPRESSION), COMPRESSION_OPT_GZIP, o(OPT_LIMIT), Integer.toString(lineCount), o(OPT_OUTPUT), localFile.toString(), landsatSrc, SELECT_SUNNY_ROWS_NO_LIMIT);\r\n    List<String> lines = IOUtils.readLines(new FileInputStream(localFile), StandardCharsets.UTF_8);\r\n    LOG.info(\"Result from select:\\n{}\", lines.get(0));\r\n    assertEquals(lineCount, lines.size());\r\n    selectCount.assertDiffEquals(\"select count\", 1);\r\n    OperationDuration duration = selectTool.getSelectDuration();\r\n    assertTrue(\"Select duration was not measured\", duration.value() > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getTempFilename",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "File getTempFilename() throws IOException\n{\r\n    File dest = File.createTempFile(\"landat\", \".csv\");\r\n    dest.delete();\r\n    return dest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testLandsatToConsole",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testLandsatToConsole() throws Throwable\n{\r\n    describe(\"select part of the landsat to the console\");\r\n    S3ATestUtils.MetricDiff readOps = new S3ATestUtils.MetricDiff(getFileSystem(), Statistic.STREAM_READ_OPERATIONS_INCOMPLETE);\r\n    run(selectConf, selectTool, D, v(CSV_OUTPUT_QUOTE_CHARACTER, \"'\"), D, v(CSV_OUTPUT_QUOTE_FIELDS, CSV_OUTPUT_QUOTE_FIELDS_ALWAYS), \"select\", o(OPT_HEADER), CSV_HEADER_OPT_USE, o(OPT_COMPRESSION), COMPRESSION_OPT_GZIP, o(OPT_LIMIT), Integer.toString(LINE_COUNT), landsatSrc, SELECT_SUNNY_ROWS_NO_LIMIT);\r\n    assertEquals(\"Lines read and printed to console\", LINE_COUNT, selectTool.getLinesRead());\r\n    readOps.assertDiffEquals(\"Read operations are still considered active\", 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectNothing",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSelectNothing() throws Throwable\n{\r\n    describe(\"an empty select is not an error\");\r\n    run(selectConf, selectTool, \"select\", o(OPT_HEADER), CSV_HEADER_OPT_USE, o(OPT_COMPRESSION), COMPRESSION_OPT_GZIP, o(OPT_INPUTFORMAT), \"csv\", o(OPT_OUTPUTFORMAT), \"csv\", o(OPT_EXPECTED), \"0\", o(OPT_LIMIT), Integer.toString(LINE_COUNT), landsatSrc, SELECT_NOTHING);\r\n    assertEquals(\"Lines read and printed to console\", 0, selectTool.getLinesRead());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testLandsatToRemoteFile",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testLandsatToRemoteFile() throws Throwable\n{\r\n    describe(\"select part of the landsat to a file\");\r\n    Path dest = path(\"testLandsatToRemoteFile.csv\");\r\n    run(selectConf, selectTool, D, v(CSV_OUTPUT_QUOTE_CHARACTER, \"'\"), D, v(CSV_OUTPUT_QUOTE_FIELDS, CSV_OUTPUT_QUOTE_FIELDS_ALWAYS), \"select\", o(OPT_HEADER), CSV_HEADER_OPT_USE, o(OPT_COMPRESSION), COMPRESSION_OPT_GZIP, o(OPT_LIMIT), Integer.toString(LINE_COUNT), o(OPT_OUTPUT), dest.toString(), landsatSrc, SELECT_SUNNY_ROWS_NO_LIMIT);\r\n    FileStatus status = getFileSystem().getFileStatus(dest);\r\n    assertEquals(\"Mismatch between bytes selected and file len in \" + status, selectTool.getBytesRead(), status.getLen());\r\n    assertIsFile(dest);\r\n    Configuration conf = getConfiguration();\r\n    SelectTool tool2 = new SelectTool(conf);\r\n    run(conf, tool2, \"select\", o(OPT_HEADER), CSV_HEADER_OPT_NONE, dest.toString(), SELECT_EVERYTHING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testUsage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUsage() throws Throwable\n{\r\n    runToFailure(EXIT_USAGE, getConfiguration(), TOO_FEW_ARGUMENTS, selectTool, \"select\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testRejectionOfNonS3FS",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRejectionOfNonS3FS() throws Throwable\n{\r\n    File dest = getTempFilename();\r\n    runToFailure(EXIT_SERVICE_UNAVAILABLE, getConfiguration(), WRONG_FILESYSTEM, selectTool, \"select\", dest.toString(), SELECT_EVERYTHING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testFailMissingFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testFailMissingFile() throws Throwable\n{\r\n    Path dest = path(\"testFailMissingFile.csv\");\r\n    runToFailure(EXIT_NOT_FOUND, getConfiguration(), \"\", selectTool, \"select\", dest.toString(), SELECT_EVERYTHING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testS3SelectDisabled",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testS3SelectDisabled() throws Throwable\n{\r\n    Configuration conf = getConfiguration();\r\n    conf.setBoolean(FS_S3A_SELECT_ENABLED, false);\r\n    disableFilesystemCaching(conf);\r\n    runToFailure(EXIT_SERVICE_UNAVAILABLE, conf, SELECT_IS_DISABLED, selectTool, \"select\", o(OPT_HEADER), CSV_HEADER_OPT_USE, o(OPT_COMPRESSION), COMPRESSION_OPT_GZIP, o(OPT_LIMIT), Integer.toString(LINE_COUNT), landsatSrc, SELECT_SUNNY_ROWS_NO_LIMIT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectBadLimit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSelectBadLimit() throws Throwable\n{\r\n    runToFailure(EXIT_USAGE, getConfiguration(), \"\", selectTool, \"select\", o(OPT_HEADER), CSV_HEADER_OPT_USE, o(OPT_COMPRESSION), COMPRESSION_OPT_GZIP, o(OPT_LIMIT), \"-1\", landsatSrc, SELECT_NOTHING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectBadInputFormat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSelectBadInputFormat() throws Throwable\n{\r\n    runToFailure(EXIT_COMMAND_ARGUMENT_ERROR, getConfiguration(), \"\", selectTool, \"select\", o(OPT_HEADER), CSV_HEADER_OPT_USE, o(OPT_INPUTFORMAT), \"pptx\", o(OPT_COMPRESSION), COMPRESSION_OPT_GZIP, landsatSrc, SELECT_NOTHING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectBadOutputFormat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSelectBadOutputFormat() throws Throwable\n{\r\n    runToFailure(EXIT_COMMAND_ARGUMENT_ERROR, getConfiguration(), \"\", selectTool, \"select\", o(OPT_HEADER), CSV_HEADER_OPT_USE, o(OPT_OUTPUTFORMAT), \"pptx\", o(OPT_COMPRESSION), COMPRESSION_OPT_GZIP, landsatSrc, SELECT_NOTHING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "o",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String o(String in)\n{\r\n    return \"-\" + in;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "v",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String v(String key, String value)\n{\r\n    return checkNotNull(key) + \"=\" + checkNotNull(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "nameThread",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void nameThread()\n{\r\n    Thread.currentThread().setName(\"JUnit-\" + methodName.getMethodName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getGlobalTimeout",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getGlobalTimeout()\n{\r\n    return S3ATestConstants.S3A_TEST_TIMEOUT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    nameThread();\r\n    Configuration conf = new Configuration();\r\n    fs = S3ATestUtils.createTestFileSystem(conf);\r\n    assumeNotNull(fs);\r\n    basePath = fs.makeQualified(S3ATestUtils.createTestPath(new Path(\"s3afilesystemcontract\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getTestBaseDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getTestBaseDir()\n{\r\n    return basePath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testMkdirsWithUmask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMkdirsWithUmask() throws Exception\n{\r\n    skip(\"Not supported\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRenameDirectoryAsExistingDirectory",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRenameDirectoryAsExistingDirectory() throws Exception\n{\r\n    assumeTrue(renameSupported());\r\n    Path src = path(\"testRenameDirectoryAsExisting/dir\");\r\n    fs.mkdirs(src);\r\n    createFile(path(src + \"/file1\"));\r\n    createFile(path(src + \"/subdir/file2\"));\r\n    Path dst = path(\"testRenameDirectoryAsExistingNew/newdir\");\r\n    fs.mkdirs(dst);\r\n    rename(src, dst, true, false, true);\r\n    assertFalse(\"Nested file1 exists\", fs.exists(path(src + \"/file1\")));\r\n    assertFalse(\"Nested file2 exists\", fs.exists(path(src + \"/subdir/file2\")));\r\n    assertTrue(\"Renamed nested file1 exists\", fs.exists(path(dst + \"/file1\")));\r\n    assertTrue(\"Renamed nested exists\", fs.exists(path(dst + \"/subdir/file2\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRenameDirectoryAsExistingFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testRenameDirectoryAsExistingFile() throws Exception\n{\r\n    assumeTrue(renameSupported());\r\n    Path src = path(\"testRenameDirectoryAsExistingFile/dir\");\r\n    fs.mkdirs(src);\r\n    Path dst = path(\"testRenameDirectoryAsExistingFileNew/newfile\");\r\n    createFile(dst);\r\n    intercept(FileAlreadyExistsException.class, () -> rename(src, dst, false, true, true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRenameDirectoryMoveToNonExistentDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRenameDirectoryMoveToNonExistentDirectory() throws Exception\n{\r\n    skip(\"does not fail\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRenameFileMoveToNonExistentDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRenameFileMoveToNonExistentDirectory() throws Exception\n{\r\n    skip(\"does not fail\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRenameFileAsExistingFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRenameFileAsExistingFile() throws Exception\n{\r\n    intercept(FileAlreadyExistsException.class, () -> super.testRenameFileAsExistingFile());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRenameNonExistentPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRenameNonExistentPath() throws Exception\n{\r\n    intercept(FileNotFoundException.class, () -> super.testRenameNonExistentPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getBlockOutputBufferName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockOutputBufferName()\n{\r\n    return Constants.FAST_UPLOAD_BYTEBUFFER;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3ADataBlocks.BlockFactory createFactory(S3AFileSystem fileSystem)\n{\r\n    return new S3ADataBlocks.ByteBufferBlockFactory(fileSystem);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getDelegationBinding",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDelegationBinding()\n{\r\n    return DELEGATION_TOKEN_ROLE_BINDING;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getFilePrefix",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getFilePrefix()\n{\r\n    return \"role\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "prepareRequest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T prepareRequest(T t)\n{\r\n    return t;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getRequestFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RequestFactory getRequestFactory()\n{\r\n    return REQUEST_FACTORY;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getOutcome",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Pair<StagingTestBase.ClientResults, StagingTestBase.ClientErrors> getOutcome()\n{\r\n    return outcome;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getLogEvents",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getLogEvents()\n{\r\n    return logEvents;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setLogEvents",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLogEvents(int logEvents)\n{\r\n    this.logEvents = logEvents;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "event",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void event(String format, Object... args)\n{\r\n    Throwable ex = null;\r\n    String s = String.format(format, args);\r\n    switch(logEvents) {\r\n        case LOG_STACK:\r\n            ex = new Exception(s);\r\n        case LOG_NAME:\r\n            LOG.info(s, ex);\r\n            break;\r\n        case LOG_NONE:\r\n        default:\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI getUri()\n{\r\n    return FS_URI;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getWorkingDirectory()\n{\r\n    return new Path(root, \"work\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "qualify",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path qualify(final Path path)\n{\r\n    return path.makeQualified(FS_URI, getWorkingDirectory());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initialize(URI name, Configuration originalConf) throws IOException\n{\r\n    conf = originalConf;\r\n    writeHelper = new WriteOperationHelper(this, conf, new EmptyS3AStatisticsContext(), noopAuditor(conf), AuditTestSupport.NOOP_SPAN);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getWriteOperationHelper",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "WriteOperationHelper getWriteOperationHelper()\n{\r\n    return writeHelper;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isMagicCommitEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isMagicCommitEnabled()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setAmazonS3Client",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setAmazonS3Client(AmazonS3 client)\n{\r\n    LOG.debug(\"Setting S3 client to {}\", client);\r\n    super.setAmazonS3Client(client);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "exists",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean exists(Path f) throws IOException\n{\r\n    event(\"exists(%s)\", f);\r\n    return mock.exists(f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "finishedWrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void finishedWrite(String key, long length, String eTag, String versionId)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataInputStream open(Path f, int bufferSize) throws IOException\n{\r\n    event(\"open(%s)\", f);\r\n    return mock.open(f, bufferSize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException\n{\r\n    event(\"create(%s)\", f);\r\n    return mock.create(f, permission, overwrite, bufferSize, replication, blockSize, progress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FSDataOutputStream append(Path f, int bufferSize, Progressable progress) throws IOException\n{\r\n    return mock.append(f, bufferSize, progress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "rename",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean rename(Path src, Path dst) throws IOException\n{\r\n    event(\"rename(%s, %s)\", src, dst);\r\n    return mock.rename(src, dst);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean delete(Path f, boolean recursive) throws IOException\n{\r\n    event(\"delete(%s, %s)\", f, recursive);\r\n    return mock.delete(f, recursive);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileStatus[] listStatus(Path f) throws IOException\n{\r\n    event(\"listStatus(%s)\", f);\r\n    return mock.listStatus(f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<LocatedFileStatus> listFiles(Path f, boolean recursive) throws IOException\n{\r\n    event(\"listFiles(%s, %s)\", f, recursive);\r\n    return new EmptyIterator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setWorkingDirectory(Path newDir)\n{\r\n    mock.setWorkingDirectory(newDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean mkdirs(Path f) throws IOException\n{\r\n    event(\"mkdirs(%s)\", f);\r\n    return mock.mkdirs(f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean mkdirs(Path f, FsPermission permission) throws IOException\n{\r\n    event(\"mkdirs(%s)\", f);\r\n    return mock.mkdirs(f, permission);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileStatus getFileStatus(Path f) throws IOException\n{\r\n    event(\"getFileStatus(%s)\", f);\r\n    return checkNotNull(mock.getFileStatus(f), \"Mock getFileStatus(%s) returned null\", f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getDefaultBlockSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDefaultBlockSize(Path f)\n{\r\n    return mock.getDefaultBlockSize(f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementStatistic",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void incrementStatistic(Statistic statistic)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementStatistic",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void incrementStatistic(Statistic statistic, long count)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementGauge",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void incrementGauge(Statistic statistic, long count)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementReadOperations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void incrementReadOperations()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementWriteOperations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void incrementWriteOperations()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementPutStartStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void incrementPutStartStatistics(long bytes)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementPutCompletedStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void incrementPutCompletedStatistics(boolean success, long bytes)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementPutProgressStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void incrementPutProgressStatistics(String key, long bytes)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getDefaultBlockSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDefaultBlockSize()\n{\r\n    return mock.getDefaultBlockSize();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "deleteObjectAtPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deleteObjectAtPath(Path f, String key, boolean isFile) throws AmazonClientException, IOException\n{\r\n    deleteObject(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "maybeCreateFakeParentDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void maybeCreateFakeParentDirectory(Path path) throws IOException, AmazonClientException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"MockS3AFileSystem{\");\r\n    sb.append(\"inner mockFS=\").append(mock);\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newCommitterStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CommitterStatistics newCommitterStatistics()\n{\r\n    return EmptyS3AStatisticsContext.EMPTY_COMMITTER_STATISTICS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "operationRetried",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void operationRetried(Exception ex)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getDurationTrackerFactory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DurationTrackerFactory getDurationTrackerFactory()\n{\r\n    return stubDurationTrackerFactory();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    resetAuditOptions(conf);\r\n    conf.setBoolean(AUDIT_ENABLED, false);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testAuditorDisabled",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testAuditorDisabled()\n{\r\n    final S3AFileSystem fs = getFileSystem();\r\n    final AuditManagerS3A auditManager = fs.getAuditManager();\r\n    Assertions.assertThat(auditManager).isInstanceOf(NoopAuditManagerS3A.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testAuditSpansAreAllTheSame",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testAuditSpansAreAllTheSame() throws Throwable\n{\r\n    final S3AFileSystem fs = getFileSystem();\r\n    final AuditSpanS3A span1 = fs.createSpan(\"span1\", null, null);\r\n    final AuditSpanS3A span2 = fs.createSpan(\"span2\", null, null);\r\n    Assertions.assertThat(span1).describedAs(\"audit span 1\").isSameAs(NOOP_SPAN);\r\n    Assertions.assertThat(span2).describedAs(\"audit span 2\").isSameAs(span1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setupDataset",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupDataset()\n{\r\n    dataset = ContractTestUtils.dataset(BLOCK_SIZE, 0, 256);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    S3ATestUtils.disableFilesystemCaching(conf);\r\n    conf.setLong(MIN_MULTIPART_THRESHOLD, MULTIPART_MIN_SIZE);\r\n    conf.setInt(MULTIPART_SIZE, MULTIPART_MIN_SIZE);\r\n    conf.set(FAST_UPLOAD_BUFFER, getBlockOutputBufferName());\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getBlockOutputBufferName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockOutputBufferName()\n{\r\n    return FAST_UPLOAD_BUFFER_ARRAY;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testZeroByteUpload",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testZeroByteUpload() throws IOException\n{\r\n    verifyUpload(\"0\", 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRegularUpload",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRegularUpload() throws IOException\n{\r\n    verifyUpload(\"regular\", 1024);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testWriteAfterStreamClose",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testWriteAfterStreamClose() throws Throwable\n{\r\n    Path dest = path(\"testWriteAfterStreamClose\");\r\n    describe(\" testWriteAfterStreamClose\");\r\n    FSDataOutputStream stream = getFileSystem().create(dest, true);\r\n    byte[] data = ContractTestUtils.dataset(16, 'a', 26);\r\n    try {\r\n        stream.write(data);\r\n        stream.close();\r\n        stream.write(data);\r\n    } finally {\r\n        IOUtils.closeStream(stream);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testBlocksClosed",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testBlocksClosed() throws Throwable\n{\r\n    Path dest = path(\"testBlocksClosed\");\r\n    describe(\" testBlocksClosed\");\r\n    FSDataOutputStream stream = getFileSystem().create(dest, true);\r\n    BlockOutputStreamStatistics statistics = S3ATestUtils.getOutputStreamStatistics(stream);\r\n    byte[] data = ContractTestUtils.dataset(16, 'a', 26);\r\n    stream.write(data);\r\n    LOG.info(\"closing output stream\");\r\n    stream.close();\r\n    assertEquals(\"total allocated blocks in \" + statistics, 1, statistics.getBlocksAllocated());\r\n    assertEquals(\"actively allocated blocks in \" + statistics, 0, statistics.getBlocksActivelyAllocated());\r\n    LOG.info(\"end of test case\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "verifyUpload",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyUpload(String name, int fileSize) throws IOException\n{\r\n    Path dest = path(name);\r\n    describe(name + \" upload to \" + dest);\r\n    ContractTestUtils.createAndVerifyFile(getFileSystem(), dest, fileSize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3ADataBlocks.BlockFactory createFactory(S3AFileSystem fileSystem)\n{\r\n    return new S3ADataBlocks.ArrayBlockFactory(fileSystem);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "markAndResetDatablock",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void markAndResetDatablock(S3ADataBlocks.BlockFactory factory) throws Exception\n{\r\n    S3AInstrumentation instrumentation = new S3AInstrumentation(new URI(\"s3a://example\"));\r\n    BlockOutputStreamStatistics outstats = instrumentation.newOutputStreamStatistics(null);\r\n    S3ADataBlocks.DataBlock block = factory.create(1, BLOCK_SIZE, outstats);\r\n    block.write(dataset, 0, dataset.length);\r\n    S3ADataBlocks.BlockUploadData uploadData = block.startUpload();\r\n    InputStream stream = uploadData.getUploadStream();\r\n    assertNotNull(stream);\r\n    assertTrue(\"Mark not supported in \" + stream, stream.markSupported());\r\n    assertEquals(0, stream.read());\r\n    stream.mark(BLOCK_SIZE);\r\n    long l = 0;\r\n    while (stream.read() != -1) {\r\n        l++;\r\n    }\r\n    stream.reset();\r\n    assertEquals(1, stream.read());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testMarkReset",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMarkReset() throws Throwable\n{\r\n    markAndResetDatablock(createFactory(getFileSystem()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testAbortAfterWrite",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testAbortAfterWrite() throws Throwable\n{\r\n    describe(\"Verify abort after a write does not create a file\");\r\n    Path dest = path(getMethodName());\r\n    FileSystem fs = getFileSystem();\r\n    ContractTestUtils.assertHasPathCapabilities(fs, dest, ABORTABLE_STREAM);\r\n    FSDataOutputStream stream = fs.create(dest, true);\r\n    byte[] data = ContractTestUtils.dataset(16, 'a', 26);\r\n    try {\r\n        ContractTestUtils.assertCapabilities(stream, new String[] { ABORTABLE_STREAM }, null);\r\n        stream.write(data);\r\n        assertCompleteAbort(stream.abort());\r\n        assertNoopAbort(stream.abort());\r\n        ContractTestUtils.assertPathsDoNotExist(fs, \"aborted file\", dest);\r\n    } finally {\r\n        IOUtils.closeStream(stream);\r\n        ContractTestUtils.assertPathsDoNotExist(fs, \"aborted file\", dest);\r\n    }\r\n    assertNoopAbort(stream.abort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testAbortAfterCloseIsHarmless",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testAbortAfterCloseIsHarmless() throws Throwable\n{\r\n    describe(\"Verify abort on a closed stream is harmless \" + \"and that the result indicates that nothing happened\");\r\n    Path dest = path(getMethodName());\r\n    FileSystem fs = getFileSystem();\r\n    byte[] data = ContractTestUtils.dataset(16, 'a', 26);\r\n    try (FSDataOutputStream stream = fs.create(dest, true)) {\r\n        stream.write(data);\r\n        assertCompleteAbort(stream.abort());\r\n        stream.close();\r\n        assertNoopAbort(stream.abort());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    final Configuration conf = super.createConfiguration();\r\n    String bucketName = getTestBucketName(conf);\r\n    removeBucketOverrides(bucketName, conf, DOWNGRADE_SYNCABLE_EXCEPTIONS);\r\n    conf.setBoolean(DOWNGRADE_SYNCABLE_EXCEPTIONS, true);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testHFlushDowngrade",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testHFlushDowngrade() throws Throwable\n{\r\n    describe(\"Verify that hflush() calls can be downgraded from fail\" + \" to ignore; the relevant counter is updated\");\r\n    Path path = methodPath();\r\n    S3AFileSystem fs = getFileSystem();\r\n    final IOStatistics fsIoStats = fs.getIOStatistics();\r\n    assertThatStatisticCounter(fsIoStats, OP_HFLUSH).isEqualTo(0);\r\n    try (FSDataOutputStream out = fs.create(path, true)) {\r\n        out.write('1');\r\n        out.hflush();\r\n        IOStatistics iostats = out.getIOStatistics();\r\n        LOG.info(\"IOStats {}\", ioStatisticsToString(iostats));\r\n        assertThatStatisticCounter(iostats, OP_HFLUSH).isEqualTo(1);\r\n        assertThatStatisticCounter(iostats, OP_HSYNC).isEqualTo(0);\r\n    }\r\n    assertThatStatisticCounter(fsIoStats, OP_HFLUSH).isEqualTo(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testHSyncDowngrade",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testHSyncDowngrade() throws Throwable\n{\r\n    describe(\"Verify that hsync() calls can be downgraded from fail\" + \" to ignore; the relevant counter is updated\");\r\n    Path path = methodPath();\r\n    S3AFileSystem fs = getFileSystem();\r\n    final IOStatistics fsIoStats = fs.getIOStatistics();\r\n    assertThatStatisticCounter(fsIoStats, OP_HSYNC).isEqualTo(0);\r\n    try (FSDataOutputStream out = fs.create(path, true)) {\r\n        out.write('1');\r\n        out.hsync();\r\n        IOStatistics iostats = out.getIOStatistics();\r\n        LOG.info(\"IOStats {}\", ioStatisticsToString(iostats));\r\n        assertThatStatisticCounter(iostats, OP_HFLUSH).isEqualTo(0);\r\n        assertThatStatisticCounter(iostats, OP_HSYNC).isEqualTo(1);\r\n    }\r\n    assertThatStatisticCounter(fsIoStats, OP_HSYNC).isEqualTo(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { \"keep-markers\", true }, { \"delete-markers\", false } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testMkdirOverDir",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testMkdirOverDir() throws Throwable\n{\r\n    describe(\"create a dir over a dir\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path baseDir = dir(methodPath());\r\n    verifyMetrics(() -> fs.mkdirs(baseDir), with(OBJECT_METADATA_REQUESTS, 0), with(OBJECT_LIST_REQUEST, FILESTATUS_DIR_PROBE_L));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testMkdirWithParent",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMkdirWithParent() throws Throwable\n{\r\n    describe(\"create a dir under a dir with a parent\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path baseDir = dir(methodPath());\r\n    Path childDir = new Path(baseDir, \"child\");\r\n    verifyMetrics(() -> fs.mkdirs(childDir), with(OBJECT_METADATA_REQUESTS, FILESTATUS_FILE_PROBE_H), with(OBJECT_LIST_REQUEST, FILESTATUS_FILE_PROBE_L + 2 * FILESTATUS_DIR_PROBE_L));\r\n    Path sibling = new Path(baseDir, \"sibling\");\r\n    verifyMetrics(() -> fs.mkdirs(sibling), with(OBJECT_METADATA_REQUESTS, FILESTATUS_FILE_PROBE_H), with(OBJECT_LIST_REQUEST, FILESTATUS_FILE_PROBE_L + 2 * FILESTATUS_DIR_PROBE_L));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testMkdirWithGrandparent",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMkdirWithGrandparent() throws Throwable\n{\r\n    describe(\"create a dir under a dir with a parent\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path baseDir = dir(methodPath());\r\n    Path subDir = new Path(baseDir, \"child/grandchild\");\r\n    verifyMetrics(() -> fs.mkdirs(subDir), with(OBJECT_METADATA_REQUESTS, 2 * FILESTATUS_FILE_PROBE_H), with(OBJECT_LIST_REQUEST, 3 * FILESTATUS_DIR_PROBE_L));\r\n    Path sibling = new Path(baseDir, \"child/sibling\");\r\n    verifyMetrics(() -> fs.mkdirs(sibling), with(OBJECT_METADATA_REQUESTS, FILESTATUS_FILE_PROBE_H), with(OBJECT_LIST_REQUEST, FILESTATUS_FILE_PROBE_L + 2 * FILESTATUS_DIR_PROBE_L));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testMkdirOverFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMkdirOverFile() throws Throwable\n{\r\n    describe(\"create a dir over a file; expect dir and file probes\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path baseDir = dir(methodPath());\r\n    Path childDir = new Path(baseDir, \"child\");\r\n    touch(fs, childDir);\r\n    verifyMetricsIntercepting(FileAlreadyExistsException.class, \"\", () -> fs.mkdirs(childDir), with(OBJECT_METADATA_REQUESTS, FILESTATUS_FILE_PROBE_H), with(OBJECT_LIST_REQUEST, FILESTATUS_DIR_PROBE_L));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testRoundTrip",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testRoundTrip() throws Throwable\n{\r\n    PartHandlePayload result = roundTrip(999, \"tag\", 1);\r\n    assertEquals(PATH, result.getPath());\r\n    assertEquals(UPLOAD, result.getUploadId());\r\n    assertEquals(999, result.getPartNumber());\r\n    assertEquals(\"tag\", result.getEtag());\r\n    assertEquals(1, result.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testRoundTrip2",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testRoundTrip2() throws Throwable\n{\r\n    long len = 1L + Integer.MAX_VALUE;\r\n    PartHandlePayload result = roundTrip(1, \"11223344\", len);\r\n    assertEquals(1, result.getPartNumber());\r\n    assertEquals(\"11223344\", result.getEtag());\r\n    assertEquals(len, result.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testNoEtag",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNoEtag() throws Throwable\n{\r\n    intercept(IllegalArgumentException.class, () -> buildPartHandlePayload(PATH, UPLOAD, 0, \"\", 1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testNoLen",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNoLen() throws Throwable\n{\r\n    intercept(IllegalArgumentException.class, () -> buildPartHandlePayload(PATH, UPLOAD, 0, \"tag\", -1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testBadPayload",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testBadPayload() throws Throwable\n{\r\n    intercept(EOFException.class, () -> parsePartHandlePayload(new byte[0]));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testBadHeader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBadHeader() throws Throwable\n{\r\n    byte[] bytes = buildPartHandlePayload(PATH, UPLOAD, 0, \"tag\", 1);\r\n    bytes[2] = 'f';\r\n    intercept(IOException.class, \"header\", () -> parsePartHandlePayload(bytes));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "roundTrip",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "PartHandlePayload roundTrip(int partNumber, String tag, long len) throws IOException\n{\r\n    byte[] bytes = buildPartHandlePayload(PATH, UPLOAD, partNumber, tag, len);\r\n    return parsePartHandlePayload(bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardown()\n{\r\n    if (workers != null) {\r\n        workers.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "testSpanMapClearedInServiceStop",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSpanMapClearedInServiceStop() throws IOException\n{\r\n    try (ActiveAuditManagerS3A auditManager = new ActiveAuditManagerS3A(emptyStatisticsStore())) {\r\n        auditManager.init(createMemoryHungryConfiguration());\r\n        auditManager.start();\r\n        auditManager.getActiveAuditSpan();\r\n        final WeakReferenceThreadMap<?> spanMap = auditManager.getActiveSpanMap();\r\n        Assertions.assertThat(spanMap.size()).describedAs(\"map size\").isEqualTo(1);\r\n        auditManager.stop();\r\n        Assertions.assertThat(spanMap.size()).describedAs(\"map size\").isEqualTo(0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "testMemoryLeak",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMemoryLeak() throws Throwable\n{\r\n    workers = Executors.newFixedThreadPool(THREAD_COUNT);\r\n    for (int i = 0; i < MANAGER_COUNT; i++) {\r\n        final long oneAuditConsumption = createAndTestOneAuditor();\r\n        LOG.info(\"manager {} memory retained {}\", i, oneAuditConsumption);\r\n    }\r\n    LOG.info(\"Total prune count {}\", pruneCount);\r\n    Assertions.assertThat(pruneCount).describedAs(\"Total prune count\").isNotZero();\r\n    Assertions.assertThat(auditManagers.stream().filter((r) -> r.get() == null).count()).describedAs(\"number of audit managers garbage collected\").isNotZero();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "createAndTestOneAuditor",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "long createAndTestOneAuditor() throws Exception\n{\r\n    long original = Runtime.getRuntime().freeMemory();\r\n    ExecutorService factory = Executors.newSingleThreadExecutor();\r\n    try {\r\n        pruneCount += factory.submit(this::createAuditorAndWorkers).get();\r\n    } finally {\r\n        factory.shutdown();\r\n        factory.awaitTermination(60, TimeUnit.SECONDS);\r\n    }\r\n    final long current = Runtime.getRuntime().freeMemory();\r\n    return current - original;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "createAuditorAndWorkers",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "int createAuditorAndWorkers() throws IOException, InterruptedException, ExecutionException\n{\r\n    try (ActiveAuditManagerS3A auditManager = new ActiveAuditManagerS3A(emptyStatisticsStore())) {\r\n        auditManager.init(createMemoryHungryConfiguration());\r\n        auditManager.start();\r\n        LOG.info(\"Using {}\", auditManager);\r\n        auditManagers.add(new WeakReference<>(auditManager));\r\n        Set<Long> threadIds = new HashSet<>();\r\n        List<Future<Result>> futures = new ArrayList<>(THREAD_COUNT);\r\n        for (int i = 0; i < THREAD_COUNT; i++) {\r\n            futures.add(workers.submit(() -> spanningOperation(auditManager)));\r\n        }\r\n        for (Future<Result> future : futures) {\r\n            final Result r = future.get();\r\n            threadIds.add(r.getThreadId());\r\n        }\r\n        final int threadsUsed = threadIds.size();\r\n        final Long[] threadIdArray = threadIds.toArray(new Long[0]);\r\n        System.gc();\r\n        final WeakReferenceThreadMap<?> spanMap = auditManager.getActiveSpanMap();\r\n        final long derefenced = threadIds.stream().filter((id) -> !spanMap.containsKey(id)).count();\r\n        if (derefenced > 0) {\r\n            LOG.info(\"{} executed across {} threads and dereferenced {} entries\", auditManager, threadsUsed, derefenced);\r\n        }\r\n        int spansRecreated = 0;\r\n        int subset = threadIdArray.length - 1;\r\n        LOG.info(\"Resolving {} thread references\", subset);\r\n        for (int i = 0; i < subset; i++) {\r\n            final long id = threadIdArray[i];\r\n            final boolean present = spanMap.containsKey(id);\r\n            Assertions.assertThat(spanMap.get(id)).describedAs(\"Span map entry for thread %d\", id).isNotNull();\r\n            if (!present) {\r\n                spansRecreated++;\r\n            }\r\n        }\r\n        LOG.info(\"Recreated {} spans\", subset);\r\n        if (derefenced > threadIdArray.length - subset) {\r\n            Assertions.assertThat(spansRecreated).describedAs(\"number of recreated spans\").isGreaterThan(0);\r\n        }\r\n        int pruned = auditManager.prune();\r\n        if (pruned > 0) {\r\n            LOG.info(\"{} executed across {} threads and pruned {} entries\", auditManager, threadsUsed, pruned);\r\n        }\r\n        Assertions.assertThat(pruned).describedAs(\"Count of references pruned\").isEqualTo(derefenced - spansRecreated);\r\n        return pruned + (int) derefenced;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "createMemoryHungryConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createMemoryHungryConfiguration()\n{\r\n    final Configuration conf = new Configuration(false);\r\n    conf.set(AUDIT_SERVICE_CLASSNAME, MemoryHungryAuditor.NAME);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "spanningOperation",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Result spanningOperation(final ActiveAuditManagerS3A auditManager) throws IOException\n{\r\n    auditManager.getActiveAuditSpan();\r\n    final AuditSpanS3A auditSpan = auditManager.createSpan(\"span\", null, null);\r\n    Assertions.assertThat(auditSpan).describedAs(\"audit span for current thread\").isNotNull();\r\n    Thread.yield();\r\n    return new Result(Thread.currentThread().getId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "testRegularPruning",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRegularPruning() throws Throwable\n{\r\n    try (ActiveAuditManagerS3A auditManager = new ActiveAuditManagerS3A(emptyStatisticsStore())) {\r\n        auditManager.init(createMemoryHungryConfiguration());\r\n        auditManager.start();\r\n        final WeakReferenceThreadMap<?> spanMap = auditManager.getActiveSpanMap();\r\n        spanMap.put(Thread.currentThread().getId() + 1, null);\r\n        int pruningCount = 0;\r\n        for (int i = 0; i < PRUNE_THRESHOLD * 2 + 1; i++) {\r\n            boolean pruned = auditManager.removeActiveSpanFromMap();\r\n            if (pruned) {\r\n                pruningCount++;\r\n            }\r\n        }\r\n        Assertions.assertThat(pruningCount).describedAs(\"Intermittent pruning count\").isEqualTo(2);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "testSpanDeactivationRemovesEntryFromMap",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testSpanDeactivationRemovesEntryFromMap() throws Throwable\n{\r\n    try (ActiveAuditManagerS3A auditManager = new ActiveAuditManagerS3A(emptyStatisticsStore())) {\r\n        auditManager.init(createMemoryHungryConfiguration());\r\n        auditManager.start();\r\n        final WeakReferenceThreadMap<?> spanMap = auditManager.getActiveSpanMap();\r\n        final AuditSpanS3A auditSpan = auditManager.createSpan(\"span\", null, null);\r\n        Assertions.assertThat(auditManager.getActiveAuditSpan()).describedAs(\"active span\").isSameAs(auditSpan);\r\n        Consumer<Boolean> assertMapHasKey = expected -> Assertions.assertThat(spanMap.containsKey(spanMap.currentThreadId())).describedAs(\"map entry for current thread\").isEqualTo(expected);\r\n        auditSpan.deactivate();\r\n        assertMapHasKey.accept(false);\r\n        final AuditSpanS3A newSpan = auditManager.getActiveAuditSpan();\r\n        Assertions.assertThat(newSpan).describedAs(\"active span\").isNotNull().matches(s -> !s.isValidSpan());\r\n        assertMapHasKey.accept(true);\r\n        auditSpan.deactivate();\r\n        assertMapHasKey.accept(true);\r\n        newSpan.deactivate();\r\n        assertMapHasKey.accept(false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "cleanupDestDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanupDestDir() throws IOException\n{\r\n    rmdir(this.outDir, getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "suitename",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String suitename()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "log",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Logger log()\n{\r\n    return LOG;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getMethodName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getMethodName()\n{\r\n    return suitename() + \"-\" + super.getMethodName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    jobId = randomJobId();\r\n    attempt0 = \"attempt_\" + jobId + \"_m_000000_0\";\r\n    taskAttempt0 = TaskAttemptID.forName(attempt0);\r\n    attempt1 = \"attempt_\" + jobId + \"_m_000001_0\";\r\n    taskAttempt1 = TaskAttemptID.forName(attempt1);\r\n    outDir = path(getMethodName());\r\n    abortMultipartUploadsUnderPath(outDir);\r\n    cleanupDestDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "teardown",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    describe(\"teardown\");\r\n    abortInTeardown.forEach(this::abortJobQuietly);\r\n    if (outDir != null) {\r\n        try (AuditSpan span = span()) {\r\n            abortMultipartUploadsUnderPath(outDir);\r\n            cleanupDestDir();\r\n        } catch (IOException e) {\r\n            log().info(\"Exception during cleanup\", e);\r\n        }\r\n    }\r\n    S3AFileSystem fileSystem = getFileSystem();\r\n    if (fileSystem != null) {\r\n        log().info(\"Statistics for {}:\\n{}\", fileSystem.getUri(), fileSystem.getInstrumentation().dump(\"  \", \" =  \", \"\\n\", true));\r\n    }\r\n    super.teardown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "checkForThreadLeakage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkForThreadLeakage()\n{\r\n    List<String> committerThreads = getCurrentThreadNames().stream().filter(n -> n.startsWith(AbstractS3ACommitter.THREAD_PREFIX)).collect(Collectors.toList());\r\n    Assertions.assertThat(committerThreads).describedAs(\"Outstanding committer threads\").isEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "abortInTeardown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortInTeardown(JobData jobData)\n{\r\n    abortInTeardown.add(jobData);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    disableFilesystemCaching(conf);\r\n    bindCommitter(conf);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "bindCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void bindCommitter(Configuration conf)\n{\r\n    super.bindCommitter(conf, getCommitterFactoryName(), getCommitterName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "createCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractS3ACommitter createCommitter(TaskAttemptContext context) throws IOException\n{\r\n    return createCommitter(getOutDir(), context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "createCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractS3ACommitter createCommitter(Path outputPath, TaskAttemptContext context) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getCommitterFactoryName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCommitterFactoryName()\n{\r\n    return CommitConstants.S3A_COMMITTER_FACTORY;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getCommitterName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCommitterName()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getOutDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getOutDir()\n{\r\n    return outDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobId()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getAttempt0",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAttempt0()\n{\r\n    return attempt0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getTaskAttempt0",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getTaskAttempt0()\n{\r\n    return taskAttempt0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getAttempt1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAttempt1()\n{\r\n    return attempt1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getTaskAttempt1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getTaskAttempt1()\n{\r\n    return taskAttempt1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "writeTextOutput",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Path writeTextOutput(TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    describe(\"write output\");\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Writing Text output for task %s\", context.getTaskAttemptID())) {\r\n        LoggingTextOutputFormat.LoggingLineRecordWriter<Object, Object> recordWriter = new LoggingTextOutputFormat<>().getRecordWriter(context);\r\n        writeOutput(recordWriter, context);\r\n        return recordWriter.getDest();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "writeOutput",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void writeOutput(RecordWriter writer, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    NullWritable nullWritable = NullWritable.get();\r\n    try (CloseWriter cw = new CloseWriter(writer, context)) {\r\n        writer.write(KEY_1, VAL_1);\r\n        writer.write(null, nullWritable);\r\n        writer.write(null, VAL_1);\r\n        writer.write(nullWritable, VAL_2);\r\n        writer.write(KEY_2, nullWritable);\r\n        writer.write(KEY_1, null);\r\n        writer.write(null, null);\r\n        writer.write(KEY_2, VAL_2);\r\n        writer.close(context);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "writeMapFileOutput",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeMapFileOutput(RecordWriter writer, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    describe(\"\\nWrite map output\");\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Writing Text output for task %s\", context.getTaskAttemptID());\r\n        CloseWriter cw = new CloseWriter(writer, context)) {\r\n        for (int i = 0; i < 10; ++i) {\r\n            Text val = ((i & 1) == 1) ? VAL_1 : VAL_2;\r\n            writer.write(new LongWritable(i), val);\r\n        }\r\n        writer.close(context);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "newJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job newJob() throws IOException\n{\r\n    return newJob(outDir, getConfiguration(), attempt0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "newJob",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Job newJob(Path dir, Configuration configuration, String taskAttemptId) throws IOException\n{\r\n    Job job = Job.getInstance(configuration);\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttemptId);\r\n    conf.setBoolean(CREATE_SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true);\r\n    FileOutputFormat.setOutputPath(job, dir);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "startJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobData startJob(boolean writeText) throws IOException, InterruptedException\n{\r\n    return startJob(standardCommitterFactory, writeText);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "startJob",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "JobData startJob(CommitterFactory factory, boolean writeText) throws IOException, InterruptedException\n{\r\n    Job job = newJob();\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt0);\r\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1);\r\n    JobContext jContext = new JobContextImpl(conf, taskAttempt0.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskAttempt0);\r\n    AbstractS3ACommitter committer = factory.createCommitter(tContext);\r\n    JobData jobData = new JobData(job, jContext, tContext, committer);\r\n    setup(jobData);\r\n    abortInTeardown(jobData);\r\n    if (writeText) {\r\n        jobData.writtenTextPath = writeTextOutput(tContext);\r\n    }\r\n    return jobData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup(JobData jobData) throws IOException\n{\r\n    AbstractS3ACommitter committer = jobData.committer;\r\n    JobContext jContext = jobData.jContext;\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    describe(\"\\nsetup job\");\r\n    try (DurationInfo d = new DurationInfo(LOG, \"setup job %s\", jContext.getJobID())) {\r\n        committer.setupJob(jContext);\r\n    }\r\n    setupCommitter(committer, tContext);\r\n    describe(\"setup complete\\n\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "setupCommitter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setupCommitter(final AbstractS3ACommitter committer, final TaskAttemptContext tContext) throws IOException\n{\r\n    try (DurationInfo d = new DurationInfo(LOG, \"setup task %s\", tContext.getTaskAttemptID())) {\r\n        committer.setupTask(tContext);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "abortJobQuietly",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortJobQuietly(JobData jobData)\n{\r\n    abortJobQuietly(jobData.committer, jobData.jContext, jobData.tContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "abortJobQuietly",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void abortJobQuietly(AbstractS3ACommitter committer, JobContext jContext, TaskAttemptContext tContext)\n{\r\n    describe(\"\\naborting task\");\r\n    try {\r\n        committer.abortTask(tContext);\r\n    } catch (IOException e) {\r\n        log().warn(\"Exception aborting task:\", e);\r\n    }\r\n    describe(\"\\naborting job\");\r\n    try {\r\n        committer.abortJob(jContext, JobStatus.State.KILLED);\r\n    } catch (IOException e) {\r\n        log().warn(\"Exception aborting job\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "commit",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void commit(AbstractS3ACommitter committer, JobContext jContext, TaskAttemptContext tContext) throws IOException\n{\r\n    try (DurationInfo d = new DurationInfo(LOG, \"committing work\", jContext.getJobID())) {\r\n        describe(\"\\ncommitting task\");\r\n        committer.commitTask(tContext);\r\n        describe(\"\\ncommitting job\");\r\n        committer.commitJob(jContext);\r\n        describe(\"commit complete\\n\");\r\n        verifyCommitterHasNoThreads(committer);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "executeWork",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void executeWork(String name, ActionToTest action) throws Exception\n{\r\n    executeWork(name, startJob(false), action);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "executeWork",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void executeWork(String name, JobData jobData, ActionToTest action) throws Exception\n{\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Executing %s\", name)) {\r\n        action.exec(jobData.job, jobData.jContext, jobData.tContext, jobData.committer);\r\n    } finally {\r\n        abortJobQuietly(jobData);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testRecoveryAndCleanup",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testRecoveryAndCleanup() throws Exception\n{\r\n    describe(\"Test (Unsupported) task recovery.\");\r\n    JobData jobData = startJob(true);\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    AbstractS3ACommitter committer = jobData.committer;\r\n    assertNotNull(\"null workPath in committer \" + committer, committer.getWorkPath());\r\n    assertNotNull(\"null outputPath in committer \" + committer, committer.getOutputPath());\r\n    commitTask(committer, tContext);\r\n    assertTaskAttemptPathDoesNotExist(committer, tContext);\r\n    Configuration conf2 = jobData.job.getConfiguration();\r\n    conf2.set(MRJobConfig.TASK_ATTEMPT_ID, attempt0);\r\n    conf2.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 2);\r\n    JobContext jContext2 = new JobContextImpl(conf2, taskAttempt0.getJobID());\r\n    TaskAttemptContext tContext2 = new TaskAttemptContextImpl(conf2, taskAttempt0);\r\n    AbstractS3ACommitter committer2 = createCommitter(tContext2);\r\n    committer2.setupJob(tContext2);\r\n    assertFalse(\"recoverySupported in \" + committer2, committer2.isRecoverySupported());\r\n    intercept(PathCommitException.class, \"recover\", () -> committer2.recoverTask(tContext2));\r\n    describe(\"aborting task attempt 2; expect nothing to clean up\");\r\n    committer2.abortTask(tContext2);\r\n    describe(\"Aborting job 2; expect pending commits to be aborted\");\r\n    committer2.abortJob(jContext2, JobStatus.State.KILLED);\r\n    assertNoMultipartUploadsPending(outDir);\r\n    verifyCommitterHasNoThreads(committer2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "assertTaskAttemptPathDoesNotExist",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertTaskAttemptPathDoesNotExist(AbstractS3ACommitter committer, TaskAttemptContext context) throws IOException\n{\r\n    Path attemptPath = committer.getTaskAttemptPath(context);\r\n    ContractTestUtils.assertPathDoesNotExist(attemptPath.getFileSystem(context.getConfiguration()), \"task attempt dir\", attemptPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "assertJobAttemptPathDoesNotExist",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertJobAttemptPathDoesNotExist(AbstractS3ACommitter committer, JobContext context) throws IOException\n{\r\n    Path attemptPath = committer.getJobAttemptPath(context);\r\n    ContractTestUtils.assertPathDoesNotExist(attemptPath.getFileSystem(context.getConfiguration()), \"job attempt dir\", attemptPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "validateContent",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void validateContent(Path dir, boolean expectSuccessMarker, String expectedJobId) throws Exception\n{\r\n    if (expectSuccessMarker) {\r\n        SuccessData successData = verifySuccessMarker(dir, expectedJobId);\r\n    }\r\n    Path expectedFile = getPart0000(dir);\r\n    log().debug(\"Validating content in {}\", expectedFile);\r\n    StringBuffer expectedOutput = new StringBuffer();\r\n    expectedOutput.append(KEY_1).append('\\t').append(VAL_1).append(\"\\n\");\r\n    expectedOutput.append(VAL_1).append(\"\\n\");\r\n    expectedOutput.append(VAL_2).append(\"\\n\");\r\n    expectedOutput.append(KEY_2).append(\"\\n\");\r\n    expectedOutput.append(KEY_1).append(\"\\n\");\r\n    expectedOutput.append(KEY_2).append('\\t').append(VAL_2).append(\"\\n\");\r\n    String output = readFile(expectedFile);\r\n    assertEquals(\"Content of \" + expectedFile, expectedOutput.toString(), output);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getPart0000",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path getPart0000(final Path dir) throws Exception\n{\r\n    final FileSystem fs = dir.getFileSystem(getConfiguration());\r\n    FileStatus[] statuses = fs.listStatus(dir, path -> path.getName().startsWith(PART_00000));\r\n    if (statuses.length != 1) {\r\n        ContractTestUtils.assertPathExists(fs, \"Output file\", new Path(dir, PART_00000));\r\n    }\r\n    return statuses[0].getPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "validateMapFileOutputContent",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void validateMapFileOutputContent(FileSystem fs, Path dir) throws Exception\n{\r\n    assertPathExists(\"Map output\", dir);\r\n    Path expectedMapDir = getPart0000(dir);\r\n    assertPathExists(\"Map output\", expectedMapDir);\r\n    assertIsDirectory(expectedMapDir);\r\n    FileStatus[] files = fs.listStatus(expectedMapDir);\r\n    assertTrue(\"No files found in \" + expectedMapDir, files.length > 0);\r\n    assertPathExists(\"index file in \" + expectedMapDir, new Path(expectedMapDir, MapFile.INDEX_FILE_NAME));\r\n    assertPathExists(\"data file in \" + expectedMapDir, new Path(expectedMapDir, MapFile.DATA_FILE_NAME));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "dumpMultipartUploads",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void dumpMultipartUploads() throws IOException\n{\r\n    countMultipartUploads(\"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testCommitLifecycle",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testCommitLifecycle() throws Exception\n{\r\n    describe(\"Full test of the expected lifecycle:\\n\" + \" start job, task, write, commit task, commit job.\\n\" + \"Verify:\\n\" + \"* no files are visible after task commit\\n\" + \"* the expected file is visible after job commit\\n\" + \"* no outstanding MPUs after job commit\");\r\n    JobData jobData = startJob(false);\r\n    JobContext jContext = jobData.jContext;\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    AbstractS3ACommitter committer = jobData.committer;\r\n    validateTaskAttemptWorkingDirectory(committer, tContext);\r\n    describe(\"1. Writing output\");\r\n    writeTextOutput(tContext);\r\n    dumpMultipartUploads();\r\n    describe(\"2. Committing task\");\r\n    assertTrue(\"No files to commit were found by \" + committer, committer.needsTaskCommit(tContext));\r\n    commitTask(committer, tContext);\r\n    try {\r\n        applyLocatedFiles(getFileSystem().listFiles(outDir, false), (status) -> assertFalse(\"task committed file to dest :\" + status, status.getPath().toString().contains(\"part\")));\r\n    } catch (FileNotFoundException ignored) {\r\n        log().info(\"Outdir {} is not created by task commit phase \", outDir);\r\n    }\r\n    describe(\"3. Committing job\");\r\n    assertMultipartUploadsPending(outDir);\r\n    commitJob(committer, jContext);\r\n    describe(\"4. Validating content\");\r\n    validateContent(outDir, shouldExpectSuccessMarker(), committer.getUUID());\r\n    assertNoMultipartUploadsPending(outDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testCommitterWithDuplicatedCommit",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCommitterWithDuplicatedCommit() throws Exception\n{\r\n    describe(\"Call a task then job commit twice;\" + \"expect the second task commit to fail.\");\r\n    JobData jobData = startJob(true);\r\n    JobContext jContext = jobData.jContext;\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    AbstractS3ACommitter committer = jobData.committer;\r\n    commit(committer, jContext, tContext);\r\n    validateContent(outDir, shouldExpectSuccessMarker(), committer.getUUID());\r\n    assertNoMultipartUploadsPending(outDir);\r\n    expectFNFEonTaskCommit(committer, tContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testTwoTaskAttemptsCommit",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testTwoTaskAttemptsCommit() throws Exception\n{\r\n    describe(\"Commit two task attempts;\" + \" expect the second attempt to succeed.\");\r\n    JobData jobData = startJob(false);\r\n    JobContext jContext = jobData.jContext;\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    AbstractS3ACommitter committer = jobData.committer;\r\n    describe(\"\\ncommitting task\");\r\n    Path outputTA1 = writeTextOutput(tContext);\r\n    Configuration conf2 = jobData.conf;\r\n    conf2.set(\"mapreduce.output.basename\", \"attempt2\");\r\n    String attempt2 = \"attempt_\" + jobId + \"_m_000000_1\";\r\n    TaskAttemptID ta2 = TaskAttemptID.forName(attempt2);\r\n    TaskAttemptContext tContext2 = new TaskAttemptContextImpl(conf2, ta2);\r\n    AbstractS3ACommitter committer2 = standardCommitterFactory.createCommitter(tContext2);\r\n    setupCommitter(committer2, tContext2);\r\n    Path outputTA2 = writeTextOutput(tContext2);\r\n    String name1 = outputTA1.getName();\r\n    String name2 = outputTA2.getName();\r\n    Assertions.assertThat(name1).describedAs(\"name of task attempt output %s\", outputTA1).isNotEqualTo(name2);\r\n    committer.commitTask(tContext);\r\n    committer2.commitTask(tContext2);\r\n    committer2.commitJob(tContext);\r\n    S3AFileSystem fs = getFileSystem();\r\n    SuccessData successData = validateSuccessFile(outDir, \"\", fs, \"query\", 1, \"\");\r\n    Assertions.assertThat(successData.getFilenames()).describedAs(\"Files committed\").hasSize(1);\r\n    assertPathExists(\"attempt2 output\", new Path(outDir, name2));\r\n    assertPathDoesNotExist(\"attempt1 output\", new Path(outDir, name1));\r\n    assertNoMultipartUploadsPending(outDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "shouldExpectSuccessMarker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldExpectSuccessMarker()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testCommitterWithFailure",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCommitterWithFailure() throws Exception\n{\r\n    describe(\"Fail the first job commit then retry\");\r\n    JobData jobData = startJob(new FailingCommitterFactory(), true);\r\n    JobContext jContext = jobData.jContext;\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    AbstractS3ACommitter committer = jobData.committer;\r\n    committer.commitTask(tContext);\r\n    expectSimulatedFailureOnJobCommit(jContext, committer);\r\n    commitJob(committer, jContext);\r\n    validateContent(outDir, shouldExpectSuccessMarker(), committer.getUUID());\r\n    expectJobCommitToFail(jContext, committer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "expectJobCommitToFail",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectJobCommitToFail(JobContext jContext, AbstractS3ACommitter committer) throws Exception\n{\r\n    expectJobCommitFailure(jContext, committer, FileNotFoundException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "expectJobCommitFailure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "E expectJobCommitFailure(JobContext jContext, AbstractS3ACommitter committer, Class<E> clazz) throws Exception\n{\r\n    return intercept(clazz, () -> {\r\n        committer.commitJob(jContext);\r\n        return committer.toString();\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "expectFNFEonTaskCommit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectFNFEonTaskCommit(AbstractS3ACommitter committer, TaskAttemptContext tContext) throws Exception\n{\r\n    intercept(FileNotFoundException.class, () -> {\r\n        committer.commitTask(tContext);\r\n        return committer.toString();\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testCommitterWithNoOutputs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCommitterWithNoOutputs() throws Exception\n{\r\n    describe(\"Have a task and job with no outputs: expect success\");\r\n    JobData jobData = startJob(new FailingCommitterFactory(), false);\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    AbstractS3ACommitter committer = jobData.committer;\r\n    committer.commitTask(tContext);\r\n    assertTaskAttemptPathDoesNotExist(committer, tContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "expectSimulatedFailureOnJobCommit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void expectSimulatedFailureOnJobCommit(JobContext jContext, AbstractS3ACommitter committer) throws Exception\n{\r\n    ((CommitterFaultInjection) committer).setFaults(CommitterFaultInjection.Faults.commitJob);\r\n    expectJobCommitFailure(jContext, committer, CommitterFaultInjectionImpl.Failure.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testMapFileOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testMapFileOutputCommitter() throws Exception\n{\r\n    describe(\"Test that the committer generates map output into a directory\\n\" + \"starting with the prefix part-\");\r\n    JobData jobData = startJob(false);\r\n    JobContext jContext = jobData.jContext;\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    AbstractS3ACommitter committer = jobData.committer;\r\n    Configuration conf = jobData.conf;\r\n    writeMapFileOutput(new MapFileOutputFormat().getRecordWriter(tContext), tContext);\r\n    commit(committer, jContext, tContext);\r\n    S3AFileSystem fs = getFileSystem();\r\n    lsR(fs, outDir, true);\r\n    String ls = ls(outDir);\r\n    describe(\"\\nvalidating\");\r\n    verifySuccessMarker(outDir, committer.getUUID());\r\n    describe(\"validate output of %s\", outDir);\r\n    validateMapFileOutputContent(fs, outDir);\r\n    describe(\"listing\");\r\n    FileStatus[] filtered = fs.listStatus(outDir, HIDDEN_FILE_FILTER);\r\n    assertEquals(\"listed children under \" + ls, 1, filtered.length);\r\n    FileStatus fileStatus = filtered[0];\r\n    assertTrue(\"Not the part file: \" + fileStatus, fileStatus.getPath().getName().startsWith(PART_00000));\r\n    describe(\"getReaders()\");\r\n    assertEquals(\"Number of MapFile.Reader entries with shared FS \" + outDir + \" : \" + ls, 1, getReaders(fs, outDir, conf).length);\r\n    describe(\"getReaders(new FS)\");\r\n    FileSystem fs2 = FileSystem.get(outDir.toUri(), conf);\r\n    assertEquals(\"Number of MapFile.Reader entries with shared FS2 \" + outDir + \" : \" + ls, 1, getReaders(fs2, outDir, conf).length);\r\n    describe(\"MapFileOutputFormat.getReaders\");\r\n    assertEquals(\"Number of MapFile.Reader entries with new FS in \" + outDir + \" : \" + ls, 1, MapFileOutputFormat.getReaders(outDir, conf).length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getReaders",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MapFile.Reader[] getReaders(FileSystem fs, Path dir, Configuration conf) throws IOException\n{\r\n    Path[] names = FileUtil.stat2Paths(fs.listStatus(dir, HIDDEN_FILE_FILTER));\r\n    Arrays.sort(names);\r\n    MapFile.Reader[] parts = new MapFile.Reader[names.length];\r\n    for (int i = 0; i < names.length; i++) {\r\n        parts[i] = new MapFile.Reader(names[i], conf);\r\n    }\r\n    return parts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testAbortTaskNoWorkDone",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAbortTaskNoWorkDone() throws Exception\n{\r\n    executeWork(\"abort task no work\", (job, jContext, tContext, committer) -> committer.abortTask(tContext));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testAbortJobNoWorkDone",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAbortJobNoWorkDone() throws Exception\n{\r\n    executeWork(\"abort task no work\", (job, jContext, tContext, committer) -> committer.abortJob(jContext, JobStatus.State.RUNNING));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testCommitJobButNotTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitJobButNotTask() throws Exception\n{\r\n    executeWork(\"commit a job while a task's work is pending, \" + \"expect task writes to be cancelled.\", (job, jContext, tContext, committer) -> {\r\n        writeTextOutput(tContext);\r\n        createCommitter(tContext).commitJob(tContext);\r\n        assertPart0000DoesNotExist(outDir);\r\n        assertNoMultipartUploadsPending(outDir);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testAbortTaskThenJob",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAbortTaskThenJob() throws Exception\n{\r\n    JobData jobData = startJob(true);\r\n    AbstractS3ACommitter committer = jobData.committer;\r\n    committer.abortTask(jobData.tContext);\r\n    intercept(FileNotFoundException.class, \"\", () -> getPart0000(committer.getWorkPath()));\r\n    committer.abortJob(jobData.jContext, JobStatus.State.FAILED);\r\n    assertJobAbortCleanedUp(jobData);\r\n    verifyCommitterHasNoThreads(committer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "assertJobAbortCleanedUp",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void assertJobAbortCleanedUp(JobData jobData) throws Exception\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    try {\r\n        FileStatus[] children = listChildren(fs, outDir);\r\n        if (children.length != 0) {\r\n            lsR(fs, outDir, true);\r\n        }\r\n        assertArrayEquals(\"Output directory not empty \" + ls(outDir), new FileStatus[0], children);\r\n    } catch (FileNotFoundException e) {\r\n    }\r\n    assertNoMultipartUploadsPending(outDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFailAbort",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testFailAbort() throws Exception\n{\r\n    describe(\"Abort the task, then job (failed), abort the job again\");\r\n    JobData jobData = startJob(true);\r\n    JobContext jContext = jobData.jContext;\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    AbstractS3ACommitter committer = jobData.committer;\r\n    committer.abortTask(tContext);\r\n    committer.getJobAttemptPath(jContext);\r\n    committer.getTaskAttemptPath(tContext);\r\n    assertPart0000DoesNotExist(outDir);\r\n    assertSuccessMarkerDoesNotExist(outDir);\r\n    describe(\"Aborting job into %s\", outDir);\r\n    committer.abortJob(jContext, JobStatus.State.FAILED);\r\n    assertTaskAttemptPathDoesNotExist(committer, tContext);\r\n    assertJobAttemptPathDoesNotExist(committer, jContext);\r\n    committer.abortJob(jContext, JobStatus.State.FAILED);\r\n    assertNoMultipartUploadsPending(outDir);\r\n    verifyCommitterHasNoThreads(committer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "assertPart0000DoesNotExist",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertPart0000DoesNotExist(Path dir) throws Exception\n{\r\n    intercept(FileNotFoundException.class, () -> getPart0000(dir));\r\n    assertPathDoesNotExist(\"expected output file\", new Path(dir, PART_00000));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testAbortJobNotTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAbortJobNotTask() throws Exception\n{\r\n    executeWork(\"abort task no work\", (job, jContext, tContext, committer) -> {\r\n        writeTextOutput(tContext);\r\n        committer.abortJob(jContext, JobStatus.State.RUNNING);\r\n        assertTaskAttemptPathDoesNotExist(committer, tContext);\r\n        assertJobAttemptPathDoesNotExist(committer, jContext);\r\n        assertNoMultipartUploadsPending(outDir);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testConcurrentCommitTaskWithSubDir",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testConcurrentCommitTaskWithSubDir() throws Exception\n{\r\n    Job job = newJob();\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    final Configuration conf = job.getConfiguration();\r\n    final JobContext jContext = new JobContextImpl(conf, taskAttempt0.getJobID());\r\n    AbstractS3ACommitter amCommitter = createCommitter(new TaskAttemptContextImpl(conf, taskAttempt0));\r\n    amCommitter.setupJob(jContext);\r\n    final TaskAttemptContext[] taCtx = new TaskAttemptContextImpl[2];\r\n    taCtx[0] = new TaskAttemptContextImpl(conf, taskAttempt0);\r\n    taCtx[1] = new TaskAttemptContextImpl(conf, taskAttempt1);\r\n    final TextOutputFormat[] tof = new LoggingTextOutputFormat[2];\r\n    for (int i = 0; i < tof.length; i++) {\r\n        tof[i] = new LoggingTextOutputFormat() {\r\n\r\n            @Override\r\n            public Path getDefaultWorkFile(TaskAttemptContext context, String extension) throws IOException {\r\n                final AbstractS3ACommitter foc = (AbstractS3ACommitter) getOutputCommitter(context);\r\n                return new Path(new Path(foc.getWorkPath(), SUB_DIR), getUniqueFile(context, getOutputName(context), extension));\r\n            }\r\n        };\r\n    }\r\n    final ExecutorService executor = HadoopExecutors.newFixedThreadPool(2);\r\n    try {\r\n        for (int i = 0; i < taCtx.length; i++) {\r\n            final int taskIdx = i;\r\n            executor.submit(() -> {\r\n                final OutputCommitter outputCommitter = tof[taskIdx].getOutputCommitter(taCtx[taskIdx]);\r\n                outputCommitter.setupTask(taCtx[taskIdx]);\r\n                final RecordWriter rw = tof[taskIdx].getRecordWriter(taCtx[taskIdx]);\r\n                writeOutput(rw, taCtx[taskIdx]);\r\n                describe(\"Committing Task %d\", taskIdx);\r\n                outputCommitter.commitTask(taCtx[taskIdx]);\r\n                return null;\r\n            });\r\n        }\r\n    } finally {\r\n        executor.shutdown();\r\n        while (!executor.awaitTermination(1, TimeUnit.SECONDS)) {\r\n            log().info(\"Awaiting thread termination!\");\r\n        }\r\n    }\r\n    describe(\"\\nCommitting Job\");\r\n    amCommitter.commitJob(jContext);\r\n    assertPathExists(\"base output directory\", outDir);\r\n    assertPart0000DoesNotExist(outDir);\r\n    Path outSubDir = new Path(outDir, SUB_DIR);\r\n    assertPathDoesNotExist(\"Must not end up with sub_dir/sub_dir\", new Path(outSubDir, SUB_DIR));\r\n    validateContent(outSubDir, false, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "createFailingCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractS3ACommitter createFailingCommitter(TaskAttemptContext tContext) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testOutputFormatIntegration",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testOutputFormatIntegration() throws Throwable\n{\r\n    Configuration conf = getConfiguration();\r\n    Job job = newJob();\r\n    job.setOutputFormatClass(LoggingTextOutputFormat.class);\r\n    conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt0);\r\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1);\r\n    JobContext jContext = new JobContextImpl(conf, taskAttempt0.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskAttempt0);\r\n    LoggingTextOutputFormat outputFormat = (LoggingTextOutputFormat) ReflectionUtils.newInstance(tContext.getOutputFormatClass(), conf);\r\n    AbstractS3ACommitter committer = (AbstractS3ACommitter) outputFormat.getOutputCommitter(tContext);\r\n    JobData jobData = new JobData(job, jContext, tContext, committer);\r\n    setup(jobData);\r\n    abortInTeardown(jobData);\r\n    LoggingTextOutputFormat.LoggingLineRecordWriter recordWriter = outputFormat.getRecordWriter(tContext);\r\n    IntWritable iw = new IntWritable(1);\r\n    recordWriter.write(iw, iw);\r\n    long expectedLength = 4;\r\n    Path dest = recordWriter.getDest();\r\n    validateTaskAttemptPathDuringWrite(dest, expectedLength);\r\n    recordWriter.close(tContext);\r\n    validateTaskAttemptPathAfterWrite(dest, expectedLength);\r\n    assertTrue(\"Committer does not have data to commit \" + committer, committer.needsTaskCommit(tContext));\r\n    commitTask(committer, tContext);\r\n    IOStatisticsSnapshot snapshot = new IOStatisticsSnapshot(committer.getIOStatistics());\r\n    String commitsCompleted = COMMITTER_TASKS_SUCCEEDED.getSymbol();\r\n    assertThatStatisticCounter(snapshot, commitsCompleted).describedAs(\"task commit count\").isEqualTo(1L);\r\n    commitJob(committer, jContext);\r\n    LOG.info(\"committer iostatistics {}\", ioStatisticsSourceToString(committer));\r\n    SuccessData successData = verifySuccessMarker(outDir, committer.getUUID());\r\n    IOStatisticsSnapshot successStats = successData.getIOStatistics();\r\n    LOG.info(\"loaded statistics {}\", successStats);\r\n    assertThatStatisticCounter(successStats, commitsCompleted).describedAs(\"task commit count\").isEqualTo(1L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testAMWorkflow",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testAMWorkflow() throws Throwable\n{\r\n    describe(\"Create a committer with a null output path & use as an AM\");\r\n    JobData jobData = startJob(true);\r\n    JobContext jContext = jobData.jContext;\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    TaskAttemptContext newAttempt = taskAttemptForJob(MRBuilderUtils.newJobId(1, 1, 1), jContext);\r\n    Configuration conf = jContext.getConfiguration();\r\n    LoggingTextOutputFormat.bind(conf);\r\n    OutputFormat<?, ?> outputFormat = ReflectionUtils.newInstance(newAttempt.getOutputFormatClass(), conf);\r\n    Path outputPath = FileOutputFormat.getOutputPath(newAttempt);\r\n    assertNotNull(\"null output path in new task attempt\", outputPath);\r\n    AbstractS3ACommitter committer2 = (AbstractS3ACommitter) outputFormat.getOutputCommitter(newAttempt);\r\n    committer2.abortTask(tContext);\r\n    verifyCommitterHasNoThreads(committer2);\r\n    assertNoMultipartUploadsPending(getOutDir());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testParallelJobsToAdjacentPaths",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testParallelJobsToAdjacentPaths() throws Throwable\n{\r\n    describe(\"Run two jobs in parallel, assert they both complete\");\r\n    JobData jobData = startJob(true);\r\n    Job job1 = jobData.job;\r\n    AbstractS3ACommitter committer1 = jobData.committer;\r\n    JobContext jContext1 = jobData.jContext;\r\n    TaskAttemptContext tContext1 = jobData.tContext;\r\n    String jobId2 = randomJobId();\r\n    String attempt20 = \"attempt_\" + jobId2 + \"_m_000000_0\";\r\n    TaskAttemptID taskAttempt20 = TaskAttemptID.forName(attempt20);\r\n    String attempt21 = \"attempt_\" + jobId2 + \"_m_000001_0\";\r\n    TaskAttemptID taskAttempt21 = TaskAttemptID.forName(attempt21);\r\n    Path job1Dest = outDir;\r\n    Path job2Dest = new Path(getOutDir().getParent(), getMethodName() + \"job2Dest\");\r\n    assertNotEquals(job1Dest, job2Dest);\r\n    Job job2 = newJob(job2Dest, unsetUUIDOptions(new JobConf(getConfiguration())), attempt20);\r\n    Configuration conf2 = job2.getConfiguration();\r\n    conf2.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1);\r\n    try {\r\n        JobContext jContext2 = new JobContextImpl(conf2, taskAttempt20.getJobID());\r\n        TaskAttemptContext tContext2 = new TaskAttemptContextImpl(conf2, taskAttempt20);\r\n        AbstractS3ACommitter committer2 = createCommitter(job2Dest, tContext2);\r\n        JobData jobData2 = new JobData(job2, jContext2, tContext2, committer2);\r\n        setup(jobData2);\r\n        abortInTeardown(jobData2);\r\n        assertNotEquals(\"Committer output paths\", committer1.getOutputPath(), committer2.getOutputPath());\r\n        assertNotEquals(\"job UUIDs\", committer1.getUUID(), committer2.getUUID());\r\n        writeTextOutput(tContext2);\r\n        commitTask(committer2, tContext2);\r\n        commitTask(committer1, tContext1);\r\n        assertMultipartUploadsPending(job1Dest);\r\n        assertMultipartUploadsPending(job2Dest);\r\n        commitJob(committer1, jContext1);\r\n        assertNoMultipartUploadsPending(job1Dest);\r\n        getPart0000(job1Dest);\r\n        assertMultipartUploadsPending(job2Dest);\r\n        commitJob(committer2, jContext2);\r\n        getPart0000(job2Dest);\r\n        assertNoMultipartUploadsPending(job2Dest);\r\n    } finally {\r\n        abortMultipartUploadsUnderPath(job2Dest);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testParallelJobsToSameDestination",
  "errType" : null,
  "containingMethodsNum" : 47,
  "sourceCodeText" : "void testParallelJobsToSameDestination() throws Throwable\n{\r\n    describe(\"Run two jobs to the same destination, assert they both complete\");\r\n    Configuration conf = getConfiguration();\r\n    conf.setBoolean(FS_S3A_COMMITTER_ABORT_PENDING_UPLOADS, false);\r\n    String stage1Id = UUID.randomUUID().toString();\r\n    conf.set(SPARK_WRITE_UUID, stage1Id);\r\n    conf.setBoolean(FS_S3A_COMMITTER_REQUIRE_UUID, true);\r\n    JobData jobData = startJob(true);\r\n    Job job1 = jobData.job;\r\n    AbstractS3ACommitter committer1 = jobData.committer;\r\n    JobContext jContext1 = jobData.jContext;\r\n    TaskAttemptContext tContext1 = jobData.tContext;\r\n    Path job1TaskOutputFile = jobData.writtenTextPath;\r\n    Assertions.assertThat(committer1.getWorkPath().toString()).describedAs(\"Work path path of %s\", committer1).contains(stage1Id);\r\n    String jobId2 = randomJobId();\r\n    String attempt2 = taskAttempt0.toString();\r\n    TaskAttemptID taskAttempt2 = taskAttempt0;\r\n    Configuration c2 = unsetUUIDOptions(new JobConf(conf));\r\n    c2.setBoolean(FS_S3A_COMMITTER_REQUIRE_UUID, true);\r\n    Job job2 = newJob(outDir, c2, attempt2);\r\n    Configuration jobConf2 = job2.getConfiguration();\r\n    jobConf2.set(\"mapreduce.output.basename\", \"task2\");\r\n    String stage2Id = UUID.randomUUID().toString();\r\n    jobConf2.set(SPARK_WRITE_UUID, stage2Id);\r\n    JobContext jContext2 = new JobContextImpl(jobConf2, taskAttempt2.getJobID());\r\n    TaskAttemptContext tContext2 = new TaskAttemptContextImpl(jobConf2, taskAttempt2);\r\n    AbstractS3ACommitter committer2 = createCommitter(outDir, tContext2);\r\n    Assertions.assertThat(committer2.getJobAttemptPath(jContext2)).describedAs(\"Job attempt path of %s\", committer2).isNotEqualTo(committer1.getJobAttemptPath(jContext1));\r\n    Assertions.assertThat(committer2.getTaskAttemptPath(tContext2)).describedAs(\"Task attempt path of %s\", committer2).isNotEqualTo(committer1.getTaskAttemptPath(tContext1));\r\n    Assertions.assertThat(committer2.getWorkPath().toString()).describedAs(\"Work path path of %s\", committer2).isNotEqualTo(committer1.getWorkPath().toString()).contains(stage2Id);\r\n    Assertions.assertThat(committer2.getUUIDSource()).describedAs(\"UUID source of %s\", committer2).isEqualTo(AbstractS3ACommitter.JobUUIDSource.SparkWriteUUID);\r\n    JobData jobData2 = new JobData(job2, jContext2, tContext2, committer2);\r\n    setup(jobData2);\r\n    abortInTeardown(jobData2);\r\n    boolean multipartInitiatedInWrite = committer2 instanceof MagicS3GuardCommitter;\r\n    LoggingTextOutputFormat.LoggingLineRecordWriter<Object, Object> recordWriter2 = new LoggingTextOutputFormat<>().getRecordWriter(tContext2);\r\n    LOG.info(\"Commit Task 1\");\r\n    commitTask(committer1, tContext1);\r\n    if (multipartInitiatedInWrite) {\r\n        LOG.info(\"With Multipart Initiated In Write: Commit Job 1\");\r\n        commitJob(committer1, jContext1);\r\n    }\r\n    writeOutput(recordWriter2, tContext2);\r\n    Path job2TaskOutputFile = recordWriter2.getDest();\r\n    LOG.info(\"Commit Task 2\");\r\n    commitTask(committer2, tContext2);\r\n    if (!multipartInitiatedInWrite) {\r\n        LOG.info(\"With Multipart NOT Initiated In Write: Commit Job 1\");\r\n        assertJobAttemptPathExists(committer1, jContext1);\r\n        commitJob(committer1, jContext1);\r\n    }\r\n    committer2.warnOnActiveUploads(outDir);\r\n    LOG.info(\"Commit Job 2\");\r\n    assertJobAttemptPathExists(committer2, jContext2);\r\n    commitJob(committer2, jContext2);\r\n    Path job1Output = new Path(outDir, job1TaskOutputFile.getName());\r\n    Path job2Output = new Path(outDir, job2TaskOutputFile.getName());\r\n    assertNotEquals(\"Job output file filenames must be different\", job1Output, job2Output);\r\n    assertPathExists(\"job 1 output\", job1Output);\r\n    assertPathExists(\"job 2 output\", job2Output);\r\n    assertNoMultipartUploadsPending(outDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testSelfGeneratedUUID",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testSelfGeneratedUUID() throws Throwable\n{\r\n    describe(\"Run two jobs to the same destination, assert they both complete\");\r\n    Configuration conf = getConfiguration();\r\n    unsetUUIDOptions(conf);\r\n    conf.setBoolean(FS_S3A_COMMITTER_GENERATE_UUID, true);\r\n    JobData jobData = startJob(false);\r\n    AbstractS3ACommitter committer = jobData.committer;\r\n    String uuid = committer.getUUID();\r\n    Assertions.assertThat(committer.getUUIDSource()).describedAs(\"UUID source of %s\", committer).isEqualTo(AbstractS3ACommitter.JobUUIDSource.GeneratedLocally);\r\n    Configuration jobConf = jobData.conf;\r\n    Assertions.assertThat(jobConf.get(FS_S3A_COMMITTER_UUID, null)).describedAs(\"Config option \" + FS_S3A_COMMITTER_UUID).isEqualTo(uuid);\r\n    Assertions.assertThat(jobConf.get(FS_S3A_COMMITTER_UUID_SOURCE, null)).describedAs(\"Config option \" + FS_S3A_COMMITTER_UUID_SOURCE).isEqualTo(AbstractS3ACommitter.JobUUIDSource.GeneratedLocally.getText());\r\n    committer.setupTask(jobData.tContext);\r\n    TaskAttemptContext tContext2 = new TaskAttemptContextImpl(conf, taskAttempt1);\r\n    AbstractS3ACommitter committer2 = createCommitter(outDir, tContext2);\r\n    Assertions.assertThat(committer2.getUUIDSource()).describedAs(\"UUID source of %s\", committer2).isEqualTo(AbstractS3ACommitter.JobUUIDSource.GeneratedLocally);\r\n    assertNotEquals(\"job UUIDs\", committer.getUUID(), committer2.getUUID());\r\n    intercept(PathCommitException.class, E_SELF_GENERATED_JOB_UUID, () -> {\r\n        committer2.setupTask(tContext2);\r\n        return committer2;\r\n    });\r\n    committer2.abortTask(tContext2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testRequirePropagatedUUID",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRequirePropagatedUUID() throws Throwable\n{\r\n    Configuration conf = getConfiguration();\r\n    unsetUUIDOptions(conf);\r\n    conf.setBoolean(FS_S3A_COMMITTER_REQUIRE_UUID, true);\r\n    conf.setBoolean(FS_S3A_COMMITTER_GENERATE_UUID, true);\r\n    intercept(PathCommitException.class, E_NO_SPARK_UUID, () -> startJob(false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "unsetUUIDOptions",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration unsetUUIDOptions(final Configuration conf)\n{\r\n    conf.unset(SPARK_WRITE_UUID);\r\n    conf.unset(FS_S3A_COMMITTER_UUID);\r\n    conf.unset(FS_S3A_COMMITTER_GENERATE_UUID);\r\n    conf.unset(FS_S3A_COMMITTER_REQUIRE_UUID);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "assertJobAttemptPathExists",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertJobAttemptPathExists(final AbstractS3ACommitter committer, final JobContext jobContext) throws IOException\n{\r\n    Path attemptPath = committer.getJobAttemptPath(jobContext);\r\n    ContractTestUtils.assertIsDirectory(attemptPath.getFileSystem(committer.getConf()), attemptPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testS3ACommitterFactoryBinding",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testS3ACommitterFactoryBinding() throws Throwable\n{\r\n    describe(\"Verify that the committer factory returns this \" + \"committer when configured to do so\");\r\n    Job job = newJob();\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt0);\r\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1);\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskAttempt0);\r\n    String name = getCommitterName();\r\n    S3ACommitterFactory factory = new S3ACommitterFactory();\r\n    assertEquals(\"Wrong committer from factory\", createCommitter(outDir, tContext).getClass(), factory.createOutputCommitter(outDir, tContext).getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "validateTaskAttemptPathDuringWrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void validateTaskAttemptPathDuringWrite(Path p, final long expectedLength) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "validateTaskAttemptPathAfterWrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void validateTaskAttemptPathAfterWrite(Path p, final long expectedLength) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "validateTaskAttemptWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void validateTaskAttemptWorkingDirectory(AbstractS3ACommitter committer, TaskAttemptContext context) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "commitTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void commitTask(final AbstractS3ACommitter committer, final TaskAttemptContext tContext) throws IOException\n{\r\n    committer.commitTask(tContext);\r\n    verifyCommitterHasNoThreads(committer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "commitJob",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void commitJob(final AbstractS3ACommitter committer, final JobContext jContext) throws IOException\n{\r\n    committer.commitJob(jContext);\r\n    verifyCommitterHasNoThreads(committer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "verifyCommitterHasNoThreads",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyCommitterHasNoThreads(AbstractS3ACommitter committer)\n{\r\n    assertFalse(\"Committer has an active thread pool\", committer.hasThreadPool());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testExceptionTranslation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testExceptionTranslation() throws Throwable\n{\r\n    intercept(AccessDeniedException.class, () -> {\r\n        throw translateException(\"test\", \"/\", new AuditFailureException(\"should be translated\"));\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testNoOpAuditorInstantiation",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testNoOpAuditorInstantiation() throws Throwable\n{\r\n    OperationAuditor auditor = createAndStartNoopAuditor(ioStatistics);\r\n    assertThat(auditor).describedAs(\"No-op auditor\").isInstanceOf(NoopAuditor.class).satisfies(o -> o.isInState(Service.STATE.STARTED));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "createAndStartNoopAuditor",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "NoopAuditor createAndStartNoopAuditor(final IOStatisticsStore store) throws IOException\n{\r\n    Configuration conf = noopAuditConfig();\r\n    OperationAuditorOptions options = OperationAuditorOptions.builder().withConfiguration(conf).withIoStatisticsStore(store);\r\n    OperationAuditor auditor = AuditIntegration.createAndInitAuditor(conf, AUDIT_SERVICE_CLASSNAME, options);\r\n    assertThat(auditor).describedAs(\"No-op auditor\").isInstanceOf(NoopAuditor.class).satisfies(o -> o.isInState(Service.STATE.INITED));\r\n    auditor.start();\r\n    return (NoopAuditor) auditor;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testCreateNonexistentAuditor",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCreateNonexistentAuditor() throws Throwable\n{\r\n    final Configuration conf = new Configuration();\r\n    OperationAuditorOptions options = OperationAuditorOptions.builder().withConfiguration(conf).withIoStatisticsStore(ioStatistics);\r\n    conf.set(AUDIT_SERVICE_CLASSNAME, \"not.a.known.class\");\r\n    intercept(RuntimeException.class, () -> AuditIntegration.createAndInitAuditor(conf, AUDIT_SERVICE_CLASSNAME, options));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testAuditManagerLifecycle",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testAuditManagerLifecycle() throws Throwable\n{\r\n    AuditManagerS3A manager = AuditIntegration.createAndStartAuditManager(noopAuditConfig(), ioStatistics);\r\n    OperationAuditor auditor = manager.getAuditor();\r\n    assertServiceStateStarted(auditor);\r\n    manager.close();\r\n    assertServiceStateStopped(auditor);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testSingleRequestHandler",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSingleRequestHandler() throws Throwable\n{\r\n    AuditManagerS3A manager = AuditIntegration.createAndStartAuditManager(noopAuditConfig(), ioStatistics);\r\n    List<RequestHandler2> handlers = manager.createRequestHandlers();\r\n    assertThat(handlers).hasSize(1);\r\n    RequestHandler2 handler = handlers.get(0);\r\n    RequestFactory requestFactory = RequestFactoryImpl.builder().withBucket(\"bucket\").build();\r\n    GetObjectMetadataRequest r = requestFactory.newGetObjectMetadataRequest(\"/\");\r\n    DefaultRequest dr = new DefaultRequest(r, \"S3\");\r\n    assertThat(handler.beforeMarshalling(r)).isNotNull();\r\n    assertThat(handler.beforeExecution(r)).isNotNull();\r\n    handler.beforeRequest(dr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testRequestHandlerLoading",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRequestHandlerLoading() throws Throwable\n{\r\n    Configuration conf = noopAuditConfig();\r\n    conf.setClassLoader(this.getClass().getClassLoader());\r\n    conf.set(AUDIT_REQUEST_HANDLERS, SimpleAWSRequestHandler.CLASS);\r\n    AuditManagerS3A manager = AuditIntegration.createAndStartAuditManager(conf, ioStatistics);\r\n    assertThat(manager.createRequestHandlers()).hasSize(2).hasAtLeastOneElementOfType(SimpleAWSRequestHandler.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testLoggingAuditorBinding",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testLoggingAuditorBinding() throws Throwable\n{\r\n    AuditManagerS3A manager = AuditIntegration.createAndStartAuditManager(AuditTestSupport.loggingAuditConfig(), ioStatistics);\r\n    OperationAuditor auditor = manager.getAuditor();\r\n    assertServiceStateStarted(auditor);\r\n    manager.close();\r\n    assertServiceStateStopped(auditor);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testNoopAuditManager",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testNoopAuditManager() throws Throwable\n{\r\n    AuditManagerS3A manager = AuditIntegration.stubAuditManager();\r\n    assertThat(manager.createStateChangeListener()).describedAs(\"transfer state change listener\").isNotNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testSpanAttachAndRetrieve",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSpanAttachAndRetrieve() throws Throwable\n{\r\n    AuditManagerS3A manager = AuditIntegration.stubAuditManager();\r\n    AuditSpanS3A span = manager.createSpan(\"op\", null, null);\r\n    GetObjectMetadataRequest request = new GetObjectMetadataRequest(\"bucket\", \"key\");\r\n    attachSpanToRequest(request, span);\r\n    AWSAuditEventCallbacks callbacks = retrieveAttachedSpan(request);\r\n    assertThat(callbacks).isSameAs(span);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testVersionCheckingHandlingNoVersions",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testVersionCheckingHandlingNoVersions() throws Throwable\n{\r\n    LOG.info(\"If an endpoint doesn't return versions, that's OK\");\r\n    ChangeTracker tracker = newTracker(ChangeDetectionPolicy.Mode.Client, ChangeDetectionPolicy.Source.VersionId, false);\r\n    assertFalse(\"Tracker should not have applied contraints \" + tracker, tracker.maybeApplyConstraint(newGetObjectRequest()));\r\n    tracker.processResponse(newResponse(null, null), \"\", 0);\r\n    assertTrackerMismatchCount(tracker, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testVersionCheckingHandlingNoVersionsVersionRequired",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testVersionCheckingHandlingNoVersionsVersionRequired() throws Throwable\n{\r\n    LOG.info(\"If an endpoint doesn't return versions but we are configured to\" + \"require them\");\r\n    ChangeTracker tracker = newTracker(ChangeDetectionPolicy.Mode.Client, ChangeDetectionPolicy.Source.VersionId, true);\r\n    expectNoVersionAttributeException(tracker, newResponse(null, null), \"policy requires VersionId\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEtagCheckingWarn",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testEtagCheckingWarn() throws Throwable\n{\r\n    LOG.info(\"If an endpoint doesn't return errors, that's OK\");\r\n    ChangeTracker tracker = newTracker(ChangeDetectionPolicy.Mode.Warn, ChangeDetectionPolicy.Source.ETag, false);\r\n    assertFalse(\"Tracker should not have applied constraints \" + tracker, tracker.maybeApplyConstraint(newGetObjectRequest()));\r\n    tracker.processResponse(newResponse(\"e1\", null), \"\", 0);\r\n    tracker.processResponse(newResponse(\"e1\", null), \"\", 0);\r\n    tracker.processResponse(newResponse(\"e2\", null), \"\", 0);\r\n    assertTrackerMismatchCount(tracker, 1);\r\n    tracker.processResponse(newResponse(\"e2\", null), \"\", 0);\r\n    assertTrackerMismatchCount(tracker, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testVersionCheckingOnClient",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testVersionCheckingOnClient() throws Throwable\n{\r\n    LOG.info(\"Verify the client-side version checker raises exceptions\");\r\n    ChangeTracker tracker = newTracker(ChangeDetectionPolicy.Mode.Client, ChangeDetectionPolicy.Source.VersionId, false);\r\n    assertFalse(\"Tracker should not have applied constraints \" + tracker, tracker.maybeApplyConstraint(newGetObjectRequest()));\r\n    tracker.processResponse(newResponse(null, \"rev1\"), \"\", 0);\r\n    assertTrackerMismatchCount(tracker, 0);\r\n    assertRevisionId(tracker, \"rev1\");\r\n    GetObjectRequest request = newGetObjectRequest();\r\n    expectChangeException(tracker, newResponse(null, \"rev2\"), \"change detected\");\r\n    assertTrackerMismatchCount(tracker, 1);\r\n    expectChangeException(tracker, newResponse(null, \"rev2\"), \"change detected\");\r\n    assertTrackerMismatchCount(tracker, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testVersionCheckingOnServer",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testVersionCheckingOnServer() throws Throwable\n{\r\n    LOG.info(\"Verify the client-side version checker handles null-ness\");\r\n    ChangeTracker tracker = newTracker(ChangeDetectionPolicy.Mode.Server, ChangeDetectionPolicy.Source.VersionId, false);\r\n    assertFalse(\"Tracker should not have applied contraints \" + tracker, tracker.maybeApplyConstraint(newGetObjectRequest()));\r\n    tracker.processResponse(newResponse(null, \"rev1\"), \"\", 0);\r\n    assertTrackerMismatchCount(tracker, 0);\r\n    assertRevisionId(tracker, \"rev1\");\r\n    GetObjectRequest request = newGetObjectRequest();\r\n    assertConstraintApplied(tracker, request);\r\n    expectChangeException(tracker, null, CHANGE_REPORTED_BY_S3);\r\n    assertTrackerMismatchCount(tracker, 1);\r\n    expectChangeException(tracker, newResponse(null, \"rev2\"), CHANGE_DETECTED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testVersionCheckingUpfrontETag",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testVersionCheckingUpfrontETag() throws Throwable\n{\r\n    ChangeTracker tracker = newTracker(ChangeDetectionPolicy.Mode.Server, ChangeDetectionPolicy.Source.ETag, false, objectAttributes(\"etag1\", \"versionid1\"));\r\n    assertEquals(\"etag1\", tracker.getRevisionId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testVersionCheckingUpfrontVersionId",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testVersionCheckingUpfrontVersionId() throws Throwable\n{\r\n    ChangeTracker tracker = newTracker(ChangeDetectionPolicy.Mode.Server, ChangeDetectionPolicy.Source.VersionId, false, objectAttributes(\"etag1\", \"versionid1\"));\r\n    assertEquals(\"versionid1\", tracker.getRevisionId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testVersionCheckingETagCopyServer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testVersionCheckingETagCopyServer() throws Throwable\n{\r\n    ChangeTracker tracker = newTracker(ChangeDetectionPolicy.Mode.Server, ChangeDetectionPolicy.Source.VersionId, false, objectAttributes(\"etag1\", \"versionid1\"));\r\n    assertConstraintApplied(tracker, newCopyObjectRequest());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testVersionCheckingETagCopyClient",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testVersionCheckingETagCopyClient() throws Throwable\n{\r\n    ChangeTracker tracker = newTracker(ChangeDetectionPolicy.Mode.Client, ChangeDetectionPolicy.Source.VersionId, false, objectAttributes(\"etag1\", \"versionid1\"));\r\n    assertFalse(\"Tracker should not have applied contraints \" + tracker, tracker.maybeApplyConstraint(newCopyObjectRequest()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCopyVersionIdRequired",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCopyVersionIdRequired() throws Throwable\n{\r\n    ChangeTracker tracker = newTracker(ChangeDetectionPolicy.Mode.Client, ChangeDetectionPolicy.Source.VersionId, true, objectAttributes(\"etag1\", \"versionId\"));\r\n    expectNoVersionAttributeException(tracker, newCopyResult(\"etag1\", null), \"policy requires VersionId\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCopyETagRequired",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCopyETagRequired() throws Throwable\n{\r\n    ChangeTracker tracker = newTracker(ChangeDetectionPolicy.Mode.Client, ChangeDetectionPolicy.Source.ETag, true, objectAttributes(\"etag1\", \"versionId\"));\r\n    expectNoVersionAttributeException(tracker, newCopyResult(null, \"versionId\"), \"policy requires ETag\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCopyVersionMismatch",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCopyVersionMismatch() throws Throwable\n{\r\n    ChangeTracker tracker = newTracker(ChangeDetectionPolicy.Mode.Server, ChangeDetectionPolicy.Source.ETag, true, objectAttributes(\"etag\", \"versionId\"));\r\n    AmazonServiceException awsException = new AmazonServiceException(\"aws exception\");\r\n    awsException.setStatusCode(ChangeTracker.SC_PRECONDITION_FAILED);\r\n    expectChangeException(tracker, awsException, \"copy\", RemoteFileChangedException.PRECONDITIONS_FAILED);\r\n    tracker.processException(new SdkBaseException(\"foo\"), \"copy\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertConstraintApplied",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertConstraintApplied(final ChangeTracker tracker, final GetObjectRequest request)\n{\r\n    assertTrue(\"Tracker should have applied contraints \" + tracker, tracker.maybeApplyConstraint(request));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertConstraintApplied",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertConstraintApplied(final ChangeTracker tracker, final CopyObjectRequest request) throws PathIOException\n{\r\n    assertTrue(\"Tracker should have applied contraints \" + tracker, tracker.maybeApplyConstraint(request));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "expectChangeException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteFileChangedException expectChangeException(final ChangeTracker tracker, final S3Object response, final String message) throws Exception\n{\r\n    return expectException(tracker, response, message, RemoteFileChangedException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "expectChangeException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteFileChangedException expectChangeException(final ChangeTracker tracker, final SdkBaseException exception, final String operation, final String message) throws Exception\n{\r\n    return expectException(tracker, exception, operation, message, RemoteFileChangedException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "expectNoVersionAttributeException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PathIOException expectNoVersionAttributeException(final ChangeTracker tracker, final S3Object response, final String message) throws Exception\n{\r\n    return expectException(tracker, response, message, NoVersionAttributeException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "expectNoVersionAttributeException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PathIOException expectNoVersionAttributeException(final ChangeTracker tracker, final CopyResult response, final String message) throws Exception\n{\r\n    return expectException(tracker, response, message, NoVersionAttributeException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "expectException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T expectException(final ChangeTracker tracker, final S3Object response, final String message, final Class<T> clazz) throws Exception\n{\r\n    return intercept(clazz, message, () -> {\r\n        tracker.processResponse(response, \"\", 0);\r\n        return tracker;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "expectException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T expectException(final ChangeTracker tracker, final CopyResult response, final String message, final Class<T> clazz) throws Exception\n{\r\n    return intercept(clazz, message, () -> {\r\n        tracker.processResponse(response);\r\n        return tracker;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "expectException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T expectException(final ChangeTracker tracker, final SdkBaseException exception, final String operation, final String message, final Class<T> clazz) throws Exception\n{\r\n    return intercept(clazz, message, () -> {\r\n        tracker.processException(exception, operation);\r\n        return tracker;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertRevisionId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertRevisionId(final ChangeTracker tracker, final String revId)\n{\r\n    assertEquals(\"Wrong revision ID in \" + tracker, revId, tracker.getRevisionId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertTrackerMismatchCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertTrackerMismatchCount(final ChangeTracker tracker, final int expectedCount)\n{\r\n    assertEquals(\"counter in tracker \" + tracker, expectedCount, tracker.getVersionMismatches());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newTracker",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ChangeTracker newTracker(final ChangeDetectionPolicy.Mode mode, final ChangeDetectionPolicy.Source source, boolean requireVersion)\n{\r\n    return newTracker(mode, source, requireVersion, objectAttributes(null, null));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newTracker",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ChangeTracker newTracker(final ChangeDetectionPolicy.Mode mode, final ChangeDetectionPolicy.Source source, boolean requireVersion, S3ObjectAttributes objectAttributes)\n{\r\n    ChangeDetectionPolicy policy = createPolicy(mode, source, requireVersion);\r\n    ChangeTracker tracker = new ChangeTracker(URI, policy, new CountingChangeTracker(), objectAttributes);\r\n    if (objectAttributes.getVersionId() == null && objectAttributes.getETag() == null) {\r\n        assertFalse(\"Tracker should not have applied constraints \" + tracker, tracker.maybeApplyConstraint(newGetObjectRequest()));\r\n    }\r\n    return tracker;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newGetObjectRequest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "GetObjectRequest newGetObjectRequest()\n{\r\n    return new GetObjectRequest(BUCKET, OBJECT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newCopyObjectRequest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CopyObjectRequest newCopyObjectRequest()\n{\r\n    return new CopyObjectRequest(BUCKET, OBJECT, BUCKET, DEST_OBJECT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newCopyResult",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "CopyResult newCopyResult(String eTag, String versionId)\n{\r\n    CopyResult copyResult = new CopyResult();\r\n    copyResult.setSourceBucketName(BUCKET);\r\n    copyResult.setSourceKey(OBJECT);\r\n    copyResult.setDestinationBucketName(BUCKET);\r\n    copyResult.setDestinationKey(DEST_OBJECT);\r\n    copyResult.setETag(eTag);\r\n    copyResult.setVersionId(versionId);\r\n    return copyResult;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newResponse",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "S3Object newResponse(String etag, String versionId)\n{\r\n    ObjectMetadata md = new ObjectMetadata();\r\n    if (etag != null) {\r\n        md.setHeader(Headers.ETAG, etag);\r\n    }\r\n    if (versionId != null) {\r\n        md.setHeader(Headers.S3_VERSION_ID, versionId);\r\n    }\r\n    S3Object response = emptyResponse();\r\n    response.setObjectMetadata(md);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "emptyResponse",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "S3Object emptyResponse()\n{\r\n    S3Object response = new S3Object();\r\n    response.setBucketName(BUCKET);\r\n    response.setKey(OBJECT);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "objectAttributes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3ObjectAttributes objectAttributes(String etag, String versionId)\n{\r\n    return new S3ObjectAttributes(BUCKET, PATH, OBJECT, null, null, etag, versionId, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "describe",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void describe(String text, Object... args)\n{\r\n    LOG.info(\"\\n\\n: {}\\n\", String.format(text, args));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { 0, false }, { 1, true }, { 3, true } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "setupCommitter",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void setupCommitter() throws Exception\n{\r\n    JobConf jobConf = getConfiguration();\r\n    jobConf.setInt(FS_S3A_COMMITTER_THREADS, numThreads);\r\n    jobConf.setBoolean(FS_S3A_COMMITTER_STAGING_UNIQUE_FILENAMES, uniqueFilenames);\r\n    jobConf.set(FS_S3A_COMMITTER_UUID, uuid());\r\n    jobConf.set(RETRY_INTERVAL, \"100ms\");\r\n    jobConf.setInt(RETRY_LIMIT, 1);\r\n    this.results = new StagingTestBase.ClientResults();\r\n    this.errors = new StagingTestBase.ClientErrors();\r\n    this.mockClient = newMockS3Client(results, errors);\r\n    this.mockFS = createAndBindMockFSInstance(jobConf, Pair.of(results, errors));\r\n    this.wrapperFS = lookupWrapperFS(jobConf);\r\n    wrapperFS.setAmazonS3Client(mockClient);\r\n    this.job = new JobContextImpl(jobConf, JOB_ID);\r\n    this.tac = new TaskAttemptContextImpl(new Configuration(job.getConfiguration()), AID);\r\n    this.jobCommitter = new MockedStagingCommitter(outputPath, tac);\r\n    jobCommitter.setupJob(job);\r\n    this.conf = tac.getConfiguration();\r\n    this.conf.setInt(MULTIPART_SIZE, 100);\r\n    tmpDir = File.createTempFile(\"testStagingCommitter\", \"\");\r\n    tmpDir.delete();\r\n    tmpDir.mkdirs();\r\n    String tmp = tmpDir.getCanonicalPath();\r\n    this.conf.set(BUFFER_DIR, String.format(\"%s/local-0/, %s/local-1 \", tmp, tmp));\r\n    this.committer = new MockedStagingCommitter(outputPath, tac);\r\n    Paths.resetTempFolderCache();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "cleanup",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup()\n{\r\n    try {\r\n        if (tmpDir != null) {\r\n            FileUtils.deleteDirectory(tmpDir);\r\n        }\r\n    } catch (IOException ignored) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "newConfig",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration newConfig()\n{\r\n    return new Configuration(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testUUIDPropagation",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testUUIDPropagation() throws Exception\n{\r\n    Configuration config = newConfig();\r\n    String uuid = uuid();\r\n    config.set(SPARK_WRITE_UUID, uuid);\r\n    config.setBoolean(FS_S3A_COMMITTER_REQUIRE_UUID, true);\r\n    Pair<String, AbstractS3ACommitter.JobUUIDSource> t3 = AbstractS3ACommitter.buildJobUUID(config, JOB_ID);\r\n    assertEquals(\"Job UUID\", uuid, t3.getLeft());\r\n    assertEquals(\"Job UUID source: \" + t3, AbstractS3ACommitter.JobUUIDSource.SparkWriteUUID, t3.getRight());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testUUIDValidation",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testUUIDValidation() throws Exception\n{\r\n    Configuration config = newConfig();\r\n    config.setBoolean(FS_S3A_COMMITTER_REQUIRE_UUID, true);\r\n    intercept(PathCommitException.class, E_NO_SPARK_UUID, () -> AbstractS3ACommitter.buildJobUUID(config, JOB_ID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testUUIDLoadOrdering",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testUUIDLoadOrdering() throws Exception\n{\r\n    Configuration config = newConfig();\r\n    config.setBoolean(FS_S3A_COMMITTER_REQUIRE_UUID, true);\r\n    String uuid = uuid();\r\n    config.set(FS_S3A_COMMITTER_UUID, uuid);\r\n    config.set(SPARK_WRITE_UUID, \"something\");\r\n    Pair<String, AbstractS3ACommitter.JobUUIDSource> t3 = AbstractS3ACommitter.buildJobUUID(config, JOB_ID);\r\n    assertEquals(\"Job UUID\", uuid, t3.getLeft());\r\n    assertEquals(\"Job UUID source: \" + t3, AbstractS3ACommitter.JobUUIDSource.CommitterUUIDProperty, t3.getRight());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testJobIDIsUUID",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testJobIDIsUUID() throws Exception\n{\r\n    Configuration config = newConfig();\r\n    Pair<String, AbstractS3ACommitter.JobUUIDSource> t3 = AbstractS3ACommitter.buildJobUUID(config, JOB_ID);\r\n    assertEquals(\"Job UUID source: \" + t3, AbstractS3ACommitter.JobUUIDSource.JobID, t3.getRight());\r\n    JobID.forName(t3.getLeft());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testSelfGeneratedUUID",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSelfGeneratedUUID() throws Exception\n{\r\n    Configuration config = newConfig();\r\n    config.setBoolean(FS_S3A_COMMITTER_GENERATE_UUID, true);\r\n    Pair<String, AbstractS3ACommitter.JobUUIDSource> t3 = AbstractS3ACommitter.buildJobUUID(config, JOB_ID);\r\n    assertEquals(\"Job UUID source: \" + t3, AbstractS3ACommitter.JobUUIDSource.GeneratedLocally, t3.getRight());\r\n    UUID.fromString(t3.getLeft());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "addUUID",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String addUUID(Configuration config)\n{\r\n    String jobUUID = uuid();\r\n    config.set(FS_S3A_COMMITTER_UUID, jobUUID);\r\n    return jobUUID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "uuid",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String uuid()\n{\r\n    return UUID.randomUUID().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testAttemptPathConstructionNoSchema",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAttemptPathConstructionNoSchema() throws Exception\n{\r\n    Configuration config = newConfig();\r\n    final String jobUUID = addUUID(config);\r\n    config.set(BUFFER_DIR, \"/tmp/mr-local-0,/tmp/mr-local-1\");\r\n    String commonPath = \"file:/tmp/mr-local-\";\r\n    Assertions.assertThat(getLocalTaskAttemptTempDir(config, jobUUID, tac.getTaskAttemptID()).toString()).describedAs(\"Missing scheme should produce local file paths\").startsWith(commonPath).contains(jobUUID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testAttemptPathsDifferentByTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAttemptPathsDifferentByTaskAttempt() throws Exception\n{\r\n    Configuration config = newConfig();\r\n    final String jobUUID = addUUID(config);\r\n    config.set(BUFFER_DIR, \"file:/tmp/mr-local-0\");\r\n    String attempt1Path = getLocalTaskAttemptTempDir(config, jobUUID, AID).toString();\r\n    String attempt2Path = getLocalTaskAttemptTempDir(config, jobUUID, AID2).toString();\r\n    Assertions.assertThat(attempt2Path).describedAs(\"local task attempt dir of TA1 must not match that of TA2\").isNotEqualTo(attempt1Path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testAttemptPathConstructionWithSchema",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAttemptPathConstructionWithSchema() throws Exception\n{\r\n    Configuration config = newConfig();\r\n    final String jobUUID = addUUID(config);\r\n    String commonPath = \"file:/tmp/mr-local-\";\r\n    config.set(BUFFER_DIR, \"file:/tmp/mr-local-0,file:/tmp/mr-local-1\");\r\n    assertThat(\"Path should be the same with file scheme\", getLocalTaskAttemptTempDir(config, jobUUID, tac.getTaskAttemptID()).toString(), StringStartsWith.startsWith(commonPath));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testAttemptPathConstructionWrongSchema",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAttemptPathConstructionWrongSchema() throws Exception\n{\r\n    Configuration config = newConfig();\r\n    final String jobUUID = addUUID(config);\r\n    config.set(BUFFER_DIR, \"hdfs://nn:8020/tmp/mr-local-0,hdfs://nn:8020/tmp/mr-local-1\");\r\n    intercept(IllegalArgumentException.class, \"Wrong FS\", () -> getLocalTaskAttemptTempDir(config, jobUUID, tac.getTaskAttemptID()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testCommitPathConstruction",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCommitPathConstruction() throws Exception\n{\r\n    Path committedTaskPath = committer.getCommittedTaskPath(tac);\r\n    assertEquals(\"Path should be in HDFS: \" + committedTaskPath, \"hdfs\", committedTaskPath.toUri().getScheme());\r\n    String ending = STAGING_UPLOADS + \"/_temporary/0/task_job_0001_r_000002\";\r\n    assertTrue(\"Did not end with \\\"\" + ending + \"\\\" :\" + committedTaskPath, committedTaskPath.toString().endsWith(ending));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testSingleTaskCommit",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testSingleTaskCommit() throws Exception\n{\r\n    Path file = new Path(commitTask(committer, tac, 1).iterator().next());\r\n    List<String> uploads = results.getUploads();\r\n    assertEquals(\"Should initiate one upload: \" + results, 1, uploads.size());\r\n    Path committedPath = committer.getCommittedTaskPath(tac);\r\n    FileSystem dfs = committedPath.getFileSystem(conf);\r\n    assertEquals(\"Should commit to HDFS: \" + committer, getDFS(), dfs);\r\n    FileStatus[] stats = dfs.listStatus(committedPath);\r\n    assertEquals(\"Should produce one commit file: \" + results, 1, stats.length);\r\n    assertEquals(\"Should name the commits file with the task ID: \" + results, \"task_job_0001_r_000002\", stats[0].getPath().getName());\r\n    PendingSet pending = PendingSet.load(dfs, stats[0]);\r\n    assertEquals(\"Should have one pending commit\", 1, pending.size());\r\n    SinglePendingCommit commit = pending.getCommits().get(0);\r\n    assertEquals(\"Should write to the correct bucket:\" + results, BUCKET, commit.getBucket());\r\n    assertEquals(\"Should write to the correct key: \" + results, OUTPUT_PREFIX + \"/\" + file.getName(), commit.getDestinationKey());\r\n    assertValidUpload(results.getTagsByUpload(), commit);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testSingleTaskEmptyFileCommit",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testSingleTaskEmptyFileCommit() throws Exception\n{\r\n    committer.setupTask(tac);\r\n    Path attemptPath = committer.getTaskAttemptPath(tac);\r\n    String rand = UUID.randomUUID().toString();\r\n    writeOutputFile(tac.getTaskAttemptID(), attemptPath, rand, 0);\r\n    committer.commitTask(tac);\r\n    List<String> uploads = results.getUploads();\r\n    assertEquals(\"Should initiate one upload\", 1, uploads.size());\r\n    Path committedPath = committer.getCommittedTaskPath(tac);\r\n    FileSystem dfs = committedPath.getFileSystem(conf);\r\n    assertEquals(\"Should commit to HDFS\", getDFS(), dfs);\r\n    assertIsFile(dfs, committedPath);\r\n    FileStatus[] stats = dfs.listStatus(committedPath);\r\n    assertEquals(\"Should produce one commit file\", 1, stats.length);\r\n    assertEquals(\"Should name the commits file with the task ID\", \"task_job_0001_r_000002\", stats[0].getPath().getName());\r\n    PendingSet pending = PendingSet.load(dfs, stats[0]);\r\n    assertEquals(\"Should have one pending commit\", 1, pending.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testSingleTaskMultiFileCommit",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testSingleTaskMultiFileCommit() throws Exception\n{\r\n    int numFiles = 3;\r\n    Set<String> files = commitTask(committer, tac, numFiles);\r\n    List<String> uploads = results.getUploads();\r\n    assertEquals(\"Should initiate multiple uploads\", numFiles, uploads.size());\r\n    Path committedPath = committer.getCommittedTaskPath(tac);\r\n    FileSystem dfs = committedPath.getFileSystem(conf);\r\n    assertEquals(\"Should commit to HDFS\", getDFS(), dfs);\r\n    assertIsFile(dfs, committedPath);\r\n    FileStatus[] stats = dfs.listStatus(committedPath);\r\n    assertEquals(\"Should produce one commit file\", 1, stats.length);\r\n    assertEquals(\"Should name the commits file with the task ID\", \"task_job_0001_r_000002\", stats[0].getPath().getName());\r\n    List<SinglePendingCommit> pending = PendingSet.load(dfs, stats[0]).getCommits();\r\n    assertEquals(\"Should have correct number of pending commits\", files.size(), pending.size());\r\n    Set<String> keys = Sets.newHashSet();\r\n    for (SinglePendingCommit commit : pending) {\r\n        assertEquals(\"Should write to the correct bucket: \" + commit, BUCKET, commit.getBucket());\r\n        assertValidUpload(results.getTagsByUpload(), commit);\r\n        keys.add(commit.getDestinationKey());\r\n    }\r\n    assertEquals(\"Should write to the correct key\", files, keys);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testTaskInitializeFailure",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testTaskInitializeFailure() throws Exception\n{\r\n    committer.setupTask(tac);\r\n    errors.failOnInit(1);\r\n    Path attemptPath = committer.getTaskAttemptPath(tac);\r\n    FileSystem fs = attemptPath.getFileSystem(conf);\r\n    writeOutputFile(tac.getTaskAttemptID(), attemptPath, UUID.randomUUID().toString(), 10);\r\n    writeOutputFile(tac.getTaskAttemptID(), attemptPath, UUID.randomUUID().toString(), 10);\r\n    intercept(AWSClientIOException.class, \"Fail on init 1\", \"Should fail during init\", () -> committer.commitTask(tac));\r\n    assertEquals(\"Should have initialized one file upload\", 1, results.getUploads().size());\r\n    assertEquals(\"Should abort the upload\", new HashSet<>(results.getUploads()), getAbortedIds(results.getAborts()));\r\n    assertPathDoesNotExist(fs, \"Should remove the attempt path\", attemptPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testTaskSingleFileUploadFailure",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testTaskSingleFileUploadFailure() throws Exception\n{\r\n    describe(\"Set up a single file upload to fail on upload 2\");\r\n    committer.setupTask(tac);\r\n    errors.failOnUpload(2);\r\n    Path attemptPath = committer.getTaskAttemptPath(tac);\r\n    FileSystem fs = attemptPath.getFileSystem(conf);\r\n    writeOutputFile(tac.getTaskAttemptID(), attemptPath, UUID.randomUUID().toString(), 10);\r\n    intercept((Class<? extends Exception>) AWSClientIOException.class, \"Fail on upload 2\", \"Should fail during upload\", () -> {\r\n        committer.commitTask(tac);\r\n        return committer.toString();\r\n    });\r\n    assertEquals(\"Should have attempted one file upload\", 1, results.getUploads().size());\r\n    assertEquals(\"Should abort the upload\", results.getUploads().get(0), results.getAborts().get(0).getUploadId());\r\n    assertPathDoesNotExist(fs, \"Should remove the attempt path\", attemptPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testTaskMultiFileUploadFailure",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testTaskMultiFileUploadFailure() throws Exception\n{\r\n    committer.setupTask(tac);\r\n    errors.failOnUpload(5);\r\n    Path attemptPath = committer.getTaskAttemptPath(tac);\r\n    FileSystem fs = attemptPath.getFileSystem(conf);\r\n    writeOutputFile(tac.getTaskAttemptID(), attemptPath, UUID.randomUUID().toString(), 10);\r\n    writeOutputFile(tac.getTaskAttemptID(), attemptPath, UUID.randomUUID().toString(), 10);\r\n    intercept((Class<? extends Exception>) AWSClientIOException.class, \"Fail on upload 5\", \"Should fail during upload\", () -> {\r\n        committer.commitTask(tac);\r\n        return committer.toString();\r\n    });\r\n    assertEquals(\"Should have attempted two file uploads\", 2, results.getUploads().size());\r\n    assertEquals(\"Should abort the upload\", new HashSet<>(results.getUploads()), getAbortedIds(results.getAborts()));\r\n    assertPathDoesNotExist(fs, \"Should remove the attempt path\", attemptPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testTaskUploadAndAbortFailure",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testTaskUploadAndAbortFailure() throws Exception\n{\r\n    committer.setupTask(tac);\r\n    errors.failOnUpload(5);\r\n    errors.failOnAbort(0);\r\n    Path attemptPath = committer.getTaskAttemptPath(tac);\r\n    FileSystem fs = attemptPath.getFileSystem(conf);\r\n    writeOutputFile(tac.getTaskAttemptID(), attemptPath, UUID.randomUUID().toString(), 10);\r\n    writeOutputFile(tac.getTaskAttemptID(), attemptPath, UUID.randomUUID().toString(), 10);\r\n    intercept((Class<? extends Exception>) AWSClientIOException.class, \"Fail on upload 5\", \"Should suppress abort failure, propagate upload failure\", () -> {\r\n        committer.commitTask(tac);\r\n        return committer.toString();\r\n    });\r\n    assertEquals(\"Should have attempted two file uploads\", 2, results.getUploads().size());\r\n    assertEquals(\"Should not have succeeded with any aborts\", new HashSet<>(), getAbortedIds(results.getAborts()));\r\n    assertPathDoesNotExist(fs, \"Should remove the attempt path\", attemptPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testSingleTaskAbort",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSingleTaskAbort() throws Exception\n{\r\n    committer.setupTask(tac);\r\n    Path attemptPath = committer.getTaskAttemptPath(tac);\r\n    FileSystem fs = attemptPath.getFileSystem(conf);\r\n    Path outPath = writeOutputFile(tac.getTaskAttemptID(), attemptPath, UUID.randomUUID().toString(), 10);\r\n    committer.abortTask(tac);\r\n    assertEquals(\"Should not upload anything\", 0, results.getUploads().size());\r\n    assertEquals(\"Should not upload anything\", 0, results.getParts().size());\r\n    assertPathDoesNotExist(fs, \"Should remove all attempt data\", outPath);\r\n    assertPathDoesNotExist(fs, \"Should remove the attempt path\", attemptPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testJobCommit",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobCommit() throws Exception\n{\r\n    Path jobAttemptPath = jobCommitter.getJobAttemptPath(job);\r\n    FileSystem fs = jobAttemptPath.getFileSystem(conf);\r\n    Set<String> uploads = runTasks(job, 4, 3);\r\n    assertNotEquals(0, uploads.size());\r\n    assertPathExists(fs, \"No job attempt path\", jobAttemptPath);\r\n    jobCommitter.commitJob(job);\r\n    assertEquals(\"Should have aborted no uploads\", 0, results.getAborts().size());\r\n    assertEquals(\"Should have deleted no uploads\", 0, results.getDeletes().size());\r\n    assertEquals(\"Should have committed all uploads\", uploads, getCommittedIds(results.getCommits()));\r\n    assertPathDoesNotExist(fs, \"jobAttemptPath not deleted\", jobAttemptPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testJobCommitFailure",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testJobCommitFailure() throws Exception\n{\r\n    Path jobAttemptPath = jobCommitter.getJobAttemptPath(job);\r\n    FileSystem fs = jobAttemptPath.getFileSystem(conf);\r\n    Set<String> uploads = runTasks(job, 4, 3);\r\n    assertPathExists(fs, \"No job attempt path\", jobAttemptPath);\r\n    errors.failOnCommit(5);\r\n    setMockLogLevel(MockS3AFileSystem.LOG_NAME);\r\n    intercept(AWSClientIOException.class, \"Fail on commit 5\", \"Should propagate the commit failure\", () -> {\r\n        jobCommitter.commitJob(job);\r\n        return jobCommitter.toString();\r\n    });\r\n    Set<String> commits = results.getCommits().stream().map(commit -> \"s3a://\" + commit.getBucketName() + \"/\" + commit.getKey()).collect(Collectors.toSet());\r\n    Set<String> deletes = results.getDeletes().stream().map(delete -> \"s3a://\" + delete.getBucketName() + \"/\" + delete.getKey()).collect(Collectors.toSet());\r\n    Assertions.assertThat(commits).describedAs(\"Committed objects compared to deleted paths %s\", results).containsExactlyInAnyOrderElementsOf(deletes);\r\n    Assertions.assertThat(results.getAborts()).describedAs(\"aborted count in %s\", results).hasSize(7);\r\n    Set<String> uploadIds = getCommittedIds(results.getCommits());\r\n    uploadIds.addAll(getAbortedIds(results.getAborts()));\r\n    Assertions.assertThat(uploadIds).describedAs(\"Combined commit/delete and aborted upload IDs\").containsExactlyInAnyOrderElementsOf(uploads);\r\n    assertPathDoesNotExist(fs, \"jobAttemptPath not deleted\", jobAttemptPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testJobAbort",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testJobAbort() throws Exception\n{\r\n    Path jobAttemptPath = jobCommitter.getJobAttemptPath(job);\r\n    FileSystem fs = jobAttemptPath.getFileSystem(conf);\r\n    Set<String> uploads = runTasks(job, 4, 3);\r\n    assertPathExists(fs, \"No job attempt path\", jobAttemptPath);\r\n    jobCommitter.abortJob(job, JobStatus.State.KILLED);\r\n    assertEquals(\"Should have committed no uploads: \" + jobCommitter, 0, results.getCommits().size());\r\n    assertEquals(\"Should have deleted no uploads: \" + jobCommitter, 0, results.getDeletes().size());\r\n    assertEquals(\"Should have aborted all uploads: \" + jobCommitter, uploads, getAbortedIds(results.getAborts()));\r\n    assertPathDoesNotExist(fs, \"jobAttemptPath not deleted\", jobAttemptPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "runTasks",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Set<String> runTasks(JobContext jobContext, int numTasks, int numFiles) throws IOException\n{\r\n    results.resetUploads();\r\n    Set<String> uploads = Sets.newHashSet();\r\n    for (int taskId = 0; taskId < numTasks; taskId += 1) {\r\n        TaskAttemptID attemptID = new TaskAttemptID(new TaskID(JOB_ID, TaskType.REDUCE, taskId), (taskId * 37) % numTasks);\r\n        TaskAttemptContext attempt = new TaskAttemptContextImpl(new Configuration(jobContext.getConfiguration()), attemptID);\r\n        MockedStagingCommitter taskCommitter = new MockedStagingCommitter(outputPath, attempt);\r\n        commitTask(taskCommitter, attempt, numFiles);\r\n    }\r\n    uploads.addAll(results.getUploads());\r\n    return uploads;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getAbortedIds",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<String> getAbortedIds(List<AbortMultipartUploadRequest> aborts)\n{\r\n    return aborts.stream().map(AbortMultipartUploadRequest::getUploadId).collect(Collectors.toSet());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getCommittedIds",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<String> getCommittedIds(List<CompleteMultipartUploadRequest> commits)\n{\r\n    return commits.stream().map(CompleteMultipartUploadRequest::getUploadId).collect(Collectors.toSet());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "commitTask",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Set<String> commitTask(StagingCommitter staging, TaskAttemptContext attempt, int numFiles) throws IOException\n{\r\n    Path attemptPath = staging.getTaskAttemptPath(attempt);\r\n    Set<String> files = Sets.newHashSet();\r\n    for (int i = 0; i < numFiles; i += 1) {\r\n        Path outPath = writeOutputFile(attempt.getTaskAttemptID(), attemptPath, UUID.randomUUID().toString(), 10 * (i + 1));\r\n        files.add(OUTPUT_PREFIX + \"/\" + outPath.getName() + (uniqueFilenames ? (\"-\" + staging.getUUID()) : \"\"));\r\n    }\r\n    staging.commitTask(attempt);\r\n    return files;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "assertValidUpload",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void assertValidUpload(Map<String, List<String>> parts, SinglePendingCommit commit)\n{\r\n    assertTrue(\"Should commit a valid uploadId\", parts.containsKey(commit.getUploadId()));\r\n    List<String> tags = parts.get(commit.getUploadId());\r\n    assertEquals(\"Should commit the correct number of file parts\", tags.size(), commit.getPartCount());\r\n    for (int i = 0; i < tags.size(); i += 1) {\r\n        assertEquals(\"Should commit the correct part tags\", tags.get(i), commit.getEtags().get(i));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "writeOutputFile",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Path writeOutputFile(TaskAttemptID id, Path dest, String content, long copies) throws IOException\n{\r\n    String fileName = ((id.getTaskType() == TaskType.REDUCE) ? \"r_\" : \"m_\") + id.getTaskID().getId() + \"_\" + id.getId() + \"_\" + UUID.randomUUID().toString();\r\n    Path outPath = new Path(dest, fileName);\r\n    FileSystem fs = outPath.getFileSystem(getConfiguration());\r\n    try (OutputStream out = fs.create(outPath)) {\r\n        byte[] bytes = content.getBytes(StandardCharsets.UTF_8);\r\n        for (int i = 0; i < copies; i += 1) {\r\n            out.write(bytes);\r\n        }\r\n    }\r\n    return outPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "setMockLogLevel",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMockLogLevel(int level)\n{\r\n    wrapperFS.setLogEvents(level);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "newJobCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PartitionedStagingCommitter newJobCommitter() throws IOException\n{\r\n    return new PartitionedStagingCommitter(outputPath, createTaskAttemptForJob());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "newTaskCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PartitionedStagingCommitter newTaskCommitter() throws IOException\n{\r\n    return new PartitionedStagingCommitter(outputPath, getTAC());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "cleanupAttempt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanupAttempt()\n{\r\n    cleanup(\"teardown\", attemptFS, attemptPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testTaskOutputListing",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testTaskOutputListing() throws Exception\n{\r\n    PartitionedStagingCommitter committer = newTaskCommitter();\r\n    attemptPath = committer.getTaskAttemptPath(getTAC());\r\n    attemptFS = attemptPath.getFileSystem(getTAC().getConfiguration());\r\n    attemptFS.delete(attemptPath, true);\r\n    try {\r\n        List<String> expectedFiles = Lists.newArrayList();\r\n        for (String dateint : Arrays.asList(\"20161115\", \"20161116\")) {\r\n            for (String hour : Arrays.asList(\"13\", \"14\")) {\r\n                String relative = \"dateint=\" + dateint + \"/hour=\" + hour + \"/\" + UUID.randomUUID().toString() + \".parquet\";\r\n                expectedFiles.add(relative);\r\n                attemptFS.create(new Path(attemptPath, relative)).close();\r\n            }\r\n        }\r\n        List<String> actualFiles = committer.getTaskOutput(getTAC()).stream().map(stat -> Paths.getRelativePath(attemptPath, stat.getPath())).collect(Collectors.toList());\r\n        Collections.sort(expectedFiles);\r\n        Collections.sort(actualFiles);\r\n        assertEquals(\"File sets should match\", expectedFiles, actualFiles);\r\n    } finally {\r\n        deleteQuietly(attemptFS, attemptPath, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testTaskOutputListingWithHiddenFiles",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testTaskOutputListingWithHiddenFiles() throws Exception\n{\r\n    PartitionedStagingCommitter committer = newTaskCommitter();\r\n    attemptPath = committer.getTaskAttemptPath(getTAC());\r\n    attemptFS = attemptPath.getFileSystem(getTAC().getConfiguration());\r\n    attemptFS.delete(attemptPath, true);\r\n    try {\r\n        List<String> expectedFiles = Lists.newArrayList();\r\n        for (String dateint : Arrays.asList(\"20161115\", \"20161116\")) {\r\n            String metadata = \"dateint=\" + dateint + \"/\" + \"_metadata\";\r\n            attemptFS.create(new Path(attemptPath, metadata)).close();\r\n            for (String hour : Arrays.asList(\"13\", \"14\")) {\r\n                String relative = \"dateint=\" + dateint + \"/hour=\" + hour + \"/\" + UUID.randomUUID().toString() + \".parquet\";\r\n                expectedFiles.add(relative);\r\n                attemptFS.create(new Path(attemptPath, relative)).close();\r\n                String partial = \"dateint=\" + dateint + \"/hour=\" + hour + \"/.\" + UUID.randomUUID().toString() + \".partial\";\r\n                attemptFS.create(new Path(attemptPath, partial)).close();\r\n            }\r\n        }\r\n        List<String> actualFiles = committer.getTaskOutput(getTAC()).stream().map(stat -> Paths.getRelativePath(attemptPath, stat.getPath())).collect(Collectors.toList());\r\n        Collections.sort(expectedFiles);\r\n        Collections.sort(actualFiles);\r\n        assertEquals(\"File sets should match\", expectedFiles, actualFiles);\r\n    } finally {\r\n        deleteQuietly(attemptFS, attemptPath, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testPartitionsResolution",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testPartitionsResolution() throws Throwable\n{\r\n    File tempDir = getTempDir();\r\n    File partitionsDir = new File(tempDir, \"partitions\");\r\n    attemptPath = new Path(partitionsDir.toURI());\r\n    attemptFS = FileSystem.getLocal(getJob().getConfiguration());\r\n    deleteQuietly(attemptFS, attemptPath, true);\r\n    attemptFS.mkdirs(attemptPath);\r\n    assertTrue(Paths.getPartitions(attemptPath, new ArrayList<>(0)).isEmpty());\r\n    String oct2017 = \"year=2017/month=10\";\r\n    Path octLog = new Path(attemptPath, oct2017 + \"/log-2017-10-04.txt\");\r\n    touch(attemptFS, octLog);\r\n    assertThat(listPartitions(attemptFS, attemptPath), hasItem(oct2017));\r\n    Path rootFile = new Path(attemptPath, \"root.txt\");\r\n    touch(attemptFS, rootFile);\r\n    assertThat(listPartitions(attemptFS, attemptPath), allOf(hasItem(oct2017), hasItem(StagingCommitterConstants.TABLE_ROOT)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "listPartitions",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Set<String> listPartitions(FileSystem fs, Path base) throws IOException\n{\r\n    List<FileStatus> statusList = mapLocatedFiles(fs.listFiles(base, true), s -> (FileStatus) s);\r\n    return Paths.getPartitions(base, statusList);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int run(String... args) throws Exception\n{\r\n    Configuration conf = new Configuration(false);\r\n    return S3GuardTool.run(conf, args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "runToFailure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void runToFailure(int status, String... args) throws Exception\n{\r\n    ExitUtil.ExitException ex = LambdaTestUtils.intercept(ExitUtil.ExitException.class, () -> run(args));\r\n    if (ex.status != status) {\r\n        throw ex;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testInfoNoArgs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testInfoNoArgs() throws Throwable\n{\r\n    runToFailure(INVALID_ARGUMENT, BucketInfo.NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testInfoWrongFilesystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testInfoWrongFilesystem() throws Throwable\n{\r\n    runToFailure(INVALID_ARGUMENT, BucketInfo.NAME, \"file://\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testNoCommand",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNoCommand() throws Throwable\n{\r\n    runToFailure(E_USAGE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testUnknownCommand",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUnknownCommand() throws Throwable\n{\r\n    runToFailure(E_USAGE, \"unknown\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createS3Client",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "AmazonS3 createS3Client(URI uri, final S3ClientCreationParameters parameters)\n{\r\n    AmazonS3 s3 = mock(AmazonS3.class);\r\n    String bucket = uri.getHost();\r\n    when(s3.doesBucketExist(bucket)).thenReturn(true);\r\n    when(s3.doesBucketExistV2(bucket)).thenReturn(true);\r\n    MultipartUploadListing noUploads = new MultipartUploadListing();\r\n    noUploads.setMultipartUploads(new ArrayList<>(0));\r\n    when(s3.listMultipartUploads(any())).thenReturn(noUploads);\r\n    when(s3.getBucketLocation(anyString())).thenReturn(Region.US_West.toString());\r\n    return s3;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    String bucketName = getTestBucketName(conf);\r\n    removeBaseAndBucketOverrides(bucketName, conf, S3A_BUCKET_PROBE, DIRECTORY_MARKER_POLICY, AUTHORITATIVE_PATH);\r\n    conf.set(DIRECTORY_MARKER_POLICY, DIRECTORY_MARKER_POLICY_DELETE);\r\n    conf.setInt(S3A_BUCKET_PROBE, 0);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    setKeepingFS(createFS(DIRECTORY_MARKER_POLICY_KEEP, null));\r\n    setDeletingFS(createFS(DIRECTORY_MARKER_POLICY_DELETE, null));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    deleteTestDirInTeardown();\r\n    super.teardown();\r\n    IOUtils.cleanupWithLogger(LOG, getKeepingFS(), getMixedFS(), getDeletingFS());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "getDeletingFS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AFileSystem getDeletingFS()\n{\r\n    return deletingFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "setDeletingFS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDeletingFS(final S3AFileSystem deletingFS)\n{\r\n    this.deletingFS = deletingFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "getKeepingFS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AFileSystem getKeepingFS()\n{\r\n    return keepingFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "setKeepingFS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setKeepingFS(S3AFileSystem keepingFS)\n{\r\n    this.keepingFS = keepingFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "getMixedFS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AFileSystem getMixedFS()\n{\r\n    return mixedFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "setMixedFS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMixedFS(S3AFileSystem mixedFS)\n{\r\n    this.mixedFS = mixedFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "tempAuditFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "File tempAuditFile() throws IOException\n{\r\n    final File audit = File.createTempFile(\"audit\", \".txt\");\r\n    audit.delete();\r\n    return audit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "expectMarkersInOutput",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void expectMarkersInOutput(final File auditFile, final int expected) throws IOException\n{\r\n    final List<String> lines = readOutput(auditFile);\r\n    Assertions.assertThat(lines).describedAs(\"Content of %s\", auditFile).hasSize(expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "readOutput",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<String> readOutput(final File outputFile) throws IOException\n{\r\n    try (FileReader reader = new FileReader(outputFile)) {\r\n        final List<String> lines = org.apache.commons.io.IOUtils.readLines(reader);\r\n        LOG.info(\"contents of output file {}\\n{}\", outputFile, StringUtils.join(\"\\n\", lines));\r\n        return lines;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "createFS",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "S3AFileSystem createFS(String markerPolicy, String authPath) throws Exception\n{\r\n    S3AFileSystem testFS = getFileSystem();\r\n    Configuration conf = new Configuration(testFS.getConf());\r\n    URI testFSUri = testFS.getUri();\r\n    String bucketName = getTestBucketName(conf);\r\n    removeBucketOverrides(bucketName, conf, DIRECTORY_MARKER_POLICY, BULK_DELETE_PAGE_SIZE, AUTHORITATIVE_PATH);\r\n    if (authPath != null) {\r\n        conf.set(AUTHORITATIVE_PATH, authPath);\r\n    }\r\n    conf.setInt(BULK_DELETE_PAGE_SIZE, 2);\r\n    conf.set(DIRECTORY_MARKER_POLICY, markerPolicy);\r\n    S3AFileSystem fs2 = new S3AFileSystem();\r\n    fs2.initialize(testFSUri, conf);\r\n    LOG.info(\"created new filesystem with policy {} and auth path {}\", markerPolicy, (authPath == null ? \"(null)\" : authPath));\r\n    return fs2;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "markerTool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MarkerTool.ScanResult markerTool(final FileSystem sourceFS, final Path path, final boolean doPurge, final int expectedMarkerCount) throws IOException\n{\r\n    return markerTool(0, sourceFS, path, doPurge, expectedMarkerCount, UNLIMITED_LISTING, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int run(Object... args) throws Exception\n{\r\n    return runS3GuardCommand(uncachedFSConfig(getConfiguration()), args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "uncachedFSConfig",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration uncachedFSConfig(final Configuration conf)\n{\r\n    Configuration c = new Configuration(conf);\r\n    disableFilesystemCaching(c);\r\n    return c;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "uncachedFSConfig",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration uncachedFSConfig(final FileSystem fs)\n{\r\n    return uncachedFSConfig(fs.getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "runToFailure",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void runToFailure(int status, Object... args) throws Exception\n{\r\n    Configuration conf = uncachedFSConfig(getConfiguration());\r\n    runS3GuardCommandToFailure(conf, status, args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "toPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path toPath(final Path base, final String name)\n{\r\n    return name.isEmpty() ? base : new Path(base, name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "markerTool",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MarkerTool.ScanResult markerTool(final int exitCode, final FileSystem sourceFS, final Path path, final boolean doPurge, final int expectedMarkers, final int limit, final boolean nonAuth) throws IOException\n{\r\n    MarkerTool.ScanResult result = MarkerTool.execMarkerTool(new MarkerTool.ScanArgsBuilder().withSourceFS(sourceFS).withPath(path).withDoPurge(doPurge).withMinMarkerCount(expectedMarkers).withMaxMarkerCount(expectedMarkers).withLimit(limit).withNonAuth(nonAuth).build());\r\n    Assertions.assertThat(result.getExitCode()).describedAs(\"Exit code of marker(%s, %s, %d) -> %s\", path, doPurge, expectedMarkers, result).isEqualTo(exitCode);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "m",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String m(String s)\n{\r\n    return \"-\" + s;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "data",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> data()\n{\r\n    return Arrays.asList(new Object[][] { { S3AInputPolicy.Normal, 0, -1, 0, _64K, 0 }, { S3AInputPolicy.Normal, 0, -1, _10MB, _64K, _10MB }, { S3AInputPolicy.Normal, _64K, _64K, _10MB, _64K, _10MB }, { S3AInputPolicy.Sequential, 0, -1, 0, _64K, 0 }, { S3AInputPolicy.Sequential, 0, -1, _10MB, _64K, _10MB }, { S3AInputPolicy.Random, 0, -1, 0, _64K, 0 }, { S3AInputPolicy.Random, 0, -1, _10MB, _64K, _10MB }, { S3AInputPolicy.Random, 0, _128K, _10MB, _64K, _128K }, { S3AInputPolicy.Random, 0, _128K, _10MB, _256K, _256K }, { S3AInputPolicy.Random, 0, 0, _10MB, _256K, _256K }, { S3AInputPolicy.Random, 0, 1, _10MB, _256K, _256K }, { S3AInputPolicy.Random, 0, _1MB, _10MB, _256K, _1MB }, { S3AInputPolicy.Random, 0, _1MB, _10MB, 0, _1MB }, { S3AInputPolicy.Random, _10MB + _64K, _1MB, _10MB, _256K, _10MB } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testInputPolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testInputPolicies() throws Throwable\n{\r\n    Assert.assertEquals(String.format(\"calculateRequestLimit(%s, %d, %d, %d, %d)\", policy, targetPos, length, contentLength, readahead), expectedLimit, S3AInputStream.calculateRequestLimit(policy, targetPos, length, contentLength, readahead));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    requestFactory = RequestFactoryImpl.builder().withBucket(\"bucket\").build();\r\n    manager = AuditIntegration.createAndStartAuditManager(createConfig(), ioStatistics);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "createConfig",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration createConfig()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardown()\n{\r\n    stopQuietly(manager);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsStore getIOStatistics()\n{\r\n    return ioStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "getRequestFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RequestFactory getRequestFactory()\n{\r\n    return requestFactory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "getManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuditManagerS3A getManager()\n{\r\n    return manager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "assertActiveSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertActiveSpan(final AuditSpan span)\n{\r\n    assertThat(activeSpan()).isSameAs(span);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "assertUnbondedSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertUnbondedSpan(final AuditSpan span)\n{\r\n    assertThat(span.isValidSpan()).describedAs(\"Validity of %s\", span).isFalse();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "activeSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpanS3A activeSpan()\n{\r\n    return manager.getActiveAuditSpan();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "head",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetObjectMetadataRequest head()\n{\r\n    return manager.beforeExecution(requestFactory.newGetObjectMetadataRequest(\"/\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "assertHeadUnaudited",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertHeadUnaudited() throws Exception\n{\r\n    intercept(AuditFailureException.class, UNAUDITED_OPERATION, this::head);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "verifyAuditFailureCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long verifyAuditFailureCount(final long expected)\n{\r\n    return verifyCounter(Statistic.AUDIT_FAILURE, expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "verifyAuditExecutionCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long verifyAuditExecutionCount(final long expected)\n{\r\n    return verifyCounter(Statistic.AUDIT_REQUEST_EXECUTION, expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "verifyCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long verifyCounter(final Statistic statistic, final long expected)\n{\r\n    IOStatisticAssertions.assertThatStatisticCounter(ioStatistics, statistic.getSymbol()).isEqualTo(expected);\r\n    return expected;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "span",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AuditSpanS3A span() throws IOException\n{\r\n    AuditSpanS3A span = manager.createSpan(OPERATION, PATH_1, PATH_2);\r\n    assertThat(span).matches(AuditSpan::isValidSpan);\r\n    return span;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "assertMapContains",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertMapContains(final Map<String, String> params, final String key, final String expected)\n{\r\n    assertThat(params.get(key)).describedAs(key).isEqualTo(expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { \"keep-markers\", true }, { \"delete-markers\", false } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testRenameFileToDifferentDirectory",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testRenameFileToDifferentDirectory() throws Throwable\n{\r\n    describe(\"rename a file to a different directory, \" + \"keeping the source dir present\");\r\n    Path baseDir = dir(methodPath());\r\n    Path srcDir = new Path(baseDir, \"1/2/3/4/5/6\");\r\n    final Path srcFilePath = file(new Path(srcDir, \"source.txt\"));\r\n    Path parent2 = srcFilePath.getParent();\r\n    Path srcFile2 = file(new Path(parent2, \"source2.txt\"));\r\n    Assertions.assertThat(srcDir).isNotSameAs(parent2);\r\n    Assertions.assertThat(srcFilePath.getParent()).isEqualTo(srcFile2.getParent());\r\n    Path destBaseDir = new Path(baseDir, \"dest\");\r\n    Path destDir = dir(new Path(destBaseDir, \"a/b/c/d\"));\r\n    Path destFilePath = new Path(destDir, \"dest.txt\");\r\n    final int directoriesInPath = directoriesInPath(destDir);\r\n    verifyMetrics(() -> execRename(srcFilePath, destFilePath), always(RENAME_SINGLE_FILE_DIFFERENT_DIR), with(DIRECTORIES_CREATED, 0), with(DIRECTORIES_DELETED, 0), withWhenKeeping(OBJECT_DELETE_REQUEST, DELETE_OBJECT_REQUEST), withWhenKeeping(FAKE_DIRECTORIES_DELETED, 0), withWhenKeeping(OBJECT_DELETE_OBJECTS, 1), probe(isDeleting() && !isBulkDelete(), OBJECT_DELETE_REQUEST, DELETE_OBJECT_REQUEST + directoriesInPath), probe(isDeleting() && isBulkDelete(), OBJECT_DELETE_REQUEST, DELETE_OBJECT_REQUEST), probe(isDeleting() && isBulkDelete(), OBJECT_BULK_DELETE_REQUEST, DELETE_MARKER_REQUEST), withWhenDeleting(FAKE_DIRECTORIES_DELETED, directoriesInPath), withWhenDeleting(OBJECT_DELETE_OBJECTS, directoriesInPath + 1));\r\n    assertIsFile(destFilePath);\r\n    assertIsDirectory(srcDir);\r\n    assertPathDoesNotExist(\"should have gone in the rename\", srcFilePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testRenameSameDirectory",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRenameSameDirectory() throws Throwable\n{\r\n    describe(\"rename a file to the same directory\");\r\n    Path baseDir = dir(methodPath());\r\n    final Path sourceFile = file(new Path(baseDir, \"source.txt\"));\r\n    Path parent2 = sourceFile.getParent();\r\n    Path destFile = new Path(parent2, \"dest\");\r\n    verifyMetrics(() -> execRename(sourceFile, destFile), always(RENAME_SINGLE_FILE_SAME_DIR), with(OBJECT_COPY_REQUESTS, 1), with(DIRECTORIES_CREATED, 0), with(OBJECT_DELETE_REQUEST, DELETE_OBJECT_REQUEST), with(FAKE_DIRECTORIES_DELETED, 0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testCostOfRootFileRename",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCostOfRootFileRename() throws Throwable\n{\r\n    describe(\"assert that a root file rename doesn't\" + \" do much in terms of parent dir operations\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    String uuid = UUID.randomUUID().toString();\r\n    Path src = file(new Path(\"/src-\" + uuid));\r\n    Path dest = new Path(\"/dest-\" + uuid);\r\n    try {\r\n        verifyMetrics(() -> {\r\n            fs.rename(src, dest);\r\n            return \"after fs.rename(/src,/dest) \" + getMetricSummary();\r\n        }, always(FILE_STATUS_FILE_PROBE.plus(GET_FILE_STATUS_FNFE).plus(COPY_OP)), with(DIRECTORIES_CREATED, 0), with(OBJECT_DELETE_REQUEST, DELETE_OBJECT_REQUEST), with(DIRECTORIES_DELETED, 0), with(FAKE_DIRECTORIES_DELETED, 0), with(FILES_DELETED, 1));\r\n    } finally {\r\n        fs.delete(src, false);\r\n        fs.delete(dest, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testCostOfRootFileDelete",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCostOfRootFileDelete() throws Throwable\n{\r\n    describe(\"assert that a root file delete doesn't\" + \" do much in terms of parent dir operations\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    String uuid = UUID.randomUUID().toString();\r\n    Path src = file(new Path(\"/src-\" + uuid));\r\n    try {\r\n        verifyMetrics(() -> {\r\n            fs.delete(src, false);\r\n            return \"after fs.delete(/dest) \" + getMetricSummary();\r\n        }, with(DIRECTORIES_CREATED, 0), with(DIRECTORIES_DELETED, 0), with(FAKE_DIRECTORIES_DELETED, 0), with(FILES_DELETED, 1), with(OBJECT_DELETE_REQUEST, DELETE_OBJECT_REQUEST), always(FILE_STATUS_FILE_PROBE));\r\n    } finally {\r\n        fs.delete(src, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    S3ATestUtils.disableFilesystemCaching(conf);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "maybeSkipTest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void maybeSkipTest() throws IOException\n{\r\n    skipIfEncryptionTestsDisabled(getConfiguration());\r\n    skipIfEncryptionNotSet(getConfiguration(), S3AEncryptionMethods.CSE_KMS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertEncrypted",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void assertEncrypted(Path path) throws IOException\n{\r\n    Map<String, byte[]> fsXAttrs = getFileSystem().getXAttrs(path);\r\n    String xAttrPrefix = \"header.\";\r\n    assertEquals(\"Key wrap algo isn't same as expected\", KMS_KEY_WRAP_ALGO, processHeader(fsXAttrs, xAttrPrefix + Headers.CRYPTO_KEYWRAP_ALGORITHM));\r\n    String keyId = getS3EncryptionKey(getTestBucketName(getConfiguration()), getConfiguration());\r\n    Assertions.assertThat(processHeader(fsXAttrs, xAttrPrefix + Headers.MATERIALS_DESCRIPTION)).describedAs(\"Materials Description should contain the content \" + \"encryption algo and should not contain the KMS keyID.\").contains(KMS_CONTENT_ENCRYPTION_ALGO).doesNotContain(keyId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "processHeader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String processHeader(Map<String, byte[]> fsXAttrs, String headerKey)\n{\r\n    return HeaderProcessing.decodeBytes(fsXAttrs.get(headerKey));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "openFS",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void openFS() throws IOException\n{\r\n    Configuration conf = getConf();\r\n    conf.setInt(SOCKET_SEND_BUFFER, 16 * 1024);\r\n    conf.setInt(SOCKET_RECV_BUFFER, 16 * 1024);\r\n    String testFile = conf.getTrimmed(KEY_CSVTEST_FILE, DEFAULT_CSVTEST_FILE);\r\n    if (testFile.isEmpty()) {\r\n        assumptionMessage = \"Empty test property: \" + KEY_CSVTEST_FILE;\r\n        LOG.warn(assumptionMessage);\r\n        testDataAvailable = false;\r\n    } else {\r\n        testData = new Path(testFile);\r\n        LOG.info(\"Using {} as input stream source\", testData);\r\n        Path path = this.testData;\r\n        bindS3aFS(path);\r\n        try {\r\n            testDataStatus = s3aFS.getFileStatus(this.testData);\r\n        } catch (IOException e) {\r\n            LOG.warn(\"Failed to read file {} specified in {}\", testFile, KEY_CSVTEST_FILE, e);\r\n            throw e;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "bindS3aFS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void bindS3aFS(Path path) throws IOException\n{\r\n    s3aFS = (S3AFileSystem) FileSystem.newInstance(path.toUri(), getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void cleanup()\n{\r\n    describe(\"cleanup\");\r\n    IOUtils.closeStream(in);\r\n    if (in != null) {\r\n        LOG.info(\"Stream statistics {}\", ioStatisticsSourceToString(in));\r\n        IOSTATS.aggregate(in.getIOStatistics());\r\n    }\r\n    if (s3aFS != null) {\r\n        LOG.info(\"FileSystem statistics {}\", ioStatisticsSourceToString(s3aFS));\r\n        FILESYSTEM_IOSTATS.aggregate(s3aFS.getIOStatistics());\r\n        IOUtils.closeStream(s3aFS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "dumpIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void dumpIOStatistics()\n{\r\n    LOG.info(\"Aggregate Stream Statistics {}\", IOSTATS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "requireCSVTestData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void requireCSVTestData()\n{\r\n    assume(assumptionMessage, testDataAvailable);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "openTestFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FSDataInputStream openTestFile() throws IOException\n{\r\n    return openTestFile(S3AInputPolicy.Normal, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "openTestFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataInputStream openTestFile(S3AInputPolicy inputPolicy, long readahead) throws IOException\n{\r\n    requireCSVTestData();\r\n    return openDataFile(s3aFS, this.testData, inputPolicy, readahead);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "openDataFile",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "FSDataInputStream openDataFile(S3AFileSystem fs, Path path, S3AInputPolicy inputPolicy, long readahead) throws IOException\n{\r\n    int bufferSize = getConf().getInt(KEY_READ_BUFFER_SIZE, DEFAULT_READ_BUFFER_SIZE);\r\n    S3AInputPolicy policy = fs.getInputPolicy();\r\n    fs.setInputPolicy(inputPolicy);\r\n    try {\r\n        FSDataInputStream stream = fs.open(path, bufferSize);\r\n        if (readahead >= 0) {\r\n            stream.setReadahead(readahead);\r\n        }\r\n        streamStatistics = getInputStreamStatistics(stream);\r\n        return stream;\r\n    } finally {\r\n        fs.setInputPolicy(policy);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "assertStreamOpenedExactlyOnce",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertStreamOpenedExactlyOnce()\n{\r\n    assertOpenOperationCount(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "assertOpenOperationCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertOpenOperationCount(long expected)\n{\r\n    assertEquals(\"open operations in\\n\" + in, expected, streamStatistics.getOpenOperations());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "logTimePerIOP",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void logTimePerIOP(String operation, NanoTimer timer, long count)\n{\r\n    LOG.info(\"Time per {}: {} nS\", operation, toHuman(timer.duration() / count));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testTimeToOpenAndReadWholeFileBlocks",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testTimeToOpenAndReadWholeFileBlocks() throws Throwable\n{\r\n    skipIfClientSideEncryption();\r\n    requireCSVTestData();\r\n    int blockSize = _1MB;\r\n    describe(\"Open the test file %s and read it in blocks of size %d\", testData, blockSize);\r\n    long len = testDataStatus.getLen();\r\n    in = openTestFile();\r\n    byte[] block = new byte[blockSize];\r\n    NanoTimer timer2 = new NanoTimer();\r\n    long count = 0;\r\n    long blockCount = len / blockSize;\r\n    long totalToRead = blockCount * blockSize;\r\n    long minimumBandwidth = 128 * 1024;\r\n    int maxResetCount = 4;\r\n    int resetCount = 0;\r\n    for (long i = 0; i < blockCount; i++) {\r\n        int offset = 0;\r\n        int remaining = blockSize;\r\n        long blockId = i + 1;\r\n        NanoTimer blockTimer = new NanoTimer();\r\n        int reads = 0;\r\n        while (remaining > 0) {\r\n            NanoTimer readTimer = new NanoTimer();\r\n            int bytesRead = in.read(block, offset, remaining);\r\n            reads++;\r\n            if (bytesRead == 1) {\r\n                break;\r\n            }\r\n            remaining -= bytesRead;\r\n            offset += bytesRead;\r\n            count += bytesRead;\r\n            readTimer.end();\r\n            if (bytesRead != 0) {\r\n                LOG.debug(\"Bytes in read #{}: {} , block bytes: {},\" + \" remaining in block: {}\" + \" duration={} nS; ns/byte: {}, bandwidth={} MB/s\", reads, bytesRead, blockSize - remaining, remaining, readTimer.duration(), readTimer.nanosPerOperation(bytesRead), readTimer.bandwidthDescription(bytesRead));\r\n            } else {\r\n                LOG.warn(\"0 bytes returned by read() operation #{}\", reads);\r\n            }\r\n        }\r\n        blockTimer.end(\"Reading block %d in %d reads\", blockId, reads);\r\n        String bw = blockTimer.bandwidthDescription(blockSize);\r\n        LOG.info(\"Bandwidth of block {}: {} MB/s: \", blockId, bw);\r\n        if (bandwidth(blockTimer, blockSize) < minimumBandwidth) {\r\n            LOG.warn(\"Bandwidth {} too low on block {}: resetting connection\", bw, blockId);\r\n            Assert.assertTrue(\"Bandwidth of \" + bw + \" too low after  \" + resetCount + \" attempts\", resetCount <= maxResetCount);\r\n            resetCount++;\r\n            getS3AInputStream(in).resetConnection();\r\n        }\r\n    }\r\n    timer2.end(\"Time to read %d bytes in %d blocks\", totalToRead, blockCount);\r\n    LOG.info(\"Overall Bandwidth {} MB/s; reset connections {}\", timer2.bandwidth(totalToRead), resetCount);\r\n    logStreamStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "bandwidth",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double bandwidth(NanoTimer timer, long bytes)\n{\r\n    return bytes * 1.0e9 / timer.duration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testLazySeekEnabled",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testLazySeekEnabled() throws Throwable\n{\r\n    describe(\"Verify that seeks do not trigger any IO\");\r\n    in = openTestFile();\r\n    long len = testDataStatus.getLen();\r\n    NanoTimer timer = new NanoTimer();\r\n    long blockCount = len / BLOCK_SIZE;\r\n    for (long i = 0; i < blockCount; i++) {\r\n        in.seek(in.getPos() + BLOCK_SIZE - 1);\r\n    }\r\n    in.seek(0);\r\n    blockCount++;\r\n    timer.end(\"Time to execute %d seeks\", blockCount);\r\n    logTimePerIOP(\"seek()\", timer, blockCount);\r\n    logStreamStatistics();\r\n    assertOpenOperationCount(0);\r\n    assertEquals(\"bytes read\", 0, streamStatistics.getBytesRead());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testReadaheadOutOfRange",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testReadaheadOutOfRange() throws Throwable\n{\r\n    try {\r\n        in = openTestFile();\r\n        in.setReadahead(-1L);\r\n        fail(\"Stream should have rejected the request \" + in);\r\n    } catch (IllegalArgumentException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testReadWithNormalPolicy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testReadWithNormalPolicy() throws Throwable\n{\r\n    describe(\"Read big blocks with a big readahead\");\r\n    skipIfClientSideEncryption();\r\n    executeSeekReadSequence(BIG_BLOCK_SIZE, BIG_BLOCK_SIZE * 2, S3AInputPolicy.Normal);\r\n    assertStreamOpenedExactlyOnce();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testDecompressionSequential128K",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testDecompressionSequential128K() throws Throwable\n{\r\n    describe(\"Decompress with a 128K readahead\");\r\n    skipIfClientSideEncryption();\r\n    executeDecompression(128 * _1KB, S3AInputPolicy.Sequential);\r\n    assertStreamOpenedExactlyOnce();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "executeDecompression",
  "errType" : [ "EOFException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void executeDecompression(long readahead, S3AInputPolicy inputPolicy) throws IOException\n{\r\n    CompressionCodecFactory factory = new CompressionCodecFactory(getConf());\r\n    CompressionCodec codec = factory.getCodec(testData);\r\n    long bytesRead = 0;\r\n    int lines = 0;\r\n    FSDataInputStream objectIn = openTestFile(inputPolicy, readahead);\r\n    IOStatistics readerStatistics = null;\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    try (LineReader lineReader = new LineReader(codec.createInputStream(objectIn), getConf())) {\r\n        readerStatistics = lineReader.getIOStatistics();\r\n        Text line = new Text();\r\n        int read;\r\n        while ((read = lineReader.readLine(line)) > 0) {\r\n            bytesRead += read;\r\n            lines++;\r\n        }\r\n    } catch (EOFException eof) {\r\n    }\r\n    timer.end(\"Time to read %d lines [%d bytes expanded, %d raw]\" + \" with readahead = %d\", lines, bytesRead, testDataStatus.getLen(), readahead);\r\n    logTimePerIOP(\"line read\", timer, lines);\r\n    logStreamStatistics();\r\n    assertNotNull(\"No IOStatistics through line reader\", readerStatistics);\r\n    LOG.info(\"statistics from reader {}\", ioStatisticsToString(readerStatistics));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "logStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void logStreamStatistics()\n{\r\n    LOG.info(String.format(\"Stream Statistics%n{}\"), streamStatistics);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "executeSeekReadSequence",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void executeSeekReadSequence(long blockSize, long readahead, S3AInputPolicy policy) throws IOException\n{\r\n    in = openTestFile(policy, readahead);\r\n    long len = testDataStatus.getLen();\r\n    NanoTimer timer = new NanoTimer();\r\n    long blockCount = len / blockSize;\r\n    LOG.info(\"Reading {} blocks, readahead = {}\", blockCount, readahead);\r\n    for (long i = 0; i < blockCount; i++) {\r\n        in.seek(in.getPos() + blockSize - 1);\r\n        assertTrue(in.read() >= 0);\r\n    }\r\n    timer.end(\"Time to execute %d seeks of distance %d with readahead = %d\", blockCount, blockSize, readahead);\r\n    logTimePerIOP(\"seek(pos + \" + blockCount + \"); read()\", timer, blockCount);\r\n    LOG.info(\"Effective bandwidth {} MB/S\", timer.bandwidthDescription(streamStatistics.getBytesRead() - streamStatistics.getBytesSkippedOnSeek()));\r\n    logStreamStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testRandomIORandomPolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testRandomIORandomPolicy() throws Throwable\n{\r\n    skipIfClientSideEncryption();\r\n    executeRandomIO(S3AInputPolicy.Random, (long) RANDOM_IO_SEQUENCE.length);\r\n    assertEquals(\"streams aborted in \" + streamStatistics, 0, streamStatistics.getAborted());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testRandomIONormalPolicy",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRandomIONormalPolicy() throws Throwable\n{\r\n    skipIfClientSideEncryption();\r\n    long expectedOpenCount = RANDOM_IO_SEQUENCE.length;\r\n    executeRandomIO(S3AInputPolicy.Normal, expectedOpenCount);\r\n    assertEquals(\"streams aborted in \" + streamStatistics, 1, streamStatistics.getAborted());\r\n    assertEquals(\"policy changes in \" + streamStatistics, 2, streamStatistics.getPolicySetCount());\r\n    assertEquals(\"input policy in \" + streamStatistics, S3AInputPolicy.Random.ordinal(), streamStatistics.getInputPolicy());\r\n    IOStatistics ioStatistics = streamStatistics.getIOStatistics();\r\n    verifyStatisticCounterValue(ioStatistics, StreamStatisticNames.STREAM_READ_ABORTED, 1);\r\n    verifyStatisticCounterValue(ioStatistics, StreamStatisticNames.STREAM_READ_SEEK_POLICY_CHANGED, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "executeRandomIO",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "ContractTestUtils.NanoTimer executeRandomIO(S3AInputPolicy policy, long expectedOpenCount) throws IOException\n{\r\n    describe(\"Random IO with policy \\\"%s\\\"\", policy);\r\n    byte[] buffer = new byte[_1MB];\r\n    long totalBytesRead = 0;\r\n    in = openTestFile(policy, 0);\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    for (int[] action : RANDOM_IO_SEQUENCE) {\r\n        int position = action[0];\r\n        int range = action[1];\r\n        in.readFully(position, buffer, 0, range);\r\n        totalBytesRead += range;\r\n    }\r\n    int reads = RANDOM_IO_SEQUENCE.length;\r\n    timer.end(\"Time to execute %d reads of total size %d bytes\", reads, totalBytesRead);\r\n    in.close();\r\n    assertOpenOperationCount(expectedOpenCount);\r\n    logTimePerIOP(\"byte read\", timer, totalBytesRead);\r\n    LOG.info(\"Effective bandwidth {} MB/S\", timer.bandwidthDescription(streamStatistics.getBytesRead() - streamStatistics.getBytesSkippedOnSeek()));\r\n    logStreamStatistics();\r\n    IOStatistics iostats = in.getIOStatistics();\r\n    long maxHttpGet = lookupMaximumStatistic(iostats, ACTION_HTTP_GET_REQUEST + SUFFIX_MAX);\r\n    assertThatStatisticMinimum(iostats, ACTION_HTTP_GET_REQUEST + SUFFIX_MIN).isGreaterThan(0).isLessThan(maxHttpGet);\r\n    MeanStatistic getMeanStat = lookupMeanStatistic(iostats, ACTION_HTTP_GET_REQUEST + SUFFIX_MEAN);\r\n    Assertions.assertThat(getMeanStat.getSamples()).describedAs(\"sample count of %s\", getMeanStat).isEqualTo(expectedOpenCount);\r\n    return timer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getS3aStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AInputStream getS3aStream()\n{\r\n    return (S3AInputStream) in.getWrappedStream();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testRandomReadOverBuffer",
  "errType" : null,
  "containingMethodsNum" : 40,
  "sourceCodeText" : "void testRandomReadOverBuffer() throws Throwable\n{\r\n    describe(\"read over a buffer, making sure that the requests\" + \" spans readahead ranges\");\r\n    int datasetLen = _32K;\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path dataFile = path(\"testReadOverBuffer.bin\");\r\n    byte[] sourceData = dataset(datasetLen, 0, 64);\r\n    writeDataset(fs, dataFile, sourceData, datasetLen, _16K, true);\r\n    byte[] buffer = new byte[datasetLen];\r\n    int readahead = _8K;\r\n    int halfReadahead = _4K;\r\n    in = openDataFile(fs, dataFile, S3AInputPolicy.Random, readahead);\r\n    LOG.info(\"Starting initial reads\");\r\n    S3AInputStream s3aStream = getS3aStream();\r\n    assertEquals(readahead, s3aStream.getReadahead());\r\n    byte[] oneByte = new byte[1];\r\n    assertEquals(1, in.read(0, oneByte, 0, 1));\r\n    assertEquals(\"remaining in\\n\" + in, readahead - 1, s3aStream.remainingInCurrentRequest());\r\n    assertEquals(\"range start in\\n\" + in, 0, s3aStream.getContentRangeStart());\r\n    assertEquals(\"range finish in\\n\" + in, readahead, s3aStream.getContentRangeFinish());\r\n    assertStreamOpenedExactlyOnce();\r\n    describe(\"Starting sequence of positioned read calls over\\n%s\", in);\r\n    NanoTimer readTimer = new NanoTimer();\r\n    int currentPos = halfReadahead;\r\n    int offset = currentPos;\r\n    int bytesRead = 0;\r\n    int readOps = 0;\r\n    while (bytesRead < halfReadahead) {\r\n        int length = buffer.length - offset;\r\n        int read = in.read(currentPos, buffer, offset, length);\r\n        bytesRead += read;\r\n        offset += read;\r\n        readOps++;\r\n        assertEquals(\"open operations on request #\" + readOps + \" after reading \" + bytesRead + \" current position in stream \" + currentPos + \" in\\n\" + fs + \"\\n \" + in, 1, streamStatistics.getOpenOperations());\r\n        for (int i = currentPos; i < currentPos + read; i++) {\r\n            assertEquals(\"Wrong value from byte \" + i, sourceData[i], buffer[i]);\r\n        }\r\n        currentPos += read;\r\n    }\r\n    assertStreamOpenedExactlyOnce();\r\n    assertEquals(readahead, currentPos);\r\n    readTimer.end(\"read %d in %d operations\", bytesRead, readOps);\r\n    bandwidth(readTimer, bytesRead);\r\n    LOG.info(\"Time per byte(): {} nS\", toHuman(readTimer.nanosPerOperation(bytesRead)));\r\n    LOG.info(\"Time per read(): {} nS\", toHuman(readTimer.nanosPerOperation(readOps)));\r\n    describe(\"read last byte\");\r\n    int read = in.read(currentPos, buffer, bytesRead, 1);\r\n    assertTrue(\"-1 from last read\", read >= 0);\r\n    assertOpenOperationCount(2);\r\n    assertEquals(\"Wrong value from read \", sourceData[currentPos], (int) buffer[currentPos]);\r\n    currentPos++;\r\n    describe(\"read() to EOF over \\n%s\", in);\r\n    long readCount = 0;\r\n    NanoTimer timer = new NanoTimer();\r\n    LOG.info(\"seeking\");\r\n    in.seek(currentPos);\r\n    LOG.info(\"reading\");\r\n    while (currentPos < datasetLen) {\r\n        int r = in.read();\r\n        assertTrue(\"Negative read() at position \" + currentPos + \" in\\n\" + in, r >= 0);\r\n        buffer[currentPos] = (byte) r;\r\n        assertEquals(\"Wrong value from read from\\n\" + in, sourceData[currentPos], r);\r\n        currentPos++;\r\n        readCount++;\r\n    }\r\n    timer.end(\"read %d bytes\", readCount);\r\n    bandwidth(timer, readCount);\r\n    LOG.info(\"Time per read(): {} nS\", toHuman(timer.nanosPerOperation(readCount)));\r\n    assertEquals(\"last read in \" + in, -1, in.read());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    removeBaseAndBucketOverrides(conf, CHANGE_DETECT_SOURCE, CHANGE_DETECT_MODE, RETRY_LIMIT, RETRY_INTERVAL);\r\n    conf.setInt(RETRY_LIMIT, 2);\r\n    conf.set(RETRY_INTERVAL, \"1ms\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testNotFoundFirstRead",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testNotFoundFirstRead() throws Exception\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    ChangeDetectionPolicy changeDetectionPolicy = fs.getChangeDetectionPolicy();\r\n    Assume.assumeFalse(\"FNF not expected when using a bucket with\" + \" object versioning\", changeDetectionPolicy.getSource() == Source.VersionId);\r\n    Path p = path(\"some-file\");\r\n    ContractTestUtils.createFile(fs, p, false, new byte[] { 20, 21, 22 });\r\n    final FSDataInputStream in = fs.open(p);\r\n    assertDeleted(p, false);\r\n    LambdaTestUtils.intercept(FileNotFoundException.class, () -> in.read());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    S3ATestUtils.removeBaseAndBucketOverrides(conf, ALLOW_REQUESTER_PAYS, ENDPOINT, S3A_BUCKET_PROBE);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRequesterPaysOptionSuccess",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testRequesterPaysOptionSuccess() throws Throwable\n{\r\n    describe(\"Test requester pays enabled case by reading last then first byte\");\r\n    Configuration conf = this.createConfiguration();\r\n    conf.setBoolean(ALLOW_REQUESTER_PAYS, true);\r\n    conf.setInt(S3A_BUCKET_PROBE, 2);\r\n    Path requesterPaysPath = getRequesterPaysPath(conf);\r\n    try (FileSystem fs = requesterPaysPath.getFileSystem(conf);\r\n        FSDataInputStream inputStream = fs.open(requesterPaysPath)) {\r\n        long fileLength = fs.getFileStatus(requesterPaysPath).getLen();\r\n        inputStream.seek(fileLength - 1);\r\n        inputStream.readByte();\r\n        inputStream.seek(0);\r\n        inputStream.readByte();\r\n        IOStatisticAssertions.assertThatStatisticCounter(inputStream.getIOStatistics(), StreamStatisticNames.STREAM_READ_OPENED).isGreaterThan(1);\r\n        fs.listFiles(requesterPaysPath.getParent(), false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRequesterPaysDisabledFails",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testRequesterPaysDisabledFails() throws Throwable\n{\r\n    describe(\"Verify expected failure for requester pays buckets when client has it disabled\");\r\n    Configuration conf = this.createConfiguration();\r\n    conf.setBoolean(ALLOW_REQUESTER_PAYS, false);\r\n    Path requesterPaysPath = getRequesterPaysPath(conf);\r\n    try (FileSystem fs = requesterPaysPath.getFileSystem(conf)) {\r\n        intercept(AccessDeniedException.class, \"403 Forbidden\", \"Expected requester pays bucket to fail without header set\", () -> fs.open(requesterPaysPath).close());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getRequesterPaysPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getRequesterPaysPath(Configuration conf)\n{\r\n    String requesterPaysFile = conf.getTrimmed(KEY_REQUESTER_PAYS_FILE, DEFAULT_REQUESTER_PAYS_FILE);\r\n    S3ATestUtils.assume(\"Empty test property: \" + KEY_REQUESTER_PAYS_FILE, !requesterPaysFile.isEmpty());\r\n    return new Path(requesterPaysFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "addDeprecatedKeys",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addDeprecatedKeys()\n{\r\n    Configuration.DeprecationDelta[] deltas = { new Configuration.DeprecationDelta(S3ATestConstants.TEST_STS_ENDPOINT, ASSUMED_ROLE_STS_ENDPOINT) };\r\n    if (deltas.length > 0) {\r\n        Configuration.addDeprecations(deltas);\r\n        Configuration.reloadExistingConfigurations();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getFsName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getFsName(Configuration conf)\n{\r\n    return conf.getTrimmed(TEST_FS_S3A_NAME, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createTestFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileSystem createTestFileSystem(Configuration conf) throws IOException\n{\r\n    return createTestFileSystem(conf, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createTestFileSystem",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "S3AFileSystem createTestFileSystem(Configuration conf, boolean purge) throws IOException\n{\r\n    String fsname = conf.getTrimmed(TEST_FS_S3A_NAME, \"\");\r\n    boolean liveTest = !StringUtils.isEmpty(fsname);\r\n    URI testURI = null;\r\n    if (liveTest) {\r\n        testURI = URI.create(fsname);\r\n        liveTest = testURI.getScheme().equals(Constants.FS_S3A);\r\n    }\r\n    Assume.assumeTrue(\"No test filesystem in \" + TEST_FS_S3A_NAME, liveTest);\r\n    S3AFileSystem fs1 = new S3AFileSystem();\r\n    if (purge) {\r\n        enableMultipartPurge(conf, PURGE_DELAY_SECONDS);\r\n    }\r\n    fs1.initialize(testURI, conf);\r\n    return fs1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "enableMultipartPurge",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void enableMultipartPurge(Configuration conf, int seconds)\n{\r\n    conf.setBoolean(PURGE_EXISTING_MULTIPART, true);\r\n    conf.setInt(PURGE_EXISTING_MULTIPART_AGE, seconds);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createTestFileContext",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "FileContext createTestFileContext(Configuration conf) throws IOException\n{\r\n    String fsname = conf.getTrimmed(TEST_FS_S3A_NAME, \"\");\r\n    boolean liveTest = !StringUtils.isEmpty(fsname);\r\n    URI testURI = null;\r\n    if (liveTest) {\r\n        testURI = URI.create(fsname);\r\n        liveTest = testURI.getScheme().equals(Constants.FS_S3A);\r\n    }\r\n    Assume.assumeTrue(\"No test filesystem in \" + TEST_FS_S3A_NAME, liveTest);\r\n    FileContext fc = FileContext.getFileContext(testURI, conf);\r\n    return fc;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "skipIfIOEContainsMessage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void skipIfIOEContainsMessage(PathIOException ioe, String... messages) throws PathIOException\n{\r\n    for (String message : messages) {\r\n        if (ioe.toString().contains(message)) {\r\n            skip(\"Skipping because: \" + message);\r\n        }\r\n    }\r\n    throw ioe;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getTestPropertyLong",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTestPropertyLong(Configuration conf, String key, long defVal)\n{\r\n    return Long.valueOf(getTestProperty(conf, key, Long.toString(defVal)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getTestPropertyBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTestPropertyBytes(Configuration conf, String key, String defVal)\n{\r\n    return org.apache.hadoop.util.StringUtils.TraditionalBinaryPrefix.string2long(getTestProperty(conf, key, defVal));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getTestPropertyInt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTestPropertyInt(Configuration conf, String key, int defVal)\n{\r\n    return (int) getTestPropertyLong(conf, key, defVal);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getTestPropertyBool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getTestPropertyBool(Configuration conf, String key, boolean defVal)\n{\r\n    return Boolean.valueOf(getTestProperty(conf, key, Boolean.toString(defVal)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getTestProperty",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getTestProperty(Configuration conf, String key, String defVal)\n{\r\n    String confVal = conf != null ? conf.getTrimmed(key, defVal) : defVal;\r\n    String propval = System.getProperty(key);\r\n    return isNotEmpty(propval) && !UNSET_PROPERTY.equals(propval) ? propval : confVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getCSVTestFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getCSVTestFile(Configuration conf)\n{\r\n    String csvFile = conf.getTrimmed(KEY_CSVTEST_FILE, DEFAULT_CSVTEST_FILE);\r\n    Assume.assumeTrue(\"CSV test file is not the default\", isNotEmpty(csvFile));\r\n    return csvFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getCSVTestPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getCSVTestPath(Configuration conf)\n{\r\n    return new Path(getCSVTestFile(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getLandsatCSVFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getLandsatCSVFile(Configuration conf)\n{\r\n    String csvFile = getCSVTestFile(conf);\r\n    Assume.assumeTrue(\"CSV test file is not the default\", DEFAULT_CSVTEST_FILE.equals(csvFile));\r\n    return csvFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getLandsatCSVPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getLandsatCSVPath(Configuration conf)\n{\r\n    return new Path(getLandsatCSVFile(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "verifyExceptionClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "E verifyExceptionClass(Class<E> clazz, Exception ex) throws Exception\n{\r\n    if (!(ex.getClass().equals(clazz))) {\r\n        throw ex;\r\n    }\r\n    return (E) ex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "disableFilesystemCaching",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void disableFilesystemCaching(Configuration conf)\n{\r\n    conf.setBoolean(FS_S3A_IMPL_DISABLE_CACHE, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "skipIfEncryptionTestsDisabled",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void skipIfEncryptionTestsDisabled(Configuration configuration)\n{\r\n    if (!configuration.getBoolean(KEY_ENCRYPTION_TESTS, true)) {\r\n        skip(\"Skipping encryption tests\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createTestPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path createTestPath(Path defVal)\n{\r\n    String testUniqueForkId = System.getProperty(S3ATestConstants.TEST_UNIQUE_FORK_ID);\r\n    return testUniqueForkId == null ? defVal : new Path(\"/\" + testUniqueForkId, \"test\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void reset(S3ATestUtils.MetricDiff... metrics)\n{\r\n    for (S3ATestUtils.MetricDiff metric : metrics) {\r\n        metric.reset();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "print",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void print(Logger log, S3ATestUtils.MetricDiff... metrics)\n{\r\n    for (S3ATestUtils.MetricDiff metric : metrics) {\r\n        log.info(metric.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "printThenReset",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void printThenReset(Logger log, S3ATestUtils.MetricDiff... metrics)\n{\r\n    print(log, metrics);\r\n    reset(metrics);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "interceptClosing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "E interceptClosing(Class<E> clazz, String contained, Callable<T> eval) throws Exception\n{\r\n    return intercept(clazz, contained, () -> {\r\n        try (Closeable c = eval.call()) {\r\n            return c.toString();\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "prepareTestConfiguration",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Configuration prepareTestConfiguration(final Configuration conf)\n{\r\n    String testUniqueForkId = System.getProperty(TEST_UNIQUE_FORK_ID);\r\n    String tmpDir = conf.get(HADOOP_TMP_DIR, \"target/build/test\");\r\n    if (testUniqueForkId != null) {\r\n        tmpDir = tmpDir + File.separator + testUniqueForkId;\r\n        conf.set(HADOOP_TMP_DIR, tmpDir);\r\n    }\r\n    conf.set(BUFFER_DIR, tmpDir);\r\n    String directoryRetention = getTestProperty(conf, DIRECTORY_MARKER_POLICY, DEFAULT_DIRECTORY_MARKER_POLICY);\r\n    conf.set(DIRECTORY_MARKER_POLICY, directoryRetention);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "unsetHadoopCredentialProviders",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void unsetHadoopCredentialProviders(final Configuration conf)\n{\r\n    conf.unset(HADOOP_SECURITY_CREDENTIAL_PROVIDER_PATH);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "buildAwsCredentialsProvider",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AWSCredentialsProvider buildAwsCredentialsProvider(final Configuration conf) throws IOException\n{\r\n    assumeSessionTestsEnabled(conf);\r\n    S3xLoginHelper.Login login = S3AUtils.getAWSAccessKeys(URI.create(\"s3a://foobar\"), conf);\r\n    if (!login.hasLogin()) {\r\n        skip(\"testSTS disabled because AWS credentials not configured\");\r\n    }\r\n    return new SimpleAWSCredentialsProvider(login);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assumeSessionTestsEnabled",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assumeSessionTestsEnabled(final Configuration conf)\n{\r\n    if (!conf.getBoolean(TEST_STS_ENABLED, true)) {\r\n        skip(\"STS functional tests disabled\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "requestSessionCredentials",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MarshalledCredentials requestSessionCredentials(final Configuration conf, final String bucket) throws IOException\n{\r\n    return requestSessionCredentials(conf, bucket, TEST_SESSION_TOKEN_DURATION_SECONDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "requestSessionCredentials",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "MarshalledCredentials requestSessionCredentials(final Configuration conf, final String bucket, final int duration) throws IOException\n{\r\n    assumeSessionTestsEnabled(conf);\r\n    MarshalledCredentials sc = MarshalledCredentialBinding.requestSessionCredentials(buildAwsCredentialsProvider(conf), S3AUtils.createAwsConf(conf, bucket, AWS_SERVICE_IDENTIFIER_STS), conf.getTrimmed(ASSUMED_ROLE_STS_ENDPOINT, DEFAULT_ASSUMED_ROLE_STS_ENDPOINT), conf.getTrimmed(ASSUMED_ROLE_STS_ENDPOINT_REGION, ASSUMED_ROLE_STS_ENDPOINT_REGION_DEFAULT), duration, new Invoker(new S3ARetryPolicy(conf), Invoker.LOG_EVENT));\r\n    sc.validate(\"requested session credentials: \", MarshalledCredentials.CredentialTypeRequired.SessionOnly);\r\n    return sc;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "roundTrip",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "T roundTrip(final T source, final Configuration conf) throws Exception\n{\r\n    DataOutputBuffer dob = new DataOutputBuffer();\r\n    source.write(dob);\r\n    DataInputBuffer dib = new DataInputBuffer();\r\n    dib.reset(dob.getData(), dob.getLength());\r\n    T after = ReflectionUtils.newInstance((Class<T>) source.getClass(), conf);\r\n    after.readFields(dib);\r\n    return after;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getTestBucketName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getTestBucketName(final Configuration conf)\n{\r\n    String bucket = checkNotNull(conf.get(TEST_FS_S3A_NAME), \"No test bucket\");\r\n    return URI.create(bucket).getHost();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "removeBucketOverrides",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void removeBucketOverrides(final String bucket, final Configuration conf, final String... options)\n{\r\n    if (StringUtils.isEmpty(bucket)) {\r\n        return;\r\n    }\r\n    final String bucketPrefix = FS_S3A_BUCKET_PREFIX + bucket + '.';\r\n    for (String option : options) {\r\n        final String stripped = option.substring(\"fs.s3a.\".length());\r\n        String target = bucketPrefix + stripped;\r\n        String v = conf.get(target);\r\n        if (v != null) {\r\n            LOG.debug(\"Removing option {}; was {}\", target, v);\r\n            conf.unset(target);\r\n        }\r\n        String extended = bucketPrefix + option;\r\n        if (conf.get(extended) != null) {\r\n            LOG.debug(\"Removing option {}\", extended);\r\n            conf.unset(extended);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "removeBaseAndBucketOverrides",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void removeBaseAndBucketOverrides(final String bucket, final Configuration conf, final String... options)\n{\r\n    for (String option : options) {\r\n        conf.unset(option);\r\n    }\r\n    removeBucketOverrides(bucket, conf, options);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "removeBaseAndBucketOverrides",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void removeBaseAndBucketOverrides(final Configuration conf, final String... options)\n{\r\n    for (String option : options) {\r\n        conf.unset(option);\r\n    }\r\n    removeBaseAndBucketOverrides(getTestBucketName(conf), conf, options);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "callQuietly",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void callQuietly(final Logger log, final CallableRaisingIOE<T> operation)\n{\r\n    try {\r\n        operation.apply();\r\n    } catch (Exception e) {\r\n        log.info(e.toString(), e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "deployService",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T deployService(final Configuration conf, final T service)\n{\r\n    service.init(conf);\r\n    service.start();\r\n    return service;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "terminateService",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T terminateService(final T service)\n{\r\n    ServiceOperations.stopQuietly(LOG, service);\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getStatusWithEmptyDirFlag",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileStatus getStatusWithEmptyDirFlag(final S3AFileSystem fs, final Path dir) throws IOException\n{\r\n    return fs.innerGetFileStatus(dir, true, StatusProbeEnum.ALL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createMockStoreContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StoreContext createMockStoreContext(boolean multiDelete, ContextAccessors accessors) throws URISyntaxException, IOException\n{\r\n    URI name = new URI(\"s3a://bucket\");\r\n    Configuration conf = new Configuration();\r\n    return new StoreContextBuilder().setFsURI(name).setBucket(\"bucket\").setConfiguration(conf).setUsername(\"alice\").setOwner(UserGroupInformation.getCurrentUser()).setExecutor(BlockingThreadPoolExecutorService.newInstance(4, 4, 10, TimeUnit.SECONDS, \"s3a-transfer-shared\")).setExecutorCapacity(DEFAULT_EXECUTOR_CAPACITY).setInvoker(new Invoker(RetryPolicies.TRY_ONCE_THEN_FAIL, Invoker.LOG_EVENT)).setInstrumentation(new EmptyS3AStatisticsContext()).setStorageStatistics(new S3AStorageStatistics()).setInputPolicy(S3AInputPolicy.Normal).setChangeDetectionPolicy(ChangeDetectionPolicy.createPolicy(ChangeDetectionPolicy.Mode.None, ChangeDetectionPolicy.Source.ETag, false)).setMultiObjectDeleteEnabled(multiDelete).setUseListV1(false).setContextAccessors(accessors).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "put",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CompletableFuture<Path> put(FileSystem fs, Path path, String text)\n{\r\n    return submit(EXECUTOR, () -> {\r\n        try (DurationInfo ignore = new DurationInfo(LOG, false, \"Creating %s\", path)) {\r\n            createFile(fs, path, true, text.getBytes(Charsets.UTF_8));\r\n            return path;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<Path> createFiles(final FileSystem fs, final Path destDir, final int depth, final int fileCount, final int dirCount) throws IOException\n{\r\n    return createDirsAndFiles(fs, destDir, depth, fileCount, dirCount, new ArrayList<>(fileCount), new ArrayList<>(dirCount));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createDirsAndFiles",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "List<Path> createDirsAndFiles(final FileSystem fs, final Path destDir, final int depth, final int fileCount, final int dirCount, final List<Path> paths, final List<Path> dirs) throws IOException\n{\r\n    buildPaths(paths, dirs, destDir, depth, fileCount, dirCount);\r\n    List<CompletableFuture<Path>> futures = new ArrayList<>(paths.size() + dirs.size());\r\n    try (DurationInfo ignore = new DurationInfo(LOG, \"Creating %d directories\", dirs.size())) {\r\n        for (Path path : dirs) {\r\n            futures.add(submit(EXECUTOR, () -> {\r\n                fs.mkdirs(path);\r\n                return path;\r\n            }));\r\n        }\r\n        waitForCompletion(futures);\r\n    }\r\n    try (DurationInfo ignore = new DurationInfo(LOG, \"Creating %d files\", paths.size())) {\r\n        for (Path path : paths) {\r\n            futures.add(put(fs, path, path.getName()));\r\n        }\r\n        waitForCompletion(futures);\r\n        return paths;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertInstanceOf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertInstanceOf(Class<?> expectedClass, Object obj)\n{\r\n    Assert.assertTrue(String.format(\"Expected instance of class %s, but is %s.\", expectedClass, obj.getClass()), expectedClass.isAssignableFrom(obj.getClass()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "buildClassListString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String buildClassListString(List<T> classes)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    for (int i = 0; i < classes.size(); ++i) {\r\n        if (i > 0) {\r\n            sb.append(',');\r\n        }\r\n        sb.append(classes.get(i).getName());\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "verifyFileStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyFileStatus(FileStatus status, long size, long blockSize, long modTime)\n{\r\n    verifyFileStatus(status, size, 0, modTime, 0, blockSize, null, null, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "verifyFileStatus",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void verifyFileStatus(FileStatus status, long size, int replication, long modTime, long accessTime, long blockSize, String owner, String group, FsPermission permission)\n{\r\n    String details = status.toString();\r\n    assertFalse(\"Not a dir: \" + details, status.isDirectory());\r\n    assertEquals(\"Mod time: \" + details, modTime, status.getModificationTime());\r\n    assertEquals(\"File size: \" + details, size, status.getLen());\r\n    assertEquals(\"Block size: \" + details, blockSize, status.getBlockSize());\r\n    if (replication > 0) {\r\n        assertEquals(\"Replication value: \" + details, replication, status.getReplication());\r\n    }\r\n    if (accessTime != 0) {\r\n        assertEquals(\"Access time: \" + details, accessTime, status.getAccessTime());\r\n    }\r\n    if (owner != null) {\r\n        assertEquals(\"Owner: \" + details, owner, status.getOwner());\r\n    }\r\n    if (group != null) {\r\n        assertEquals(\"Group: \" + details, group, status.getGroup());\r\n    }\r\n    if (permission != null) {\r\n        assertEquals(\"Permission: \" + details, permission, status.getPermission());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "verifyDirStatus",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void verifyDirStatus(S3AFileStatus status, int replication, String owner)\n{\r\n    String details = status.toString();\r\n    assertTrue(\"Is a dir: \" + details, status.isDirectory());\r\n    assertEquals(\"zero length: \" + details, 0, status.getLen());\r\n    assertTrue(\"Mod time: \" + details, status.getModificationTime() > 0);\r\n    assertEquals(\"Replication value: \" + details, replication, status.getReplication());\r\n    assertEquals(\"Access time: \" + details, 0, status.getAccessTime());\r\n    assertEquals(\"Owner: \" + details, owner, status.getOwner());\r\n    assertEquals(\"Group: \" + details, owner, status.getGroup());\r\n    assertEquals(\"Permission: \" + details, FsPermission.getDefault(), status.getPermission());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertOptionEquals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertOptionEquals(Configuration conf, String key, String expected)\n{\r\n    String actual = conf.get(key);\r\n    String origin = actual == null ? \"(none)\" : \"[\" + StringUtils.join(conf.getPropertySources(key), \", \") + \"]\";\r\n    Assertions.assertThat(actual).describedAs(\"Value of %s with origin %s\", key, origin).isEqualTo(expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assume",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assume(String message, boolean condition)\n{\r\n    if (!condition) {\r\n        LOG.warn(message);\r\n    }\r\n    Assume.assumeTrue(message, condition);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getOutputStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BlockOutputStreamStatistics getOutputStreamStatistics(FSDataOutputStream out)\n{\r\n    S3ABlockOutputStream blockOutputStream = (S3ABlockOutputStream) out.getWrappedStream();\r\n    return blockOutputStream.getStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String read(FileSystem fs, Path path) throws IOException\n{\r\n    FileStatus status = fs.getFileStatus(path);\r\n    try (FSDataInputStream in = fs.open(path)) {\r\n        byte[] buf = new byte[(int) status.getLen()];\r\n        in.readFully(0, buf);\r\n        return new String(buf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "readWithStatus",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String readWithStatus(final FileSystem fs, final FileStatus status) throws IOException\n{\r\n    final CompletableFuture<FSDataInputStream> future = fs.openFile(status.getPath()).withFileStatus(status).build();\r\n    try (FSDataInputStream in = awaitFuture(future)) {\r\n        byte[] buf = new byte[(int) status.getLen()];\r\n        in.readFully(0, buf);\r\n        return new String(buf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "lsR",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long lsR(FileSystem fileSystem, Path path, boolean recursive) throws Exception\n{\r\n    if (path == null) {\r\n        LOG.info(\"Empty path\");\r\n        return 0;\r\n    }\r\n    return S3AUtils.applyLocatedFiles(fileSystem.listFiles(path, recursive), (status) -> LOG.info(\"{}\", status));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assumeMagicCommitEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assumeMagicCommitEnabled(S3AFileSystem fs) throws IOException\n{\r\n    assume(\"Magic commit option disabled on \" + fs, fs.hasPathCapability(fs.getWorkingDirectory(), CommitConstants.STORE_CAPABILITY_MAGIC_COMMITTER));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "authenticationContains",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean authenticationContains(Configuration conf, String providerClassname)\n{\r\n    return conf.getTrimmedStringCollection(AWS_CREDENTIALS_PROVIDER).contains(providerClassname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "checkListingDoesNotContainPath",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void checkListingDoesNotContainPath(S3AFileSystem fs, Path filePath) throws IOException\n{\r\n    final RemoteIterator<LocatedFileStatus> listIter = fs.listFiles(filePath.getParent(), false);\r\n    while (listIter.hasNext()) {\r\n        final LocatedFileStatus lfs = listIter.next();\r\n        assertNotEquals(\"Listing was not supposed to include \" + filePath, filePath, lfs.getPath());\r\n    }\r\n    LOG.info(\"{}; file omitted from listFiles listing as expected.\", filePath);\r\n    final FileStatus[] fileStatuses = fs.listStatus(filePath.getParent());\r\n    for (FileStatus fileStatus : fileStatuses) {\r\n        assertNotEquals(\"Listing was not supposed to include \" + filePath, filePath, fileStatus.getPath());\r\n    }\r\n    LOG.info(\"{}; file omitted from listStatus as expected.\", filePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "checkListingContainsPath",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void checkListingContainsPath(S3AFileSystem fs, Path filePath) throws IOException\n{\r\n    boolean listFilesHasIt = false;\r\n    boolean listStatusHasIt = false;\r\n    final RemoteIterator<LocatedFileStatus> listIter = fs.listFiles(filePath.getParent(), false);\r\n    while (listIter.hasNext()) {\r\n        final LocatedFileStatus lfs = listIter.next();\r\n        if (filePath.equals(lfs.getPath())) {\r\n            listFilesHasIt = true;\r\n        }\r\n    }\r\n    final FileStatus[] fileStatuses = fs.listStatus(filePath.getParent());\r\n    for (FileStatus fileStatus : fileStatuses) {\r\n        if (filePath.equals(fileStatus.getPath())) {\r\n            listStatusHasIt = true;\r\n        }\r\n    }\r\n    assertTrue(\"fs.listFiles didn't include \" + filePath, listFilesHasIt);\r\n    assertTrue(\"fs.listStatus didn't include \" + filePath, listStatusHasIt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listInitialThreadsForLifecycleChecks",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Set<String> listInitialThreadsForLifecycleChecks()\n{\r\n    Set<String> threadSet = getCurrentThreadNames();\r\n    threadSet.add(\"org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner\");\r\n    threadSet.add(\"java-sdk-progress-listener-callback-thread\");\r\n    threadSet.add(\"java-sdk-http-connection-reaper\");\r\n    threadSet.add(\"process reaper\");\r\n    threadSet.add(\"MutableQuantiles-0\");\r\n    threadSet.add(\"Attach Listener\");\r\n    return threadSet;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getCurrentThreadNames",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<String> getCurrentThreadNames()\n{\r\n    TreeSet<String> threads = Thread.getAllStackTraces().keySet().stream().map(Thread::getName).filter(n -> n.startsWith(\"JUnit\")).filter(n -> n.startsWith(\"surefire\")).collect(Collectors.toCollection(TreeSet::new));\r\n    return threads;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "innerGetFileStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileStatus innerGetFileStatus(S3AFileSystem fs, Path path, boolean needEmptyDirectoryFlag, Set<StatusProbeEnum> probes) throws IOException\n{\r\n    return fs.innerGetFileStatus(path, needEmptyDirectoryFlag, probes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "skipIfEncryptionNotSet",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void skipIfEncryptionNotSet(Configuration configuration, S3AEncryptionMethods s3AEncryptionMethod) throws IOException\n{\r\n    String bucket = getTestBucketName(configuration);\r\n    final EncryptionSecrets secrets = buildEncryptionSecrets(bucket, configuration);\r\n    if (!s3AEncryptionMethod.getMethod().equals(secrets.getEncryptionMethod().getMethod()) || StringUtils.isBlank(secrets.getEncryptionKey())) {\r\n        skip(S3_ENCRYPTION_KEY + \" is not set for \" + s3AEncryptionMethod.getMethod() + \" or \" + S3_ENCRYPTION_ALGORITHM + \" is not set to \" + s3AEncryptionMethod.getMethod() + \" in \" + secrets);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    jobId = randomJobId();\r\n    attempt0 = \"attempt_\" + jobId + \"_m_000000_0\";\r\n    taskAttempt0 = TaskAttemptID.forName(attempt0);\r\n    outDir = path(getMethodName());\r\n    factory = new S3ACommitterFactory();\r\n    Configuration conf = new Configuration();\r\n    conf.set(FileOutputFormat.OUTDIR, outDir.toUri().toString());\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt0);\r\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1);\r\n    filesystemConfRef = getFileSystem().getConf();\r\n    tContext = new TaskAttemptContextImpl(conf, taskAttempt0);\r\n    taskConfRef = tContext.getConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testEverything",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testEverything() throws Throwable\n{\r\n    testImplicitFileBinding();\r\n    testBindingsInTask();\r\n    testBindingsInFSConfig();\r\n    testInvalidFileBinding();\r\n    testInvalidTaskBinding();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testImplicitFileBinding",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testImplicitFileBinding() throws Throwable\n{\r\n    taskConfRef.unset(FS_S3A_COMMITTER_NAME);\r\n    filesystemConfRef.unset(FS_S3A_COMMITTER_NAME);\r\n    assertFactoryCreatesExpectedCommitter(FileOutputCommitter.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testBindingsInTask",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBindingsInTask() throws Throwable\n{\r\n    filesystemConfRef.set(FS_S3A_COMMITTER_NAME, \"INVALID\");\r\n    taskConfRef.set(FS_S3A_COMMITTER_NAME, COMMITTER_NAME_FILE);\r\n    assertFactoryCreatesExpectedCommitter(FileOutputCommitter.class);\r\n    for (Object[] binding : bindings) {\r\n        taskConfRef.set(FS_S3A_COMMITTER_NAME, (String) binding[0]);\r\n        assertFactoryCreatesExpectedCommitter((Class) binding[1]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testBindingsInFSConfig",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBindingsInFSConfig() throws Throwable\n{\r\n    taskConfRef.unset(FS_S3A_COMMITTER_NAME);\r\n    filesystemConfRef.set(FS_S3A_COMMITTER_NAME, COMMITTER_NAME_FILE);\r\n    assertFactoryCreatesExpectedCommitter(FileOutputCommitter.class);\r\n    for (Object[] binding : bindings) {\r\n        taskConfRef.set(FS_S3A_COMMITTER_NAME, (String) binding[0]);\r\n        assertFactoryCreatesExpectedCommitter((Class) binding[1]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testInvalidFileBinding",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testInvalidFileBinding() throws Throwable\n{\r\n    taskConfRef.unset(FS_S3A_COMMITTER_NAME);\r\n    filesystemConfRef.set(FS_S3A_COMMITTER_NAME, INVALID_NAME);\r\n    LambdaTestUtils.intercept(PathCommitException.class, INVALID_NAME, () -> createCommitter());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testInvalidTaskBinding",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testInvalidTaskBinding() throws Throwable\n{\r\n    filesystemConfRef.unset(FS_S3A_COMMITTER_NAME);\r\n    taskConfRef.set(FS_S3A_COMMITTER_NAME, INVALID_NAME);\r\n    LambdaTestUtils.intercept(PathCommitException.class, INVALID_NAME, () -> createCommitter());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "assertFactoryCreatesExpectedCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertFactoryCreatesExpectedCommitter(final Class expected) throws IOException\n{\r\n    assertEquals(\"Wrong Committer from factory\", expected, createCommitter().getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "createCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PathOutputCommitter createCommitter() throws IOException\n{\r\n    return factory.createOutputCommitter(outDir, tContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    csvPath = path(getMethodName() + \".csv\");\r\n    Assume.assumeTrue(\"S3 Select is not enabled\", getFileSystem().hasPathCapability(csvPath, S3_SELECT_CAPABILITY));\r\n    selectConf = new Configuration(false);\r\n    selectConf.setBoolean(SELECT_ERRORS_INCLUDE_SQL, true);\r\n    createStandardCsvFile(getFileSystem(), csvPath, ALL_QUOTES);\r\n    brokenCSV = path(\"testParseBrokenCSVFile\");\r\n    createStandardCsvFile(getFileSystem(), brokenCSV, true, ALL_QUOTES, ALL_ROWS_COUNT, ALL_ROWS_COUNT, \",\", \"\\n\", \"\\\"\", csv -> csv.line(\"# comment\").row(ALL_QUOTES, \"bad\", \"Tuesday\", 0, \"entry-bad\", \"yes\", false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    describe(\"teardown\");\r\n    try {\r\n        if (csvPath != null) {\r\n            getFileSystem().delete(csvPath, false);\r\n        }\r\n        if (brokenCSV != null) {\r\n            getFileSystem().delete(brokenCSV, false);\r\n        }\r\n    } finally {\r\n        super.teardown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testCapabilityProbe",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCapabilityProbe() throws Throwable\n{\r\n    assertTrue(\"Select is not available on \" + getFileSystem(), isSelectAvailable(getFileSystem()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testReadWholeFileClassicAPI",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testReadWholeFileClassicAPI() throws Throwable\n{\r\n    describe(\"create and read the whole file. Verifies setup working\");\r\n    int lines;\r\n    try (BufferedReader reader = new BufferedReader(new InputStreamReader(getFileSystem().open(csvPath)))) {\r\n        lines = 0;\r\n        String line;\r\n        while ((line = reader.readLine()) != null) {\r\n            lines++;\r\n            LOG.info(\"{}\", line);\r\n        }\r\n    }\r\n    assertEquals(\"line count\", ALL_ROWS_COUNT_WITH_HEADER, lines);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectWholeFileNoHeader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSelectWholeFileNoHeader() throws Throwable\n{\r\n    describe(\"Select the entire file, expect all rows but the header\");\r\n    expectSelected(ALL_ROWS_COUNT, selectConf, CSV_HEADER_OPT_USE, \"SELECT * FROM S3OBJECT\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectFirstColumnNoHeader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSelectFirstColumnNoHeader() throws Throwable\n{\r\n    describe(\"Select the entire file, expect all rows but the header\");\r\n    expectSelected(ALL_ROWS_COUNT_WITH_HEADER, selectConf, CSV_HEADER_OPT_NONE, \"SELECT s._1 FROM S3OBJECT s\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectSelfNoHeader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSelectSelfNoHeader() throws Throwable\n{\r\n    describe(\"Select the entire file, expect all rows but the header\");\r\n    expectSelected(ALL_ROWS_COUNT_WITH_HEADER, selectConf, CSV_HEADER_OPT_NONE, \"SELECT s._1 FROM S3OBJECT s WHERE s._1 = s._1\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectSelfUseHeader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSelectSelfUseHeader() throws Throwable\n{\r\n    describe(\"Select the entire file, expect all rows including the header\");\r\n    expectSelected(ALL_ROWS_COUNT, selectConf, CSV_HEADER_OPT_USE, \"SELECT s.id FROM S3OBJECT s WHERE s.id = s.id\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectID2UseHeader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSelectID2UseHeader() throws Throwable\n{\r\n    describe(\"Select where ID=2; use the header\");\r\n    expectSelected(1, selectConf, CSV_HEADER_OPT_USE, \"SELECT s.id FROM S3OBJECT s WHERE s.id = '2'\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectNoMatchingID",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSelectNoMatchingID() throws Throwable\n{\r\n    describe(\"Select where there is no match; expect nothing back\");\r\n    expectSelected(0, selectConf, CSV_HEADER_OPT_USE, \"SELECT s.id FROM S3OBJECT s WHERE s.id = '0x8000'\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectId1",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSelectId1() throws Throwable\n{\r\n    describe(\"Select the first element in the file\");\r\n    expectSelected(1, selectConf, CSV_HEADER_OPT_NONE, \"SELECT * FROM S3OBJECT s WHERE s._1 = '1'\", TRUE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectEmptySQL",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSelectEmptySQL() throws Throwable\n{\r\n    describe(\"An empty SQL statement fails fast\");\r\n    FutureDataInputStreamBuilder builder = getFileSystem().openFile(csvPath).must(SELECT_SQL, \"\");\r\n    interceptFuture(IllegalArgumentException.class, SELECT_SQL, builder.build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectEmptyFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSelectEmptyFile() throws Throwable\n{\r\n    describe(\"Select everything from an empty file\");\r\n    Path path = path(\"testSelectEmptyFile\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    ContractTestUtils.touch(fs, path);\r\n    parseToLines(fs.openFile(path).must(SELECT_SQL, SELECT_EVERYTHING).withFileStatus(fs.getFileStatus(path)).build().get(), 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectEmptyFileWithConditions",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSelectEmptyFileWithConditions() throws Throwable\n{\r\n    describe(\"Select everything from an empty file with a more complex SQL\");\r\n    Path path = path(\"testSelectEmptyFileWithConditions\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    ContractTestUtils.touch(fs, path);\r\n    String sql = \"SELECT * FROM S3OBJECT s WHERE s._1 = `TRUE`\";\r\n    CompletableFuture<FSDataInputStream> future = fs.openFile(path).must(SELECT_SQL, sql).build();\r\n    assertEquals(\"Not at the end of the file\", -1, future.get().read());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectSeek",
  "errType" : null,
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void testSelectSeek() throws Throwable\n{\r\n    describe(\"Verify forward seeks work, not others\");\r\n    Path path = csvPath;\r\n    S3AFileSystem fs = getFileSystem();\r\n    int len = (int) fs.getFileStatus(path).getLen();\r\n    byte[] fullData = new byte[len];\r\n    int actualLen;\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"Initial read of %s\", path);\r\n        FSDataInputStream sourceStream = select(fs, path, selectConf, SELECT_EVERYTHING)) {\r\n        actualLen = IOUtils.read(sourceStream, fullData);\r\n    }\r\n    int seekRange = 20;\r\n    try (FSDataInputStream seekStream = select(fs, path, selectConf, SELECT_EVERYTHING)) {\r\n        SelectInputStream sis = (SelectInputStream) seekStream.getWrappedStream();\r\n        S3AInputStreamStatistics streamStats = sis.getS3AStreamStatistics();\r\n        seekStream.seek(0);\r\n        assertEquals(\"first byte read\", fullData[0], seekStream.read());\r\n        seekStream.seek(1);\r\n        seekStream.seek(1);\r\n        PathIOException ex = intercept(PathIOException.class, SelectInputStream.SEEK_UNSUPPORTED, () -> seekStream.seek(0));\r\n        LOG.info(\"Seek error is as expected\", ex);\r\n        byte[] buffer = new byte[1];\r\n        long pos = seekStream.getPos();\r\n        seekStream.readFully(pos, buffer);\r\n        intercept(PathIOException.class, SelectInputStream.SEEK_UNSUPPORTED, () -> seekStream.readFully(0, buffer));\r\n        assertPosition(seekStream, pos + 1);\r\n        intercept(PathIOException.class, SelectInputStream.SEEK_UNSUPPORTED, () -> seekStream.readFully(pos, buffer));\r\n        seekStream.setReadahead(null);\r\n        assertEquals(\"Readahead in \", Constants.DEFAULT_READAHEAD_RANGE, sis.getReadahead());\r\n        long target = seekStream.getPos() + seekRange;\r\n        seek(seekStream, target);\r\n        assertPosition(seekStream, target);\r\n        assertEquals(\"byte at seek position\", fullData[(int) seekStream.getPos()], seekStream.read());\r\n        assertEquals(\"Seek bytes skipped in \" + streamStats, seekRange, streamStats.getBytesSkippedOnSeek());\r\n        intercept(IllegalArgumentException.class, S3AInputStream.E_NEGATIVE_READAHEAD_VALUE, () -> seekStream.setReadahead(-1L));\r\n        int read = seekStream.read(seekStream.getPos() + 2, buffer, 0, 1);\r\n        assertEquals(1, read);\r\n        logIntercepted(expectSeekEOF(seekStream, actualLen * 2));\r\n        assertPosition(seekStream, actualLen);\r\n        assertEquals(-1, seekStream.read());\r\n        LOG.info(\"Seek statistics {}\", streamStats);\r\n        assertFalse(\"Failed to seek to new source in \" + seekStream, seekStream.seekToNewSource(0));\r\n        seekStream.setReadahead(0L);\r\n        seekStream.close();\r\n        LOG.info(\"Final stream state {}\", sis);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "assertPosition",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertPosition(Seekable stream, long pos) throws IOException\n{\r\n    assertEquals(\"Wrong stream position in \" + stream, pos, stream.getPos());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectOddLinesNoHeader",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSelectOddLinesNoHeader() throws Throwable\n{\r\n    describe(\"Select odd lines, ignoring the header\");\r\n    expectSelected(ODD_ROWS_COUNT, selectConf, CSV_HEADER_OPT_IGNORE, \"SELECT * FROM S3OBJECT s WHERE s._5 = `TRUE`\");\r\n    long bytesRead = getFileSystem().getInstrumentation().getCounterValue(Statistic.STREAM_READ_BYTES);\r\n    assertNotEquals(\"No bytes read count in filesystem instrumentation counter\", 0, bytesRead);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectOddLinesHeader",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSelectOddLinesHeader() throws Throwable\n{\r\n    describe(\"Select the odd values\");\r\n    List<String> selected = expectSelected(ODD_ROWS_COUNT, selectConf, CSV_HEADER_OPT_USE, SELECT_ODD_ROWS);\r\n    assertThat(selected, hasItem(ENTRY_0001));\r\n    assertThat(selected, not(hasItem(ENTRY_0002)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectOddLinesHeaderTSVOutput",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSelectOddLinesHeaderTSVOutput() throws Throwable\n{\r\n    describe(\"Select the odd values with tab spaced output\");\r\n    selectConf.set(CSV_OUTPUT_FIELD_DELIMITER, \"\\t\");\r\n    selectConf.set(CSV_OUTPUT_QUOTE_CHARACTER, \"'\");\r\n    selectConf.set(CSV_OUTPUT_QUOTE_FIELDS, CSV_OUTPUT_QUOTE_FIELDS_AS_NEEEDED);\r\n    selectConf.set(CSV_OUTPUT_RECORD_DELIMITER, \"\\r\");\r\n    List<String> selected = expectSelected(ODD_ROWS_COUNT, selectConf, CSV_HEADER_OPT_USE, SELECT_ODD_ENTRIES_BOOL);\r\n    String row1 = selected.get(0);\r\n    String[] columns = row1.split(\"\\t\", -1);\r\n    assertEquals(\"Wrong column count from tab split line <\" + row1 + \">\", CSV_COLUMN_COUNT, columns.length);\r\n    assertEquals(\"Wrong column value from tab split line <\" + row1 + \">\", \"entry-0001\", columns[3]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectNotOperationHeader",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSelectNotOperationHeader() throws Throwable\n{\r\n    describe(\"Select the even values with a NOT call; quote the header name\");\r\n    List<String> selected = expectSelected(EVEN_ROWS_COUNT, selectConf, CSV_HEADER_OPT_USE, \"SELECT s.name FROM S3OBJECT s WHERE NOT s.\\\"odd\\\" = %s\", TRUE);\r\n    assertThat(selected, not(hasItem(ENTRY_0001)));\r\n    assertThat(selected, hasItem(ENTRY_0002));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testBackslashExpansion",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBackslashExpansion() throws Throwable\n{\r\n    assertEquals(\"\\t\\r\\n\", expandBackslashChars(\"\\t\\r\\n\"));\r\n    assertEquals(\"\\t\", expandBackslashChars(\"\\\\t\"));\r\n    assertEquals(\"\\r\", expandBackslashChars(\"\\\\r\"));\r\n    assertEquals(\"\\r \\n\", expandBackslashChars(\"\\\\r \\\\n\"));\r\n    assertEquals(\"\\\\\", expandBackslashChars(\"\\\\\\\\\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectFileExample",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSelectFileExample() throws Throwable\n{\r\n    describe(\"Select the entire file, expect all rows but the header\");\r\n    int len = (int) getFileSystem().getFileStatus(csvPath).getLen();\r\n    FutureDataInputStreamBuilder builder = getFileSystem().openFile(csvPath).must(\"fs.s3a.select.sql\", SELECT_ODD_ENTRIES).must(\"fs.s3a.select.input.format\", \"CSV\").must(\"fs.s3a.select.input.compression\", \"NONE\").must(\"fs.s3a.select.input.csv.header\", \"use\").must(\"fs.s3a.select.output.format\", \"CSV\");\r\n    CompletableFuture<FSDataInputStream> future = builder.build();\r\n    try (FSDataInputStream select = future.get()) {\r\n        byte[] bytes = new byte[len];\r\n        int actual = select.read(bytes);\r\n        LOG.info(\"file length is {}; length of selected data is {}\", len, actual);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectUnsupportedInputFormat",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSelectUnsupportedInputFormat() throws Throwable\n{\r\n    describe(\"Request an Unsupported input format\");\r\n    FutureDataInputStreamBuilder builder = getFileSystem().openFile(csvPath).must(SELECT_SQL, SELECT_ODD_ENTRIES).must(SELECT_INPUT_FORMAT, \"pptx\");\r\n    interceptFuture(IllegalArgumentException.class, \"pptx\", builder.build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectUnsupportedOutputFormat",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSelectUnsupportedOutputFormat() throws Throwable\n{\r\n    describe(\"Request a (currently) Unsupported output format\");\r\n    FutureDataInputStreamBuilder builder = getFileSystem().openFile(csvPath).must(SELECT_SQL, SELECT_ODD_ENTRIES).must(SELECT_INPUT_FORMAT, \"csv\").must(SELECT_OUTPUT_FORMAT, \"json\");\r\n    interceptFuture(IllegalArgumentException.class, \"json\", builder.build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectMissingFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSelectMissingFile() throws Throwable\n{\r\n    describe(\"Select a missing file, expect it to surface in the future\");\r\n    Path missing = path(\"missing\");\r\n    FutureDataInputStreamBuilder builder = getFileSystem().openFile(missing).must(SELECT_SQL, SELECT_ODD_ENTRIES);\r\n    interceptFuture(FileNotFoundException.class, \"\", builder.build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectDirectoryFails",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSelectDirectoryFails() throws Throwable\n{\r\n    describe(\"Verify that secondary select options are only valid on select\" + \" queries\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path dir = path(\"dir\");\r\n    fs.mkdirs(dir);\r\n    FutureDataInputStreamBuilder builder = getFileSystem().openFile(dir).must(SELECT_SQL, SELECT_ODD_ENTRIES);\r\n    interceptFuture(FileNotFoundException.class, \"\", builder.build());\r\n    builder = getFileSystem().openFile(dir.getParent()).must(SELECT_SQL, SELECT_ODD_ENTRIES);\r\n    interceptFuture(FileNotFoundException.class, \"\", builder.build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectRootFails",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSelectRootFails() throws Throwable\n{\r\n    describe(\"verify root dir selection is rejected\");\r\n    FutureDataInputStreamBuilder builder = getFileSystem().openFile(path(\"/\")).must(SELECT_SQL, SELECT_ODD_ENTRIES);\r\n    interceptFuture(IOException.class, \"\", builder.build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testCloseWithAbort",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCloseWithAbort() throws Throwable\n{\r\n    describe(\"Close the stream with the readahead outstanding\");\r\n    S3ATestUtils.MetricDiff readOps = new S3ATestUtils.MetricDiff(getFileSystem(), Statistic.STREAM_READ_OPERATIONS_INCOMPLETE);\r\n    selectConf.setInt(READAHEAD_RANGE, 2);\r\n    FSDataInputStream stream = select(getFileSystem(), csvPath, selectConf, \"SELECT * FROM S3OBJECT s\");\r\n    SelectInputStream sis = (SelectInputStream) stream.getWrappedStream();\r\n    assertEquals(\"Readahead on \" + sis, 2, sis.getReadahead());\r\n    stream.setReadahead(1L);\r\n    assertEquals(\"Readahead on \" + sis, 1, sis.getReadahead());\r\n    stream.read();\r\n    S3AInputStreamStatistics stats = (S3AInputStreamStatistics) sis.getS3AStreamStatistics();\r\n    assertEquals(\"Read count in \" + sis, 1, stats.getBytesRead());\r\n    stream.close();\r\n    assertEquals(\"Abort count in \" + sis, 1, stats.getAborted());\r\n    readOps.assertDiffEquals(\"Read operations are still considered active\", 0);\r\n    intercept(PathIOException.class, FSExceptionMessages.STREAM_IS_CLOSED, () -> stream.read());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testCloseWithNoAbort",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testCloseWithNoAbort() throws Throwable\n{\r\n    describe(\"Close the stream with the readahead outstandingV\");\r\n    FSDataInputStream stream = select(getFileSystem(), csvPath, selectConf, \"SELECT * FROM S3OBJECT s\");\r\n    stream.setReadahead(0x1000L);\r\n    SelectInputStream sis = (SelectInputStream) stream.getWrappedStream();\r\n    S3AInputStreamStatistics stats = (S3AInputStreamStatistics) sis.getS3AStreamStatistics();\r\n    stream.close();\r\n    assertEquals(\"Close count in \" + sis, 1, stats.getClosed());\r\n    assertEquals(\"Abort count in \" + sis, 0, stats.getAborted());\r\n    assertTrue(\"No bytes read in close of \" + sis, stats.getBytesReadInClose() > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testFileContextIntegration",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testFileContextIntegration() throws Throwable\n{\r\n    describe(\"Test that select works through FileContext\");\r\n    FileContext fc = S3ATestUtils.createTestFileContext(getConfiguration());\r\n    selectConf.set(CSV_INPUT_HEADER, CSV_HEADER_OPT_USE);\r\n    List<String> selected = verifySelectionCount(ODD_ROWS_COUNT, SELECT_ODD_ENTRIES_INT, parseToLines(select(fc, csvPath, selectConf, SELECT_ODD_ROWS)));\r\n    assertThat(selected, hasItem(ENTRY_0001));\r\n    assertThat(selected, not(hasItem(ENTRY_0002)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectOptionsOnlyOnSelectCalls",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSelectOptionsOnlyOnSelectCalls() throws Throwable\n{\r\n    describe(\"Secondary select options are only valid on select\" + \" queries\");\r\n    String key = CSV_INPUT_HEADER;\r\n    intercept(IllegalArgumentException.class, key, () -> getFileSystem().openFile(csvPath).must(key, CSV_HEADER_OPT_USE).build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectMustBeEnabled",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSelectMustBeEnabled() throws Throwable\n{\r\n    describe(\"Verify that the FS must have S3 select enabled.\");\r\n    Configuration conf = new Configuration(getFileSystem().getConf());\r\n    conf.setBoolean(FS_S3A_SELECT_ENABLED, false);\r\n    try (FileSystem fs2 = FileSystem.newInstance(csvPath.toUri(), conf)) {\r\n        intercept(UnsupportedOperationException.class, SELECT_UNSUPPORTED, () -> {\r\n            assertFalse(\"S3 Select Capability must be disabled on \" + fs2, isSelectAvailable(fs2));\r\n            return fs2.openFile(csvPath).must(SELECT_SQL, SELECT_ODD_ROWS).build();\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectOptionsRejectedOnNormalOpen",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSelectOptionsRejectedOnNormalOpen() throws Throwable\n{\r\n    describe(\"Verify that a normal open fails on select must() options\");\r\n    intercept(IllegalArgumentException.class, AbstractFSBuilderImpl.UNKNOWN_MANDATORY_KEY, () -> getFileSystem().openFile(csvPath).must(CSV_INPUT_HEADER, CSV_HEADER_OPT_USE).build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectOddRecordsWithHeader",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSelectOddRecordsWithHeader() throws Throwable\n{\r\n    describe(\"work through a record reader\");\r\n    JobConf conf = createJobConf();\r\n    inputMust(conf, CSV_INPUT_HEADER, CSV_HEADER_OPT_USE);\r\n    expectRecordsRead(ODD_ROWS_COUNT, conf, SELECT_ODD_ENTRIES_DECIMAL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectDatestampsConverted",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSelectDatestampsConverted() throws Throwable\n{\r\n    describe(\"timestamp conversion in record IIO\");\r\n    JobConf conf = createJobConf();\r\n    inputMust(conf, CSV_INPUT_HEADER, CSV_HEADER_OPT_USE);\r\n    inputMust(conf, CSV_OUTPUT_QUOTE_FIELDS, CSV_OUTPUT_QUOTE_FIELDS_AS_NEEEDED);\r\n    String sql = SELECT_TO_DATE;\r\n    List<String> records = expectRecordsRead(ALL_ROWS_COUNT, conf, sql);\r\n    LOG.info(\"Result of {}\\n{}\", sql, prepareToPrint(records));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectNoMatch",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSelectNoMatch() throws Throwable\n{\r\n    describe(\"when there's no match to a query, 0 records are returned,\");\r\n    JobConf conf = createJobConf();\r\n    inputMust(conf, CSV_INPUT_HEADER, CSV_HEADER_OPT_USE);\r\n    expectRecordsRead(0, conf, \"SELECT * FROM S3OBJECT s WHERE s.odd = \" + q(\"maybe\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectOddRecordsIgnoreHeader",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSelectOddRecordsIgnoreHeader() throws Throwable\n{\r\n    describe(\"work through a record reader\");\r\n    JobConf conf = createJobConf();\r\n    inputOpt(conf, CSV_INPUT_HEADER, CSV_HEADER_OPT_NONE);\r\n    inputMust(conf, CSV_INPUT_HEADER, CSV_HEADER_OPT_IGNORE);\r\n    expectRecordsRead(EVEN_ROWS_COUNT, conf, SELECT_EVEN_ROWS_NO_HEADER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectRecordsUnknownMustOpt",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSelectRecordsUnknownMustOpt() throws Throwable\n{\r\n    describe(\"verify reader key validation is remapped\");\r\n    JobConf conf = createJobConf();\r\n    inputOpt(conf, CSV_INPUT_HEADER, CSV_HEADER_OPT_NONE);\r\n    inputMust(conf, CSV_INPUT_HEADER + \".something\", CSV_HEADER_OPT_IGNORE);\r\n    intercept(IllegalArgumentException.class, AbstractFSBuilderImpl.UNKNOWN_MANDATORY_KEY, () -> readRecords(conf, SELECT_EVEN_ROWS_NO_HEADER));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectOddRecordsWithHeaderV1",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSelectOddRecordsWithHeaderV1() throws Throwable\n{\r\n    describe(\"work through a V1 record reader\");\r\n    JobConf conf = createJobConf();\r\n    inputMust(conf, CSV_INPUT_HEADER, CSV_HEADER_OPT_USE);\r\n    inputMust(conf, CSV_OUTPUT_FIELD_DELIMITER, \"\\\\t\");\r\n    inputMust(conf, CSV_OUTPUT_QUOTE_CHARACTER, \"'\");\r\n    inputMust(conf, CSV_OUTPUT_QUOTE_FIELDS, CSV_OUTPUT_QUOTE_FIELDS_AS_NEEEDED);\r\n    inputMust(conf, CSV_OUTPUT_RECORD_DELIMITER, \"\\n\");\r\n    verifySelectionCount(ODD_ROWS_COUNT, SELECT_ODD_ROWS, readRecordsV1(conf, SELECT_ODD_ROWS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "createJobConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobConf createJobConf()\n{\r\n    JobConf conf = new JobConf(getConfiguration());\r\n    enablePassthroughCodec(conf, \".csv\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectOddRecordsIgnoreHeaderV1",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSelectOddRecordsIgnoreHeaderV1() throws Throwable\n{\r\n    describe(\"work through a V1 record reader\");\r\n    JobConf conf = createJobConf();\r\n    inputOpt(conf, CSV_INPUT_HEADER, CSV_HEADER_OPT_NONE);\r\n    inputMust(conf, CSV_INPUT_HEADER, CSV_HEADER_OPT_IGNORE);\r\n    inputMust(conf, INPUT_FADVISE, INPUT_FADV_NORMAL);\r\n    inputMust(conf, SELECT_ERRORS_INCLUDE_SQL, \"true\");\r\n    verifySelectionCount(EVEN_ROWS_COUNT, SELECT_EVEN_ROWS_NO_HEADER, readRecordsV1(conf, SELECT_EVEN_ROWS_NO_HEADER));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "expectRecordsRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> expectRecordsRead(final int expected, final JobConf conf, final String sql) throws Exception\n{\r\n    return verifySelectionCount(expected, sql, readRecords(conf, sql));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "readRecords",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> readRecords(JobConf conf, String sql) throws Exception\n{\r\n    return readRecords(conf, csvPath, sql, createLineRecordReader(), ALL_ROWS_COUNT_WITH_HEADER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "readRecordsV1",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<String> readRecordsV1(JobConf conf, String sql) throws Exception\n{\r\n    inputMust(conf, SELECT_SQL, sql);\r\n    return super.readRecordsV1(conf, createLineRecordReaderV1(conf, csvPath), new LongWritable(), new Text(), ALL_ROWS_COUNT_WITH_HEADER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "expectSelected",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<String> expectSelected(final int expected, final Configuration conf, final String header, final String sql, final Object... args) throws Exception\n{\r\n    conf.set(CSV_INPUT_HEADER, header);\r\n    return verifySelectionCount(expected, sql(sql, args), selectCsvFile(conf, sql, args));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "selectCsvFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> selectCsvFile(final Configuration conf, final String sql, final Object... args) throws Exception\n{\r\n    return parseToLines(select(getFileSystem(), csvPath, conf, sql, args));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testCommentsSkipped",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCommentsSkipped() throws Throwable\n{\r\n    describe(\"Verify that comments are skipped\");\r\n    selectConf.set(CSV_INPUT_HEADER, CSV_HEADER_OPT_USE);\r\n    List<String> lines = verifySelectionCount(ALL_ROWS_COUNT_WITH_HEADER, \"select s.id\", parseToLines(select(getFileSystem(), brokenCSV, selectConf, \"SELECT * FROM S3OBJECT s\")));\r\n    LOG.info(\"\\n{}\", prepareToPrint(lines));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testEmptyColumnsRegenerated",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testEmptyColumnsRegenerated() throws Throwable\n{\r\n    describe(\"if you ask for a column but your row doesn't have it,\" + \" an empty column is inserted\");\r\n    selectConf.set(CSV_INPUT_HEADER, CSV_HEADER_OPT_USE);\r\n    List<String> lines = verifySelectionCount(ALL_ROWS_COUNT_WITH_HEADER, \"select s.oddrange\", parseToLines(select(getFileSystem(), brokenCSV, selectConf, \"SELECT s.oddrange FROM S3OBJECT s\")));\r\n    LOG.info(\"\\n{}\", prepareToPrint(lines));\r\n    assertEquals(\"Final oddrange column is not regenerated empty\", \"\\\"\\\"\", lines.get(lines.size() - 1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testIntCastFailure",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testIntCastFailure() throws Throwable\n{\r\n    describe(\"Verify that int casts fail\");\r\n    expectSelectFailure(E_CAST_FAILED, SELECT_ODD_ENTRIES_INT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectToDateParseFailure",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSelectToDateParseFailure() throws Throwable\n{\r\n    describe(\"Verify date parsing failure\");\r\n    expectSelectFailure(E_CAST_FAILED, SELECT_TO_DATE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testParseInvalidPathComponent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testParseInvalidPathComponent() throws Throwable\n{\r\n    describe(\"Verify bad SQL parseing\");\r\n    expectSelectFailure(E_PARSE_INVALID_PATH_COMPONENT, \"SELECT * FROM S3OBJECT WHERE s.'oddf' = true\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectInvalidTableAlias",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSelectInvalidTableAlias() throws Throwable\n{\r\n    describe(\"select with unknown column name\");\r\n    expectSelectFailure(E_INVALID_TABLE_ALIAS, \"SELECT * FROM S3OBJECT WHERE s.\\\"oddf\\\" = 'true'\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectGeneratedAliases",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSelectGeneratedAliases() throws Throwable\n{\r\n    describe(\"select with a ._2 column when headers are enabled\");\r\n    expectSelectFailure(E_INVALID_TABLE_ALIAS, \"SELECT * FROM S3OBJECT WHERE s._2 = 'true'\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "expectSelectFailure",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AWSServiceIOException expectSelectFailure(String expectedErrorCode, String sql) throws Exception\n{\r\n    selectConf.set(CSV_INPUT_HEADER, CSV_HEADER_OPT_USE);\r\n    return verifyErrorCode(expectedErrorCode, intercept(AWSBadRequestException.class, () -> prepareToPrint(parseToLines(select(getFileSystem(), brokenCSV, selectConf, sql)))));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testInputSplit",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testInputSplit() throws Throwable\n{\r\n    describe(\"Verify that only a single file is used for splits\");\r\n    JobConf conf = new JobConf(getConfiguration());\r\n    inputMust(conf, CSV_INPUT_HEADER, CSV_HEADER_OPT_USE);\r\n    final Path input = csvPath;\r\n    S3AFileSystem fs = getFileSystem();\r\n    final Path output = path(\"testLandsatSelect\").makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n    conf.set(FileInputFormat.INPUT_DIR, input.toString());\r\n    conf.set(FileOutputFormat.OUTDIR, output.toString());\r\n    final Job job = Job.getInstance(conf, \"testInputSplit\");\r\n    JobContext jobCtx = new JobContextImpl(job.getConfiguration(), getTaskAttempt0().getJobID());\r\n    TextInputFormat tif = new TextInputFormat();\r\n    List<InputSplit> splits = tif.getSplits(jobCtx);\r\n    assertThat(\"split count wrong\", splits, hasSize(1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    Configuration conf = getConfig();\r\n    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).racks(null).build();\r\n    clusterFS = cluster.getFileSystem();\r\n    localFS = FileSystem.getLocal(clusterFS.getConf());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    clusterFS = null;\r\n    localFS = null;\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getCluster",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MiniDFSCluster getCluster()\n{\r\n    return cluster;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getClusterFS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSystem getClusterFS()\n{\r\n    return clusterFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getLocalFS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LocalFileSystem getLocalFS()\n{\r\n    return localFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "test301ContainsEndpoint",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void test301ContainsEndpoint() throws Exception\n{\r\n    String bucket = \"bucket.s3-us-west-2.amazonaws.com\";\r\n    int sc301 = 301;\r\n    AmazonS3Exception s3Exception = createS3Exception(\"wrong endpoint\", sc301, Collections.singletonMap(S3AUtils.ENDPOINT_KEY, bucket));\r\n    AWSRedirectException ex = verifyTranslated(AWSRedirectException.class, s3Exception);\r\n    assertStatusCode(sc301, ex);\r\n    assertNotNull(ex.getMessage());\r\n    assertContained(ex.getMessage(), bucket);\r\n    assertContained(ex.getMessage(), ENDPOINT);\r\n    assertExceptionContains(ENDPOINT, ex, \"endpoint\");\r\n    assertExceptionContains(bucket, ex, \"bucket name\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertContained",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertContained(String text, String contained)\n{\r\n    assertTrue(\"string \\\"\" + contained + \"\\\" not found in \\\"\" + text + \"\\\"\", text != null && text.contains(contained));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "verifyTranslated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyTranslated(int status, Class<E> expected) throws Exception\n{\r\n    verifyTranslated(expected, createS3Exception(status));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "test400isBad",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test400isBad() throws Exception\n{\r\n    verifyTranslated(400, AWSBadRequestException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "test401isNotPermittedFound",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test401isNotPermittedFound() throws Exception\n{\r\n    verifyTranslated(401, AccessDeniedException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "test403isNotPermittedFound",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test403isNotPermittedFound() throws Exception\n{\r\n    verifyTranslated(403, AccessDeniedException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "test404isNotFound",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test404isNotFound() throws Exception\n{\r\n    verifyTranslated(SC_404, FileNotFoundException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testUnknownBucketException",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testUnknownBucketException() throws Exception\n{\r\n    AmazonS3Exception ex404 = createS3Exception(SC_404);\r\n    ex404.setErrorCode(ErrorTranslation.AwsErrorCodes.E_NO_SUCH_BUCKET);\r\n    verifyTranslated(UnknownStoreException.class, ex404);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "test410isNotFound",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test410isNotFound() throws Exception\n{\r\n    verifyTranslated(410, FileNotFoundException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "test416isEOF",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test416isEOF() throws Exception\n{\r\n    verifyTranslated(416, EOFException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testGenericS3Exception",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGenericS3Exception() throws Exception\n{\r\n    AWSS3IOException ex = verifyTranslated(AWSS3IOException.class, createS3Exception(451));\r\n    assertStatusCode(451, ex);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testGenericServiceS3Exception",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGenericServiceS3Exception() throws Exception\n{\r\n    AmazonServiceException ase = new AmazonServiceException(\"unwind\");\r\n    ase.setStatusCode(500);\r\n    AWSServiceIOException ex = verifyTranslated(AWSStatus500Exception.class, ase);\r\n    assertStatusCode(500, ex);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertStatusCode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertStatusCode(int expected, AWSServiceIOException ex)\n{\r\n    assertNotNull(\"Null exception\", ex);\r\n    if (expected != ex.getStatusCode()) {\r\n        throw new AssertionError(\"Expected status code \" + expected + \"but got \" + ex.getStatusCode(), ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testGenericClientException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGenericClientException() throws Exception\n{\r\n    verifyTranslated(AWSClientIOException.class, new AmazonClientException(\"\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createS3Exception",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AmazonS3Exception createS3Exception(int code)\n{\r\n    return createS3Exception(\"\", code, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createS3Exception",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AmazonS3Exception createS3Exception(String message, int code, Map<String, String> additionalDetails)\n{\r\n    AmazonS3Exception source = new AmazonS3Exception(message);\r\n    source.setStatusCode(code);\r\n    source.setAdditionalDetails(additionalDetails);\r\n    return source;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "verifyTranslated",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "E verifyTranslated(Class<E> clazz, AmazonClientException exception) throws Exception\n{\r\n    IOException ioe = translateException(\"test\", \"/\", exception);\r\n    assertExceptionContains(exception.getMessage(), ioe, \"Translated Exception should contain the error message of the \" + \"actual exception\");\r\n    return verifyExceptionClass(clazz, ioe);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertContainsInterrupted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertContainsInterrupted(boolean expected, Throwable thrown) throws Throwable\n{\r\n    boolean wasInterrupted = containsInterruptedException(thrown) != null;\r\n    if (wasInterrupted != expected) {\r\n        throw thrown;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testInterruptExceptionDetecting",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testInterruptExceptionDetecting() throws Throwable\n{\r\n    InterruptedException interrupted = new InterruptedException(\"irq\");\r\n    assertContainsInterrupted(true, interrupted);\r\n    IOException ioe = new IOException(\"ioe\");\r\n    assertContainsInterrupted(false, ioe);\r\n    assertContainsInterrupted(true, ioe.initCause(interrupted));\r\n    assertContainsInterrupted(true, new InterruptedIOException(\"ioirq\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testExtractInterrupted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testExtractInterrupted() throws Throwable\n{\r\n    throw extractException(\"\", \"\", new ExecutionException(new AmazonClientException(new InterruptedException(\"\"))));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testExtractInterruptedIO",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testExtractInterruptedIO() throws Throwable\n{\r\n    throw extractException(\"\", \"\", new ExecutionException(new AmazonClientException(new InterruptedIOException(\"\"))));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "testGetContentSummaryDir",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testGetContentSummaryDir() throws Throwable\n{\r\n    describe(\"getContentSummary on test dir with children\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path baseDir = methodPath();\r\n    fs.mkdirs(new Path(baseDir, \"a\"));\r\n    fs.mkdirs(new Path(baseDir, \"a/b\"));\r\n    fs.mkdirs(new Path(baseDir, \"a/b/a\"));\r\n    fs.mkdirs(new Path(baseDir, \"d/e/f\"));\r\n    Path filePath = new Path(baseDir, \"a/b/file\");\r\n    touch(fs, filePath);\r\n    final ContentSummary summary = fs.getContentSummary(baseDir);\r\n    Assertions.assertThat(summary.getDirectoryCount()).as(\"Summary \" + summary).isEqualTo(7);\r\n    Assertions.assertThat(summary.getFileCount()).as(\"Summary \" + summary).isEqualTo(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileSystem getFileSystem()\n{\r\n    return (S3AFileSystem) super.getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\fileContext",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp() throws IOException, Exception\n{\r\n    conf = new Configuration();\r\n    fc1 = S3ATestUtils.createTestFileContext(conf);\r\n    fc2 = S3ATestUtils.createTestFileContext(conf);\r\n    super.setUp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\fileContext",
  "methodName" : "testFileStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testFileStatus() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "nameThread",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void nameThread()\n{\r\n    Thread.currentThread().setName(\"JUnit\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testByteBufferIO",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void testByteBufferIO() throws Throwable\n{\r\n    try (S3ADataBlocks.ByteBufferBlockFactory factory = new S3ADataBlocks.ByteBufferBlockFactory(null)) {\r\n        int limit = 128;\r\n        S3ADataBlocks.ByteBufferBlockFactory.ByteBufferBlock block = factory.create(1, limit, null);\r\n        assertOutstandingBuffers(factory, 1);\r\n        byte[] buffer = ContractTestUtils.toAsciiByteArray(\"test data\");\r\n        int bufferLen = buffer.length;\r\n        block.write(buffer, 0, bufferLen);\r\n        assertEquals(bufferLen, block.dataSize());\r\n        assertEquals(\"capacity in \" + block, limit - bufferLen, block.remainingCapacity());\r\n        assertTrue(\"hasCapacity(64) in \" + block, block.hasCapacity(64));\r\n        assertTrue(\"No capacity in \" + block, block.hasCapacity(limit - bufferLen));\r\n        S3ADataBlocks.BlockUploadData blockUploadData = block.startUpload();\r\n        S3ADataBlocks.ByteBufferBlockFactory.ByteBufferBlock.ByteBufferInputStream stream = (S3ADataBlocks.ByteBufferBlockFactory.ByteBufferBlock.ByteBufferInputStream) blockUploadData.getUploadStream();\r\n        assertTrue(\"Mark not supported in \" + stream, stream.markSupported());\r\n        assertTrue(\"!hasRemaining() in \" + stream, stream.hasRemaining());\r\n        int expected = bufferLen;\r\n        assertEquals(\"wrong available() in \" + stream, expected, stream.available());\r\n        assertEquals('t', stream.read());\r\n        stream.mark(limit);\r\n        expected--;\r\n        assertEquals(\"wrong available() in \" + stream, expected, stream.available());\r\n        int offset = 5;\r\n        byte[] in = new byte[limit];\r\n        assertEquals(2, stream.read(in, offset, 2));\r\n        assertEquals('e', in[offset]);\r\n        assertEquals('s', in[offset + 1]);\r\n        expected -= 2;\r\n        assertEquals(\"wrong available() in \" + stream, expected, stream.available());\r\n        byte[] remainder = new byte[limit];\r\n        int c;\r\n        int index = 0;\r\n        while ((c = stream.read()) >= 0) {\r\n            remainder[index++] = (byte) c;\r\n        }\r\n        assertEquals(expected, index);\r\n        assertEquals('a', remainder[--index]);\r\n        assertEquals(\"wrong available() in \" + stream, 0, stream.available());\r\n        assertTrue(\"hasRemaining() in \" + stream, !stream.hasRemaining());\r\n        stream.reset();\r\n        assertEquals('e', stream.read());\r\n        stream.close();\r\n        assertOutstandingBuffers(factory, 1);\r\n        block.close();\r\n        assertOutstandingBuffers(factory, 0);\r\n        stream.close();\r\n        assertOutstandingBuffers(factory, 0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertOutstandingBuffers",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertOutstandingBuffers(S3ADataBlocks.ByteBufferBlockFactory factory, int expectedCount)\n{\r\n    assertEquals(\"outstanding buffers in \" + factory, expectedCount, factory.getOutstandingBufferCount());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    rootPath = getFileSystem().makeQualified(new Path(\"/\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "test_100_audit_root_noauth",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void test_100_audit_root_noauth() throws Throwable\n{\r\n    describe(\"Run a verbose audit\");\r\n    final File audit = tempAuditFile();\r\n    run(MARKERS, V, AUDIT, m(OPT_OUT), audit, rootPath);\r\n    readOutput(audit);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "test_200_clean_root",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void test_200_clean_root() throws Throwable\n{\r\n    describe(\"Clean the root path\");\r\n    final File audit = tempAuditFile();\r\n    run(MARKERS, V, CLEAN, m(OPT_OUT), audit, rootPath);\r\n    readOutput(audit);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    probeForAssumedRoleARN(getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getDelegationBinding",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDelegationBinding()\n{\r\n    return DELEGATION_TOKEN_ROLE_BINDING;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getTokenKind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getTokenKind()\n{\r\n    return ROLE_TOKEN_KIND;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "verifyRestrictedPermissions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyRestrictedPermissions(final S3AFileSystem delegatedFS) throws Exception\n{\r\n    intercept(AccessDeniedException.class, () -> readLandsatMetadata(delegatedFS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testFile",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testFile() throws Exception\n{\r\n    Path path = new Path(\"/file\");\r\n    String key = path.toUri().getPath().substring(1);\r\n    ObjectMetadata meta = new ObjectMetadata();\r\n    meta.setContentLength(1L);\r\n    meta.setLastModified(new Date(2L));\r\n    when(s3.getObjectMetadata(argThat(correctGetMetadataRequest(BUCKET, key)))).thenReturn(meta);\r\n    FileStatus stat = fs.getFileStatus(path);\r\n    assertNotNull(stat);\r\n    assertEquals(fs.makeQualified(path), stat.getPath());\r\n    assertTrue(stat.isFile());\r\n    assertEquals(meta.getContentLength(), stat.getLen());\r\n    assertEquals(meta.getLastModified().getTime(), stat.getModificationTime());\r\n    ContractTestUtils.assertNotErasureCoded(fs, path);\r\n    assertTrue(path + \" should have erasure coding unset in \" + \"FileStatus#toString(): \" + stat, stat.toString().contains(\"isErasureCoded=false\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testFakeDirectory",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testFakeDirectory() throws Exception\n{\r\n    Path path = new Path(\"/dir\");\r\n    String key = path.toUri().getPath().substring(1);\r\n    when(s3.getObjectMetadata(argThat(correctGetMetadataRequest(BUCKET, key)))).thenThrow(NOT_FOUND);\r\n    String keyDir = key + \"/\";\r\n    ListObjectsV2Result listResult = new ListObjectsV2Result();\r\n    S3ObjectSummary objectSummary = new S3ObjectSummary();\r\n    objectSummary.setKey(keyDir);\r\n    objectSummary.setSize(0L);\r\n    listResult.getObjectSummaries().add(objectSummary);\r\n    when(s3.listObjectsV2(argThat(matchListV2Request(BUCKET, keyDir)))).thenReturn(listResult);\r\n    FileStatus stat = fs.getFileStatus(path);\r\n    assertNotNull(stat);\r\n    assertEquals(fs.makeQualified(path), stat.getPath());\r\n    assertTrue(stat.isDirectory());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testImplicitDirectory",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testImplicitDirectory() throws Exception\n{\r\n    Path path = new Path(\"/dir\");\r\n    String key = path.toUri().getPath().substring(1);\r\n    when(s3.getObjectMetadata(argThat(correctGetMetadataRequest(BUCKET, key)))).thenThrow(NOT_FOUND);\r\n    when(s3.getObjectMetadata(argThat(correctGetMetadataRequest(BUCKET, key + \"/\")))).thenThrow(NOT_FOUND);\r\n    setupListMocks(Collections.singletonList(\"dir/\"), Collections.emptyList());\r\n    FileStatus stat = fs.getFileStatus(path);\r\n    assertNotNull(stat);\r\n    assertEquals(fs.makeQualified(path), stat.getPath());\r\n    assertTrue(stat.isDirectory());\r\n    ContractTestUtils.assertNotErasureCoded(fs, path);\r\n    assertTrue(path + \" should have erasure coding unset in \" + \"FileStatus#toString(): \" + stat, stat.toString().contains(\"isErasureCoded=false\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRoot",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRoot() throws Exception\n{\r\n    Path path = new Path(\"/\");\r\n    String key = path.toUri().getPath().substring(1);\r\n    when(s3.getObjectMetadata(argThat(correctGetMetadataRequest(BUCKET, key)))).thenThrow(NOT_FOUND);\r\n    when(s3.getObjectMetadata(argThat(correctGetMetadataRequest(BUCKET, key + \"/\")))).thenThrow(NOT_FOUND);\r\n    setupListMocks(Collections.emptyList(), Collections.emptyList());\r\n    FileStatus stat = fs.getFileStatus(path);\r\n    assertNotNull(stat);\r\n    assertEquals(fs.makeQualified(path), stat.getPath());\r\n    assertTrue(stat.isDirectory());\r\n    assertTrue(stat.getPath().isRoot());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testNotFound",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testNotFound() throws Exception\n{\r\n    Path path = new Path(\"/dir\");\r\n    String key = path.toUri().getPath().substring(1);\r\n    when(s3.getObjectMetadata(argThat(correctGetMetadataRequest(BUCKET, key)))).thenThrow(NOT_FOUND);\r\n    when(s3.getObjectMetadata(argThat(correctGetMetadataRequest(BUCKET, key + \"/\")))).thenThrow(NOT_FOUND);\r\n    setupListMocks(Collections.emptyList(), Collections.emptyList());\r\n    exception.expect(FileNotFoundException.class);\r\n    fs.getFileStatus(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setupListMocks",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setupListMocks(List<String> prefixes, List<S3ObjectSummary> summaries)\n{\r\n    ObjectListing objects = mock(ObjectListing.class);\r\n    when(objects.getCommonPrefixes()).thenReturn(prefixes);\r\n    when(objects.getObjectSummaries()).thenReturn(summaries);\r\n    when(s3.listObjects(any(ListObjectsRequest.class))).thenReturn(objects);\r\n    ListObjectsV2Result v2Result = mock(ListObjectsV2Result.class);\r\n    when(v2Result.getCommonPrefixes()).thenReturn(prefixes);\r\n    when(v2Result.getObjectSummaries()).thenReturn(summaries);\r\n    when(s3.listObjectsV2(any(ListObjectsV2Request.class))).thenReturn(v2Result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "correctGetMetadataRequest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ArgumentMatcher<GetObjectMetadataRequest> correctGetMetadataRequest(String bucket, String key)\n{\r\n    return request -> request != null && request.getBucketName().equals(bucket) && request.getKey().equals(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "matchListV2Request",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ArgumentMatcher<ListObjectsV2Request> matchListV2Request(String bucket, String key)\n{\r\n    return (ListObjectsV2Request request) -> {\r\n        return request != null && request.getBucketName().equals(bucket) && request.getPrefix().equals(key);\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "resetMetricDiffs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void resetMetricDiffs()\n{\r\n    metricDiffs.values().forEach(S3ATestUtils.MetricDiff::reset);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3ATestUtils.MetricDiff get(Statistic stat)\n{\r\n    S3ATestUtils.MetricDiff diff = requireNonNull(metricDiffs.get(stat.getSymbol()), () -> \"No metric tracking for \" + stat);\r\n    return diff;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "exec",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "T exec(Callable<T> eval, ExpectedProbe... expectedA) throws Exception\n{\r\n    List<ExpectedProbe> expected = Arrays.asList(expectedA);\r\n    resetMetricDiffs();\r\n    assumeProbesEnabled(expected);\r\n    T r = eval.call();\r\n    String text = \"operation returning \" + (r != null ? r.toString() : \"null\");\r\n    LOG.info(\"{}\", text);\r\n    LOG.info(\"state {}\", this.toString());\r\n    LOG.info(\"probes {}\", expected);\r\n    LOG.info(\"IOStatistics {}\", ioStatisticsToPrettyString(ioStatistics));\r\n    for (ExpectedProbe ed : expected) {\r\n        ed.verify(this, text);\r\n    }\r\n    return r;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "assumeProbesEnabled",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assumeProbesEnabled(List<ExpectedProbe> expected)\n{\r\n    boolean enabled = false;\r\n    for (ExpectedProbe ed : expected) {\r\n        enabled |= ed.isEnabled();\r\n    }\r\n    String pstr = expected.stream().map(Object::toString).collect(Collectors.joining(\", \"));\r\n    Assumptions.assumeThat(enabled).describedAs(\"metrics to probe for are not enabled in %s\", pstr).isTrue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "intercepting",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "E intercepting(Class<E> clazz, String text, Callable<T> eval, ExpectedProbe... expected) throws Exception\n{\r\n    return exec(() -> intercept(clazz, text, eval), expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return metricDiffs.values().stream().map(S3ATestUtils.MetricDiff::toString).collect(Collectors.joining(\", \"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "builder",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Builder builder(S3AFileSystem fs)\n{\r\n    return new Builder(fs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "always",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ExpectedProbe always()\n{\r\n    return ALWAYS_PROBE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "probe",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExpectedProbe probe(final Statistic statistic, final int expected)\n{\r\n    return probe(expected >= 0, statistic, expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "probe",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ExpectedProbe probe(final boolean enabled, final Statistic statistic, final int expected)\n{\r\n    return enabled ? new ExpectSingleStatistic(statistic, expected) : EMPTY_PROBE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "probes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExpectedProbe probes(final boolean enabled, final ExpectedProbe... plist)\n{\r\n    return enabled ? new ProbeList(Arrays.asList(plist)) : EMPTY_PROBE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "expect",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExpectedProbe expect(boolean enabled, OperationCost cost)\n{\r\n    return probes(enabled, probe(OBJECT_METADATA_REQUESTS, cost.head()), probe(OBJECT_LIST_REQUEST, cost.list()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testProvidedFileStatusIteratorEnd",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testProvidedFileStatusIteratorEnd() throws Exception\n{\r\n    S3AFileStatus s3aStatus = new S3AFileStatus(100, 0, new Path(\"s3a://blah/blah\"), 8192, null, null, null);\r\n    S3AFileStatus[] statuses = { s3aStatus };\r\n    RemoteIterator<S3AFileStatus> it = Listing.toProvidedFileStatusIterator(statuses);\r\n    Assert.assertTrue(\"hasNext() should return true first time\", it.hasNext());\r\n    Assert.assertEquals(\"first element from iterator\", s3aStatus, it.next());\r\n    Assert.assertFalse(\"hasNext() should now be false\", it.hasNext());\r\n    intercept(NoSuchElementException.class, it::next);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "createObjectAttributes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3ObjectAttributes createObjectAttributes(Path path, String eTag, String versionId, long len)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "createObjectAttributes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3ObjectAttributes createObjectAttributes(S3AFileStatus fileStatus)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "createReadContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AReadOpContext createReadContext(FileStatus fileStatus)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "finishRename",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void finishRename(Path sourceRenamed, Path destCreated) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "deleteObjectAtPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void deleteObjectAtPath(Path path, String key, boolean isFile) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "listFilesAndDirectoryMarkers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RemoteIterator<S3ALocatedFileStatus> listFilesAndDirectoryMarkers(final Path path, final S3AFileStatus status, final boolean includeSelf) throws IOException\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "copyFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CopyResult copyFile(String srcKey, String destKey, S3ObjectAttributes srcAttributes, S3AReadOpContext readContext) throws IOException\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "removeKeys",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void removeKeys(List<DeleteObjectsRequest.KeyVersion> keysToDelete, boolean deleteFakeDir) throws MultiObjectDeleteException, AmazonClientException, IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "listObjects",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RemoteIterator<S3AFileStatus> listObjects(Path path, String key) throws IOException\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics",
  "methodName" : "inputStreamStatisticKeys",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> inputStreamStatisticKeys()\n{\r\n    return Arrays.asList(StreamStatisticNames.STREAM_READ_ABORTED, StreamStatisticNames.STREAM_READ_BYTES_DISCARDED_ABORT, StreamStatisticNames.STREAM_READ_CLOSED, StreamStatisticNames.STREAM_READ_BYTES_DISCARDED_CLOSE, StreamStatisticNames.STREAM_READ_CLOSE_OPERATIONS, StreamStatisticNames.STREAM_READ_OPENED, StreamStatisticNames.STREAM_READ_BYTES, StreamStatisticNames.STREAM_READ_EXCEPTIONS, StreamStatisticNames.STREAM_READ_FULLY_OPERATIONS, StreamStatisticNames.STREAM_READ_OPERATIONS, StreamStatisticNames.STREAM_READ_OPERATIONS_INCOMPLETE, StreamStatisticNames.STREAM_READ_VERSION_MISMATCHES, StreamStatisticNames.STREAM_READ_SEEK_OPERATIONS, StreamStatisticNames.STREAM_READ_SEEK_BACKWARD_OPERATIONS, StreamStatisticNames.STREAM_READ_SEEK_FORWARD_OPERATIONS, StreamStatisticNames.STREAM_READ_SEEK_BYTES_BACKWARDS, StreamStatisticNames.STREAM_READ_SEEK_BYTES_DISCARDED, StreamStatisticNames.STREAM_READ_SEEK_BYTES_SKIPPED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics",
  "methodName" : "outputStreamStatisticKeys",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> outputStreamStatisticKeys()\n{\r\n    return Arrays.asList(STREAM_WRITE_BYTES, STREAM_WRITE_BLOCK_UPLOADS, STREAM_WRITE_EXCEPTIONS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    scaleTestDir = new Path(getTestPath(), getTestSuiteName());\r\n    hugefile = new Path(scaleTestDir, \"hugefile\");\r\n    hugefileRenamed = new Path(scaleTestDir, \"hugefileRenamed\");\r\n    filesize = getTestPropertyBytes(getConf(), KEY_HUGE_FILESIZE, DEFAULT_HUGE_FILESIZE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getTestSuiteName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTestSuiteName()\n{\r\n    return getBlockOutputBufferName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "createScaleConfiguration",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "Configuration createScaleConfiguration()\n{\r\n    Configuration conf = super.createScaleConfiguration();\r\n    partitionSize = (int) getTestPropertyBytes(conf, KEY_HUGE_PARTITION_SIZE, DEFAULT_HUGE_PARTITION_SIZE);\r\n    assertTrue(\"Partition size too small: \" + partitionSize, partitionSize >= MULTIPART_MIN_SIZE);\r\n    conf.setLong(SOCKET_SEND_BUFFER, _1MB);\r\n    conf.setLong(SOCKET_RECV_BUFFER, _1MB);\r\n    conf.setLong(MIN_MULTIPART_THRESHOLD, partitionSize);\r\n    conf.setInt(MULTIPART_SIZE, partitionSize);\r\n    conf.set(USER_AGENT_PREFIX, \"STestS3AHugeFileCreate\");\r\n    conf.set(FAST_UPLOAD_BUFFER, getBlockOutputBufferName());\r\n    S3ATestUtils.disableFilesystemCaching(conf);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getBlockOutputBufferName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockOutputBufferName()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "test_010_CreateHugeFile",
  "errType" : [ "ClassCastException" ],
  "containingMethodsNum" : 35,
  "sourceCodeText" : "void test_010_CreateHugeFile() throws IOException\n{\r\n    long filesizeMB = filesize / _1MB;\r\n    deleteHugeFile();\r\n    Path fileToCreate = getPathOfFileToCreate();\r\n    describe(\"Creating file %s of size %d MB\" + \" with partition size %d buffered by %s\", fileToCreate, filesizeMB, partitionSize, getBlockOutputBufferName());\r\n    int timeout = getTestTimeoutSeconds();\r\n    int bandwidth = _1MB;\r\n    long uploadTime = filesize / bandwidth;\r\n    assertTrue(String.format(\"Timeout set in %s seconds is too low;\" + \" estimating upload time of %d seconds at 1 MB/s.\" + \" Rerun tests with -D%s=%d\", timeout, uploadTime, KEY_TEST_TIMEOUT, uploadTime * 2), uploadTime < timeout);\r\n    assertEquals(\"File size set in \" + KEY_HUGE_FILESIZE + \" = \" + filesize + \" is not a multiple of \" + uploadBlockSize, 0, filesize % uploadBlockSize);\r\n    byte[] data = new byte[uploadBlockSize];\r\n    for (int i = 0; i < uploadBlockSize; i++) {\r\n        data[i] = (byte) (i % 256);\r\n    }\r\n    long blocks = filesize / uploadBlockSize;\r\n    long blocksPerMB = _1MB / uploadBlockSize;\r\n    S3AFileSystem fs = getFileSystem();\r\n    IOStatistics iostats = fs.getIOStatistics();\r\n    String putRequests = Statistic.OBJECT_PUT_REQUESTS.getSymbol();\r\n    String putBytes = Statistic.OBJECT_PUT_BYTES.getSymbol();\r\n    Statistic putRequestsActive = Statistic.OBJECT_PUT_REQUESTS_ACTIVE;\r\n    Statistic putBytesPending = Statistic.OBJECT_PUT_BYTES_PENDING;\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    BlockOutputStreamStatistics streamStatistics;\r\n    long blocksPer10MB = blocksPerMB * 10;\r\n    ProgressCallback progress = new ProgressCallback(timer);\r\n    try (FSDataOutputStream out = fs.create(fileToCreate, true, uploadBlockSize, progress)) {\r\n        try {\r\n            streamStatistics = getOutputStreamStatistics(out);\r\n        } catch (ClassCastException e) {\r\n            LOG.info(\"Wrapped output stream is not block stream: {}\", out.getWrappedStream());\r\n            streamStatistics = null;\r\n        }\r\n        for (long block = 1; block <= blocks; block++) {\r\n            out.write(data);\r\n            long written = block * uploadBlockSize;\r\n            if (block % blocksPer10MB == 0 || written == filesize) {\r\n                long percentage = written * 100 / filesize;\r\n                double elapsedTime = timer.elapsedTime() / 1.0e9;\r\n                double writtenMB = 1.0 * written / _1MB;\r\n                LOG.info(String.format(\"[%02d%%] Buffered %.2f MB out of %d MB;\" + \" PUT %d bytes (%d pending) in %d operations (%d active);\" + \" elapsedTime=%.2fs; write to buffer bandwidth=%.2f MB/s\", percentage, writtenMB, filesizeMB, iostats.counters().get(putBytes), gaugeValue(putBytesPending), iostats.counters().get(putRequests), gaugeValue(putRequestsActive), elapsedTime, writtenMB / elapsedTime));\r\n            }\r\n        }\r\n        LOG.info(\"Closing stream {}\", out);\r\n        LOG.info(\"Statistics : {}\", streamStatistics);\r\n        ContractTestUtils.NanoTimer closeTimer = new ContractTestUtils.NanoTimer();\r\n        out.close();\r\n        closeTimer.end(\"time to close() output stream\");\r\n    }\r\n    timer.end(\"time to write %d MB in blocks of %d\", filesizeMB, uploadBlockSize);\r\n    logFSState();\r\n    bandwidth(timer, filesize);\r\n    LOG.info(\"Statistics after stream closed: {}\", streamStatistics);\r\n    LOG.info(\"IOStatistics after upload: {}\", demandStringifyIOStatistics(iostats));\r\n    long putRequestCount = lookupCounterStatistic(iostats, putRequests);\r\n    long putByteCount = lookupCounterStatistic(iostats, putBytes);\r\n    Assertions.assertThat(putRequestCount).describedAs(\"Put request count from filesystem stats %s\", iostats).isGreaterThan(0);\r\n    Assertions.assertThat(putByteCount).describedAs(\"%s count from filesystem stats %s\", putBytes, iostats).isGreaterThan(0);\r\n    LOG.info(\"PUT {} bytes in {} operations; {} MB/operation\", putByteCount, putRequestCount, putByteCount / (putRequestCount * _1MB));\r\n    LOG.info(\"Time per PUT {} nS\", toHuman(timer.nanosPerOperation(putRequestCount)));\r\n    verifyStatisticGaugeValue(iostats, putRequestsActive.getSymbol(), 0);\r\n    verifyStatisticGaugeValue(iostats, STREAM_WRITE_BLOCK_UPLOADS_BYTES_PENDING.getSymbol(), 0);\r\n    progress.verifyNoFailures(\"Put file \" + fileToCreate + \" of size \" + filesize);\r\n    if (streamStatistics != null) {\r\n        assertEquals(\"actively allocated blocks in \" + streamStatistics, 0, streamStatistics.getBlocksActivelyAllocated());\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getPathOfFileToCreate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getPathOfFileToCreate()\n{\r\n    return this.hugefile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getScaleTestDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getScaleTestDir()\n{\r\n    return scaleTestDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getHugefile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getHugefile()\n{\r\n    return hugefile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "setHugefile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHugefile(Path hugefile)\n{\r\n    this.hugefile = hugefile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getHugefileRenamed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getHugefileRenamed()\n{\r\n    return hugefileRenamed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getUploadBlockSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getUploadBlockSize()\n{\r\n    return uploadBlockSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getPartitionSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getPartitionSize()\n{\r\n    return partitionSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "assumeHugeFileExists",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assumeHugeFileExists() throws IOException\n{\r\n    assumeFileExists(this.hugefile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "assumeFileExists",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void assumeFileExists(Path file) throws IOException\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    ContractTestUtils.assertPathExists(fs, \"huge file not created\", file);\r\n    FileStatus status = fs.getFileStatus(file);\r\n    ContractTestUtils.assertIsFile(file, status);\r\n    assertTrue(\"File \" + file + \" is empty\", status.getLen() > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "logFSState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void logFSState()\n{\r\n    LOG.info(\"File System state after operation:\\n{}\", getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "test_030_postCreationAssertions",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void test_030_postCreationAssertions() throws Throwable\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    ContractTestUtils.assertPathExists(fs, \"Huge file\", hugefile);\r\n    FileStatus status = fs.getFileStatus(hugefile);\r\n    ContractTestUtils.assertIsFile(hugefile, status);\r\n    assertEquals(\"File size in \" + status, filesize, status.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "test_040_PositionedReadHugeFile",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void test_040_PositionedReadHugeFile() throws Throwable\n{\r\n    assumeHugeFileExists();\r\n    final String encryption = getConf().getTrimmed(Constants.S3_ENCRYPTION_ALGORITHM);\r\n    boolean encrypted = encryption != null;\r\n    if (encrypted) {\r\n        LOG.info(\"File is encrypted with algorithm {}\", encryption);\r\n    }\r\n    String filetype = encrypted ? \"encrypted file\" : \"file\";\r\n    describe(\"Positioned reads of %s %s\", filetype, hugefile);\r\n    S3AFileSystem fs = getFileSystem();\r\n    FileStatus status = fs.getFileStatus(hugefile);\r\n    long size = status.getLen();\r\n    int ops = 0;\r\n    final int bufferSize = 8192;\r\n    byte[] buffer = new byte[bufferSize];\r\n    long eof = size - 1;\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    ContractTestUtils.NanoTimer readAtByte0, readAtByte0Again, readAtEOF;\r\n    try (FSDataInputStream in = fs.open(hugefile, uploadBlockSize)) {\r\n        readAtByte0 = new ContractTestUtils.NanoTimer();\r\n        in.readFully(0, buffer);\r\n        readAtByte0.end(\"time to read data at start of file\");\r\n        ops++;\r\n        readAtEOF = new ContractTestUtils.NanoTimer();\r\n        in.readFully(eof - bufferSize, buffer);\r\n        readAtEOF.end(\"time to read data at end of file\");\r\n        ops++;\r\n        readAtByte0Again = new ContractTestUtils.NanoTimer();\r\n        in.readFully(0, buffer);\r\n        readAtByte0Again.end(\"time to read data at start of file again\");\r\n        ops++;\r\n        LOG.info(\"Final stream state: {}\", in);\r\n    }\r\n    long mb = Math.max(size / _1MB, 1);\r\n    logFSState();\r\n    timer.end(\"time to perform positioned reads of %s of %d MB \", filetype, mb);\r\n    LOG.info(\"Time per positioned read = {} nS\", toHuman(timer.nanosPerOperation(ops)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "test_050_readHugeFile",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void test_050_readHugeFile() throws Throwable\n{\r\n    assumeHugeFileExists();\r\n    describe(\"Reading %s\", hugefile);\r\n    S3AFileSystem fs = getFileSystem();\r\n    FileStatus status = fs.getFileStatus(hugefile);\r\n    long size = status.getLen();\r\n    long blocks = size / uploadBlockSize;\r\n    byte[] data = new byte[uploadBlockSize];\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    try (FSDataInputStream in = fs.open(hugefile, uploadBlockSize)) {\r\n        for (long block = 0; block < blocks; block++) {\r\n            in.readFully(data);\r\n        }\r\n        LOG.info(\"Final stream state: {}\", in);\r\n    }\r\n    long mb = Math.max(size / _1MB, 1);\r\n    timer.end(\"time to read file of %d MB \", mb);\r\n    LOG.info(\"Time per MB to read = {} nS\", toHuman(timer.nanosPerOperation(mb)));\r\n    bandwidth(timer, size);\r\n    logFSState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "test_090_verifyRenameSourceEncryption",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void test_090_verifyRenameSourceEncryption() throws IOException\n{\r\n    if (isEncrypted(getFileSystem())) {\r\n        assertEncrypted(getHugefile());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "assertEncrypted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void assertEncrypted(Path hugeFile) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "isEncrypted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isEncrypted(S3AFileSystem fileSystem)\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "test_100_renameHugeFile",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void test_100_renameHugeFile() throws Throwable\n{\r\n    assumeHugeFileExists();\r\n    describe(\"renaming %s to %s\", hugefile, hugefileRenamed);\r\n    S3AFileSystem fs = getFileSystem();\r\n    FileStatus status = fs.getFileStatus(hugefile);\r\n    long size = status.getLen();\r\n    fs.delete(hugefileRenamed, false);\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    fs.rename(hugefile, hugefileRenamed);\r\n    long mb = Math.max(size / _1MB, 1);\r\n    timer.end(\"time to rename file of %d MB\", mb);\r\n    LOG.info(\"Time per MB to rename = {} nS\", toHuman(timer.nanosPerOperation(mb)));\r\n    bandwidth(timer, size);\r\n    logFSState();\r\n    FileStatus destFileStatus = fs.getFileStatus(hugefileRenamed);\r\n    assertEquals(size, destFileStatus.getLen());\r\n    ContractTestUtils.NanoTimer timer2 = new ContractTestUtils.NanoTimer();\r\n    fs.rename(hugefileRenamed, hugefile);\r\n    timer2.end(\"Renaming back\");\r\n    LOG.info(\"Time per MB to rename = {} nS\", toHuman(timer2.nanosPerOperation(mb)));\r\n    bandwidth(timer2, size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "test_110_verifyRenameDestEncryption",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void test_110_verifyRenameDestEncryption() throws IOException\n{\r\n    if (isEncrypted(getFileSystem())) {\r\n        assertEncrypted(hugefile);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "test_800_DeleteHugeFiles",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void test_800_DeleteHugeFiles() throws IOException\n{\r\n    try {\r\n        deleteHugeFile();\r\n        delete(hugefileRenamed, false);\r\n    } finally {\r\n        ContractTestUtils.rm(getFileSystem(), getTestPath(), true, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "test_900_dumpStats",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_900_dumpStats()\n{\r\n    LOG.info(\"Statistics\\n{}\", ioStatisticsSourceToString(getFileSystem()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "deleteHugeFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deleteHugeFile() throws IOException\n{\r\n    delete(hugefile, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void delete(Path path, boolean recursive) throws IOException\n{\r\n    describe(\"Deleting %s\", path);\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    getFileSystem().delete(path, recursive);\r\n    timer.end(\"time to delete %s\", path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testNoBucketProbing",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testNoBucketProbing() throws Exception\n{\r\n    describe(\"Disable init-time probes and expect FS operations to fail\");\r\n    Configuration conf = createConfigurationWithProbe(0);\r\n    fs = FileSystem.get(uri, conf);\r\n    Path root = new Path(uri);\r\n    assertTrue(\"root path should always exist\", fs.exists(root));\r\n    assertTrue(\"getFileStatus on root should always return a directory\", fs.getFileStatus(root).isDirectory());\r\n    expectUnknownStore(() -> fs.listStatus(root));\r\n    Path src = new Path(root, \"testfile\");\r\n    Path dest = new Path(root, \"dst\");\r\n    expectUnknownStore(() -> fs.getFileStatus(src));\r\n    expectUnknownStore(() -> fs.exists(src));\r\n    assertFalse(\"isFile(\" + src + \")\" + \" was expected to complete by returning false\", fs.isFile(src));\r\n    expectUnknownStore(() -> fs.isDirectory(src));\r\n    expectUnknownStore(() -> fs.mkdirs(src));\r\n    expectUnknownStore(() -> fs.delete(src));\r\n    expectUnknownStore(() -> fs.rename(src, dest));\r\n    byte[] data = dataset(1024, 'a', 'z');\r\n    expectUnknownStore(() -> writeDataset(fs, src, data, data.length, 1024 * 1024, true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "expectUnknownStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectUnknownStore(Callable<T> eval) throws Exception\n{\r\n    intercept(UnknownStoreException.class, eval);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "expectUnknownStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectUnknownStore(LambdaTestUtils.VoidCallable eval) throws Exception\n{\r\n    intercept(UnknownStoreException.class, eval);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfigurationWithProbe",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createConfigurationWithProbe(final int probe)\n{\r\n    Configuration conf = new Configuration(getFileSystem().getConf());\r\n    S3ATestUtils.disableFilesystemCaching(conf);\r\n    conf.setInt(S3A_BUCKET_PROBE, probe);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testBucketProbingV1",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testBucketProbingV1() throws Exception\n{\r\n    describe(\"Test the V1 bucket probe\");\r\n    Configuration configuration = createConfigurationWithProbe(1);\r\n    expectUnknownStore(() -> FileSystem.get(uri, configuration));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testBucketProbingV2",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBucketProbingV2() throws Exception\n{\r\n    describe(\"Test the V2 bucket probe\");\r\n    Configuration configuration = createConfigurationWithProbe(2);\r\n    expectUnknownStore(() -> FileSystem.get(uri, configuration));\r\n    configuration.setInt(S3A_BUCKET_PROBE, 3);\r\n    expectUnknownStore(() -> FileSystem.get(uri, configuration));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testBucketProbingParameterValidation",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testBucketProbingParameterValidation() throws Exception\n{\r\n    describe(\"Test bucket probe parameter %s validation\", S3A_BUCKET_PROBE);\r\n    Configuration configuration = createConfigurationWithProbe(-1);\r\n    intercept(IllegalArgumentException.class, \"Value of \" + S3A_BUCKET_PROBE + \" should be >= 0\", \"Should throw IllegalArgumentException\", () -> FileSystem.get(uri, configuration));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testAccessPointProbingV2",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAccessPointProbingV2() throws Exception\n{\r\n    describe(\"Test V2 bucket probing using an AccessPoint ARN\");\r\n    Configuration configuration = createConfigurationWithProbe(2);\r\n    String accessPointArn = \"arn:aws:s3:eu-west-1:123456789012:accesspoint/\" + randomBucket;\r\n    configuration.set(String.format(InternalConstants.ARN_BUCKET_OPTION, randomBucket), accessPointArn);\r\n    expectUnknownStore(() -> FileSystem.get(uri, configuration));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testAccessPointRequired",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAccessPointRequired() throws Exception\n{\r\n    describe(\"Test V2 bucket probing with 'fs.s3a.accesspoint.required' property.\");\r\n    Configuration configuration = createConfigurationWithProbe(2);\r\n    configuration.set(AWS_S3_ACCESSPOINT_REQUIRED, \"true\");\r\n    intercept(PathIOException.class, InternalConstants.AP_REQUIRED_EXCEPTION, \"Should throw IOException if Access Points are required but not configured.\", () -> FileSystem.get(uri, configuration));\r\n    String accessPointArn = \"arn:aws:s3:eu-west-1:123456789012:accesspoint/\" + randomBucket;\r\n    configuration.set(String.format(InternalConstants.ARN_BUCKET_OPTION, randomBucket), accessPointArn);\r\n    expectUnknownStore(() -> FileSystem.get(uri, configuration));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    Configuration configuration = super.getConfiguration();\r\n    S3ATestUtils.disableFilesystemCaching(configuration);\r\n    return configuration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    IOUtils.cleanupWithLogger(getLogger(), fs);\r\n    super.teardown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getCredentials",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AWSCredentials getCredentials()\n{\r\n    COUNTER.incrementAndGet();\r\n    throw new CredentialInitializationException(\"no credentials\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "refresh",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void refresh()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getInvocationCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getInvocationCount()\n{\r\n    return COUNTER.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "classSetup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void classSetup() throws Exception\n{\r\n    landsatUri = new URI(S3ATestConstants.DEFAULT_CSVTEST_FILE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testSessionTokenKind",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSessionTokenKind() throws Throwable\n{\r\n    AbstractS3ATokenIdentifier identifier = new SessionTokenIdentifier();\r\n    assertEquals(SESSION_TOKEN_KIND, identifier.getKind());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testSessionTokenIssueDate",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSessionTokenIssueDate() throws Throwable\n{\r\n    AbstractS3ATokenIdentifier identifier = new SessionTokenIdentifier();\r\n    assertEquals(SESSION_TOKEN_KIND, identifier.getKind());\r\n    assertTrue(\"issue date is not set\", identifier.getIssueDate() > 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testSessionTokenDecode",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSessionTokenDecode() throws Throwable\n{\r\n    Text alice = new Text(\"alice\");\r\n    Text renewer = new Text(\"yarn\");\r\n    AbstractS3ATokenIdentifier identifier = new SessionTokenIdentifier(SESSION_TOKEN_KIND, alice, renewer, new URI(\"s3a://landsat-pds/\"), new MarshalledCredentials(\"a\", \"b\", \"\"), new EncryptionSecrets(S3AEncryptionMethods.SSE_S3, \"\"), \"origin\");\r\n    Token<AbstractS3ATokenIdentifier> t1 = new Token<>(identifier, new SessionSecretManager());\r\n    AbstractS3ATokenIdentifier decoded = t1.decodeIdentifier();\r\n    decoded.validate();\r\n    MarshalledCredentials creds = ((SessionTokenIdentifier) decoded).getMarshalledCredentials();\r\n    assertNotNull(\"credentials\", MarshalledCredentialBinding.toAWSCredentials(creds, MarshalledCredentials.CredentialTypeRequired.AnyNonEmpty, \"\"));\r\n    assertEquals(alice, decoded.getOwner());\r\n    UserGroupInformation decodedUser = decoded.getUser();\r\n    assertEquals(\"name of \" + decodedUser, \"alice\", decodedUser.getUserName());\r\n    assertEquals(\"renewer\", renewer, decoded.getRenewer());\r\n    assertEquals(\"Authentication method of \" + decodedUser, UserGroupInformation.AuthenticationMethod.TOKEN, decodedUser.getAuthenticationMethod());\r\n    assertEquals(\"origin\", decoded.getOrigin());\r\n    assertEquals(\"issue date\", identifier.getIssueDate(), decoded.getIssueDate());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testFullTokenKind",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFullTokenKind() throws Throwable\n{\r\n    AbstractS3ATokenIdentifier identifier = new FullCredentialsTokenIdentifier();\r\n    assertEquals(FULL_TOKEN_KIND, identifier.getKind());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testSessionTokenIdentifierRoundTrip",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSessionTokenIdentifierRoundTrip() throws Throwable\n{\r\n    Text renewer = new Text(\"yarn\");\r\n    SessionTokenIdentifier id = new SessionTokenIdentifier(SESSION_TOKEN_KIND, new Text(), renewer, landsatUri, new MarshalledCredentials(\"a\", \"b\", \"c\"), new EncryptionSecrets(), \"\");\r\n    SessionTokenIdentifier result = S3ATestUtils.roundTrip(id, null);\r\n    String ids = id.toString();\r\n    assertEquals(\"URI in \" + ids, id.getUri(), result.getUri());\r\n    assertEquals(\"credentials in \" + ids, id.getMarshalledCredentials(), result.getMarshalledCredentials());\r\n    assertEquals(\"renewer in \" + ids, renewer, id.getRenewer());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testSessionTokenIdentifierRoundTripNoRenewer",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSessionTokenIdentifierRoundTripNoRenewer() throws Throwable\n{\r\n    SessionTokenIdentifier id = new SessionTokenIdentifier(SESSION_TOKEN_KIND, new Text(), null, landsatUri, new MarshalledCredentials(\"a\", \"b\", \"c\"), new EncryptionSecrets(), \"\");\r\n    SessionTokenIdentifier result = S3ATestUtils.roundTrip(id, null);\r\n    String ids = id.toString();\r\n    assertEquals(\"URI in \" + ids, id.getUri(), result.getUri());\r\n    assertEquals(\"credentials in \" + ids, id.getMarshalledCredentials(), result.getMarshalledCredentials());\r\n    assertEquals(\"renewer in \" + ids, new Text(), id.getRenewer());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testRoleTokenIdentifierRoundTrip",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRoleTokenIdentifierRoundTrip() throws Throwable\n{\r\n    RoleTokenIdentifier id = new RoleTokenIdentifier(landsatUri, new Text(), new Text(), new MarshalledCredentials(\"a\", \"b\", \"c\"), new EncryptionSecrets(), \"\");\r\n    RoleTokenIdentifier result = S3ATestUtils.roundTrip(id, null);\r\n    String ids = id.toString();\r\n    assertEquals(\"URI in \" + ids, id.getUri(), result.getUri());\r\n    assertEquals(\"credentials in \" + ids, id.getMarshalledCredentials(), result.getMarshalledCredentials());\r\n    assertEquals(\"renewer in \" + ids, new Text(), id.getRenewer());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testFullTokenIdentifierRoundTrip",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testFullTokenIdentifierRoundTrip() throws Throwable\n{\r\n    Text renewer = new Text(\"renewerName\");\r\n    FullCredentialsTokenIdentifier id = new FullCredentialsTokenIdentifier(landsatUri, new Text(), renewer, new MarshalledCredentials(\"a\", \"b\", \"\"), new EncryptionSecrets(), \"\");\r\n    FullCredentialsTokenIdentifier result = S3ATestUtils.roundTrip(id, null);\r\n    String ids = id.toString();\r\n    assertEquals(\"URI in \" + ids, id.getUri(), result.getUri());\r\n    assertEquals(\"credentials in \" + ids, id.getMarshalledCredentials(), result.getMarshalledCredentials());\r\n    assertEquals(\"renewer in \" + ids, renewer, result.getRenewer());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getBlockOutputBufferName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockOutputBufferName()\n{\r\n    return Constants.FAST_UPLOAD_BUFFER_DISK;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createFactory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3ADataBlocks.BlockFactory createFactory(S3AFileSystem fileSystem)\n{\r\n    Assume.assumeTrue(\"mark/reset nopt supoprted\", false);\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return \"s3a\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "getTestPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTestPath()\n{\r\n    return S3ATestUtils.createTestPath(super.getTestPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    Configuration c = new Configuration();\r\n    skipIfEncryptionNotSet(c, SSE_KMS);\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getBlockOutputBufferName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockOutputBufferName()\n{\r\n    return Constants.FAST_UPLOAD_BUFFER_ARRAY;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "isEncrypted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isEncrypted(S3AFileSystem fileSystem)\n{\r\n    Configuration c = new Configuration();\r\n    return StringUtils.isNotBlank(getS3EncryptionKey(getTestBucketName(c), c));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "assertEncrypted",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertEncrypted(Path hugeFile) throws IOException\n{\r\n    Configuration c = new Configuration();\r\n    String kmsKey = getS3EncryptionKey(getTestBucketName(c), c);\r\n    EncryptionTestUtils.assertEncrypted(getFileSystem(), hugeFile, SSE_KMS, kmsKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "beforeTest",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void beforeTest()\n{\r\n    SignerForTest1.reset();\r\n    SignerForTest2.reset();\r\n    SignerInitializerForTest.reset();\r\n    SignerForInitializerTest.reset();\r\n    SignerInitializer2ForTest.reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testPredefinedSignerInitialization",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testPredefinedSignerInitialization() throws IOException\n{\r\n    Configuration config = new Configuration();\r\n    config.set(CUSTOM_SIGNERS, \"AWS4SignerType\");\r\n    SignerManager signerManager = new SignerManager(\"dontcare\", null, config, UserGroupInformation.getCurrentUser());\r\n    signerManager.initCustomSigners();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testCustomSignerFailureIfNotRegistered",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCustomSignerFailureIfNotRegistered() throws Exception\n{\r\n    Configuration config = new Configuration();\r\n    config.set(CUSTOM_SIGNERS, \"testsignerUnregistered\");\r\n    SignerManager signerManager = new SignerManager(\"dontcare\", null, config, UserGroupInformation.getCurrentUser());\r\n    signerManager.initCustomSigners();\r\n    LambdaTestUtils.intercept(Exception.class, () -> SignerFactory.createSigner(\"testsignerUnregistered\", null));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testCustomSignerInitialization",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCustomSignerInitialization() throws IOException\n{\r\n    Configuration config = new Configuration();\r\n    config.set(CUSTOM_SIGNERS, \"testsigner1:\" + SignerForTest1.class.getName());\r\n    SignerManager signerManager = new SignerManager(\"dontcare\", null, config, UserGroupInformation.getCurrentUser());\r\n    signerManager.initCustomSigners();\r\n    Signer s1 = SignerFactory.createSigner(\"testsigner1\", null);\r\n    s1.sign(null, null);\r\n    Assertions.assertThat(SignerForTest1.initialized).as(SignerForTest1.class.getName() + \" not initialized\").isEqualTo(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testMultipleCustomSignerInitialization",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testMultipleCustomSignerInitialization() throws IOException\n{\r\n    Configuration config = new Configuration();\r\n    config.set(CUSTOM_SIGNERS, \"testsigner1:\" + SignerForTest1.class.getName() + \",\" + \"testsigner2:\" + SignerForTest2.class.getName());\r\n    SignerManager signerManager = new SignerManager(\"dontcare\", null, config, UserGroupInformation.getCurrentUser());\r\n    signerManager.initCustomSigners();\r\n    Signer s1 = SignerFactory.createSigner(\"testsigner1\", null);\r\n    s1.sign(null, null);\r\n    Assertions.assertThat(SignerForTest1.initialized).as(SignerForTest1.class.getName() + \" not initialized\").isEqualTo(true);\r\n    Signer s2 = SignerFactory.createSigner(\"testsigner2\", null);\r\n    s2.sign(null, null);\r\n    Assertions.assertThat(SignerForTest2.initialized).as(SignerForTest2.class.getName() + \" not initialized\").isEqualTo(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testSimpleSignerInitializer",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSimpleSignerInitializer() throws IOException\n{\r\n    Configuration config = new Configuration();\r\n    config.set(CUSTOM_SIGNERS, \"testsigner1:\" + SignerForTest1.class.getName() + \":\" + SignerInitializerForTest.class.getName());\r\n    Token<? extends TokenIdentifier> token = createTokenForTest(\"identifier\");\r\n    DelegationTokenProvider dtProvider = new DelegationTokenProviderForTest(token);\r\n    UserGroupInformation ugi = UserGroupInformation.createRemoteUser(\"testuser\");\r\n    SignerManager signerManager = new SignerManager(\"bucket1\", dtProvider, config, ugi);\r\n    signerManager.initCustomSigners();\r\n    Assertions.assertThat(SignerInitializerForTest.instanceCount).as(SignerInitializerForTest.class.getName() + \" creation count mismatch\").isEqualTo(1);\r\n    Assertions.assertThat(SignerInitializerForTest.registerCount).as(SignerInitializerForTest.class.getName() + \" registration count mismatch\").isEqualTo(1);\r\n    Assertions.assertThat(SignerInitializerForTest.unregisterCount).as(SignerInitializerForTest.class.getName() + \" registration count mismatch\").isEqualTo(0);\r\n    signerManager.close();\r\n    Assertions.assertThat(SignerInitializerForTest.unregisterCount).as(SignerInitializerForTest.class.getName() + \" registration count mismatch\").isEqualTo(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testMultipleSignerInitializers",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testMultipleSignerInitializers() throws IOException\n{\r\n    Configuration config = new Configuration();\r\n    config.set(CUSTOM_SIGNERS, \"testsigner1:\" + SignerForTest1.class.getName() + \":\" + SignerInitializerForTest.class.getName() + \",\" + \"testsigner2:\" + SignerForTest2.class.getName() + \",\" + \"testsigner3:\" + SignerForTest2.class.getName() + \":\" + SignerInitializer2ForTest.class.getName());\r\n    Token<? extends TokenIdentifier> token = createTokenForTest(\"identifier\");\r\n    DelegationTokenProvider dtProvider = new DelegationTokenProviderForTest(token);\r\n    UserGroupInformation ugi = UserGroupInformation.createRemoteUser(\"testuser\");\r\n    SignerManager signerManager = new SignerManager(\"bucket1\", dtProvider, config, ugi);\r\n    signerManager.initCustomSigners();\r\n    Assertions.assertThat(SignerInitializerForTest.instanceCount).as(SignerInitializerForTest.class.getName() + \" creation count mismatch\").isEqualTo(1);\r\n    Assertions.assertThat(SignerInitializerForTest.registerCount).as(SignerInitializerForTest.class.getName() + \" registration count mismatch\").isEqualTo(1);\r\n    Assertions.assertThat(SignerInitializerForTest.unregisterCount).as(SignerInitializerForTest.class.getName() + \" registration count mismatch\").isEqualTo(0);\r\n    Assertions.assertThat(SignerInitializer2ForTest.instanceCount).as(SignerInitializer2ForTest.class.getName() + \" creation count mismatch\").isEqualTo(1);\r\n    Assertions.assertThat(SignerInitializer2ForTest.registerCount).as(SignerInitializer2ForTest.class.getName() + \" registration count mismatch\").isEqualTo(1);\r\n    Assertions.assertThat(SignerInitializer2ForTest.unregisterCount).as(SignerInitializer2ForTest.class.getName() + \" registration count mismatch\").isEqualTo(0);\r\n    signerManager.close();\r\n    Assertions.assertThat(SignerInitializerForTest.unregisterCount).as(SignerInitializerForTest.class.getName() + \" registration count mismatch\").isEqualTo(1);\r\n    Assertions.assertThat(SignerInitializer2ForTest.unregisterCount).as(SignerInitializer2ForTest.class.getName() + \" registration count mismatch\").isEqualTo(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testSignerInitializerMultipleInstances",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testSignerInitializerMultipleInstances() throws IOException, InterruptedException\n{\r\n    String id1 = \"id1\";\r\n    String id2 = \"id2\";\r\n    String id3 = \"id3\";\r\n    UserGroupInformation ugiU1 = UserGroupInformation.createRemoteUser(TESTUSER1);\r\n    UserGroupInformation ugiU2 = UserGroupInformation.createRemoteUser(TESTUSER2);\r\n    SignerManager signerManagerU1B1 = fakeS3AInstanceCreation(id1, SignerForInitializerTest.class, SignerInitializerForTest.class, BUCKET1, ugiU1);\r\n    SignerManager signerManagerU2B1 = fakeS3AInstanceCreation(id2, SignerForInitializerTest.class, SignerInitializerForTest.class, BUCKET1, ugiU2);\r\n    SignerManager signerManagerU2B2 = fakeS3AInstanceCreation(id3, SignerForInitializerTest.class, SignerInitializerForTest.class, BUCKET2, ugiU2);\r\n    Assertions.assertThat(SignerInitializerForTest.instanceCount).as(SignerInitializerForTest.class.getName() + \" creation count mismatch\").isEqualTo(3);\r\n    Assertions.assertThat(SignerInitializerForTest.registerCount).as(SignerInitializerForTest.class.getName() + \" registration count mismatch\").isEqualTo(3);\r\n    Assertions.assertThat(SignerInitializerForTest.unregisterCount).as(SignerInitializerForTest.class.getName() + \" registration count mismatch\").isEqualTo(0);\r\n    attemptSignAndVerify(id1, BUCKET1, ugiU1, false);\r\n    attemptSignAndVerify(id2, BUCKET1, ugiU2, false);\r\n    attemptSignAndVerify(id3, BUCKET2, ugiU2, false);\r\n    attemptSignAndVerify(\"dontcare\", BUCKET2, ugiU1, true);\r\n    closeAndVerifyNull(signerManagerU1B1, BUCKET1, ugiU1, 2);\r\n    closeAndVerifyNull(signerManagerU2B2, BUCKET2, ugiU2, 1);\r\n    closeAndVerifyNull(signerManagerU2B1, BUCKET1, ugiU2, 0);\r\n    Assertions.assertThat(SignerInitializerForTest.unregisterCount).as(SignerInitializerForTest.class.getName() + \" registration count mismatch\").isEqualTo(3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "attemptSignAndVerify",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void attemptSignAndVerify(String identifier, String bucket, UserGroupInformation ugi, boolean expectNullStoreInfo) throws IOException, InterruptedException\n{\r\n    ugi.doAs((PrivilegedExceptionAction<Void>) () -> {\r\n        Signer signer = new SignerForInitializerTest();\r\n        SignableRequest<?> signableRequest = constructSignableRequest(bucket);\r\n        signer.sign(signableRequest, null);\r\n        verifyStoreValueInSigner(expectNullStoreInfo, bucket, identifier);\r\n        return null;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "verifyStoreValueInSigner",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void verifyStoreValueInSigner(boolean expectNull, String bucketName, String identifier) throws IOException\n{\r\n    if (expectNull) {\r\n        Assertions.assertThat(SignerForInitializerTest.retrievedStoreValue).as(\"Retrieved store value expected to be null\").isNull();\r\n    } else {\r\n        StoreValue storeValue = SignerForInitializerTest.retrievedStoreValue;\r\n        Assertions.assertThat(storeValue).as(\"StoreValue should not be null\").isNotNull();\r\n        Assertions.assertThat(storeValue.getBucketName()).as(\"Bucket Name mismatch\").isEqualTo(bucketName);\r\n        Configuration conf = storeValue.getStoreConf();\r\n        Assertions.assertThat(conf).as(\"Configuration should not be null\").isNotNull();\r\n        Assertions.assertThat(conf.get(TEST_KEY_IDENTIFIER)).as(\"Identifier mistmatch\").isEqualTo(identifier);\r\n        Token<? extends TokenIdentifier> token = storeValue.getDtProvider().getFsDelegationToken();\r\n        String tokenId = new String(token.getIdentifier(), StandardCharsets.UTF_8);\r\n        Assertions.assertThat(tokenId).as(\"Mismatch in delegation token identifier\").isEqualTo(createTokenIdentifierString(identifier, bucketName, UserGroupInformation.getCurrentUser().getShortUserName()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "closeAndVerifyNull",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void closeAndVerifyNull(Closeable closeable, String bucketName, UserGroupInformation ugi, int expectedCount) throws IOException, InterruptedException\n{\r\n    closeable.close();\r\n    attemptSignAndVerify(\"dontcare\", bucketName, ugi, true);\r\n    Assertions.assertThat(SignerInitializerForTest.storeCache.size()).as(\"StoreCache size mismatch\").isEqualTo(expectedCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "createTokenForTest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Token<? extends TokenIdentifier> createTokenForTest(String idString)\n{\r\n    byte[] identifier = idString.getBytes(StandardCharsets.UTF_8);\r\n    byte[] password = \"notapassword\".getBytes(StandardCharsets.UTF_8);\r\n    Token<? extends TokenIdentifier> token = new Token<>(identifier, password, TEST_TOKEN_KIND, TEST_TOKEN_SERVICE);\r\n    return token;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "fakeS3AInstanceCreation",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "SignerManager fakeS3AInstanceCreation(String identifier, Class<? extends Signer> signerClazz, Class<? extends AwsSignerInitializer> signerInitializerClazz, String bucketName, UserGroupInformation ugi)\n{\r\n    Objects.requireNonNull(signerClazz, \"SignerClazz missing\");\r\n    Objects.requireNonNull(signerInitializerClazz, \"SignerInitializerClazzMissing\");\r\n    Configuration config = new Configuration();\r\n    config.set(TEST_KEY_IDENTIFIER, identifier);\r\n    config.set(CUSTOM_SIGNERS, signerClazz.getCanonicalName() + \":\" + signerClazz.getName() + \":\" + signerInitializerClazz.getName());\r\n    Token<? extends TokenIdentifier> token1 = createTokenForTest(createTokenIdentifierString(identifier, bucketName, ugi.getShortUserName()));\r\n    DelegationTokenProvider dtProvider1 = new DelegationTokenProviderForTest(token1);\r\n    SignerManager signerManager = new SignerManager(bucketName, dtProvider1, config, ugi);\r\n    signerManager.initCustomSigners();\r\n    return signerManager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "createTokenIdentifierString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String createTokenIdentifierString(String identifier, String bucketName, String user)\n{\r\n    return identifier + \"_\" + bucketName + \"_\" + user;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "constructSignableRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SignableRequest<?> constructSignableRequest(String bucketName) throws URISyntaxException\n{\r\n    DefaultRequest signableRequest = new DefaultRequest(AmazonWebServiceRequest.NOOP, \"fakeservice\");\r\n    URI uri = new URI(\"s3://\" + bucketName + \"/\");\r\n    signableRequest.setEndpoint(uri);\r\n    return signableRequest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    testPath = path(\"/tests3ascale\");\r\n    LOG.debug(\"Scale test operation count = {}\", getOperationCount());\r\n    enabled = getTestPropertyBool(getConf(), KEY_SCALE_TESTS_ENABLED, DEFAULT_SCALE_TESTS_ENABLED);\r\n    assume(\"Scale test disabled: to enable set property \" + KEY_SCALE_TESTS_ENABLED, isEnabled());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "demandCreateConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration demandCreateConfiguration()\n{\r\n    conf = createScaleConfiguration();\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return demandCreateConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "createScaleConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createScaleConfiguration()\n{\r\n    return super.createConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getTestPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getTestPath()\n{\r\n    return testPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getOperationCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getOperationCount()\n{\r\n    return getConf().getLong(KEY_OPERATION_COUNT, DEFAULT_OPERATION_COUNT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getTestTimeoutSeconds",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTestTimeoutSeconds()\n{\r\n    return getTestPropertyInt(demandCreateConfiguration(), KEY_TEST_TIMEOUT, SCALE_TEST_TIMEOUT_SECONDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return getTestTimeoutSeconds() * 1000;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getInputStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AInputStreamStatistics getInputStreamStatistics(FSDataInputStream in)\n{\r\n    return getS3AInputStream(in).getS3AStreamStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getS3AInputStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AInputStream getS3AInputStream(FSDataInputStream in)\n{\r\n    InputStream inner = in.getWrappedStream();\r\n    if (inner instanceof S3AInputStream) {\r\n        return (S3AInputStream) inner;\r\n    } else {\r\n        throw new AssertionError(\"Not an S3AInputStream: \" + inner);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "gaugeValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long gaugeValue(Statistic statistic)\n{\r\n    return lookupGaugeStatistic(getFileSystem().getIOStatistics(), statistic.getSymbol());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "isEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isEnabled()\n{\r\n    return enabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "isParallelExecution",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isParallelExecution()\n{\r\n    return Boolean.getBoolean(S3ATestConstants.KEY_PARALLEL_TEST_EXECUTION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    resetAuditOptions(conf);\r\n    enableLoggingAuditor(conf);\r\n    conf.set(AUDIT_REQUEST_HANDLERS, SimpleAWSRequestHandler.CLASS);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "iostats",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOStatistics iostats()\n{\r\n    return getFileSystem().getIOStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testInvokeOutOfSpanRejected",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testInvokeOutOfSpanRejected() throws Throwable\n{\r\n    describe(\"Operations against S3 will be rejected outside of a span\");\r\n    final S3AFileSystem fs = getFileSystem();\r\n    final long failures0 = lookupCounterStatistic(iostats(), AUDIT_FAILURE.getSymbol());\r\n    final long exec0 = lookupCounterStatistic(iostats(), AUDIT_REQUEST_EXECUTION.getSymbol());\r\n    fs.createSpan(\"span\", null, null).close();\r\n    final WriteOperationHelper writer = fs.getWriteOperationHelper();\r\n    Assertions.assertThat(writer.getAuditSpan()).matches(s -> !s.isValidSpan(), \"Span is not valid\");\r\n    final AccessDeniedException ex = intercept(AccessDeniedException.class, UNAUDITED_OPERATION, () -> writer.listMultipartUploads(\"/\"));\r\n    if (!(ex.getCause() instanceof AuditFailureException)) {\r\n        throw ex;\r\n    }\r\n    assertThatStatisticCounter(iostats(), AUDIT_REQUEST_EXECUTION.getSymbol()).isGreaterThan(exec0);\r\n    assertThatStatisticCounter(iostats(), AUDIT_FAILURE.getSymbol()).isGreaterThan(failures0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testRequestHandlerBinding",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRequestHandlerBinding() throws Throwable\n{\r\n    describe(\"Verify that extra request handlers can be added and that they\" + \" will be invoked during request execution\");\r\n    final long baseCount = SimpleAWSRequestHandler.getInvocationCount();\r\n    final S3AFileSystem fs = getFileSystem();\r\n    final long exec0 = lookupCounterStatistic(iostats(), AUDIT_REQUEST_EXECUTION.getSymbol());\r\n    fs.listStatus(path(\"/\"));\r\n    Assertions.assertThat(SimpleAWSRequestHandler.getInvocationCount()).describedAs(\"Invocation count of plugged in request handler\").isGreaterThan(baseCount);\r\n    assertThatStatisticCounter(iostats(), AUDIT_REQUEST_EXECUTION.getSymbol()).isGreaterThan(exec0);\r\n    assertThatStatisticCounter(iostats(), AUDIT_FAILURE.getSymbol()).isZero();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "testS3AFilesArePrivate",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testS3AFilesArePrivate() throws Throwable\n{\r\n    S3AFileStatus status = new S3AFileStatus(false, PATH, \"self\");\r\n    assertTrue(\"Not encrypted: \" + status, status.isEncrypted());\r\n    assertNotExecutable(status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "testS3AFilesArePrivateOtherContstructor",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testS3AFilesArePrivateOtherContstructor() throws Throwable\n{\r\n    S3AFileStatus status = new S3AFileStatus(0, 0, PATH, 1, \"self\", null, null);\r\n    assertTrue(\"Not encrypted: \" + status, status.isEncrypted());\r\n    assertNotExecutable(status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "assertNotExecutable",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertNotExecutable(final S3AFileStatus status) throws IOException\n{\r\n    Map<URI, FileStatus> cache = new HashMap<>();\r\n    cache.put(PATH.toUri(), status);\r\n    assertFalse(\"Should not have been executable \" + status, ClientDistributedCacheManager.ancestorsHaveExecutePermissions(null, PATH, cache));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    fs = S3ATestUtils.createTestFileSystem(conf);\r\n    ChangeDetectionPolicy changeDetectionPolicy = getLandsatFS().getChangeDetectionPolicy();\r\n    Assume.assumeFalse(\"the standard landsat bucket doesn't have versioning\", changeDetectionPolicy.getSource() == Source.VersionId && changeDetectionPolicy.isRequireVersion());\r\n    rootPath = path(\"ITestS3SelectMRJob\");\r\n    Path workingDir = path(\"working\");\r\n    fs.setWorkingDirectory(workingDir);\r\n    fs.mkdirs(new Path(rootPath, \"input/\"));\r\n    yarnCluster = new MiniYARNCluster(\"ITestS3SelectMRJob\", 1, 1, 1);\r\n    yarnCluster.init(conf);\r\n    yarnCluster.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    if (yarnCluster != null) {\r\n        yarnCluster.stop();\r\n    }\r\n    super.teardown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testLandsatSelect",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testLandsatSelect() throws Exception\n{\r\n    final Path input = getLandsatGZ();\r\n    final Path output = path(\"testLandsatSelect\").makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n    final Job job = Job.getInstance(conf, \"process level count\");\r\n    job.setJarByClass(WordCount.class);\r\n    job.setMapperClass(WordCount.TokenizerMapper.class);\r\n    job.setCombinerClass(WordCount.IntSumReducer.class);\r\n    job.setReducerClass(WordCount.IntSumReducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    FileInputFormat.addInputPath(job, input);\r\n    FileOutputFormat.setOutputPath(job, output);\r\n    final JobConf jobConf = (JobConf) job.getConfiguration();\r\n    jobConf.set(FS_S3A_COMMITTER_NAME, StagingCommitter.NAME);\r\n    jobConf.setBoolean(FS_S3A_COMMITTER_STAGING_UNIQUE_FILENAMES, false);\r\n    final String query = ITestS3SelectLandsat.SELECT_PROCESSING_LEVEL_NO_LIMIT;\r\n    inputMust(jobConf, SELECT_SQL, query);\r\n    inputMust(jobConf, SELECT_INPUT_COMPRESSION, COMPRESSION_OPT_GZIP);\r\n    inputMust(jobConf, SELECT_INPUT_FORMAT, SELECT_FORMAT_CSV);\r\n    inputMust(jobConf, CSV_INPUT_HEADER, CSV_HEADER_OPT_USE);\r\n    inputMust(jobConf, SELECT_OUTPUT_FORMAT, SELECT_FORMAT_CSV);\r\n    inputMust(jobConf, CSV_OUTPUT_QUOTE_FIELDS, CSV_OUTPUT_QUOTE_FIELDS_AS_NEEEDED);\r\n    enablePassthroughCodec(jobConf, \".gz\");\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"SQL \" + query)) {\r\n        int exitCode = job.waitForCompletion(true) ? 0 : 1;\r\n        assertEquals(\"Returned error code.\", 0, exitCode);\r\n    }\r\n    Path successPath = new Path(output, \"_SUCCESS\");\r\n    SuccessData success = SuccessData.load(fs, successPath);\r\n    LOG.info(\"Job _SUCCESS\\n{}\", success);\r\n    LOG.info(\"Results for query \\n{}\", query);\r\n    final AtomicLong parts = new AtomicLong(0);\r\n    S3AUtils.applyLocatedFiles(fs.listFiles(output, false), (status) -> {\r\n        Path path = status.getPath();\r\n        if (path.getName().startsWith(\"part-\")) {\r\n            parts.incrementAndGet();\r\n            String result = readStringFromFile(path);\r\n            LOG.info(\"{}\\n{}\", path, result);\r\n            String[] lines = result.split(\"\\n\", -1);\r\n            int l = lines.length;\r\n            assertTrue(\"Wrong number of lines (\" + l + \") in \" + result, l > 0 && l < 15);\r\n        }\r\n    });\r\n    assertEquals(\"More part files created than expected\", 1, parts.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "readStringFromFile",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String readStringFromFile(Path path) throws IOException\n{\r\n    int bytesLen = (int) fs.getFileStatus(path).getLen();\r\n    byte[] buffer = new byte[bytesLen];\r\n    return FutureIOSupport.awaitFuture(fs.openFile(path).build().thenApply(in -> {\r\n        try {\r\n            IOUtils.readFully(in, buffer, 0, bytesLen);\r\n            return new String(buffer);\r\n        } catch (IOException ex) {\r\n            throw new UncheckedIOException(ex);\r\n        }\r\n    }));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { \"keep-markers\", true }, { \"delete-markers\", false } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfLocatedFileStatusOnFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCostOfLocatedFileStatusOnFile() throws Throwable\n{\r\n    describe(\"performing listLocatedStatus on a file\");\r\n    Path file = file(methodPath());\r\n    S3AFileSystem fs = getFileSystem();\r\n    verifyMetrics(() -> fs.listLocatedStatus(file), always(FILE_STATUS_FILE_PROBE.plus(LIST_LOCATED_STATUS_LIST_OP)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfListLocatedStatusOnEmptyDir",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCostOfListLocatedStatusOnEmptyDir() throws Throwable\n{\r\n    describe(\"performing listLocatedStatus on an empty dir\");\r\n    Path dir = dir(methodPath());\r\n    S3AFileSystem fs = getFileSystem();\r\n    verifyMetrics(() -> fs.listLocatedStatus(dir), always(LIST_LOCATED_STATUS_LIST_OP.plus(GET_FILE_STATUS_ON_EMPTY_DIR)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfListLocatedStatusOnNonEmptyDir",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCostOfListLocatedStatusOnNonEmptyDir() throws Throwable\n{\r\n    describe(\"performing listLocatedStatus on a non empty dir\");\r\n    Path dir = dir(methodPath());\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path file = file(new Path(dir, \"file.txt\"));\r\n    verifyMetrics(() -> fs.listLocatedStatus(dir), always(LIST_LOCATED_STATUS_LIST_OP));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfListFilesOnFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCostOfListFilesOnFile() throws Throwable\n{\r\n    describe(\"Performing listFiles() on a file\");\r\n    Path file = path(getMethodName() + \".txt\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    touch(fs, file);\r\n    verifyMetrics(() -> fs.listFiles(file, true), always(LIST_LOCATED_STATUS_LIST_OP.plus(GET_FILE_STATUS_ON_FILE)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfListFilesOnEmptyDir",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCostOfListFilesOnEmptyDir() throws Throwable\n{\r\n    describe(\"Perpforming listFiles() on an empty dir with marker\");\r\n    Path dir = path(getMethodName());\r\n    S3AFileSystem fs = getFileSystem();\r\n    fs.mkdirs(dir);\r\n    verifyMetrics(() -> fs.listFiles(dir, true), always(LIST_FILES_LIST_OP.plus(GET_FILE_STATUS_ON_EMPTY_DIR)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfListFilesOnNonEmptyDir",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCostOfListFilesOnNonEmptyDir() throws Throwable\n{\r\n    describe(\"Performing listFiles() on a non empty dir\");\r\n    Path dir = path(getMethodName());\r\n    S3AFileSystem fs = getFileSystem();\r\n    fs.mkdirs(dir);\r\n    Path file = new Path(dir, \"file.txt\");\r\n    touch(fs, file);\r\n    verifyMetrics(() -> fs.listFiles(dir, true), always(LIST_FILES_LIST_OP));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfListFilesOnNonExistingDir",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCostOfListFilesOnNonExistingDir() throws Throwable\n{\r\n    describe(\"Performing listFiles() on a non existing dir\");\r\n    Path dir = path(getMethodName());\r\n    S3AFileSystem fs = getFileSystem();\r\n    verifyMetricsIntercepting(FileNotFoundException.class, \"\", () -> fs.listFiles(dir, true), always(LIST_FILES_LIST_OP.plus(GET_FILE_STATUS_FNFE)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfListStatusOnFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCostOfListStatusOnFile() throws Throwable\n{\r\n    describe(\"Performing listStatus() on a file\");\r\n    Path file = path(getMethodName() + \".txt\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    touch(fs, file);\r\n    verifyMetrics(() -> fs.listStatus(file), always(LIST_STATUS_LIST_OP.plus(GET_FILE_STATUS_ON_FILE)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfListStatusOnEmptyDir",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCostOfListStatusOnEmptyDir() throws Throwable\n{\r\n    describe(\"Performing listStatus() on an empty dir\");\r\n    Path dir = path(getMethodName());\r\n    S3AFileSystem fs = getFileSystem();\r\n    fs.mkdirs(dir);\r\n    verifyMetrics(() -> fs.listStatus(dir), always(LIST_STATUS_LIST_OP.plus(GET_FILE_STATUS_ON_EMPTY_DIR)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfListStatusOnNonEmptyDir",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCostOfListStatusOnNonEmptyDir() throws Throwable\n{\r\n    describe(\"Performing listStatus() on a non empty dir\");\r\n    Path dir = path(getMethodName());\r\n    S3AFileSystem fs = getFileSystem();\r\n    fs.mkdirs(dir);\r\n    Path file = new Path(dir, \"file.txt\");\r\n    touch(fs, file);\r\n    verifyMetrics(() -> fs.listStatus(dir), always(LIST_STATUS_LIST_OP));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfGetFileStatusOnFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCostOfGetFileStatusOnFile() throws Throwable\n{\r\n    describe(\"performing getFileStatus on a file\");\r\n    Path simpleFile = file(methodPath());\r\n    S3AFileStatus status = verifyInnerGetFileStatus(simpleFile, true, StatusProbeEnum.ALL, GET_FILE_STATUS_ON_FILE);\r\n    assertTrue(\"not a file: \" + status, status.isFile());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfGetFileStatusOnEmptyDir",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCostOfGetFileStatusOnEmptyDir() throws Throwable\n{\r\n    describe(\"performing getFileStatus on an empty directory\");\r\n    Path dir = dir(methodPath());\r\n    S3AFileStatus status = verifyInnerGetFileStatus(dir, true, StatusProbeEnum.ALL, GET_FILE_STATUS_ON_DIR_MARKER);\r\n    assertSame(\"not empty: \" + status, Tristate.TRUE, status.isEmptyDirectory());\r\n    verifyInnerGetFileStatus(dir, false, StatusProbeEnum.DIRECTORIES, FILE_STATUS_DIR_PROBE);\r\n    isDir(dir, true, FILE_STATUS_DIR_PROBE);\r\n    isFile(dir, false, FILE_STATUS_FILE_PROBE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfGetFileStatusOnMissingFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCostOfGetFileStatusOnMissingFile() throws Throwable\n{\r\n    describe(\"performing getFileStatus on a missing file\");\r\n    interceptGetFileStatusFNFE(methodPath(), false, StatusProbeEnum.ALL, GET_FILE_STATUS_FNFE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfRootFileStatus",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCostOfRootFileStatus() throws Throwable\n{\r\n    Path root = path(\"/\");\r\n    S3AFileStatus rootStatus = verifyInnerGetFileStatus(root, false, StatusProbeEnum.ALL, ROOT_FILE_STATUS_PROBE);\r\n    String rootStatusContent = rootStatus.toString();\r\n    Assertions.assertThat(rootStatus.isDirectory()).describedAs(\"Status returned should be a directory \" + rootStatusContent).isEqualTo(true);\r\n    Assertions.assertThat(rootStatus.isEmptyDirectory()).isEqualTo(Tristate.UNKNOWN);\r\n    rootStatus = verifyInnerGetFileStatus(root, true, StatusProbeEnum.ALL, FILE_STATUS_DIR_PROBE);\r\n    Assertions.assertThat(rootStatus.isDirectory()).describedAs(\"Status returned should be a directory \" + rootStatusContent).isEqualTo(true);\r\n    Assertions.assertThat(rootStatus.isEmptyDirectory()).isNotEqualByComparingTo(Tristate.UNKNOWN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testIsDirIsFileMissingPath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testIsDirIsFileMissingPath() throws Throwable\n{\r\n    describe(\"performing isDir and isFile on a missing file\");\r\n    Path path = methodPath();\r\n    isDir(path, false, FILE_STATUS_DIR_PROBE);\r\n    isFile(path, false, FILE_STATUS_FILE_PROBE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfGetFileStatusOnNonEmptyDir",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCostOfGetFileStatusOnNonEmptyDir() throws Throwable\n{\r\n    describe(\"performing getFileStatus on a non-empty directory\");\r\n    Path dir = dir(methodPath());\r\n    file(new Path(dir, \"simple.txt\"));\r\n    S3AFileStatus status = verifyInnerGetFileStatus(dir, true, StatusProbeEnum.ALL, GET_FILE_STATUS_ON_DIR);\r\n    assertEmptyDirStatus(status, Tristate.FALSE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfCopyFromLocalFile",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCostOfCopyFromLocalFile() throws Throwable\n{\r\n    describe(\"testCostOfCopyFromLocalFile\");\r\n    File localTestDir = getTestDir(\"tmp\");\r\n    localTestDir.mkdirs();\r\n    File tmpFile = File.createTempFile(\"tests3acost\", \".txt\", localTestDir);\r\n    tmpFile.delete();\r\n    try {\r\n        URI localFileURI = tmpFile.toURI();\r\n        FileSystem localFS = FileSystem.get(localFileURI, getFileSystem().getConf());\r\n        Path localPath = new Path(localFileURI);\r\n        int len = 10 * 1024;\r\n        byte[] data = dataset(len, 'A', 'Z');\r\n        writeDataset(localFS, localPath, data, len, 1024, true);\r\n        S3AFileSystem s3a = getFileSystem();\r\n        Path remotePath = methodPath();\r\n        verifyMetrics(() -> {\r\n            s3a.copyFromLocalFile(false, true, localPath, remotePath);\r\n            return \"copy\";\r\n        }, with(INVOCATION_COPY_FROM_LOCAL_FILE, 1), with(OBJECT_PUT_REQUESTS, 1), with(OBJECT_PUT_BYTES, len));\r\n        verifyFileContents(s3a, remotePath, data);\r\n        LOG.info(\"Filesystem {}\", s3a);\r\n    } finally {\r\n        tmpFile.delete();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testDirProbes",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testDirProbes() throws Throwable\n{\r\n    describe(\"Test directory probe cost\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path emptydir = dir(methodPath());\r\n    interceptGetFileStatusFNFE(emptydir, false, StatusProbeEnum.HEAD_ONLY, FILE_STATUS_FILE_PROBE);\r\n    S3AFileStatus status = verifyInnerGetFileStatus(emptydir, true, StatusProbeEnum.LIST_ONLY, FILE_STATUS_DIR_PROBE);\r\n    assertEmptyDirStatus(status, Tristate.TRUE);\r\n    interceptGetFileStatusFNFE(emptydir, false, EnumSet.noneOf(StatusProbeEnum.class), NO_IO);\r\n    String emptyDirTrailingSlash = fs.pathToKey(emptydir.getParent()) + \"/\" + emptydir.getName() + \"/\";\r\n    interceptOperation(FileNotFoundException.class, \"\", NO_IO, () -> fs.s3GetFileStatus(emptydir, emptyDirTrailingSlash, StatusProbeEnum.HEAD_ONLY, false));\r\n    status = verify(FILE_STATUS_DIR_PROBE, () -> fs.s3GetFileStatus(emptydir, emptyDirTrailingSlash, StatusProbeEnum.LIST_ONLY, true));\r\n    assertEquals(emptydir, status.getPath());\r\n    assertEmptyDirStatus(status, Tristate.TRUE);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testNeedEmptyDirectoryProbeRequiresList",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testNeedEmptyDirectoryProbeRequiresList() throws Throwable\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    intercept(IllegalArgumentException.class, \"\", () -> fs.s3GetFileStatus(new Path(\"/something\"), \"/something\", StatusProbeEnum.HEAD_ONLY, true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCreateCost",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCreateCost() throws Throwable\n{\r\n    describe(\"Test file creation cost\");\r\n    Path testFile = methodPath();\r\n    create(testFile, false, CREATE_FILE_NO_OVERWRITE);\r\n    create(testFile, true, CREATE_FILE_OVERWRITE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCreateCostFileExists",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCreateCostFileExists() throws Throwable\n{\r\n    describe(\"Test cost of create file failing with existing file\");\r\n    Path testFile = file(methodPath());\r\n    interceptOperation(FileAlreadyExistsException.class, \"\", FILE_STATUS_FILE_PROBE, () -> file(testFile, false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCreateCostDirExists",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCreateCostDirExists() throws Throwable\n{\r\n    describe(\"Test cost of create file failing with existing dir\");\r\n    Path testFile = dir(methodPath());\r\n    interceptOperation(FileAlreadyExistsException.class, \"\", GET_FILE_STATUS_ON_DIR_MARKER, () -> file(testFile, false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCreateBuilder",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCreateBuilder() throws Throwable\n{\r\n    describe(\"Test builder file creation cost\");\r\n    Path testFile = methodPath();\r\n    dir(testFile.getParent());\r\n    buildFile(testFile, false, false, GET_FILE_STATUS_FNFE.plus(FILE_STATUS_DIR_PROBE));\r\n    buildFile(testFile, true, true, FILE_STATUS_DIR_PROBE);\r\n    interceptOperation(FileAlreadyExistsException.class, \"\", GET_FILE_STATUS_ON_FILE, () -> buildFile(testFile, false, true, GET_FILE_STATUS_ON_FILE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfGlobStatus",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCostOfGlobStatus() throws Throwable\n{\r\n    describe(\"Test globStatus has expected cost\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path basePath = path(\"testCostOfGlobStatus/nextFolder/\");\r\n    int filesToCreate = 10;\r\n    for (int i = 0; i < filesToCreate; i++) {\r\n        create(basePath.suffix(\"/\" + i));\r\n    }\r\n    fs.globStatus(basePath.suffix(\"/*\"));\r\n    verify(LIST_STATUS_LIST_OP, () -> fs.globStatus(basePath.suffix(\"/*\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCostOfGlobStatusNoSymlinkResolution",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCostOfGlobStatusNoSymlinkResolution() throws Throwable\n{\r\n    describe(\"Test globStatus does not attempt to resolve symlinks\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path basePath = path(\"testCostOfGlobStatusNoSymlinkResolution/f/\");\r\n    String fileName = \"/notASymlinkDOntResolveMeLikeOne\";\r\n    create(basePath.suffix(fileName));\r\n    verify(LIST_STATUS_LIST_OP, () -> fs.globStatus(basePath.suffix(\"/*\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "createScaleConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createScaleConfiguration()\n{\r\n    Configuration configuration = super.createScaleConfiguration();\r\n    configuration.setBoolean(Constants.ENABLE_MULTI_DELETE, false);\r\n    return configuration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "suitename",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String suitename()\n{\r\n    return \"ITestDirectoryCommitProtocol\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "getCommitterName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCommitterName()\n{\r\n    return CommitConstants.COMMITTER_NAME_DIRECTORY;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "createCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractS3ACommitter createCommitter(Path outputPath, TaskAttemptContext context) throws IOException\n{\r\n    return new DirectoryStagingCommitter(outputPath, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "createFailingCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractS3ACommitter createFailingCommitter(TaskAttemptContext tContext) throws IOException\n{\r\n    return new CommitterWithFailedThenSucceed(getOutDir(), tContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "testValidateDefaultConflictMode",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testValidateDefaultConflictMode() throws Throwable\n{\r\n    describe(\"Checking default conflict mode adoption\");\r\n    Configuration baseConf = new Configuration(true);\r\n    String[] sources = baseConf.getPropertySources(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE);\r\n    String sourceStr = Arrays.stream(sources).collect(Collectors.joining(\",\"));\r\n    String baseConfVal = baseConf.getTrimmed(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE);\r\n    assertEquals(\"conflict mode in core config from \" + sourceStr, CONFLICT_MODE_APPEND, baseConfVal);\r\n    Configuration fsConf = getFileSystem().getConf();\r\n    String conflictModeDefVal = fsConf.getTrimmed(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE);\r\n    assertEquals(\"conflict mode in filesystem\", CONFLICT_MODE_APPEND, conflictModeDefVal);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getKdc",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MiniKdc getKdc()\n{\r\n    return kdc;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getKeytab",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "File getKeytab()\n{\r\n    return keytab;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getKeytabPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getKeytabPath()\n{\r\n    return keytab.getAbsolutePath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createBobUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UserGroupInformation createBobUser() throws IOException\n{\r\n    return loginUserFromKeytabAndReturnUGI(bobPrincipal, keytab.getAbsolutePath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createAliceUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UserGroupInformation createAliceUser() throws IOException\n{\r\n    return loginUserFromKeytabAndReturnUGI(alicePrincipal, keytab.getAbsolutePath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getWorkDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "File getWorkDir()\n{\r\n    return workDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getKrbInstance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getKrbInstance()\n{\r\n    return krbInstance;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getLoginUsername",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLoginUsername()\n{\r\n    return loginUsername;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getLoginPrincipal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLoginPrincipal()\n{\r\n    return loginPrincipal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "withRealm",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String withRealm(String user)\n{\r\n    return user + \"@EXAMPLE.COM\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void serviceInit(final Configuration conf) throws Exception\n{\r\n    patchConfigAtInit(conf);\r\n    super.serviceInit(conf);\r\n    Properties kdcConf = MiniKdc.createConf();\r\n    workDir = GenericTestUtils.getTestDir(\"kerberos\");\r\n    workDir.mkdirs();\r\n    kdc = new MiniKdc(kdcConf, workDir);\r\n    krbInstance = LOCALHOST_NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    super.serviceStart();\r\n    kdc.start();\r\n    keytab = new File(workDir, \"keytab.bin\");\r\n    loginUsername = UserGroupInformation.getLoginUser().getShortUserName();\r\n    loginPrincipal = loginUsername + \"/\" + krbInstance;\r\n    alicePrincipal = ALICE + \"/\" + krbInstance;\r\n    bobPrincipal = BOB + \"/\" + krbInstance;\r\n    kdc.createPrincipal(keytab, alicePrincipal, bobPrincipal, \"HTTP/\" + krbInstance, HTTP_LOCALHOST, loginPrincipal);\r\n    final File keystoresDir = new File(workDir, \"ssl\");\r\n    keystoresDir.mkdirs();\r\n    sslConfDir = KeyStoreTestUtil.getClasspathDir(this.getClass());\r\n    KeyStoreTestUtil.setupSSLConfig(keystoresDir.getAbsolutePath(), sslConfDir, getConfig(), false);\r\n    clientSSLConfigFileName = KeyStoreTestUtil.getClientSSLConfigFileName();\r\n    serverSSLConfigFileName = KeyStoreTestUtil.getServerSSLConfigFileName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    super.serviceStop();\r\n    kdc.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "patchConfigAtInit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void patchConfigAtInit(final Configuration conf)\n{\r\n    int timeout = 60 * 60_1000;\r\n    conf.setInt(\"jvm.pause.info-threshold.ms\", timeout);\r\n    conf.setInt(\"jvm.pause.warn-threshold.ms\", timeout);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "patchConfigWithHDFSBindings",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void patchConfigWithHDFSBindings(final Configuration conf)\n{\r\n    Preconditions.checkState(isInState(STATE.STARTED));\r\n    enableKerberos(conf);\r\n    String path = getKeytabPath();\r\n    String spnegoPrincipal = \"*\";\r\n    String localhost = LOCALHOST_NAME;\r\n    String instance = getKrbInstance();\r\n    String hdfsPrincipal = getLoginPrincipal();\r\n    patchConfigAtInit(conf);\r\n    conf.setLong(CommonConfigurationKeys.FS_DU_INTERVAL_KEY, Long.MAX_VALUE);\r\n    conf.set(DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, hdfsPrincipal);\r\n    conf.set(DFS_NAMENODE_KEYTAB_FILE_KEY, path);\r\n    conf.set(DFS_DATANODE_KERBEROS_PRINCIPAL_KEY, hdfsPrincipal);\r\n    conf.set(DFS_DATANODE_KEYTAB_FILE_KEY, path);\r\n    conf.set(DFS_WEB_AUTHENTICATION_KERBEROS_PRINCIPAL_KEY, spnegoPrincipal);\r\n    conf.set(DFS_JOURNALNODE_KEYTAB_FILE_KEY, path);\r\n    conf.set(DFS_JOURNALNODE_KERBEROS_PRINCIPAL_KEY, hdfsPrincipal);\r\n    conf.set(DFS_JOURNALNODE_KERBEROS_INTERNAL_SPNEGO_PRINCIPAL_KEY, spnegoPrincipal);\r\n    conf.setBoolean(DFS_BLOCK_ACCESS_TOKEN_ENABLE_KEY, true);\r\n    conf.set(DFS_DATA_TRANSFER_PROTECTION_KEY, \"authentication\");\r\n    conf.set(DFS_HTTP_POLICY_KEY, HttpConfig.Policy.HTTPS_ONLY.name());\r\n    conf.set(DFS_NAMENODE_HTTPS_ADDRESS_KEY, \"localhost:0\");\r\n    conf.set(DFS_DATANODE_HTTPS_ADDRESS_KEY, \"localhost:0\");\r\n    conf.set(DFS_HTTP_POLICY_KEY, HttpConfig.Policy.HTTPS_ONLY.name());\r\n    conf.set(DFS_CLIENT_HTTPS_KEYSTORE_RESOURCE_KEY, KeyStoreTestUtil.getClientSSLConfigFileName());\r\n    conf.set(DFS_SERVER_HTTPS_KEYSTORE_RESOURCE_KEY, KeyStoreTestUtil.getServerSSLConfigFileName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "patchConfigWithYARNBindings",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void patchConfigWithYARNBindings(final Configuration conf)\n{\r\n    Preconditions.checkState(isInState(STATE.STARTED));\r\n    enableKerberos(conf);\r\n    patchConfigAtInit(conf);\r\n    String path = getKeytabPath();\r\n    String localhost = LOCALHOST_NAME;\r\n    String yarnPrincipal = withRealm(getLoginPrincipal());\r\n    conf.set(RM_PRINCIPAL, yarnPrincipal);\r\n    conf.set(RM_KEYTAB, path);\r\n    conf.set(RM_HOSTNAME, localhost);\r\n    conf.set(RM_BIND_HOST, localhost);\r\n    conf.set(RM_ADDRESS, localhost + \":\" + DEFAULT_RM_PORT);\r\n    conf.set(NM_PRINCIPAL, yarnPrincipal);\r\n    conf.set(NM_KEYTAB, path);\r\n    conf.set(NM_ADDRESS, localhost + \":\" + DEFAULT_NM_PORT);\r\n    conf.setBoolean(TIMELINE_SERVICE_ENABLED, false);\r\n    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_EMIT_TIMELINE_DATA, false);\r\n    conf.set(JHAdminConfig.MR_HISTORY_KEYTAB, path);\r\n    conf.set(JHAdminConfig.MR_HISTORY_PRINCIPAL, yarnPrincipal);\r\n    conf.set(JHAdminConfig.MR_HISTORY_ADDRESS, localhost + \":\" + DEFAULT_MR_HISTORY_PORT);\r\n    conf.setBoolean(JHAdminConfig.MR_HISTORY_CLEANER_ENABLE, false);\r\n    conf.setInt(RM_AM_MAX_ATTEMPTS, 1);\r\n    conf.setInt(YarnConfiguration.RESOURCEMANAGER_CONNECT_MAX_WAIT_MS, 100);\r\n    conf.setInt(YarnConfiguration.RESOURCEMANAGER_CONNECT_RETRY_INTERVAL_MS, 10_000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "resetUGI",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void resetUGI()\n{\r\n    UserGroupInformation.reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "userOnHost",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String userOnHost(final String shortname)\n{\r\n    return shortname + \"/\" + krbInstance + \"@\" + getRealm();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getRealm",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRealm()\n{\r\n    return kdc.getRealm();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "loginUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void loginUser(final String user) throws IOException\n{\r\n    UserGroupInformation.loginUserFromKeytab(user, getKeytabPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "loginPrincipal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void loginPrincipal() throws IOException\n{\r\n    loginUser(getLoginPrincipal());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "assertSecurityEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertSecurityEnabled()\n{\r\n    assertTrue(\"Security is needed for this test\", UserGroupInformation.isSecurityEnabled());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "closeUserFileSystems",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void closeUserFileSystems(UserGroupInformation ugi) throws IOException\n{\r\n    if (ugi != null) {\r\n        FileSystem.closeAllForUGI(ugi);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "enableKerberos",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void enableKerberos(Configuration conf)\n{\r\n    conf.set(HADOOP_SECURITY_AUTHENTICATION, UserGroupInformation.AuthenticationMethod.KERBEROS.name());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEndpoint",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testEndpoint() throws Exception\n{\r\n    conf = new Configuration();\r\n    String endpoint = conf.getTrimmed(S3ATestConstants.CONFIGURATION_TEST_ENDPOINT, \"\");\r\n    if (endpoint.isEmpty()) {\r\n        LOG.warn(\"Custom endpoint test skipped as \" + S3ATestConstants.CONFIGURATION_TEST_ENDPOINT + \"config \" + \"setting was not detected\");\r\n    } else {\r\n        conf.set(Constants.ENDPOINT, endpoint);\r\n        fs = S3ATestUtils.createTestFileSystem(conf);\r\n        AmazonS3 s3 = fs.getAmazonS3ClientForTesting(\"test endpoint\");\r\n        String endPointRegion = \"\";\r\n        String[] endpointParts = StringUtils.split(endpoint, '.');\r\n        if (endpointParts.length == 3) {\r\n            endPointRegion = endpointParts[0].substring(3);\r\n        } else if (endpointParts.length == 4) {\r\n            endPointRegion = endpointParts[1];\r\n        } else {\r\n            fail(\"Unexpected endpoint\");\r\n        }\r\n        assertEquals(\"Endpoint config setting and bucket location differ: \", endPointRegion, s3.getBucketLocation(fs.getUri().getHost()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testProxyConnection",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testProxyConnection() throws Exception\n{\r\n    useFailFastConfiguration();\r\n    conf.set(Constants.PROXY_HOST, \"127.0.0.1\");\r\n    conf.setInt(Constants.PROXY_PORT, 1);\r\n    String proxy = conf.get(Constants.PROXY_HOST) + \":\" + conf.get(Constants.PROXY_PORT);\r\n    expectFSCreateFailure(AWSClientIOException.class, conf, \"when using proxy \" + proxy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "useFailFastConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void useFailFastConfiguration()\n{\r\n    conf = new Configuration();\r\n    conf.setInt(Constants.MAX_ERROR_RETRIES, 2);\r\n    conf.setInt(Constants.RETRY_LIMIT, 2);\r\n    conf.set(RETRY_INTERVAL, \"100ms\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "expectFSCreateFailure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "E expectFSCreateFailure(Class<E> clazz, Configuration conf, String text) throws Exception\n{\r\n    return intercept(clazz, () -> {\r\n        fs = S3ATestUtils.createTestFileSystem(conf);\r\n        fs.listFiles(new Path(\"/\"), false);\r\n        return \"expected failure creating FS \" + text + \" got \" + fs;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testProxyPortWithoutHost",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testProxyPortWithoutHost() throws Exception\n{\r\n    useFailFastConfiguration();\r\n    conf.unset(Constants.PROXY_HOST);\r\n    conf.setInt(Constants.PROXY_PORT, 1);\r\n    IllegalArgumentException e = expectFSCreateFailure(IllegalArgumentException.class, conf, \"Expected a connection error for proxy server\");\r\n    String msg = e.toString();\r\n    if (!msg.contains(Constants.PROXY_HOST) && !msg.contains(Constants.PROXY_PORT)) {\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testAutomaticProxyPortSelection",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testAutomaticProxyPortSelection() throws Exception\n{\r\n    useFailFastConfiguration();\r\n    conf.unset(Constants.PROXY_PORT);\r\n    conf.set(Constants.PROXY_HOST, \"127.0.0.1\");\r\n    conf.set(Constants.SECURE_CONNECTIONS, \"true\");\r\n    expectFSCreateFailure(AWSClientIOException.class, conf, \"Expected a connection error for proxy server\");\r\n    conf.set(Constants.SECURE_CONNECTIONS, \"false\");\r\n    expectFSCreateFailure(AWSClientIOException.class, conf, \"Expected a connection error for proxy server\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testUsernameInconsistentWithPassword",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testUsernameInconsistentWithPassword() throws Exception\n{\r\n    useFailFastConfiguration();\r\n    conf.set(Constants.PROXY_HOST, \"127.0.0.1\");\r\n    conf.setInt(Constants.PROXY_PORT, 1);\r\n    conf.set(Constants.PROXY_USERNAME, \"user\");\r\n    IllegalArgumentException e = expectFSCreateFailure(IllegalArgumentException.class, conf, \"Expected a connection error for proxy server\");\r\n    assertIsProxyUsernameError(e);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertIsProxyUsernameError",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertIsProxyUsernameError(final IllegalArgumentException e)\n{\r\n    String msg = e.toString();\r\n    if (!msg.contains(Constants.PROXY_USERNAME) && !msg.contains(Constants.PROXY_PASSWORD)) {\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testUsernameInconsistentWithPassword2",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testUsernameInconsistentWithPassword2() throws Exception\n{\r\n    useFailFastConfiguration();\r\n    conf.set(Constants.PROXY_HOST, \"127.0.0.1\");\r\n    conf.setInt(Constants.PROXY_PORT, 1);\r\n    conf.set(Constants.PROXY_PASSWORD, \"password\");\r\n    IllegalArgumentException e = expectFSCreateFailure(IllegalArgumentException.class, conf, \"Expected a connection error for proxy server\");\r\n    assertIsProxyUsernameError(e);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCredsFromCredentialProvider",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCredsFromCredentialProvider() throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    final File file = tempDir.newFile(\"test.jks\");\r\n    final URI jks = ProviderUtils.nestURIForLocalJavaKeyStoreProvider(file.toURI());\r\n    conf.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, jks.toString());\r\n    provisionAccessKeys(conf);\r\n    conf.set(Constants.ACCESS_KEY, EXAMPLE_ID + \"LJM\");\r\n    S3xLoginHelper.Login creds = S3AUtils.getAWSAccessKeys(new URI(\"s3a://foobar\"), conf);\r\n    assertEquals(\"AccessKey incorrect.\", EXAMPLE_ID, creds.getUser());\r\n    assertEquals(\"SecretKey incorrect.\", EXAMPLE_KEY, creds.getPassword());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "provisionAccessKeys",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void provisionAccessKeys(final Configuration conf) throws Exception\n{\r\n    final CredentialProvider provider = CredentialProviderFactory.getProviders(conf).get(0);\r\n    provider.createCredentialEntry(Constants.ACCESS_KEY, EXAMPLE_ID.toCharArray());\r\n    provider.createCredentialEntry(Constants.SECRET_KEY, EXAMPLE_KEY.toCharArray());\r\n    provider.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSecretFromCredentialProviderIDFromConfig",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSecretFromCredentialProviderIDFromConfig() throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    final File file = tempDir.newFile(\"test.jks\");\r\n    final URI jks = ProviderUtils.nestURIForLocalJavaKeyStoreProvider(file.toURI());\r\n    conf.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, jks.toString());\r\n    final CredentialProvider provider = CredentialProviderFactory.getProviders(conf).get(0);\r\n    provider.createCredentialEntry(Constants.SECRET_KEY, EXAMPLE_KEY.toCharArray());\r\n    provider.flush();\r\n    conf.set(Constants.ACCESS_KEY, EXAMPLE_ID);\r\n    S3xLoginHelper.Login creds = S3AUtils.getAWSAccessKeys(new URI(\"s3a://foobar\"), conf);\r\n    assertEquals(\"AccessKey incorrect.\", EXAMPLE_ID, creds.getUser());\r\n    assertEquals(\"SecretKey incorrect.\", EXAMPLE_KEY, creds.getPassword());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testIDFromCredentialProviderSecretFromConfig",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testIDFromCredentialProviderSecretFromConfig() throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    final File file = tempDir.newFile(\"test.jks\");\r\n    final URI jks = ProviderUtils.nestURIForLocalJavaKeyStoreProvider(file.toURI());\r\n    conf.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, jks.toString());\r\n    final CredentialProvider provider = CredentialProviderFactory.getProviders(conf).get(0);\r\n    provider.createCredentialEntry(Constants.ACCESS_KEY, EXAMPLE_ID.toCharArray());\r\n    provider.flush();\r\n    conf.set(Constants.SECRET_KEY, EXAMPLE_KEY);\r\n    S3xLoginHelper.Login creds = S3AUtils.getAWSAccessKeys(new URI(\"s3a://foobar\"), conf);\r\n    assertEquals(\"AccessKey incorrect.\", EXAMPLE_ID, creds.getUser());\r\n    assertEquals(\"SecretKey incorrect.\", EXAMPLE_KEY, creds.getPassword());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testExcludingS3ACredentialProvider",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testExcludingS3ACredentialProvider() throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    final File file = tempDir.newFile(\"test.jks\");\r\n    final URI jks = ProviderUtils.nestURIForLocalJavaKeyStoreProvider(file.toURI());\r\n    conf.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, \"jceks://s3a/foobar,\" + jks.toString());\r\n    Configuration c = ProviderUtils.excludeIncompatibleCredentialProviders(conf, S3AFileSystem.class);\r\n    String newPath = conf.get(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH);\r\n    assertFalse(\"Provider Path incorrect\", newPath.contains(\"s3a://\"));\r\n    provisionAccessKeys(c);\r\n    conf.set(Constants.ACCESS_KEY, EXAMPLE_ID + \"LJM\");\r\n    URI uri2 = new URI(\"s3a://foobar\");\r\n    S3xLoginHelper.Login creds = S3AUtils.getAWSAccessKeys(uri2, conf);\r\n    assertEquals(\"AccessKey incorrect.\", EXAMPLE_ID, creds.getUser());\r\n    assertEquals(\"SecretKey incorrect.\", EXAMPLE_KEY, creds.getPassword());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "shouldBeAbleToSwitchOnS3PathStyleAccessViaConfigProperty",
  "errType" : [ "AWSS3IOException", "IllegalArgumentException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void shouldBeAbleToSwitchOnS3PathStyleAccessViaConfigProperty() throws Exception\n{\r\n    conf = new Configuration();\r\n    conf.set(Constants.PATH_STYLE_ACCESS, Boolean.toString(true));\r\n    assertTrue(conf.getBoolean(Constants.PATH_STYLE_ACCESS, false));\r\n    try {\r\n        fs = S3ATestUtils.createTestFileSystem(conf);\r\n        assertNotNull(fs);\r\n        AmazonS3 s3 = fs.getAmazonS3ClientForTesting(\"configuration\");\r\n        assertNotNull(s3);\r\n        S3ClientOptions clientOptions = getField(s3, S3ClientOptions.class, \"clientOptions\");\r\n        assertTrue(\"Expected to find path style access to be switched on!\", clientOptions.isPathStyleAccess());\r\n        byte[] file = ContractTestUtils.toAsciiByteArray(\"test file\");\r\n        ContractTestUtils.writeAndRead(fs, new Path(\"/path/style/access/testFile\"), file, file.length, (int) conf.getLongBytes(Constants.FS_S3A_BLOCK_SIZE, file.length), false, true);\r\n    } catch (final AWSS3IOException e) {\r\n        LOG.error(\"Caught exception: \", e);\r\n        assertEquals(HttpStatus.SC_MOVED_PERMANENTLY, e.getStatusCode());\r\n    } catch (final IllegalArgumentException e) {\r\n        if (!fs.getBucket().contains(\"arn:\")) {\r\n            LOG.error(\"Caught unexpected exception: \", e);\r\n            throw e;\r\n        }\r\n        GenericTestUtils.assertExceptionContains(AP_ILLEGAL_ACCESS, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testDefaultUserAgent",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testDefaultUserAgent() throws Exception\n{\r\n    conf = new Configuration();\r\n    fs = S3ATestUtils.createTestFileSystem(conf);\r\n    assertNotNull(fs);\r\n    AmazonS3 s3 = fs.getAmazonS3ClientForTesting(\"User Agent\");\r\n    assertNotNull(s3);\r\n    ClientConfiguration awsConf = getField(s3, ClientConfiguration.class, \"clientConfiguration\");\r\n    assertEquals(\"Hadoop \" + VersionInfo.getVersion(), awsConf.getUserAgentPrefix());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCustomUserAgent",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCustomUserAgent() throws Exception\n{\r\n    conf = new Configuration();\r\n    conf.set(Constants.USER_AGENT_PREFIX, \"MyApp\");\r\n    fs = S3ATestUtils.createTestFileSystem(conf);\r\n    assertNotNull(fs);\r\n    AmazonS3 s3 = fs.getAmazonS3ClientForTesting(\"User agent\");\r\n    assertNotNull(s3);\r\n    ClientConfiguration awsConf = getField(s3, ClientConfiguration.class, \"clientConfiguration\");\r\n    assertEquals(\"MyApp, Hadoop \" + VersionInfo.getVersion(), awsConf.getUserAgentPrefix());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRequestTimeout",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRequestTimeout() throws Exception\n{\r\n    conf = new Configuration();\r\n    conf.set(REQUEST_TIMEOUT, \"120\");\r\n    fs = S3ATestUtils.createTestFileSystem(conf);\r\n    AmazonS3 s3 = fs.getAmazonS3ClientForTesting(\"Request timeout (ms)\");\r\n    ClientConfiguration awsConf = getField(s3, ClientConfiguration.class, \"clientConfiguration\");\r\n    assertEquals(\"Configured \" + REQUEST_TIMEOUT + \" is different than what AWS sdk configuration uses internally\", 120000, awsConf.getRequestTimeout());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCloseIdempotent",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCloseIdempotent() throws Throwable\n{\r\n    conf = new Configuration();\r\n    fs = S3ATestUtils.createTestFileSystem(conf);\r\n    AWSCredentialProviderList credentials = fs.shareCredentials(\"testCloseIdempotent\");\r\n    credentials.close();\r\n    fs.close();\r\n    assertTrue(\"Closing FS didn't close credentials \" + credentials, credentials.isClosed());\r\n    assertEquals(\"refcount not zero in \" + credentials, 0, credentials.getRefCount());\r\n    fs.close();\r\n    assertEquals(\"refcount not zero in \" + credentials, 0, credentials.getRefCount());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testDirectoryAllocatorDefval",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testDirectoryAllocatorDefval() throws Throwable\n{\r\n    conf = new Configuration();\r\n    conf.unset(Constants.BUFFER_DIR);\r\n    fs = S3ATestUtils.createTestFileSystem(conf);\r\n    File tmp = fs.createTmpFileForWrite(\"out-\", 1024, conf);\r\n    assertTrue(\"not found: \" + tmp, tmp.exists());\r\n    tmp.delete();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testDirectoryAllocatorRR",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDirectoryAllocatorRR() throws Throwable\n{\r\n    File dir1 = GenericTestUtils.getRandomizedTestDir();\r\n    File dir2 = GenericTestUtils.getRandomizedTestDir();\r\n    dir1.mkdirs();\r\n    dir2.mkdirs();\r\n    conf = new Configuration();\r\n    conf.set(Constants.BUFFER_DIR, dir1 + \", \" + dir2);\r\n    fs = S3ATestUtils.createTestFileSystem(conf);\r\n    File tmp1 = fs.createTmpFileForWrite(\"out-\", 1024, conf);\r\n    tmp1.delete();\r\n    File tmp2 = fs.createTmpFileForWrite(\"out-\", 1024, conf);\r\n    tmp2.delete();\r\n    assertNotEquals(\"round robin not working\", tmp1.getParent(), tmp2.getParent());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testReadAheadRange",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testReadAheadRange() throws Exception\n{\r\n    conf = new Configuration();\r\n    conf.set(Constants.READAHEAD_RANGE, \"300K\");\r\n    fs = S3ATestUtils.createTestFileSystem(conf);\r\n    assertNotNull(fs);\r\n    long readAheadRange = fs.getReadAheadRange();\r\n    assertNotNull(readAheadRange);\r\n    assertEquals(\"Read Ahead Range Incorrect.\", 300 * 1024, readAheadRange);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testUsernameFromUGI",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testUsernameFromUGI() throws Throwable\n{\r\n    final String alice = \"alice\";\r\n    UserGroupInformation fakeUser = UserGroupInformation.createUserForTesting(alice, new String[] { \"users\", \"administrators\" });\r\n    conf = new Configuration();\r\n    fs = fakeUser.doAs(new PrivilegedExceptionAction<S3AFileSystem>() {\r\n\r\n        @Override\r\n        public S3AFileSystem run() throws Exception {\r\n            return S3ATestUtils.createTestFileSystem(conf);\r\n        }\r\n    });\r\n    assertEquals(\"username\", alice, fs.getUsername());\r\n    FileStatus status = fs.getFileStatus(new Path(\"/\"));\r\n    assertEquals(\"owner in \" + status, alice, status.getOwner());\r\n    assertEquals(\"group in \" + status, alice, status.getGroup());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getField",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "T getField(Object target, Class<T> fieldType, String fieldName) throws IllegalAccessException\n{\r\n    Object obj = FieldUtils.readField(target, fieldName, true);\r\n    assertNotNull(String.format(\"Could not read field named %s in object with class %s.\", fieldName, target.getClass().getName()), obj);\r\n    assertTrue(String.format(\"Unexpected type found for field named %s, expected %s, actual %s.\", fieldName, fieldType.getName(), obj.getClass().getName()), fieldType.isAssignableFrom(obj.getClass()));\r\n    return fieldType.cast(obj);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testConfOptionPropagationToFS",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testConfOptionPropagationToFS() throws Exception\n{\r\n    Configuration config = new Configuration();\r\n    String testFSName = config.getTrimmed(TEST_FS_S3A_NAME, \"\");\r\n    String bucket = new URI(testFSName).getHost();\r\n    setBucketOption(config, bucket, \"propagation\", \"propagated\");\r\n    fs = S3ATestUtils.createTestFileSystem(config);\r\n    Configuration updated = fs.getConf();\r\n    assertOptionEquals(updated, \"fs.s3a.propagation\", \"propagated\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testS3SpecificSignerOverride",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testS3SpecificSignerOverride() throws IOException\n{\r\n    ClientConfiguration clientConfiguration = null;\r\n    Configuration config;\r\n    String signerOverride = \"testSigner\";\r\n    String s3SignerOverride = \"testS3Signer\";\r\n    config = new Configuration();\r\n    config.set(SIGNING_ALGORITHM_S3, s3SignerOverride);\r\n    clientConfiguration = S3AUtils.createAwsConf(config, \"dontcare\", AWS_SERVICE_IDENTIFIER_S3);\r\n    Assert.assertEquals(s3SignerOverride, clientConfiguration.getSignerOverride());\r\n    clientConfiguration = S3AUtils.createAwsConf(config, \"dontcare\", AWS_SERVICE_IDENTIFIER_STS);\r\n    Assert.assertNull(clientConfiguration.getSignerOverride());\r\n    config = new Configuration();\r\n    config.set(SIGNING_ALGORITHM, signerOverride);\r\n    config.set(SIGNING_ALGORITHM_S3, s3SignerOverride);\r\n    clientConfiguration = S3AUtils.createAwsConf(config, \"dontcare\", AWS_SERVICE_IDENTIFIER_S3);\r\n    Assert.assertEquals(s3SignerOverride, clientConfiguration.getSignerOverride());\r\n    clientConfiguration = S3AUtils.createAwsConf(config, \"dontcare\", AWS_SERVICE_IDENTIFIER_STS);\r\n    Assert.assertEquals(signerOverride, clientConfiguration.getSignerOverride());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    auditor = (LoggingAuditor) getManager().getAuditor();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "createConfig",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfig()\n{\r\n    final Configuration conf = loggingAuditConfig();\r\n    conf.set(REFERRER_HEADER_FILTER, \"x1, x2, x3\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testHttpReferrerPatchesTheRequest",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testHttpReferrerPatchesTheRequest() throws Throwable\n{\r\n    AuditSpan span = span();\r\n    long ts = span.getTimestamp();\r\n    GetObjectMetadataRequest request = head();\r\n    Map<String, String> headers = request.getCustomRequestHeaders();\r\n    assertThat(headers).describedAs(\"Custom headers\").containsKey(HEADER_REFERRER);\r\n    String header = headers.get(HEADER_REFERRER);\r\n    LOG.info(\"Header is {}\", header);\r\n    Map<String, String> params = HttpReferrerAuditHeader.extractQueryParameters(header);\r\n    assertMapContains(params, PARAM_PRINCIPAL, UserGroupInformation.getCurrentUser().getUserName());\r\n    assertMapContains(params, PARAM_FILESYSTEM_ID, auditor.getAuditorId());\r\n    assertMapContains(params, PARAM_OP, OPERATION);\r\n    assertMapContains(params, PARAM_PATH, PATH_1);\r\n    assertMapContains(params, PARAM_PATH2, PATH_2);\r\n    String threadID = CommonAuditContext.currentThreadID();\r\n    assertMapContains(params, PARAM_THREAD0, threadID);\r\n    assertMapContains(params, PARAM_THREAD1, threadID);\r\n    assertMapContains(params, PARAM_ID, span.getSpanId());\r\n    assertThat(span.getTimestamp()).describedAs(\"Timestamp of \" + span).isEqualTo(ts);\r\n    assertMapContains(params, PARAM_TIMESTAMP, Long.toString(ts));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testHeaderComplexPaths",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testHeaderComplexPaths() throws Throwable\n{\r\n    String p1 = \"s3a://dotted.bucket/path: value/subdir\";\r\n    String p2 = \"s3a://key/\";\r\n    AuditSpan span = getManager().createSpan(OPERATION, p1, p2);\r\n    long ts = span.getTimestamp();\r\n    Map<String, String> params = issueRequestAndExtractParameters();\r\n    assertMapContains(params, PARAM_PRINCIPAL, UserGroupInformation.getCurrentUser().getUserName());\r\n    assertMapContains(params, PARAM_FILESYSTEM_ID, auditor.getAuditorId());\r\n    assertMapContains(params, PARAM_OP, OPERATION);\r\n    assertMapContains(params, PARAM_PATH, p1);\r\n    assertMapContains(params, PARAM_PATH2, p2);\r\n    String threadID = CommonAuditContext.currentThreadID();\r\n    assertMapContains(params, PARAM_THREAD0, threadID);\r\n    assertMapContains(params, PARAM_THREAD1, threadID);\r\n    assertMapContains(params, PARAM_ID, span.getSpanId());\r\n    assertThat(span.getTimestamp()).describedAs(\"Timestamp of \" + span).isEqualTo(ts);\r\n    assertMapContains(params, PARAM_TIMESTAMP, Long.toString(ts));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "issueRequestAndExtractParameters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Map<String, String> issueRequestAndExtractParameters() throws URISyntaxException\n{\r\n    head();\r\n    return HttpReferrerAuditHeader.extractQueryParameters(auditor.getLastHeader());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testHeaderFiltering",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testHeaderFiltering() throws Throwable\n{\r\n    AuditSpan span = getManager().createSpan(OPERATION, null, null);\r\n    auditor.addAttribute(\"x0\", \"x0\");\r\n    auditor.addAttribute(\"x2\", \"x2\");\r\n    final Map<String, String> params = issueRequestAndExtractParameters();\r\n    assertThat(params).doesNotContainKey(\"x2\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testMatchAWSLogEntry",
  "errType" : [ "IllegalStateException", "IllegalStateException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testMatchAWSLogEntry() throws Throwable\n{\r\n    LOG.info(\"Matcher pattern is\\n'{}'\", LOG_ENTRY_PATTERN);\r\n    LOG.info(\"Log entry is\\n'{}'\", SAMPLE_LOG_ENTRY);\r\n    final Matcher matcher = LOG_ENTRY_PATTERN.matcher(SAMPLE_LOG_ENTRY);\r\n    assertThat(matcher.matches()).describedAs(\"matches() \" + DESCRIPTION).isTrue();\r\n    final int groupCount = matcher.groupCount();\r\n    assertThat(groupCount).describedAs(\"Group count of \" + DESCRIPTION).isGreaterThanOrEqualTo(AWS_LOG_REGEXP_GROUPS.size());\r\n    for (String name : AWS_LOG_REGEXP_GROUPS) {\r\n        try {\r\n            final String group = matcher.group(name);\r\n            LOG.info(\"[{}]: '{}'\", name, group);\r\n        } catch (IllegalStateException e) {\r\n            throw new AssertionError(\"No match for group <\" + name + \">: \" + e, e);\r\n        }\r\n    }\r\n    for (int i = 1; i <= groupCount; i++) {\r\n        try {\r\n            final String group = matcher.group(i);\r\n            LOG.info(\"[{}]: '{}'\", i, group);\r\n        } catch (IllegalStateException e) {\r\n            throw new AssertionError(\"No match for group \" + i + \": \" + e, e);\r\n        }\r\n    }\r\n    assertThat(nonBlankGroup(matcher, VERB_GROUP)).describedAs(\"HTTP Verb\").isEqualTo(S3LogVerbs.PUT);\r\n    final String referrer = nonBlankGroup(matcher, REFERRER_GROUP);\r\n    Map<String, String> params = HttpReferrerAuditHeader.extractQueryParameters(referrer);\r\n    LOG.info(\"Parsed referrer\");\r\n    for (Map.Entry<String, String> entry : params.entrySet()) {\r\n        LOG.info(\"{} = \\\"{}\\\"\", entry.getKey(), entry.getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "nonBlankGroup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String nonBlankGroup(final Matcher matcher, final String group)\n{\r\n    final String g = matcher.group(group);\r\n    assertThat(g).describedAs(\"Value of group %s\", group).isNotBlank();\r\n    return g;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testStripWrappedQuotes",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testStripWrappedQuotes() throws Throwable\n{\r\n    expectStrippedField(\"\", \"\");\r\n    expectStrippedField(\"\\\"UA\\\"\", \"UA\");\r\n    expectStrippedField(\"\\\"\\\"\\\"\\\"\", \"\");\r\n    expectStrippedField(\"\\\"\\\"\\\"b\\\"\", \"b\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "expectStrippedField",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectStrippedField(final String str, final String ex)\n{\r\n    assertThat(maybeStripWrappedQuotes(str)).describedAs(\"Stripped <%s>\", str).isEqualTo(ex);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { 0 }, { 1 }, { 3 }, { 8 }, { 16 } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "isParallel",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isParallel()\n{\r\n    return numThreads > 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup()\n{\r\n    items = IntStream.rangeClosed(1, ITEM_COUNT).mapToObj(i -> new Item(i, String.format(\"With %d threads\", numThreads))).collect(Collectors.toList());\r\n    if (numThreads > 0) {\r\n        threadPool = Executors.newFixedThreadPool(numThreads, new ThreadFactoryBuilder().setDaemon(true).setNameFormat(getMethodName() + \"-pool-%d\").build());\r\n        submitter = new PoolSubmitter();\r\n    } else {\r\n        submitter = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardown()\n{\r\n    if (threadPool != null) {\r\n        threadPool.shutdown();\r\n        threadPool = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "builder",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Tasks.Builder<Item> builder()\n{\r\n    return Tasks.foreach(items).executeWith(submitter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "assertRun",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertRun(Tasks.Builder<Item> builder, CounterTask task) throws IOException\n{\r\n    boolean b = builder.run(task);\r\n    assertTrue(\"Run of \" + task + \" failed\", b);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "assertFailed",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertFailed(Tasks.Builder<Item> builder, CounterTask task) throws IOException\n{\r\n    boolean b = builder.run(task);\r\n    assertFalse(\"Run of \" + task + \" unexpectedly succeeded\", b);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "itemsToString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String itemsToString()\n{\r\n    return \"[\" + items.stream().map(Item::toString).collect(Collectors.joining(\"\\n\")) + \"]\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testSimpleInvocation",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSimpleInvocation() throws Throwable\n{\r\n    CounterTask t = new CounterTask(\"simple\", 0, Item::commit);\r\n    assertRun(builder(), t);\r\n    t.assertInvoked(\"\", ITEM_COUNT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFailNoStoppingSuppressed",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testFailNoStoppingSuppressed() throws Throwable\n{\r\n    assertFailed(builder().suppressExceptions(), failingTask);\r\n    failingTask.assertInvoked(\"Continued through operations\", ITEM_COUNT);\r\n    items.forEach(Item::assertCommittedOrFailed);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFailFastSuppressed",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testFailFastSuppressed() throws Throwable\n{\r\n    assertFailed(builder().suppressExceptions().stopOnFailure(), failingTask);\r\n    if (isParallel()) {\r\n        failingTask.assertInvokedAtLeast(\"stop fast\", FAILPOINT);\r\n    } else {\r\n        failingTask.assertInvoked(\"stop fast\", FAILPOINT);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFailedCallAbortSuppressed",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testFailedCallAbortSuppressed() throws Throwable\n{\r\n    assertFailed(builder().stopOnFailure().suppressExceptions().abortWith(aborter), failingTask);\r\n    failingTask.assertInvokedAtLeast(\"success\", FAILPOINT);\r\n    if (!isParallel()) {\r\n        aborter.assertInvokedAtLeast(\"abort\", 1);\r\n        items.stream().filter(i -> !i.committed).map(Item::assertAborted);\r\n        items.stream().filter(i -> i.committed).forEach(i -> assertFalse(i.toString(), i.aborted));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFailedCalledWhenNotStoppingSuppressed",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testFailedCalledWhenNotStoppingSuppressed() throws Throwable\n{\r\n    assertFailed(builder().suppressExceptions().onFailure(failures), failingTask);\r\n    failingTask.assertInvokedAtLeast(\"success\", FAILPOINT);\r\n    failures.assertInvoked(\"failure event\", 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFailFastCallRevertSuppressed",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testFailFastCallRevertSuppressed() throws Throwable\n{\r\n    assertFailed(builder().stopOnFailure().revertWith(reverter).abortWith(aborter).suppressExceptions().onFailure(failures), failingTask);\r\n    failingTask.assertInvokedAtLeast(\"success\", FAILPOINT);\r\n    if (!isParallel()) {\r\n        aborter.assertInvokedAtLeast(\"abort\", 1);\r\n        items.stream().filter(i -> !i.committed).filter(i -> !i.failed).forEach(Item::assertAborted);\r\n    }\r\n    items.stream().filter(i -> i.committed && !i.failed).forEach(Item::assertReverted);\r\n    items.stream().filter(i -> i.reverted).forEach(Item::assertCommitted);\r\n    failures.assertInvoked(\"failure event\", 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFailSlowCallRevertSuppressed",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testFailSlowCallRevertSuppressed() throws Throwable\n{\r\n    assertFailed(builder().suppressExceptions().revertWith(reverter).onFailure(failures), failingTask);\r\n    failingTask.assertInvokedAtLeast(\"success\", FAILPOINT);\r\n    int failing = failures.getItem().id;\r\n    items.stream().filter(i -> i.id != failing).filter(i -> i.committed).forEach(Item::assertReverted);\r\n    items.stream().filter(i -> i.reverted).forEach(Item::assertCommitted);\r\n    failures.assertInvoked(\"failure event\", 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFailFastExceptions",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testFailFastExceptions() throws Throwable\n{\r\n    intercept(IOException.class, () -> builder().stopOnFailure().run(failingTask));\r\n    if (isParallel()) {\r\n        failingTask.assertInvokedAtLeast(\"stop fast\", FAILPOINT);\r\n    } else {\r\n        failingTask.assertInvoked(\"stop fast\", FAILPOINT);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFailSlowExceptions",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testFailSlowExceptions() throws Throwable\n{\r\n    intercept(IOException.class, () -> builder().run(failingTask));\r\n    failingTask.assertInvoked(\"continued through operations\", ITEM_COUNT);\r\n    items.forEach(Item::assertCommittedOrFailed);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFailFastExceptionsWithAbortFailure",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testFailFastExceptionsWithAbortFailure() throws Throwable\n{\r\n    CounterTask failFirst = new CounterTask(\"task\", 1, Item::commit);\r\n    CounterTask a = new CounterTask(\"aborter\", 1, Item::abort);\r\n    intercept(IOException.class, () -> builder().stopOnFailure().abortWith(a).run(failFirst));\r\n    if (!isParallel()) {\r\n        a.assertInvokedAtLeast(\"abort\", ITEM_COUNT - 1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFailFastExceptionsWithAbortFailureStopped",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testFailFastExceptionsWithAbortFailureStopped() throws Throwable\n{\r\n    CounterTask failFirst = new CounterTask(\"task\", 1, Item::commit);\r\n    CounterTask a = new CounterTask(\"aborter\", 1, Item::abort);\r\n    intercept(IOException.class, () -> builder().stopOnFailure().stopAbortsOnFailure().abortWith(a).run(failFirst));\r\n    if (!isParallel()) {\r\n        a.assertInvoked(\"abort\", 1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testRevertAllSuppressed",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testRevertAllSuppressed() throws Throwable\n{\r\n    CounterTask failLast = new CounterTask(\"task\", ITEM_COUNT, Item::commit);\r\n    assertFailed(builder().suppressExceptions().stopOnFailure().revertWith(reverter).abortWith(aborter).onFailure(failures), failLast);\r\n    failLast.assertInvoked(\"success\", ITEM_COUNT);\r\n    int abCount = aborter.getCount();\r\n    int revCount = reverter.getCount();\r\n    assertEquals(ITEM_COUNT, 1 + abCount + revCount);\r\n    int failing = failures.getItem().id;\r\n    items.stream().filter(i -> i.id != failing).filter(i -> i.committed).forEach(Item::assertReverted);\r\n    items.stream().filter(i -> i.id != failing).filter(i -> !i.committed).forEach(Item::assertAborted);\r\n    items.stream().filter(i -> i.reverted).forEach(Item::assertCommitted);\r\n    failures.assertInvoked(\"failure event\", 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "mockS3ABuilder",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "S3ABlockOutputStream.BlockOutputStreamBuilder mockS3ABuilder()\n{\r\n    ExecutorService executorService = mock(ExecutorService.class);\r\n    Progressable progressable = mock(Progressable.class);\r\n    S3ADataBlocks.BlockFactory blockFactory = mock(S3ADataBlocks.BlockFactory.class);\r\n    long blockSize = Constants.DEFAULT_MULTIPART_SIZE;\r\n    WriteOperationHelper oHelper = mock(WriteOperationHelper.class);\r\n    PutTracker putTracker = mock(PutTracker.class);\r\n    final S3ABlockOutputStream.BlockOutputStreamBuilder builder = S3ABlockOutputStream.builder().withBlockFactory(blockFactory).withBlockSize(blockSize).withExecutorService(executorService).withKey(\"\").withProgress(progressable).withPutTracker(putTracker).withWriteOperations(oHelper);\r\n    return builder;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    final S3ABlockOutputStream.BlockOutputStreamBuilder builder = mockS3ABuilder();\r\n    stream = spy(new S3ABlockOutputStream(builder));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testFlushNoOpWhenStreamClosed",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testFlushNoOpWhenStreamClosed() throws Exception\n{\r\n    doThrow(new IOException()).when(stream).checkOpen();\r\n    stream.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testWriteOperationHelperPartLimits",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testWriteOperationHelperPartLimits() throws Throwable\n{\r\n    S3AFileSystem s3a = mock(S3AFileSystem.class);\r\n    when(s3a.getBucket()).thenReturn(\"bucket\");\r\n    when(s3a.getRequestFactory()).thenReturn(MockS3AFileSystem.REQUEST_FACTORY);\r\n    final Configuration conf = new Configuration();\r\n    WriteOperationHelper woh = new WriteOperationHelper(s3a, conf, new EmptyS3AStatisticsContext(), noopAuditor(conf), AuditTestSupport.NOOP_SPAN);\r\n    ByteArrayInputStream inputStream = new ByteArrayInputStream(\"a\".getBytes());\r\n    String key = \"destKey\";\r\n    woh.newUploadPartRequest(key, \"uploadId\", 1, 1024, inputStream, null, 0L);\r\n    intercept(PathIOException.class, key, () -> woh.newUploadPartRequest(key, \"uploadId\", 50000, 1024, inputStream, null, 0L));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testStreamClosedAfterAbort",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testStreamClosedAfterAbort() throws Exception\n{\r\n    stream.abort();\r\n    intercept(IOException.class, () -> stream.checkOpen());\r\n    doThrow(new StreamClosedException()).when(stream).checkOpen();\r\n    intercept(StreamClosedException.class, () -> stream.write(new byte[] { 'a', 'b', 'c' }));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCallingCloseAfterCallingAbort",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCallingCloseAfterCallingAbort() throws Exception\n{\r\n    stream.abort();\r\n    stream.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSyncableUnsupported",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSyncableUnsupported() throws Exception\n{\r\n    final S3ABlockOutputStream.BlockOutputStreamBuilder builder = mockS3ABuilder();\r\n    builder.withDowngradeSyncableExceptions(false);\r\n    stream = spy(new S3ABlockOutputStream(builder));\r\n    intercept(UnsupportedOperationException.class, () -> stream.hflush());\r\n    intercept(UnsupportedOperationException.class, () -> stream.hsync());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSyncableDowngrade",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSyncableDowngrade() throws Exception\n{\r\n    final S3ABlockOutputStream.BlockOutputStreamBuilder builder = mockS3ABuilder();\r\n    builder.withDowngradeSyncableExceptions(true);\r\n    stream = spy(new S3ABlockOutputStream(builder));\r\n    stream.hflush();\r\n    stream.hsync();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "bindRolePolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration bindRolePolicy(final Configuration conf, final Policy policy) throws JsonProcessingException\n{\r\n    String p = MODEL.toJson(policy);\r\n    LOG.info(\"Setting role policy to policy of size {}:\\n{}\", p.length(), p);\r\n    conf.set(ASSUMED_ROLE_POLICY, p);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "bindRolePolicyStatements",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration bindRolePolicyStatements(final Configuration conf, final Statement... statements) throws JsonProcessingException\n{\r\n    return bindRolePolicy(conf, policy(statements));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "assertDeleteForbidden",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertDeleteForbidden(final FileSystem fs, final Path path) throws Exception\n{\r\n    intercept(AccessDeniedException.class, \"\", () -> fs.delete(path, true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "assertTouchForbidden",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertTouchForbidden(final FileSystem fs, final Path path) throws Exception\n{\r\n    intercept(AccessDeniedException.class, \"\", \"Caller could create file at \" + path, () -> {\r\n        touch(fs, path);\r\n        return fs.getFileStatus(path);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "newAssumedRoleConfig",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Configuration newAssumedRoleConfig(final Configuration srcConf, final String roleARN)\n{\r\n    Configuration conf = new Configuration(srcConf);\r\n    removeBaseAndBucketOverrides(conf, DELEGATION_TOKEN_BINDING, ASSUMED_ROLE_ARN, AWS_CREDENTIALS_PROVIDER);\r\n    conf.set(AWS_CREDENTIALS_PROVIDER, AssumedRoleCredentialProvider.NAME);\r\n    conf.set(ASSUMED_ROLE_ARN, roleARN);\r\n    conf.set(ASSUMED_ROLE_SESSION_NAME, \"test\");\r\n    conf.set(ASSUMED_ROLE_SESSION_DURATION, \"15m\");\r\n    disableFilesystemCaching(conf);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "forbidden",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AccessDeniedException forbidden(final String contained, final Callable<T> eval) throws Exception\n{\r\n    return forbidden(\"\", contained, eval);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "forbidden",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AccessDeniedException forbidden(final String message, final String contained, final Callable<T> eval) throws Exception\n{\r\n    return intercept(AccessDeniedException.class, contained, message, eval);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "probeForAssumedRoleARN",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String probeForAssumedRoleARN(Configuration conf)\n{\r\n    String arn = conf.getTrimmed(ASSUMED_ROLE_ARN, \"\");\r\n    Assume.assumeTrue(\"No ARN defined in \" + ASSUMED_ROLE_ARN, !arn.isEmpty());\r\n    return arn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "assertCredentialsEqual",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertCredentialsEqual(final String message, final MarshalledCredentials expected, final MarshalledCredentials actual)\n{\r\n    assertEquals(message + \": access key\", expected.getAccessKey(), actual.getAccessKey());\r\n    assertTrue(message + \": secret key\", expected.getSecretKey().equals(actual.getSecretKey()));\r\n    assertEquals(message + \": session token\", expected.getSessionToken(), actual.getSessionToken());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "touchFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<Path> touchFiles(final FileSystem fs, final Path destDir, final int range) throws IOException\n{\r\n    List<Path> paths = IntStream.rangeClosed(1, range).mapToObj((i) -> new Path(destDir, \"file-\" + i)).collect(Collectors.toList());\r\n    for (Path path : paths) {\r\n        touch(fs, path);\r\n    }\r\n    return paths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\fileContext",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws IOException, Exception\n{\r\n    Configuration conf = new Configuration();\r\n    fc = S3ATestUtils.createTestFileContext(conf);\r\n    super.setUp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\fileContext",
  "methodName" : "listCorruptedBlocksSupported",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean listCorruptedBlocksSupported()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\fileContext",
  "methodName" : "testCreateFlagAppendExistingFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testCreateFlagAppendExistingFile() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\fileContext",
  "methodName" : "testCreateFlagCreateAppendExistingFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testCreateFlagCreateAppendExistingFile() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\fileContext",
  "methodName" : "testBuilderCreateAppendExistingFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testBuilderCreateAppendExistingFile() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\fileContext",
  "methodName" : "testSetVerifyChecksum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testSetVerifyChecksum() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    Configuration conf = createConfiguration();\r\n    fs = new S3AFileSystem();\r\n    URI uri = URI.create(FS_S3A + \"://\" + BUCKET);\r\n    conf.unset(Constants.S3_ENCRYPTION_ALGORITHM);\r\n    fs.initialize(uri, conf);\r\n    s3 = fs.getAmazonS3ClientForTesting(\"mocking\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setClass(S3_CLIENT_FACTORY_IMPL, MockS3ClientFactory.class, S3ClientFactory.class);\r\n    conf.setLong(Constants.MULTIPART_SIZE, MULTIPART_MIN_SIZE);\r\n    conf.setInt(Constants.S3A_BUCKET_PROBE, 1);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    if (fs != null) {\r\n        fs.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testLandsatBucketUnguarded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLandsatBucketUnguarded() throws Throwable\n{\r\n    run(BucketInfo.NAME, \"-\" + BucketInfo.UNGUARDED_FLAG, getLandsatCSVFile(getConfiguration()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testLandsatBucketRequireGuarded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLandsatBucketRequireGuarded() throws Throwable\n{\r\n    runToFailure(E_BAD_STATE, BucketInfo.NAME, \"-\" + BucketInfo.GUARDED_FLAG, getLandsatCSVFile(ITestS3GuardTool.this.getConfiguration()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testLandsatBucketRequireUnencrypted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLandsatBucketRequireUnencrypted() throws Throwable\n{\r\n    run(BucketInfo.NAME, \"-\" + BucketInfo.ENCRYPTION_FLAG, \"none\", getLandsatCSVFile(getConfiguration()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testLandsatBucketRequireEncrypted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLandsatBucketRequireEncrypted() throws Throwable\n{\r\n    runToFailure(E_BAD_STATE, BucketInfo.NAME, \"-\" + BucketInfo.ENCRYPTION_FLAG, \"AES256\", getLandsatCSVFile(ITestS3GuardTool.this.getConfiguration()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testStoreInfo",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testStoreInfo() throws Throwable\n{\r\n    S3GuardTool.BucketInfo cmd = toClose(new S3GuardTool.BucketInfo(getFileSystem().getConf()));\r\n    String output = exec(cmd, cmd.getName(), \"-\" + BucketInfo.UNGUARDED_FLAG, getFileSystem().getUri().toString());\r\n    LOG.info(\"Exec output=\\n{}\", output);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testUploads",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testUploads() throws Throwable\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path path = path(UPLOAD_PREFIX + \"/\" + UPLOAD_NAME);\r\n    describe(\"Cleaning up any leftover uploads from previous runs.\");\r\n    final String key = fs.pathToKey(path);\r\n    try {\r\n        clearAnyUploads(fs, path);\r\n        assertNoUploadsAt(fs, path.getParent());\r\n        describe(\"Confirming CLI lists nothing.\");\r\n        assertNumUploads(path, 0);\r\n        describe(\"Uploading single part.\");\r\n        createPartUpload(fs, key, 128, 1);\r\n        assertEquals(\"Should be one upload\", 1, countUploadsAt(fs, path));\r\n        describe(\"Confirming CLI lists one part\");\r\n        assertNumUploads(path, 1);\r\n        assertNumUploads(path.getParent(), 1);\r\n        describe(\"Deleting part via CLI\");\r\n        assertNumDeleted(fs, path, 1);\r\n        describe(\"Confirming deletion via API\");\r\n        assertEquals(\"Should be no uploads\", 0, countUploadsAt(fs, path));\r\n        describe(\"Confirming CLI lists nothing.\");\r\n        assertNumUploads(path, 0);\r\n    } catch (Throwable t) {\r\n        clearAnyUploads(fs, path);\r\n        throw t;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testUploadListByAge",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testUploadListByAge() throws Throwable\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path path = path(UPLOAD_PREFIX + \"/\" + UPLOAD_NAME);\r\n    describe(\"Cleaning up any leftover uploads from previous runs.\");\r\n    clearAnyUploads(fs, path);\r\n    describe(\"Uploading single part.\");\r\n    final String key = fs.pathToKey(path);\r\n    createPartUpload(fs, key, 128, 1);\r\n    try {\r\n        assertEquals(\"Should be one upload\", 1, countUploadsAt(fs, path));\r\n        describe(\"Confirming CLI older age doesn't list\");\r\n        assertNumUploadsAge(path, 0, 600);\r\n        describe(\"Confirming CLI older age doesn't delete\");\r\n        uploadCommandAssertCount(fs, ABORT_FORCE_OPTIONS, path, 0, 600);\r\n        describe(\"Sleeping 1 second then confirming upload still there\");\r\n        Thread.sleep(1000);\r\n        LambdaTestUtils.eventually(5000, 1000, () -> {\r\n            assertNumUploadsAge(path, 1, 1);\r\n        });\r\n        describe(\"Doing aged deletion\");\r\n        uploadCommandAssertCount(fs, ABORT_FORCE_OPTIONS, path, 1, 1);\r\n        describe(\"Confirming age deletion happened\");\r\n        assertEquals(\"Should be no uploads\", 0, countUploadsAt(fs, path));\r\n    } catch (Throwable t) {\r\n        clearAnyUploads(fs, path);\r\n        throw t;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testUploadNegativeExpect",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUploadNegativeExpect() throws Throwable\n{\r\n    runToFailure(E_BAD_STATE, Uploads.NAME, \"-expect\", \"1\", path(\"/we/are/almost/postive/this/doesnt/exist/fhfsadfoijew\").toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "assertNumUploads",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertNumUploads(Path path, int numUploads) throws Exception\n{\r\n    assertNumUploadsAge(path, numUploads, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "assertNumUploadsAge",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertNumUploadsAge(Path path, int numUploads, int ageSeconds) throws Exception\n{\r\n    if (ageSeconds > 0) {\r\n        run(Uploads.NAME, \"-expect\", String.valueOf(numUploads), \"-seconds\", String.valueOf(ageSeconds), path.toString());\r\n    } else {\r\n        run(Uploads.NAME, \"-expect\", String.valueOf(numUploads), path.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "assertNumDeleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertNumDeleted(S3AFileSystem fs, Path path, int numDeleted) throws Exception\n{\r\n    uploadCommandAssertCount(fs, ABORT_FORCE_OPTIONS, path, numDeleted, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "uploadCommandAssertCount",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void uploadCommandAssertCount(S3AFileSystem fs, String[] options, Path path, int numUploads, int ageSeconds) throws Exception\n{\r\n    List<String> allOptions = new ArrayList<>();\r\n    List<String> output = new ArrayList<>();\r\n    S3GuardTool.Uploads cmd = new S3GuardTool.Uploads(fs.getConf());\r\n    ByteArrayOutputStream buf = new ByteArrayOutputStream();\r\n    allOptions.add(cmd.getName());\r\n    allOptions.addAll(Arrays.asList(options));\r\n    if (ageSeconds > 0) {\r\n        allOptions.add(\"-\" + Uploads.SECONDS_FLAG);\r\n        allOptions.add(String.valueOf(ageSeconds));\r\n    }\r\n    allOptions.add(path.toString());\r\n    exec(0, \"\", cmd, buf, allOptions.toArray(new String[0]));\r\n    try (BufferedReader reader = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(buf.toByteArray())))) {\r\n        String line;\r\n        while ((line = reader.readLine()) != null) {\r\n            String[] fields = line.split(\"\\\\s\");\r\n            if (fields.length == 4 && fields[0].equals(Uploads.TOTAL)) {\r\n                int parsedUploads = Integer.parseInt(fields[1]);\r\n                LOG.debug(\"Matched CLI output: {} {} {} {}\", fields[0], fields[1], fields[2], fields[3]);\r\n                assertEquals(\"Unexpected number of uploads\", numUploads, parsedUploads);\r\n                return;\r\n            }\r\n            LOG.debug(\"Not matched: {}\", line);\r\n            output.add(line);\r\n        }\r\n    }\r\n    fail(\"Command output did not match: \\n\" + StringUtils.join(\"\\n\", output));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    getLogger().info(\"FS details {}\", getFileSystem());\r\n    super.teardown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    disableFilesystemCaching(conf);\r\n    conf.setInt(Constants.MAX_PAGING_KEYS, 2);\r\n    conf.setInt(LIST_VERSION, 1);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (out != null) {\r\n        out.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getPath()\n{\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getSeparator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSeparator()\n{\r\n    return separator;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getEol",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getEol()\n{\r\n    return eol;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "row",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "CsvFile row(long quotes, Object... columns)\n{\r\n    for (int i = 0; i < columns.length; i++) {\r\n        if (i != 0) {\r\n            out.write(separator);\r\n        }\r\n        boolean toQuote = (quotes & 1) == 1;\r\n        quotes = quotes >>> 1;\r\n        if (toQuote) {\r\n            out.write(quote);\r\n        }\r\n        out.write(columns[i].toString());\r\n        if (toQuote) {\r\n            out.write(quote);\r\n        }\r\n    }\r\n    out.write(eol);\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "line",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "CsvFile line(String line)\n{\r\n    out.write(line);\r\n    out.write(eol);\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getOut",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "PrintWriter getOut()\n{\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testMetricsRegister",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMetricsRegister() throws IOException, InterruptedException\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path dest = path(\"testMetricsRegister\");\r\n    ContractTestUtils.touch(fs, dest);\r\n    MutableCounterLong fileCreated = (MutableCounterLong) fs.getInstrumentation().getRegistry().get(Statistic.FILES_CREATED.getSymbol());\r\n    assertEquals(\"Metrics system should report single file created event\", 1, fileCreated.value());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testStreamStatistics() throws IOException\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path file = path(\"testStreamStatistics\");\r\n    byte[] data = \"abcdefghijklmnopqrstuvwxyz\".getBytes();\r\n    ContractTestUtils.createFile(fs, file, false, data);\r\n    InputStream inputStream = fs.open(file);\r\n    try {\r\n        while (inputStream.read(data) != -1) {\r\n            LOG.debug(\"Read batch of data from input stream...\");\r\n        }\r\n        LOG.info(\"Final stream statistics: {}\", ioStatisticsSourceToString(inputStream));\r\n    } finally {\r\n        inputStream.close();\r\n    }\r\n    final String statName = Statistic.STREAM_READ_BYTES.getSymbol();\r\n    final S3AInstrumentation instrumentation = fs.getInstrumentation();\r\n    final long counterValue = instrumentation.getCounterValue(statName);\r\n    final int expectedBytesRead = 26;\r\n    Assertions.assertThat(counterValue).describedAs(\"Counter %s from instrumentation %s\", statName, instrumentation).isEqualTo(expectedBytesRead);\r\n    MutableCounterLong read = (MutableCounterLong) instrumentation.getRegistry().get(statName);\r\n    assertEquals(\"Stream statistics were not merged\", expectedBytesRead, read.value());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { \"session\", DELEGATION_TOKEN_SESSION_BINDING, SESSION_TOKEN_KIND }, { \"full\", DELEGATION_TOKEN_FULL_CREDENTIALS_BINDING, FULL_TOKEN_KIND }, { \"role\", DELEGATION_TOKEN_ROLE_BINDING, ROLE_TOKEN_KIND } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "setupCluster",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setupCluster() throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    assumeSessionTestsEnabled(conf);\r\n    disableFilesystemCaching(conf);\r\n    cluster = deployService(conf, new MiniKerberizedHadoopCluster());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws Exception\n{\r\n    cluster = terminateService(cluster);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "YarnConfiguration createConfiguration()\n{\r\n    Configuration parent = super.createConfiguration();\r\n    YarnConfiguration conf = new YarnConfiguration(parent);\r\n    cluster.patchConfigWithYARNBindings(conf);\r\n    conf.setInt(YarnConfiguration.RESOURCEMANAGER_CONNECT_MAX_WAIT_MS, 100);\r\n    conf.setInt(YarnConfiguration.RESOURCEMANAGER_CONNECT_RETRY_INTERVAL_MS, 10_000);\r\n    String host = jobResource.getHost();\r\n    conf.set(String.format(\"fs.s3a.bucket.%s.endpoint\", host), \"\");\r\n    enableDelegationTokens(conf, tokenBinding);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "YarnConfiguration getConfiguration()\n{\r\n    return (YarnConfiguration) super.getConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    cluster.loginPrincipal();\r\n    super.setup();\r\n    Configuration conf = getConfiguration();\r\n    if (DELEGATION_TOKEN_ROLE_BINDING.equals(tokenBinding)) {\r\n        probeForAssumedRoleARN(getConfiguration());\r\n    }\r\n    UserGroupInformation.setConfiguration(conf);\r\n    assertSecurityEnabled();\r\n    LOG.info(\"Starting MiniMRCluster\");\r\n    yarn = deployService(conf, new MiniMRYarnCluster(\"ITestDelegatedMRJob\", 1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    describe(\"Teardown operations\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    if (fs != null && destPath != null) {\r\n        fs.delete(destPath, true);\r\n    }\r\n    yarn = terminateService(yarn);\r\n    super.teardown();\r\n    closeUserFileSystems(UserGroupInformation.getCurrentUser());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getTestTimeoutSeconds",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTestTimeoutSeconds()\n{\r\n    return getTestPropertyInt(new Configuration(), KEY_TEST_TIMEOUT, SCALE_TEST_TIMEOUT_SECONDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return getTestTimeoutSeconds() * 1000;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testCommonCrawlLookup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCommonCrawlLookup() throws Throwable\n{\r\n    FileSystem resourceFS = EXTRA_JOB_RESOURCE_PATH.getFileSystem(getConfiguration());\r\n    FileStatus status = resourceFS.getFileStatus(EXTRA_JOB_RESOURCE_PATH);\r\n    LOG.info(\"Extra job resource is {}\", status);\r\n    assertTrue(\"Not encrypted: \" + status, status.isEncrypted());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testJobSubmissionCollectsTokens",
  "errType" : null,
  "containingMethodsNum" : 32,
  "sourceCodeText" : "void testJobSubmissionCollectsTokens() throws Exception\n{\r\n    describe(\"Mock Job test\");\r\n    JobConf conf = new JobConf(getConfiguration());\r\n    Path input = new Path(DEFAULT_CSVTEST_FILE);\r\n    final FileSystem sourceFS = input.getFileSystem(conf);\r\n    final S3AFileSystem fs = getFileSystem();\r\n    destPath = path(getMethodName());\r\n    fs.delete(destPath, true);\r\n    fs.mkdirs(destPath);\r\n    Path output = new Path(destPath, \"output/\");\r\n    output = output.makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n    MockJob job = new MockJob(conf, \"word count\");\r\n    job.setJarByClass(WordCount.class);\r\n    job.setMapperClass(WordCount.TokenizerMapper.class);\r\n    job.setCombinerClass(WordCount.IntSumReducer.class);\r\n    job.setReducerClass(WordCount.IntSumReducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    FileInputFormat.addInputPath(job, input);\r\n    FileOutputFormat.setOutputPath(job, output);\r\n    job.setMaxMapAttempts(1);\r\n    job.setMaxReduceAttempts(1);\r\n    URI partitionUri = new URI(EXTRA_JOB_RESOURCE_PATH.toString() + \"#_partition.lst\");\r\n    job.addCacheFile(partitionUri);\r\n    describe(\"Executing Mock Job Submission to %s\", output);\r\n    job.submit();\r\n    final JobStatus status = job.getStatus();\r\n    assertEquals(\"not a mock job\", MockJob.NAME, status.getSchedulingInfo());\r\n    assertEquals(\"Job State\", JobStatus.State.RUNNING, status.getState());\r\n    final Credentials submittedCredentials = requireNonNull(job.getSubmittedCredentials(), \"job submitted credentials\");\r\n    final Collection<Token<? extends TokenIdentifier>> tokens = submittedCredentials.getAllTokens();\r\n    LOG.info(\"Token Count = {}\", tokens.size());\r\n    for (Token<? extends TokenIdentifier> token : tokens) {\r\n        LOG.info(\"{}\", token);\r\n    }\r\n    lookupToken(submittedCredentials, sourceFS.getUri(), tokenKind);\r\n    lookupToken(submittedCredentials, fs.getUri(), tokenKind);\r\n    lookupToken(submittedCredentials, EXTRA_JOB_RESOURCE_PATH.getFileSystem(conf).getUri(), tokenKind);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    assumeRoleTests();\r\n    restrictedDir = super.path(\"restricted\");\r\n    Configuration conf = newAssumedRoleConfig(getConfiguration(), getAssumedRoleARN());\r\n    bindRolePolicyStatements(conf, STATEMENT_ALLOW_SSE_KMS_RW, statement(true, S3_ALL_BUCKETS, S3_BUCKET_READ_OPERATIONS), new RoleModel.Statement(RoleModel.Effects.Allow).addActions(S3_PATH_RW_OPERATIONS).addResources(directory(restrictedDir)));\r\n    roleFS = (S3AFileSystem) restrictedDir.getFileSystem(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    cleanupWithLogger(LOG, roleFS);\r\n    roleFS = null;\r\n    super.teardown();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "assumeRoleTests",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assumeRoleTests()\n{\r\n    assume(\"No ARN for role tests\", !getAssumedRoleARN().isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileSystem getFileSystem()\n{\r\n    return roleFS != null ? roleFS : getFullFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getFullFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileSystem getFullFileSystem()\n{\r\n    return super.getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "path",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path path(String filepath) throws IOException\n{\r\n    return new Path(restrictedDir, filepath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getAssumedRoleARN",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getAssumedRoleARN()\n{\r\n    return getContract().getConf().getTrimmed(ASSUMED_ROLE_ARN, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "teardownClusters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownClusters() throws IOException\n{\r\n    terminateCluster(clusterBinding);\r\n    clusterBinding = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ClusterBinding createCluster(final JobConf conf, final boolean useHDFS) throws IOException\n{\r\n    prepareTestConfiguration(conf);\r\n    conf.setBoolean(JHAdminConfig.MR_HISTORY_CLEANER_ENABLE, false);\r\n    conf.setLong(CommonConfigurationKeys.FS_DU_INTERVAL_KEY, Long.MAX_VALUE);\r\n    conf.setBoolean(YarnConfiguration.NM_DISK_HEALTH_CHECK_ENABLE, false);\r\n    conf.setInt(YarnConfiguration.NM_MAX_PER_DISK_UTILIZATION_PERCENTAGE, 100);\r\n    String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd-HH.mm.ss.SS\"));\r\n    String clusterName = \"yarn-\" + timestamp;\r\n    MiniDFSClusterService miniDFSClusterService = useHDFS ? deployService(conf, new MiniDFSClusterService()) : null;\r\n    MiniMRYarnCluster yarnCluster = deployService(conf, new MiniMRYarnCluster(clusterName, NO_OF_NODEMANAGERS));\r\n    return new ClusterBinding(clusterName, miniDFSClusterService, yarnCluster);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "terminateCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void terminateCluster(ClusterBinding cluster)\n{\r\n    if (cluster != null) {\r\n        cluster.terminate();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getClusterBinding",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClusterBinding getClusterBinding()\n{\r\n    return clusterBinding;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getYarn",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MiniMRYarnCluster getYarn()\n{\r\n    return getClusterBinding().getYarn();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getClusterFS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getClusterFS() throws IOException\n{\r\n    return getClusterBinding().getClusterFS();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "committerName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String committerName()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "demandCreateClusterBinding",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ClusterBinding demandCreateClusterBinding() throws Exception\n{\r\n    return createCluster(new JobConf(), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    scaleTest = getTestPropertyBool(getConfiguration(), KEY_SCALE_TESTS_ENABLED, DEFAULT_SCALE_TESTS_ENABLED);\r\n    if (getClusterBinding() == null) {\r\n        clusterBinding = demandCreateClusterBinding();\r\n    }\r\n    assertNotNull(\"cluster is not bound\", getClusterBinding());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return SCALE_TEST_TIMEOUT_SECONDS * 1000;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "newJobConf",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobConf newJobConf() throws IOException\n{\r\n    JobConf jobConf = new JobConf(getYarn().getConfig());\r\n    jobConf.addResource(getConfiguration());\r\n    applyCustomConfigOptions(jobConf);\r\n    return jobConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "createJob",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Job createJob(Configuration jobConf) throws IOException\n{\r\n    Job mrJob = Job.getInstance(jobConf, getMethodName());\r\n    patchConfigurationForCommitter(mrJob.getConfiguration());\r\n    return mrJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "patchConfigurationForCommitter",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Configuration patchConfigurationForCommitter(final Configuration jobConf)\n{\r\n    jobConf.setBoolean(FS_S3A_COMMITTER_STAGING_UNIQUE_FILENAMES, isUniqueFilenames());\r\n    bindCommitter(jobConf, CommitConstants.S3A_COMMITTER_FACTORY, committerName());\r\n    jobConf.setBoolean(KEY_SCALE_TESTS_ENABLED, isScaleTest());\r\n    String staging = stagingFilesDir.getRoot().getAbsolutePath();\r\n    LOG.info(\"Staging temp dir is {}\", staging);\r\n    jobConf.set(FS_S3A_COMMITTER_STAGING_TMP_PATH, staging);\r\n    return jobConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getTestFileCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTestFileCount()\n{\r\n    return isScaleTest() ? SCALE_TEST_FILE_COUNT : TEST_FILE_COUNT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "applyCustomConfigOptions",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void applyCustomConfigOptions(JobConf jobConf) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "customPostExecutionValidation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void customPostExecutionValidation(Path destPath, SuccessData successData) throws Exception\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "requireScaleTestsEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void requireScaleTestsEnabled()\n{\r\n    assume(\"Scale test disabled: to enable set property \" + KEY_SCALE_TESTS_ENABLED, isScaleTest());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "isScaleTest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isScaleTest()\n{\r\n    return scaleTest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "isUniqueFilenames",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isUniqueFilenames()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testUUIDPart",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUUIDPart()\n{\r\n    assertUUIDAdded(\"/part-0000\", \"/part-0000-UUID\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testUUIDPartSuffix",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUUIDPartSuffix()\n{\r\n    assertUUIDAdded(\"/part-0000.gz.csv\", \"/part-0000-UUID.gz.csv\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testUUIDDottedPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUUIDDottedPath()\n{\r\n    assertUUIDAdded(\"/parent.dir/part-0000\", \"/parent.dir/part-0000-UUID\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testUUIDPartUUID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUUIDPartUUID()\n{\r\n    assertUUIDAdded(\"/part-0000-UUID.gz.csv\", \"/part-0000-UUID.gz.csv\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testUUIDParentUUID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUUIDParentUUID()\n{\r\n    assertUUIDAdded(\"/UUID/part-0000.gz.csv\", \"/UUID/part-0000.gz.csv\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testUUIDDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUUIDDir() throws Throwable\n{\r\n    intercept(IllegalStateException.class, () -> addUUID(\"/dest/\", \"UUID\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testUUIDEmptyDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUUIDEmptyDir() throws Throwable\n{\r\n    intercept(IllegalArgumentException.class, () -> addUUID(\"\", \"UUID\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testEmptyUUID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testEmptyUUID() throws Throwable\n{\r\n    intercept(IllegalArgumentException.class, () -> addUUID(\"part-0000.gz\", \"\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "assertUUIDAdded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertUUIDAdded(String path, String expected)\n{\r\n    assertEquals(\"from \" + path, expected, addUUID(path, \"UUID\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testRelativizeOneLevel",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRelativizeOneLevel()\n{\r\n    String suffix = \"year=2017\";\r\n    Path path = new Path(DATA + suffix);\r\n    assertEquals(suffix, getRelativePath(BASE, path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testRelativizeTwoLevel",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRelativizeTwoLevel()\n{\r\n    String suffix = \"year=2017/month=10\";\r\n    Path path = path(BASE, suffix);\r\n    assertEquals(suffix, getRelativePath(BASE, path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testRelativizeSelf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRelativizeSelf()\n{\r\n    assertEquals(\"\", getRelativePath(BASE, BASE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testRelativizeParent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRelativizeParent()\n{\r\n    assertEquals(\"/\", getRelativePath(BASE, BASE.getParent()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testGetPartition",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetPartition()\n{\r\n    assertEquals(\"year=2017/month=10\", getPartition(\"year=2017/month=10/part-0000.avro\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testMPUCommitDir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testMPUCommitDir() throws Throwable\n{\r\n    Configuration conf = new Configuration();\r\n    LocalFileSystem localFS = FileSystem.getLocal(conf);\r\n    Path dir = getMultipartUploadCommitsDirectory(localFS, conf, \"UUID\");\r\n    assertTrue(dir.toString().endsWith(\"UUID/\" + StagingCommitterConstants.STAGING_UPLOADS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getDelegationBinding",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDelegationBinding()\n{\r\n    return DELEGATION_TOKEN_ROLE_BINDING;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getTokenKind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getTokenKind()\n{\r\n    return ROLE_TOKEN_KIND;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    probeForAssumedRoleARN(getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "verifyCredentialPropagation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractS3ATokenIdentifier verifyCredentialPropagation(final S3AFileSystem fs, final MarshalledCredentials marshalledCredentials, final Configuration conf) throws Exception\n{\r\n    intercept(DelegationTokenIOException.class, E_NO_SESSION_TOKENS_FOR_ROLE_BINDING, () -> super.verifyCredentialPropagation(fs, marshalledCredentials, conf));\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testBindingWithoutARN",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testBindingWithoutARN() throws Throwable\n{\r\n    describe(\"verify that a role binding only needs a role ARN when creating\" + \" a new token\");\r\n    Configuration conf = new Configuration(getConfiguration());\r\n    conf.unset(DelegationConstants.DELEGATION_TOKEN_ROLE_ARN);\r\n    try (S3ADelegationTokens delegationTokens2 = new S3ADelegationTokens()) {\r\n        final S3AFileSystem fs = getFileSystem();\r\n        delegationTokens2.bindToFileSystem(fs.getUri(), fs.createStoreContext(), fs.createDelegationOperations());\r\n        delegationTokens2.init(conf);\r\n        delegationTokens2.start();\r\n        intercept(IllegalStateException.class, E_NO_ARN, () -> delegationTokens2.createDelegationToken(new EncryptionSecrets(), null));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testCreateRoleModel",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCreateRoleModel() throws Throwable\n{\r\n    describe(\"self contained role model retrieval\");\r\n    EnumSet<AWSPolicyProvider.AccessLevel> access = EnumSet.of(AWSPolicyProvider.AccessLevel.READ, AWSPolicyProvider.AccessLevel.WRITE);\r\n    S3AFileSystem fs = getFileSystem();\r\n    List<RoleModel.Statement> rules = fs.listAWSPolicyRules(access);\r\n    assertTrue(\"No AWS policy rules from FS\", !rules.isEmpty());\r\n    String ruleset = new RoleModel().toJson(new RoleModel.Policy(rules));\r\n    LOG.info(\"Access policy for {}\\n{}\", fs.getUri(), ruleset);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    S3AFileSystem.initializeClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testBucketConfigurationPropagation",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testBucketConfigurationPropagation() throws Throwable\n{\r\n    Configuration config = new Configuration(false);\r\n    setBucketOption(config, \"b\", \"base\", \"1024\");\r\n    String basekey = \"fs.s3a.base\";\r\n    assertOptionEquals(config, basekey, null);\r\n    String bucketKey = \"fs.s3a.bucket.b.base\";\r\n    assertOptionEquals(config, bucketKey, \"1024\");\r\n    Configuration updated = propagateBucketOptions(config, \"b\");\r\n    assertOptionEquals(updated, basekey, \"1024\");\r\n    assertOptionEquals(config, basekey, null);\r\n    String[] sources = updated.getPropertySources(basekey);\r\n    assertEquals(1, sources.length);\r\n    Assertions.assertThat(sources).describedAs(\"base key property sources\").hasSize(1);\r\n    Assertions.assertThat(sources[0]).describedAs(\"Property source\").contains(bucketKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testBucketConfigurationPropagationResolution",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testBucketConfigurationPropagationResolution() throws Throwable\n{\r\n    Configuration config = new Configuration(false);\r\n    String basekey = \"fs.s3a.base\";\r\n    String baseref = \"fs.s3a.baseref\";\r\n    String baseref2 = \"fs.s3a.baseref2\";\r\n    config.set(basekey, \"orig\");\r\n    config.set(baseref2, \"${fs.s3a.base}\");\r\n    setBucketOption(config, \"b\", basekey, \"1024\");\r\n    setBucketOption(config, \"b\", baseref, \"${fs.s3a.base}\");\r\n    Configuration updated = propagateBucketOptions(config, \"b\");\r\n    assertOptionEquals(updated, basekey, \"1024\");\r\n    assertOptionEquals(updated, baseref, \"1024\");\r\n    assertOptionEquals(updated, baseref2, \"1024\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testMultipleBucketConfigurations",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMultipleBucketConfigurations() throws Throwable\n{\r\n    Configuration config = new Configuration(false);\r\n    setBucketOption(config, \"b\", USER_AGENT_PREFIX, \"UA-b\");\r\n    setBucketOption(config, \"c\", USER_AGENT_PREFIX, \"UA-c\");\r\n    config.set(USER_AGENT_PREFIX, \"UA-orig\");\r\n    Configuration updated = propagateBucketOptions(config, \"c\");\r\n    assertOptionEquals(updated, USER_AGENT_PREFIX, \"UA-c\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testClearBucketOption",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testClearBucketOption() throws Throwable\n{\r\n    Configuration config = new Configuration();\r\n    config.set(USER_AGENT_PREFIX, \"base\");\r\n    setBucketOption(config, \"bucket\", USER_AGENT_PREFIX, \"overridden\");\r\n    clearBucketOption(config, \"bucket\", USER_AGENT_PREFIX);\r\n    Configuration updated = propagateBucketOptions(config, \"c\");\r\n    assertOptionEquals(updated, USER_AGENT_PREFIX, \"base\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testBucketConfigurationSkipsUnmodifiable",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testBucketConfigurationSkipsUnmodifiable() throws Throwable\n{\r\n    Configuration config = new Configuration(false);\r\n    String impl = \"fs.s3a.impl\";\r\n    config.set(impl, \"orig\");\r\n    setBucketOption(config, \"b\", impl, \"b\");\r\n    String changeDetectionMode = CHANGE_DETECT_MODE;\r\n    String client = CHANGE_DETECT_MODE_CLIENT;\r\n    setBucketOption(config, \"b\", changeDetectionMode, client);\r\n    setBucketOption(config, \"b\", \"impl2\", \"b2\");\r\n    setBucketOption(config, \"b\", \"bucket.b.loop\", \"b3\");\r\n    assertOptionEquals(config, \"fs.s3a.bucket.b.impl\", \"b\");\r\n    Configuration updated = propagateBucketOptions(config, \"b\");\r\n    assertOptionEquals(updated, impl, \"orig\");\r\n    assertOptionEquals(updated, \"fs.s3a.impl2\", \"b2\");\r\n    assertOptionEquals(updated, changeDetectionMode, client);\r\n    assertOptionEquals(updated, \"fs.s3a.bucket.b.loop\", null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSecurityCredentialPropagationNoOverride",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSecurityCredentialPropagationNoOverride() throws Exception\n{\r\n    Configuration config = new Configuration();\r\n    config.set(CREDENTIAL_PROVIDER_PATH, \"base\");\r\n    patchSecurityCredentialProviders(config);\r\n    assertOptionEquals(config, CREDENTIAL_PROVIDER_PATH, \"base\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSecurityCredentialPropagationOverrideNoBase",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSecurityCredentialPropagationOverrideNoBase() throws Exception\n{\r\n    Configuration config = new Configuration();\r\n    config.unset(CREDENTIAL_PROVIDER_PATH);\r\n    config.set(S3A_SECURITY_CREDENTIAL_PROVIDER_PATH, \"override\");\r\n    patchSecurityCredentialProviders(config);\r\n    assertOptionEquals(config, CREDENTIAL_PROVIDER_PATH, \"override\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSecurityCredentialPropagationOverride",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSecurityCredentialPropagationOverride() throws Exception\n{\r\n    Configuration config = new Configuration();\r\n    config.set(CREDENTIAL_PROVIDER_PATH, \"base\");\r\n    config.set(S3A_SECURITY_CREDENTIAL_PROVIDER_PATH, \"override\");\r\n    patchSecurityCredentialProviders(config);\r\n    assertOptionEquals(config, CREDENTIAL_PROVIDER_PATH, \"override,base\");\r\n    Collection<String> all = config.getStringCollection(CREDENTIAL_PROVIDER_PATH);\r\n    assertTrue(all.contains(\"override\"));\r\n    assertTrue(all.contains(\"base\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSecurityCredentialPropagationEndToEnd",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSecurityCredentialPropagationEndToEnd() throws Exception\n{\r\n    Configuration config = new Configuration();\r\n    config.set(CREDENTIAL_PROVIDER_PATH, \"base\");\r\n    setBucketOption(config, \"b\", S3A_SECURITY_CREDENTIAL_PROVIDER_PATH, \"override\");\r\n    Configuration updated = propagateBucketOptions(config, \"b\");\r\n    patchSecurityCredentialProviders(updated);\r\n    assertOptionEquals(updated, CREDENTIAL_PROVIDER_PATH, \"override,base\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testBucketConfigurationDeprecatedEncryptionAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBucketConfigurationDeprecatedEncryptionAlgorithm() throws Throwable\n{\r\n    Configuration config = new Configuration(false);\r\n    config.set(S3_ENCRYPTION_ALGORITHM, NEW_ALGORITHM_KEY_GLOBAL);\r\n    setBucketOption(config, \"b\", SERVER_SIDE_ENCRYPTION_ALGORITHM, OLD_ALGORITHM_KEY_BUCKET);\r\n    Configuration updated = propagateBucketOptions(config, \"b\");\r\n    String value = getEncryptionAlgorithm(\"b\", updated).getMethod();\r\n    Assertions.assertThat(value).describedAs(\"lookupPassword(%s)\", S3_ENCRYPTION_ALGORITHM).isEqualTo(OLD_ALGORITHM_KEY_BUCKET);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testJceksDeprecatedEncryptionAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testJceksDeprecatedEncryptionAlgorithm() throws Exception\n{\r\n    final Configuration conf = new Configuration(false);\r\n    final File file = tempDir.newFile(\"test.jks\");\r\n    final URI jks = ProviderUtils.nestURIForLocalJavaKeyStoreProvider(file.toURI());\r\n    conf.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, jks.toString());\r\n    final CredentialProvider provider = CredentialProviderFactory.getProviders(conf).get(0);\r\n    provider.createCredentialEntry(S3_ENCRYPTION_ALGORITHM, NEW_ALGORITHM_KEY_GLOBAL.toCharArray());\r\n    provider.createCredentialEntry(S3_ENCRYPTION_KEY, \"global s3 encryption key\".toCharArray());\r\n    provider.createCredentialEntry(FS_S3A_BUCKET_PREFIX + \"b.\" + SERVER_SIDE_ENCRYPTION_ALGORITHM, OLD_ALGORITHM_KEY_BUCKET.toCharArray());\r\n    final String bucketKey = \"bucket-server-side-encryption-key\";\r\n    provider.createCredentialEntry(FS_S3A_BUCKET_PREFIX + \"b.\" + SERVER_SIDE_ENCRYPTION_KEY, bucketKey.toCharArray());\r\n    provider.flush();\r\n    final EncryptionSecrets secrets = S3AUtils.buildEncryptionSecrets(\"b\", conf);\r\n    Assertions.assertThat(secrets.getEncryptionMethod().getMethod()).describedAs(\"buildEncryptionSecrets() encryption algorithm resolved to %s\", secrets).isEqualTo(OLD_ALGORITHM_KEY_BUCKET);\r\n    Assertions.assertThat(secrets.getEncryptionKey()).describedAs(\"buildEncryptionSecrets() encryption key resolved to %s\", secrets).isEqualTo(bucketKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "newJobCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DirectoryCommitterForTesting newJobCommitter() throws Exception\n{\r\n    return new DirectoryCommitterForTesting(outputPath, createTaskAttemptForJob());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "setupStaging",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setupStaging() throws Exception\n{\r\n    stagingDir = File.createTempFile(\"staging\", \"\");\r\n    stagingDir.delete();\r\n    stagingDir.mkdir();\r\n    stagingPath = new Path(stagingDir.toURI());\r\n    localFS = FileSystem.getLocal(new Configuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "teardownStaging",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownStaging() throws IOException\n{\r\n    try {\r\n        if (stagingDir != null) {\r\n            FileUtils.deleteDirectory(stagingDir);\r\n        }\r\n    } catch (IOException ignored) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "createJobConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobConf createJobConf()\n{\r\n    JobConf conf = super.createJobConf();\r\n    conf.setInt(CommitConstants.FS_S3A_COMMITTER_THREADS, 100);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getJobConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration getJobConf()\n{\r\n    return getJob().getConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "test_010_createTaskFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_010_createTaskFiles() throws Exception\n{\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"Creating %d test files in %s\", TOTAL_COMMIT_COUNT, stagingDir)) {\r\n        createTasks();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "createTasks",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void createTasks() throws IOException\n{\r\n    String tag = \"9062dcf18ffaee254821303bbd11c72b\";\r\n    List<PartETag> etags = IntStream.rangeClosed(1, BLOCKS_PER_TASK + 1).mapToObj(i -> new PartETag(i, tag)).collect(Collectors.toList());\r\n    SinglePendingCommit base = new SinglePendingCommit();\r\n    base.setBucket(BUCKET);\r\n    base.setJobId(\"0000\");\r\n    base.setLength(914688000);\r\n    base.bindCommitData(etags);\r\n    base.setDestinationKey(\"/base\");\r\n    base.setUploadId(\"uploadId\");\r\n    base.setUri(outputPathUri.toString());\r\n    SinglePendingCommit[] singles = new SinglePendingCommit[FILES_PER_TASK];\r\n    byte[] bytes = base.toBytes();\r\n    for (int i = 0; i < FILES_PER_TASK; i++) {\r\n        singles[i] = SinglePendingCommit.serializer().fromBytes(bytes);\r\n    }\r\n    int uploadCount = 0;\r\n    for (int task = 0; task < TASKS; task++) {\r\n        PendingSet pending = new PendingSet();\r\n        String taskId = String.format(\"task-%04d\", task);\r\n        for (int i = 0; i < FILES_PER_TASK; i++) {\r\n            String uploadId = String.format(\"%05d-task-%04d-file-%02d\", uploadCount, task, i);\r\n            Path p = new Path(outputPath, \"datasets/examples/testdirectoryscale/\" + \"year=2019/month=09/day=26/hour=20/second=53\" + uploadId);\r\n            URI dest = p.toUri();\r\n            SinglePendingCommit commit = singles[i];\r\n            String key = dest.getPath();\r\n            commit.setDestinationKey(key);\r\n            commit.setUri(dest.toString());\r\n            commit.touch(Instant.now().toEpochMilli());\r\n            commit.setTaskId(taskId);\r\n            commit.setUploadId(uploadId);\r\n            pending.add(commit);\r\n            activeUploads.put(uploadId, key);\r\n        }\r\n        Path path = new Path(stagingPath, String.format(\"task-%04d.\" + PENDINGSET_SUFFIX, task));\r\n        pending.save(localFS, path, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "test_020_loadFilesToAttempt",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void test_020_loadFilesToAttempt() throws Exception\n{\r\n    DirectoryStagingCommitter committer = newJobCommitter();\r\n    Configuration jobConf = getJobConf();\r\n    jobConf.set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, CONFLICT_MODE_APPEND);\r\n    FileSystem mockS3 = getMockS3A();\r\n    pathIsDirectory(mockS3, outputPath);\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"listing pending uploads\")) {\r\n        AbstractS3ACommitter.ActiveCommit activeCommit = committer.listPendingUploadsToCommit(getJob());\r\n        Assertions.assertThat(activeCommit.getSourceFiles()).describedAs(\"Source files of %s\", activeCommit).hasSize(TASKS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "test_030_commitFiles",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void test_030_commitFiles() throws Exception\n{\r\n    DirectoryCommitterForTesting committer = newJobCommitter();\r\n    StagingTestBase.ClientResults results = getMockResults();\r\n    results.addUploads(activeUploads);\r\n    Configuration jobConf = getJobConf();\r\n    jobConf.set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, CONFLICT_MODE_APPEND);\r\n    S3AFileSystem mockS3 = getMockS3A();\r\n    pathIsDirectory(mockS3, outputPath);\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"Committing Job\")) {\r\n        committer.commitJob(getJob());\r\n    }\r\n    Assertions.assertThat(results.getCommits()).describedAs(\"commit count\").hasSize(TOTAL_COMMIT_COUNT);\r\n    AbstractS3ACommitter.ActiveCommit activeCommit = committer.activeCommit;\r\n    Assertions.assertThat(activeCommit.getCommittedObjects()).describedAs(\"committed objects in active commit\").hasSize(Math.min(TOTAL_COMMIT_COUNT, CommitConstants.SUCCESS_MARKER_FILE_LIMIT));\r\n    Assertions.assertThat(activeCommit.getCommittedFileCount()).describedAs(\"committed objects in active commit\").isEqualTo(TOTAL_COMMIT_COUNT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "test_040_abortFiles",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void test_040_abortFiles() throws Exception\n{\r\n    DirectoryStagingCommitter committer = newJobCommitter();\r\n    getMockResults().addUploads(activeUploads);\r\n    Configuration jobConf = getJobConf();\r\n    jobConf.set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, CONFLICT_MODE_APPEND);\r\n    FileSystem mockS3 = getMockS3A();\r\n    pathIsDirectory(mockS3, outputPath);\r\n    committer.abortJob(getJob(), JobStatus.State.FAILED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getBlockOutputBufferName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockOutputBufferName()\n{\r\n    return Constants.FAST_UPLOAD_BUFFER_DISK;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "newJobCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PartitionedStagingCommitter newJobCommitter() throws IOException\n{\r\n    return new PartitionedStagingCommitter(outputPath, createTaskAttemptForJob());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "newTaskCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PartitionedStagingCommitter newTaskCommitter() throws Exception\n{\r\n    return new PartitionedStagingCommitter(outputPath, getTAC());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "createRelativeFileList",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createRelativeFileList()\n{\r\n    for (String dateint : Arrays.asList(\"20161115\", \"20161116\")) {\r\n        for (String hour : Arrays.asList(\"14\", \"15\")) {\r\n            String relative = \"dateint=\" + dateint + \"/hour=\" + hour + \"/\" + UUID.randomUUID().toString() + \".parquet\";\r\n            relativeFiles.add(relative);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testBadConflictMode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBadConflictMode() throws Throwable\n{\r\n    getJob().getConfiguration().set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, \"merge\");\r\n    intercept(IllegalArgumentException.class, \"MERGE\", \"committer conflict\", this::newJobCommitter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testDefault",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testDefault() throws Exception\n{\r\n    JobContext job = getJob();\r\n    job.getConfiguration().unset(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE);\r\n    final PartitionedStagingCommitter committer = newTaskCommitter();\r\n    committer.setupTask(getTAC());\r\n    assertConflictResolution(committer, job, ConflictResolution.APPEND);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testFail",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testFail() throws Exception\n{\r\n    FileSystem mockS3 = getMockS3A();\r\n    getTAC().getConfiguration().set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, CONFLICT_MODE_FAIL);\r\n    final PartitionedStagingCommitter committer = newTaskCommitter();\r\n    committer.setupTask(getTAC());\r\n    createTestOutputFiles(relativeFiles, committer.getTaskAttemptPath(getTAC()), getTAC().getConfiguration());\r\n    reset(mockS3);\r\n    Path existsPath = new Path(outputPath, relativeFiles.get(1)).getParent();\r\n    pathExists(mockS3, existsPath);\r\n    intercept(PathExistsException.class, \"\", \"Should complain because a partition already exists: \" + existsPath, () -> committer.commitTask(getTAC()));\r\n    reset(mockS3);\r\n    committer.commitTask(getTAC());\r\n    verifyFilesCreated(committer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testAppend",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testAppend() throws Exception\n{\r\n    FileSystem mockS3 = getMockS3A();\r\n    getTAC().getConfiguration().set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, CONFLICT_MODE_APPEND);\r\n    PartitionedStagingCommitter committer = newTaskCommitter();\r\n    committer.setupTask(getTAC());\r\n    createTestOutputFiles(relativeFiles, committer.getTaskAttemptPath(getTAC()), getTAC().getConfiguration());\r\n    reset(mockS3);\r\n    pathExists(mockS3, new Path(outputPath, relativeFiles.get(2)).getParent());\r\n    committer.commitTask(getTAC());\r\n    verifyFilesCreated(committer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "verifyFilesCreated",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void verifyFilesCreated(final PartitionedStagingCommitter committer)\n{\r\n    Set<String> files = Sets.newHashSet();\r\n    for (InitiateMultipartUploadRequest request : getMockResults().getRequests().values()) {\r\n        assertEquals(BUCKET, request.getBucketName());\r\n        files.add(request.getKey());\r\n    }\r\n    Assertions.assertThat(files).describedAs(\"Should have the right number of uploads\").hasSize(relativeFiles.size());\r\n    Set<String> expected = buildExpectedList(committer);\r\n    Assertions.assertThat(files).describedAs(\"Should have correct paths\").containsExactlyInAnyOrderElementsOf(expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testReplace",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testReplace() throws Exception\n{\r\n    FileSystem mockS3 = getMockS3A();\r\n    getTAC().getConfiguration().set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, CONFLICT_MODE_REPLACE);\r\n    PartitionedStagingCommitter committer = newTaskCommitter();\r\n    committer.setupTask(getTAC());\r\n    createTestOutputFiles(relativeFiles, committer.getTaskAttemptPath(getTAC()), getTAC().getConfiguration());\r\n    reset(mockS3);\r\n    pathExists(mockS3, new Path(outputPath, relativeFiles.get(3)).getParent());\r\n    committer.commitTask(getTAC());\r\n    verifyFilesCreated(committer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "buildExpectedList",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Set<String> buildExpectedList(StagingCommitter committer)\n{\r\n    Set<String> expected = Sets.newHashSet();\r\n    boolean unique = committer.useUniqueFilenames();\r\n    for (String relative : relativeFiles) {\r\n        expected.add(OUTPUT_PREFIX + \"/\" + (unique ? Paths.addUUID(relative, committer.getUUID()) : relative));\r\n    }\r\n    return expected;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "uri",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI uri(String s)\n{\r\n    try {\r\n        return new URI(s);\r\n    } catch (URISyntaxException e) {\r\n        throw new RuntimeException(e.toString(), e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "assertMatchesEndpoint",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertMatchesEndpoint(URI uri)\n{\r\n    assertEquals(\"Source \" + uri, ENDPOINT, S3xLoginHelper.buildFSURI(uri));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "assertInvalid",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertInvalid(URI uri) throws Exception\n{\r\n    intercept(IllegalArgumentException.class, S3xLoginHelper.LOGIN_WARNING, () -> S3xLoginHelper.buildFSURI(uri));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "assertMatchesLogin",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "S3xLoginHelper.Login assertMatchesLogin(String user, String pass, URI uri)\n{\r\n    S3xLoginHelper.Login expected = new S3xLoginHelper.Login(user, pass);\r\n    S3xLoginHelper.Login actual = S3xLoginHelper.extractLoginDetails(uri);\r\n    if (!expected.equals(actual)) {\r\n        Assert.fail(\"Source \" + uri + \" login expected=:\" + toString(expected) + \" actual=\" + toString(actual));\r\n    }\r\n    return actual;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "testSimpleFSURI",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSimpleFSURI() throws Throwable\n{\r\n    assertMatchesEndpoint(ENDPOINT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "testLoginSimple",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testLoginSimple() throws Throwable\n{\r\n    S3xLoginHelper.Login login = assertMatchesLogin(\"\", \"\", ENDPOINT);\r\n    assertFalse(\"Login of \" + login, login.hasLogin());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "testLoginWithUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLoginWithUser() throws Throwable\n{\r\n    assertMatchesLogin(USER, \"\", USER_NO_PASS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "testLoginWithUserAndColon",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLoginWithUserAndColon() throws Throwable\n{\r\n    assertMatchesLogin(USER, \"\", WITH_USER_AND_COLON);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "testLoginNoUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLoginNoUser() throws Throwable\n{\r\n    assertMatchesLogin(\"\", \"\", NO_USER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "testLoginNoUserNoPass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLoginNoUserNoPass() throws Throwable\n{\r\n    assertMatchesLogin(\"\", \"\", NO_USER_NO_PASS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "testLoginNoUserNoPassTwoColon",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLoginNoUserNoPassTwoColon() throws Throwable\n{\r\n    assertMatchesLogin(\"\", \"\", NO_USER_NO_PASS_TWO_COLON);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "testFsUriWithUserAndPass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFsUriWithUserAndPass() throws Throwable\n{\r\n    assertInvalid(WITH_USER_AND_PASS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "testFsUriWithSlashInPass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFsUriWithSlashInPass() throws Throwable\n{\r\n    assertInvalid(WITH_SLASH_IN_PASS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "testFsUriWithPlusInPass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFsUriWithPlusInPass() throws Throwable\n{\r\n    assertInvalid(WITH_PLUS_IN_PASS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "testFsUriWithPlusRawInPass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFsUriWithPlusRawInPass() throws Throwable\n{\r\n    assertInvalid(WITH_PLUS_RAW_IN_PASS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "testFsUriWithUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFsUriWithUser() throws Throwable\n{\r\n    assertInvalid(USER_NO_PASS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "testFsUriWithUserAndColon",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFsUriWithUserAndColon() throws Throwable\n{\r\n    assertInvalid(WITH_USER_AND_COLON);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "testFsiNoUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFsiNoUser() throws Throwable\n{\r\n    assertMatchesEndpoint(NO_USER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "testFsUriNoUserNoPass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFsUriNoUserNoPass() throws Throwable\n{\r\n    assertMatchesEndpoint(NO_USER_NO_PASS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "testFsUriNoUserNoPassTwoColon",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFsUriNoUserNoPassTwoColon() throws Throwable\n{\r\n    assertMatchesEndpoint(NO_USER_NO_PASS_TWO_COLON);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String toString(S3xLoginHelper.Login login)\n{\r\n    final StringBuilder sb = new StringBuilder(\"LoginTuple{\");\r\n    sb.append(\"<'\").append(login.getUser()).append('\\'');\r\n    sb.append(\", '\").append(login.getPassword()).append('\\'');\r\n    sb.append('>');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testRequestFactoryWithEncryption",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRequestFactoryWithEncryption() throws Throwable\n{\r\n    RequestFactory factory = RequestFactoryImpl.builder().withBucket(\"bucket\").withEncryptionSecrets(new EncryptionSecrets(S3AEncryptionMethods.SSE_KMS, \"kms:key\")).build();\r\n    createFactoryObjects(factory);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testRequestFactoryWithCannedACL",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRequestFactoryWithCannedACL() throws Throwable\n{\r\n    CannedAccessControlList acl = CannedAccessControlList.BucketOwnerFullControl;\r\n    RequestFactory factory = RequestFactoryImpl.builder().withBucket(\"bucket\").withCannedACL(acl).build();\r\n    String path = \"path\";\r\n    String path2 = \"path2\";\r\n    ObjectMetadata md = factory.newObjectMetadata(128);\r\n    Assertions.assertThat(factory.newPutObjectRequest(path, md, new ByteArrayInputStream(new byte[0])).getCannedAcl()).describedAs(\"ACL of PUT\").isEqualTo(acl);\r\n    Assertions.assertThat(factory.newCopyObjectRequest(path, path2, md).getCannedAccessControlList()).describedAs(\"ACL of COPY\").isEqualTo(acl);\r\n    Assertions.assertThat(factory.newMultipartUploadRequest(path).getCannedACL()).describedAs(\"ACL of MPU\").isEqualTo(acl);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testRequestFactoryWithProcessor",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testRequestFactoryWithProcessor() throws Throwable\n{\r\n    CountRequests countRequests = new CountRequests();\r\n    RequestFactory factory = RequestFactoryImpl.builder().withBucket(\"bucket\").withRequestPreparer(countRequests).build();\r\n    createFactoryObjects(factory);\r\n    assertThat(countRequests.counter.get()).describedAs(\"request preparation count\").isEqualTo(requestsAnalyzed);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "a",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AWSRequestAnalyzer.RequestInfo a(T request)\n{\r\n    AWSRequestAnalyzer.RequestInfo info = analyzer.analyze(request);\r\n    LOG.info(\"{}\", info);\r\n    requestsAnalyzed++;\r\n    return info;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "createFactoryObjects",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void createFactoryObjects(RequestFactory factory)\n{\r\n    String path = \"path\";\r\n    String path2 = \"path2\";\r\n    String id = \"1\";\r\n    ObjectMetadata md = factory.newObjectMetadata(128);\r\n    a(factory.newAbortMultipartUploadRequest(path, id));\r\n    a(factory.newCompleteMultipartUploadRequest(path, id, new ArrayList<>()));\r\n    a(factory.newCopyObjectRequest(path, path2, md));\r\n    a(factory.newDeleteObjectRequest(path));\r\n    a(factory.newBulkDeleteRequest(new ArrayList<>()));\r\n    a(factory.newDirectoryMarkerRequest(path));\r\n    a(factory.newGetObjectRequest(path));\r\n    a(factory.newGetObjectMetadataRequest(path));\r\n    a(factory.newListMultipartUploadsRequest(path));\r\n    a(factory.newListObjectsV1Request(path, \"/\", 1));\r\n    a(factory.newListNextBatchOfObjectsRequest(new ObjectListing()));\r\n    a(factory.newListObjectsV2Request(path, \"/\", 1));\r\n    a(factory.newMultipartUploadRequest(path));\r\n    File srcfile = new File(\"/tmp/a\");\r\n    a(factory.newPutObjectRequest(path, factory.newObjectMetadata(-1), srcfile));\r\n    ByteArrayInputStream stream = new ByteArrayInputStream(new byte[0]);\r\n    a(factory.newPutObjectRequest(path, md, stream));\r\n    a(factory.newSelectRequest(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testMultipartUploadRequest",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testMultipartUploadRequest() throws Throwable\n{\r\n    CountRequests countRequests = new CountRequests();\r\n    RequestFactory factory = RequestFactoryImpl.builder().withBucket(\"bucket\").withRequestPreparer(countRequests).build();\r\n    String path = \"path\";\r\n    String path2 = \"path2\";\r\n    String id = \"1\";\r\n    File srcfile = File.createTempFile(\"file\", \"\");\r\n    try {\r\n        ByteArrayInputStream stream = new ByteArrayInputStream(new byte[0]);\r\n        a(factory.newUploadPartRequest(path, id, 1, 0, stream, null, 0));\r\n        a(factory.newUploadPartRequest(path, id, 2, 128_000_000, null, srcfile, 0));\r\n        intercept(IllegalArgumentException.class, () -> factory.newUploadPartRequest(path, id, 3, 128_000_000, null, srcfile, 128));\r\n    } finally {\r\n        srcfile.delete();\r\n    }\r\n    assertThat(countRequests.counter.get()).describedAs(\"request preparation count\").isEqualTo(requestsAnalyzed);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    String bucketName = getTestBucketName(conf);\r\n    String arnKey = String.format(InternalConstants.ARN_BUCKET_OPTION, bucketName);\r\n    String arn = conf.getTrimmed(arnKey, \"\");\r\n    removeBaseAndBucketOverrides(bucketName, conf, DIRECTORY_MARKER_POLICY, AUTHORITATIVE_PATH);\r\n    conf.set(DIRECTORY_MARKER_POLICY, keepMarkers ? DIRECTORY_MARKER_POLICY_KEEP : DIRECTORY_MARKER_POLICY_DELETE);\r\n    disableFilesystemCaching(conf);\r\n    if (!arn.isEmpty()) {\r\n        conf.set(arnKey, arn);\r\n    }\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    S3AFileSystem fs = getFileSystem();\r\n    isKeeping = isKeepingMarkers();\r\n    isDeleting = !isKeeping;\r\n    DirectoryPolicy markerPolicy = fs.getDirectoryMarkerPolicy();\r\n    Assertions.assertThat(markerPolicy.getMarkerPolicy()).describedAs(\"Marker policy for filesystem %s\", fs).isEqualTo(isKeepingMarkers() ? DirectoryPolicy.MarkerPolicy.Keep : DirectoryPolicy.MarkerPolicy.Delete);\r\n    OperationCostValidator.Builder builder = OperationCostValidator.builder(getFileSystem());\r\n    EnumSet.allOf(Statistic.class).stream().filter(s -> s.getType() == StatisticTypeEnum.TYPE_COUNTER || s.getType() == StatisticTypeEnum.TYPE_DURATION).forEach(s -> builder.withMetric(s));\r\n    costValidator = builder.build();\r\n    final Configuration fsConf = getFileSystem().getConf();\r\n    isBulkDelete = fsConf.getBoolean(Constants.ENABLE_MULTI_DELETE, true);\r\n    deleteMarkerStatistic = isBulkDelete() ? OBJECT_BULK_DELETE_REQUEST : OBJECT_DELETE_REQUEST;\r\n    setSpanSource(fs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "isDeleting",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isDeleting()\n{\r\n    return isDeleting;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "isKeepingMarkers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isKeepingMarkers()\n{\r\n    return keepMarkers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "getMetricSummary",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getMetricSummary()\n{\r\n    return costValidator;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "buildFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path buildFile(Path path, boolean overwrite, boolean recursive, OperationCost cost) throws Exception\n{\r\n    resetStatistics();\r\n    verify(cost, () -> {\r\n        FSDataOutputStreamBuilder builder = getFileSystem().createFile(path).overwrite(overwrite);\r\n        if (recursive) {\r\n            builder.recursive();\r\n        }\r\n        FSDataOutputStream stream = builder.build();\r\n        stream.close();\r\n        return stream.toString();\r\n    });\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "dir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path dir(Path p) throws IOException\n{\r\n    mkdirs(p);\r\n    return p;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "file",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path file(Path p) throws IOException\n{\r\n    return file(p, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "file",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path file(Path path, final boolean overwrite) throws IOException\n{\r\n    getFileSystem().create(path, overwrite).close();\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path create(Path path) throws Exception\n{\r\n    return create(path, true, CREATE_FILE_OVERWRITE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path create(Path path, boolean overwrite, OperationCost cost) throws Exception\n{\r\n    return verify(cost, () -> file(path, overwrite));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "execRename",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String execRename(final Path source, final Path dest) throws IOException\n{\r\n    getFileSystem().rename(source, dest);\r\n    return String.format(\"rename(%s, %s): %s\", dest, source, getMetricSummary());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "directoriesInPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int directoriesInPath(Path path)\n{\r\n    return path.isRoot() ? 0 : 1 + directoriesInPath(path.getParent());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "resetStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void resetStatistics()\n{\r\n    costValidator.resetMetricDiffs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "verifyMetrics",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T verifyMetrics(Callable<T> eval, OperationCostValidator.ExpectedProbe... expected) throws Exception\n{\r\n    span();\r\n    return costValidator.exec(eval, expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "verifyMetricsIntercepting",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "E verifyMetricsIntercepting(Class<E> clazz, String text, Callable<T> eval, OperationCostValidator.ExpectedProbe... expected) throws Exception\n{\r\n    span();\r\n    return costValidator.intercepting(clazz, text, eval, expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "interceptOperation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "E interceptOperation(Class<E> clazz, String text, OperationCost cost, Callable<T> eval) throws Exception\n{\r\n    return verifyMetricsIntercepting(clazz, text, eval, always(cost));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "always",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OperationCostValidator.ExpectedProbe always(OperationCost cost)\n{\r\n    return expect(true, cost);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "whenKeeping",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OperationCostValidator.ExpectedProbe whenKeeping(OperationCost cost)\n{\r\n    return expect(isKeepingMarkers(), cost);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "whenDeleting",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OperationCostValidator.ExpectedProbe whenDeleting(OperationCost cost)\n{\r\n    return expect(isDeleting(), cost);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "verify",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T verify(OperationCost cost, Callable<T> eval) throws Exception\n{\r\n    return verifyMetrics(eval, always(cost), OperationCostValidator.always());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "verifyInnerGetFileStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileStatus verifyInnerGetFileStatus(Path path, boolean needEmptyDirectoryFlag, Set<StatusProbeEnum> probes, OperationCost cost) throws Exception\n{\r\n    return verify(cost, () -> innerGetFileStatus(getFileSystem(), path, needEmptyDirectoryFlag, probes));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "interceptGetFileStatusFNFE",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void interceptGetFileStatusFNFE(Path path, boolean needEmptyDirectoryFlag, Set<StatusProbeEnum> probes, OperationCost cost) throws Exception\n{\r\n    try (AuditSpan span = span()) {\r\n        interceptOperation(FileNotFoundException.class, \"\", cost, () -> innerGetFileStatus(getFileSystem(), path, needEmptyDirectoryFlag, probes));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "isDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void isDir(Path path, boolean expected, OperationCost cost) throws Exception\n{\r\n    boolean b = verify(cost, () -> getFileSystem().isDirectory(path));\r\n    Assertions.assertThat(b).describedAs(\"isDirectory(%s)\", path).isEqualTo(expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "isFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void isFile(Path path, boolean expected, OperationCost cost) throws Exception\n{\r\n    boolean b = verify(cost, () -> getFileSystem().isFile(path));\r\n    Assertions.assertThat(b).describedAs(\"isFile(%s)\", path).isEqualTo(expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "with",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OperationCostValidator.ExpectedProbe with(final Statistic stat, final int expected)\n{\r\n    return probe(stat, expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "withWhenKeeping",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OperationCostValidator.ExpectedProbe withWhenKeeping(final Statistic stat, final int expected)\n{\r\n    return probe(isKeepingMarkers(), stat, expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "withWhenDeleting",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OperationCostValidator.ExpectedProbe withWhenDeleting(final Statistic stat, final int expected)\n{\r\n    return probe(isDeleting(), stat, expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "assertEmptyDirStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertEmptyDirStatus(final S3AFileStatus status, final Tristate expected)\n{\r\n    Assertions.assertThat(status.isEmptyDirectory()).describedAs(dynamicDescription(() -> \"FileStatus says directory is not empty: \" + status + \"\\n\" + ContractTestUtils.ls(getFileSystem(), status.getPath()))).isEqualTo(expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "isBulkDelete",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isBulkDelete()\n{\r\n    return isBulkDelete;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "getDeleteMarkerStatistic",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Statistic getDeleteMarkerStatistic()\n{\r\n    return deleteMarkerStatistic;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    resetAuditOptions(conf);\r\n    conf.set(AUDIT_SERVICE_CLASSNAME, AccessCheckingAuditor.CLASS);\r\n    conf.setBoolean(AUDIT_ENABLED, true);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    auditor = (AccessCheckingAuditor) getFileSystem().getAuditor();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testFileAccessAllowed",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testFileAccessAllowed() throws Throwable\n{\r\n    describe(\"Enable checkaccess and verify it works with expected\" + \" statistics\");\r\n    auditor.setAccessAllowed(true);\r\n    Path path = methodPath();\r\n    S3AFileSystem fs = getFileSystem();\r\n    touch(fs, path);\r\n    verifyMetrics(() -> access(fs, path), with(INVOCATION_ACCESS, 1), always(FILE_STATUS_FILE_PROBE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testDirAccessAllowed",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testDirAccessAllowed() throws Throwable\n{\r\n    describe(\"Enable checkaccess and verify it works with a dir\");\r\n    auditor.setAccessAllowed(true);\r\n    Path path = methodPath();\r\n    S3AFileSystem fs = getFileSystem();\r\n    mkdirs(path);\r\n    verifyMetrics(() -> access(fs, path), with(INVOCATION_ACCESS, 1), always(FILE_STATUS_ALL_PROBES));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testRootDirAccessAllowed",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRootDirAccessAllowed() throws Throwable\n{\r\n    describe(\"Enable checkaccess and verify root dir access\");\r\n    auditor.setAccessAllowed(true);\r\n    Path path = new Path(\"/\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    mkdirs(path);\r\n    verifyMetrics(() -> access(fs, path), with(INVOCATION_ACCESS, 1), always(ROOT_FILE_STATUS_PROBE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testFileAccessDenied",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testFileAccessDenied() throws Throwable\n{\r\n    describe(\"Disable checkaccess and verify it fails\");\r\n    auditor.setAccessAllowed(false);\r\n    Path path = methodPath();\r\n    S3AFileSystem fs = getFileSystem();\r\n    touch(fs, path);\r\n    verifyMetricsIntercepting(AccessControlException.class, \"\\\"\" + path + \"\\\"\", () -> access(fs, path), with(INVOCATION_ACCESS, 1), with(AUDIT_ACCESS_CHECK_FAILURE, 1), with(AUDIT_REQUEST_EXECUTION, 1), always(FILE_STATUS_FILE_PROBE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testDirAccessDenied",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testDirAccessDenied() throws Throwable\n{\r\n    describe(\"Disable checkaccess and verify it dir access denied\");\r\n    auditor.setAccessAllowed(false);\r\n    Path path = methodPath();\r\n    S3AFileSystem fs = getFileSystem();\r\n    mkdirs(path);\r\n    verifyMetricsIntercepting(AccessControlException.class, \"\\\"\" + path + \"\\\"\", () -> access(fs, path), with(INVOCATION_ACCESS, 1), with(AUDIT_REQUEST_EXECUTION, 2), with(STORE_IO_REQUEST, 2), with(AUDIT_ACCESS_CHECK_FAILURE, 1), always(FILE_STATUS_ALL_PROBES));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testMissingPathAccessFNFE",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMissingPathAccessFNFE() throws Throwable\n{\r\n    describe(\"access() on missing path goes to S3 and fails with FNFE\");\r\n    auditor.setAccessAllowed(false);\r\n    Path path = methodPath();\r\n    S3AFileSystem fs = getFileSystem();\r\n    verifyMetricsIntercepting(FileNotFoundException.class, path.toString(), () -> access(fs, path), with(INVOCATION_ACCESS, 1), with(AUDIT_REQUEST_EXECUTION, 2), with(AUDIT_ACCESS_CHECK_FAILURE, 0), always(FILE_STATUS_ALL_PROBES));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "access",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String access(final S3AFileSystem fs, final Path path) throws AccessControlException, IOException\n{\r\n    fs.access(path, FsAction.ALL);\r\n    return ioStatisticsToPrettyString(fs.getIOStatistics());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "createScaleConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration createScaleConfiguration()\n{\r\n    Configuration configuration = super.createScaleConfiguration();\r\n    removeBaseAndBucketOverrides(configuration, MULTIPART_SIZE, UPLOAD_PART_COUNT_LIMIT);\r\n    configuration.setLong(MULTIPART_SIZE, MPU_SIZE);\r\n    configuration.setLong(UPLOAD_PART_COUNT_LIMIT, 2);\r\n    return configuration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testTwoPartUpload",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testTwoPartUpload() throws Throwable\n{\r\n    Path file = path(getMethodName());\r\n    createFile(getFileSystem(), file, true, dataset(6 * _1MB, 'a', 'z' - 'a'));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testUploadOverLimitFailure",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testUploadOverLimitFailure() throws Throwable\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path file = path(getMethodName());\r\n    intercept(PathIOException.class, () -> createFile(fs, file, false, dataset(15 * _1MB, 'a', 'z' - 'a')));\r\n    assertPathDoesNotExist(\"upload must not have completed\", file);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testCommitLimitFailure",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testCommitLimitFailure() throws Throwable\n{\r\n    describe(\"verify commit uploads fail-safe when MPU limits exceeded\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    CommitOperations actions = new CommitOperations(fs);\r\n    File tempFile = File.createTempFile(\"commit\", \".txt\");\r\n    FileUtils.writeByteArrayToFile(tempFile, dataset(15 * _1MB, 'a', 'z' - 'a'));\r\n    Path dest = methodPath();\r\n    final S3AInstrumentation instrumentation = fs.getInstrumentation();\r\n    final long initial = instrumentation.getCounterValue(Statistic.COMMITTER_COMMITS_ABORTED);\r\n    intercept(PathIOException.class, () -> actions.uploadFileToPendingCommit(tempFile, dest, null, MPU_SIZE, new ProgressCounter()));\r\n    assertPathDoesNotExist(\"upload must not have completed\", dest);\r\n    final long after = instrumentation.getCounterValue(Statistic.COMMITTER_COMMITS_ABORTED);\r\n    Assertions.assertThat(after).describedAs(\"commit abort count\").isEqualTo(initial + 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testAbortAfterTwoPartUpload",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testAbortAfterTwoPartUpload() throws Throwable\n{\r\n    Path file = path(getMethodName());\r\n    byte[] data = dataset(6 * _1MB, 'a', 'z' - 'a');\r\n    S3AFileSystem fs = getFileSystem();\r\n    FSDataOutputStream stream = fs.create(file, true);\r\n    try {\r\n        stream.write(data);\r\n        assertCompleteAbort(stream.abort());\r\n        assertPathDoesNotExist(\"upload must not have completed\", file);\r\n    } finally {\r\n        IOUtils.closeStream(stream);\r\n        assertPathDoesNotExist(\"upload must not have completed\", file);\r\n    }\r\n    verifyStreamWasAborted(fs, stream);\r\n    assertNoopAbort(stream.abort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testAbortWhenOverwritingAFile",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testAbortWhenOverwritingAFile() throws Throwable\n{\r\n    Path file = path(getMethodName());\r\n    S3AFileSystem fs = getFileSystem();\r\n    byte[] smallData = writeTextFile(fs, file, \"original\", true);\r\n    byte[] data = dataset(6 * _1MB, 'a', 'z' - 'a');\r\n    FSDataOutputStream stream = fs.create(file, true);\r\n    try {\r\n        ContractTestUtils.assertCapabilities(stream, new String[] { ABORTABLE_STREAM }, null);\r\n        stream.write(data);\r\n        assertCompleteAbort(stream.abort());\r\n        verifyFileContents(fs, file, smallData);\r\n    } finally {\r\n        IOUtils.closeStream(stream);\r\n    }\r\n    verifyFileContents(fs, file, smallData);\r\n    verifyStreamWasAborted(fs, stream);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "verifyStreamWasAborted",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void verifyStreamWasAborted(final S3AFileSystem fs, final FSDataOutputStream stream)\n{\r\n    final IOStatistics iostats = stream.getIOStatistics();\r\n    final String sstr = ioStatisticsToPrettyString(iostats);\r\n    LOG.info(\"IOStatistics for stream: {}\", sstr);\r\n    verifyStatisticCounterValue(iostats, INVOCATION_ABORT.getSymbol(), 1);\r\n    verifyStatisticCounterValue(iostats, OBJECT_MULTIPART_UPLOAD_ABORTED.getSymbol(), 1);\r\n    final IOStatistics fsIostats = fs.getIOStatistics();\r\n    assertThatStatisticCounter(fsIostats, INVOCATION_ABORT.getSymbol()).isGreaterThanOrEqualTo(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "createSessionToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createSessionToken() throws URISyntaxException\n{\r\n    bucketURI = new URI(\"s3a://bucket1\");\r\n    credentials = new MarshalledCredentials(\"accessKey\", \"secretKey\", \"sessionToken\");\r\n    credentials.setRoleARN(\"roleARN\");\r\n    expiration = 1970;\r\n    credentials.setExpiration(expiration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testRoundTrip",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRoundTrip() throws Throwable\n{\r\n    MarshalledCredentials c2 = S3ATestUtils.roundTrip(this.credentials, new Configuration());\r\n    assertEquals(credentials, c2);\r\n    assertEquals(\"accessKey\", c2.getAccessKey());\r\n    assertEquals(\"secretKey\", c2.getSecretKey());\r\n    assertEquals(\"sessionToken\", c2.getSessionToken());\r\n    assertEquals(expiration, c2.getExpiration());\r\n    assertEquals(credentials, c2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testRoundTripNoSessionData",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testRoundTripNoSessionData() throws Throwable\n{\r\n    MarshalledCredentials c = new MarshalledCredentials();\r\n    c.setAccessKey(\"A\");\r\n    c.setSecretKey(\"K\");\r\n    MarshalledCredentials c2 = S3ATestUtils.roundTrip(c, new Configuration());\r\n    assertEquals(c, c2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testRoundTripEncryptionData",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRoundTripEncryptionData() throws Throwable\n{\r\n    EncryptionSecrets secrets = new EncryptionSecrets(S3AEncryptionMethods.SSE_KMS, \"key\");\r\n    EncryptionSecrets result = S3ATestUtils.roundTrip(secrets, new Configuration());\r\n    assertEquals(\"round trip\", secrets, result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testMarshalledCredentialProviderSession",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testMarshalledCredentialProviderSession() throws Throwable\n{\r\n    MarshalledCredentialProvider provider = new MarshalledCredentialProvider(\"test\", bucketURI, new Configuration(false), credentials, MarshalledCredentials.CredentialTypeRequired.SessionOnly);\r\n    AWSCredentials aws = provider.getCredentials();\r\n    assertEquals(credentials.toString(), credentials.getAccessKey(), aws.getAWSAccessKeyId());\r\n    assertEquals(credentials.toString(), credentials.getSecretKey(), aws.getAWSSecretKey());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testCredentialTypeMismatch",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCredentialTypeMismatch() throws Throwable\n{\r\n    MarshalledCredentialProvider provider = new MarshalledCredentialProvider(\"test\", bucketURI, new Configuration(false), credentials, MarshalledCredentials.CredentialTypeRequired.FullOnly);\r\n    intercept(NoAuthWithAWSException.class, \"test\", () -> provider.getCredentials());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testCredentialProviderNullURI",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCredentialProviderNullURI() throws Throwable\n{\r\n    intercept(NullPointerException.class, \"\", () -> new MarshalledCredentialProvider(\"test\", null, new Configuration(false), credentials, MarshalledCredentials.CredentialTypeRequired.FullOnly));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\fileContext",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws IOException, Exception\n{\r\n    Configuration conf = new Configuration();\r\n    fc = S3ATestUtils.createTestFileContext(conf);\r\n    super.setUp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\fileContext",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (fc != null) {\r\n        super.tearDown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    enableChecksums(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    final Configuration conf = super.createConfiguration();\r\n    removeBaseAndBucketOverrides(conf, S3_ENCRYPTION_ALGORITHM, S3_ENCRYPTION_KEY, SERVER_SIDE_ENCRYPTION_ALGORITHM, SERVER_SIDE_ENCRYPTION_KEY);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "enableChecksums",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void enableChecksums(final boolean enabled)\n{\r\n    getFileSystem().getConf().setBoolean(Constants.ETAG_CHECKSUM_ENABLED, enabled);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCreateNonRecursiveSuccess",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCreateNonRecursiveSuccess() throws IOException\n{\r\n    Path shouldWork = path(\"nonrecursivenode\");\r\n    try (FSDataOutputStream out = createNonRecursive(shouldWork)) {\r\n        out.write(0);\r\n        out.close();\r\n    }\r\n    assertIsFile(shouldWork);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCreateNonRecursiveNoParent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCreateNonRecursiveNoParent() throws IOException\n{\r\n    createNonRecursive(path(\"/recursive/node\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCreateNonRecursiveParentIsFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCreateNonRecursiveParentIsFile() throws IOException\n{\r\n    Path parent = path(\"/file.txt\");\r\n    touch(getFileSystem(), parent);\r\n    createNonRecursive(new Path(parent, \"fail\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testPutObjectDirect",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testPutObjectDirect() throws Throwable\n{\r\n    final S3AFileSystem fs = getFileSystem();\r\n    try (AuditSpan span = span()) {\r\n        ObjectMetadata metadata = fs.newObjectMetadata(-1);\r\n        metadata.setContentLength(-1);\r\n        Path path = path(\"putDirect\");\r\n        final PutObjectRequest put = new PutObjectRequest(fs.getBucket(), path.toUri().getPath(), new ByteArrayInputStream(\"PUT\".getBytes()), metadata);\r\n        LambdaTestUtils.intercept(IllegalStateException.class, () -> fs.putObjectDirect(put));\r\n        assertPathDoesNotExist(\"put object was created\", path);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FSDataOutputStream createNonRecursive(Path path) throws IOException\n{\r\n    return getFileSystem().createNonRecursive(path, false, 4096, (short) 3, (short) 4096, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "touchFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path touchFile(String name) throws IOException\n{\r\n    Path path = path(name);\r\n    touch(getFileSystem(), path);\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "mkFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path mkFile(String name, byte[] data) throws IOException\n{\r\n    final Path f = path(name);\r\n    createFile(getFileSystem(), f, true, data);\r\n    return f;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEmptyFileChecksums",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testEmptyFileChecksums() throws Throwable\n{\r\n    assumeNoDefaultEncryption();\r\n    final S3AFileSystem fs = getFileSystem();\r\n    Path file1 = touchFile(\"file1\");\r\n    EtagChecksum checksum1 = fs.getFileChecksum(file1, 0);\r\n    LOG.info(\"Checksum for {}: {}\", file1, checksum1);\r\n    assertHasPathCapabilities(fs, file1, CommonPathCapabilities.FS_CHECKSUMS);\r\n    assertNotNull(\"Null file 1 checksum\", checksum1);\r\n    assertNotEquals(\"file 1 checksum\", 0, checksum1.getLength());\r\n    assertEquals(\"checksums of empty files\", checksum1, fs.getFileChecksum(touchFile(\"file2\"), 0));\r\n    Assertions.assertThat(fs.getXAttr(file1, XA_ETAG)).describedAs(\"etag from xattr\").isEqualTo(checksum1.getBytes());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assumeNoDefaultEncryption",
  "errType" : [ "AccessDeniedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assumeNoDefaultEncryption() throws IOException\n{\r\n    try {\r\n        skipIfClientSideEncryption();\r\n        Assume.assumeThat(getDefaultEncryption(), nullValue());\r\n    } catch (AccessDeniedException e) {\r\n        LOG.warn(\"User does not have permission to call getBucketEncryption()\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testChecksumDisabled",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testChecksumDisabled() throws Throwable\n{\r\n    enableChecksums(false);\r\n    final S3AFileSystem fs = getFileSystem();\r\n    Path file1 = touchFile(\"file1\");\r\n    EtagChecksum checksum1 = fs.getFileChecksum(file1, 0);\r\n    assertLacksPathCapabilities(fs, file1, CommonPathCapabilities.FS_CHECKSUMS);\r\n    assertNull(\"Checksums are being generated\", checksum1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testNonEmptyFileChecksums",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testNonEmptyFileChecksums() throws Throwable\n{\r\n    final S3AFileSystem fs = getFileSystem();\r\n    final Path file3 = mkFile(\"file3\", HELLO);\r\n    final EtagChecksum checksum1 = fs.getFileChecksum(file3, 0);\r\n    assertNotNull(\"file 3 checksum\", checksum1);\r\n    final Path file4 = touchFile(\"file4\");\r\n    final EtagChecksum checksum2 = fs.getFileChecksum(file4, 0);\r\n    assertNotEquals(\"checksums\", checksum1, checksum2);\r\n    createFile(fs, file4, true, \"hello, world\".getBytes(StandardCharsets.UTF_8));\r\n    assertNotEquals(checksum2, fs.getFileChecksum(file4, 0));\r\n    Assertions.assertThat(fs.getXAttr(file3, XA_ETAG)).describedAs(\"etag from xattr\").isEqualTo(checksum1.getBytes());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testNonEmptyFileChecksumsUnencrypted",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testNonEmptyFileChecksumsUnencrypted() throws Throwable\n{\r\n    Assume.assumeTrue(encryptionAlgorithm().equals(S3AEncryptionMethods.NONE));\r\n    assumeNoDefaultEncryption();\r\n    final S3AFileSystem fs = getFileSystem();\r\n    final EtagChecksum checksum1 = fs.getFileChecksum(mkFile(\"file5\", HELLO), 0);\r\n    assertNotNull(\"file 3 checksum\", checksum1);\r\n    assertEquals(\"checksums\", checksum1, fs.getFileChecksum(mkFile(\"file6\", HELLO), 0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "encryptionAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AEncryptionMethods encryptionAlgorithm()\n{\r\n    return getFileSystem().getS3EncryptionAlgorithm();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testNegativeLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNegativeLength() throws Throwable\n{\r\n    LambdaTestUtils.intercept(IllegalArgumentException.class, () -> getFileSystem().getFileChecksum(mkFile(\"negative\", HELLO), -1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testNegativeLengthDisabledChecksum",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testNegativeLengthDisabledChecksum() throws Throwable\n{\r\n    enableChecksums(false);\r\n    LambdaTestUtils.intercept(IllegalArgumentException.class, () -> getFileSystem().getFileChecksum(mkFile(\"negative\", HELLO), -1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testChecksumLengthPastEOF",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testChecksumLengthPastEOF() throws Throwable\n{\r\n    enableChecksums(true);\r\n    final S3AFileSystem fs = getFileSystem();\r\n    Path f = mkFile(\"file5\", HELLO);\r\n    EtagChecksum l = fs.getFileChecksum(f, HELLO.length);\r\n    assertNotNull(\"Null checksum\", l);\r\n    assertEquals(l, fs.getFileChecksum(f, HELLO.length * 2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testS3AToStringUnitialized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testS3AToStringUnitialized() throws Throwable\n{\r\n    try (S3AFileSystem fs = new S3AFileSystem()) {\r\n        fs.toString();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testS3AIOStatisticsUninitialized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testS3AIOStatisticsUninitialized() throws Throwable\n{\r\n    try (S3AFileSystem fs = new S3AFileSystem()) {\r\n        fs.getIOStatistics();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testPathFixup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testPathFixup() throws Throwable\n{\r\n    final S3AFileSystem fs = getFileSystem();\r\n    Path path = fs.makeQualified(new Path(\"path\"));\r\n    String trailing = path.toUri().toString() + \"/\";\r\n    verifyNoTrailingSlash(\"path from string\", new Path(trailing));\r\n    URI trailingURI = verifyTrailingSlash(\"trailingURI\", new URI(trailing));\r\n    Path pathFromTrailingURI = verifyTrailingSlash(\"pathFromTrailingURI\", new Path(trailingURI));\r\n    verifyNoTrailingSlash(\"path from fs.makeQualified()\", fs.makeQualified(pathFromTrailingURI));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testPathDoubleSlashFixup",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testPathDoubleSlashFixup() throws Throwable\n{\r\n    final S3AFileSystem fs = getFileSystem();\r\n    Path path = fs.makeQualified(new Path(\"path\"));\r\n    String trailing2 = path.toUri().toString() + \"//\";\r\n    verifyNoTrailingSlash(\"path from string\", new Path(trailing2));\r\n    URI trailingURI = new URI(trailing2);\r\n    Path pathFromTrailingURI = verifyTrailingSlash(\"pathFromTrailingURI\", new Path(trailingURI));\r\n    verifyNoTrailingSlash(\"path from fs.makeQualified()\", fs.makeQualified(pathFromTrailingURI));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRootPathFixup",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRootPathFixup() throws Throwable\n{\r\n    final S3AFileSystem fs = getFileSystem();\r\n    String baseFsURI = fs.getUri().toString();\r\n    Path rootPath_from_FS_URI = verifyNoTrailingSlash(\"root\", new Path(baseFsURI));\r\n    String trailing = verifyTrailingSlash(\"FS URI\", baseFsURI + \"/\");\r\n    Path root_path_from_trailing_string = verifyTrailingSlash(\"root path from string\", new Path(trailing));\r\n    URI trailingURI = verifyTrailingSlash(\"trailingURI\", new URI(trailing));\r\n    Path pathFromTrailingURI = verifyTrailingSlash(\"pathFromTrailingURI\", new Path(trailingURI));\r\n    Path pathFromQualify = verifyTrailingSlash(\"path from fs.makeQualified()\", fs.makeQualified(pathFromTrailingURI));\r\n    assertEquals(root_path_from_trailing_string, pathFromQualify);\r\n    Path pathFromRootQualify = verifyNoTrailingSlash(\"path from fs.makeQualified(\" + baseFsURI + \")\", fs.makeQualified(rootPath_from_FS_URI));\r\n    assertEquals(rootPath_from_FS_URI, pathFromRootQualify);\r\n    assertNotEquals(rootPath_from_FS_URI, root_path_from_trailing_string);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "verifyTrailingSlash",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "T verifyTrailingSlash(String role, T o)\n{\r\n    String s = o.toString();\r\n    assertTrue(role + \" lacks trailing slash \" + s, s.endsWith(\"/\"));\r\n    assertFalse(role + \" has double trailing slash \" + s, s.endsWith(\"//\"));\r\n    return o;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "verifyNoTrailingSlash",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T verifyNoTrailingSlash(String role, T o)\n{\r\n    String s = o.toString();\r\n    assertFalse(role + \" has trailing slash \" + s, s.endsWith(\"/\"));\r\n    return o;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getDefaultEncryption",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "GetBucketEncryptionResult getDefaultEncryption() throws IOException\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    AmazonS3 s3 = fs.getAmazonS3ClientForTesting(\"check default encryption\");\r\n    try {\r\n        return Invoker.once(\"getBucketEncryption()\", fs.getBucket(), () -> s3.getBucketEncryption(fs.getBucket()));\r\n    } catch (FileNotFoundException e) {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEncryption",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testEncryption() throws Throwable\n{\r\n    describe(\"Test to verify client-side encryption for different file sizes.\");\r\n    for (int size : SIZES) {\r\n        validateEncryptionForFileSize(size);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEncryptionOverRename",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testEncryptionOverRename() throws Throwable\n{\r\n    describe(\"Test for AWS CSE on Rename Operation.\");\r\n    maybeSkipTest();\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path src = path(getMethodName());\r\n    byte[] data = dataset(SMALL_FILE_SIZE, 'a', 'z');\r\n    writeDataset(fs, src, data, data.length, SMALL_FILE_SIZE, true, false);\r\n    ContractTestUtils.verifyFileContents(fs, src, data);\r\n    Path dest = path(src.getName() + \"-copy\");\r\n    fs.rename(src, dest);\r\n    ContractTestUtils.verifyFileContents(fs, dest, data);\r\n    assertEncrypted(dest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testDirectoryListingFileLengths",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testDirectoryListingFileLengths() throws IOException\n{\r\n    describe(\"Test to verify directory listing calls gives correct content \" + \"lengths\");\r\n    maybeSkipTest();\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path parentDir = path(getMethodName());\r\n    for (int i : SIZES) {\r\n        Path child = new Path(parentDir, getMethodName() + i);\r\n        writeThenReadFile(child, i);\r\n    }\r\n    List<Integer> fileLengthDirListing = new ArrayList<>();\r\n    for (FileStatus fileStatus : fs.listStatus(parentDir)) {\r\n        fileLengthDirListing.add((int) fileStatus.getLen());\r\n    }\r\n    Assertions.assertThat(fileLengthDirListing).describedAs(\"File lengths aren't the same \" + \"as expected from FileStatus dir. listing\").containsExactlyInAnyOrderElementsOf(SIZES);\r\n    RemoteIterator<LocatedFileStatus> listDir = fs.listFiles(parentDir, true);\r\n    List<Integer> fileLengthListLocated = new ArrayList<>();\r\n    while (listDir.hasNext()) {\r\n        LocatedFileStatus fileStatus = listDir.next();\r\n        fileLengthListLocated.add((int) fileStatus.getLen());\r\n    }\r\n    Assertions.assertThat(fileLengthListLocated).describedAs(\"File lengths isn't same \" + \"as expected from LocatedFileStatus dir. listing\").containsExactlyInAnyOrderElementsOf(SIZES);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testBigFilePutAndGet",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testBigFilePutAndGet() throws IOException\n{\r\n    maybeSkipTest();\r\n    assume(\"Scale test disabled: to enable set property \" + KEY_SCALE_TESTS_ENABLED, getTestPropertyBool(getConfiguration(), KEY_SCALE_TESTS_ENABLED, DEFAULT_SCALE_TESTS_ENABLED));\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path filePath = path(getMethodName());\r\n    byte[] fileContent = dataset(BIG_FILE_SIZE, 'a', 26);\r\n    int offsetSeek = fileContent[BIG_FILE_SIZE - 4];\r\n    createFile(fs, filePath, true, fileContent);\r\n    LOG.info(\"Multi-part upload successful...\");\r\n    try (FSDataInputStream in = fs.open(filePath)) {\r\n        in.seek(BIG_FILE_SIZE - 4);\r\n        assertEquals(\"Byte at a specific position not equal to actual byte\", offsetSeek, in.read());\r\n        in.seek(0);\r\n        assertEquals(\"Byte at a specific position not equal to actual byte\", 'a', in.read());\r\n        in.seek(MULTIPART_MIN_SIZE - 1);\r\n        int byteBeforeBlockEnd = fileContent[MULTIPART_MIN_SIZE];\r\n        assertEquals(\"Byte before multipart block end mismatch\", byteBeforeBlockEnd - 1, in.read());\r\n        assertEquals(\"Byte at multipart end mismatch\", byteBeforeBlockEnd, in.read());\r\n        assertEquals(\"Byte after multipart end mismatch\", byteBeforeBlockEnd + 1, in.read());\r\n        in.seek(BIG_FILE_SIZE + 1);\r\n        assertEquals(\"Byte at eof mismatch\", -1, in.read());\r\n        in.readFully(0, fileContent);\r\n        verifyFileContents(fs, filePath, fileContent);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEncryptionEnabledAndDisabledFS",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testEncryptionEnabledAndDisabledFS() throws Exception\n{\r\n    maybeSkipTest();\r\n    S3AFileSystem cseDisabledFS = new S3AFileSystem();\r\n    Configuration cseDisabledConf = getConfiguration();\r\n    S3AFileSystem cseEnabledFS = getFileSystem();\r\n    Path unEncryptedFilePath = path(getMethodName());\r\n    Path encryptedFilePath = path(getMethodName() + \"cse\");\r\n    removeBaseAndBucketOverrides(getTestBucketName(cseDisabledConf), cseDisabledConf, S3_ENCRYPTION_ALGORITHM, S3_ENCRYPTION_KEY, SERVER_SIDE_ENCRYPTION_ALGORITHM, SERVER_SIDE_ENCRYPTION_KEY);\r\n    cseDisabledFS.initialize(getFileSystem().getUri(), cseDisabledConf);\r\n    IOStatistics cseDisabledIOStats = cseDisabledFS.getIOStatistics();\r\n    IOStatistics cseEnabledIOStatistics = cseEnabledFS.getIOStatistics();\r\n    IOStatisticAssertions.assertThatStatisticGauge(cseDisabledIOStats, Statistic.CLIENT_SIDE_ENCRYPTION_ENABLED.getSymbol()).isEqualTo(0L);\r\n    IOStatisticAssertions.assertThatStatisticGauge(cseEnabledIOStatistics, Statistic.CLIENT_SIDE_ENCRYPTION_ENABLED.getSymbol()).isEqualTo(1L);\r\n    try (FSDataOutputStream out = cseDisabledFS.create(unEncryptedFilePath)) {\r\n        out.write(new byte[SMALL_FILE_SIZE]);\r\n    }\r\n    try (FSDataInputStream in = cseEnabledFS.open(unEncryptedFilePath)) {\r\n        FileStatus encryptedFSFileStatus = cseEnabledFS.getFileStatus(unEncryptedFilePath);\r\n        assertEquals(\"Mismatch in content length bytes\", SMALL_FILE_SIZE, encryptedFSFileStatus.getLen());\r\n        intercept(SecurityException.class, \"\", \"SecurityException should be thrown\", () -> {\r\n            in.read(new byte[SMALL_FILE_SIZE]);\r\n            return \"Exception should be raised if unencrypted data is read by \" + \"a CSE enabled FS\";\r\n        });\r\n    }\r\n    try (FSDataOutputStream out = cseEnabledFS.create(encryptedFilePath)) {\r\n        out.write('a');\r\n    }\r\n    try (FSDataInputStream in = cseDisabledFS.open(encryptedFilePath)) {\r\n        FileStatus unEncryptedFSFileStatus = cseDisabledFS.getFileStatus(encryptedFilePath);\r\n        assertNotEquals(\"Mismatch in content length\", 1, unEncryptedFSFileStatus.getLen());\r\n        Assertions.assertThat(in.read()).describedAs(\"Encrypted data shouldn't be equal to actual content \" + \"without deciphering\").isNotEqualTo('a');\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    S3ATestUtils.removeBaseAndBucketOverrides(conf, Constants.MULTIPART_SIZE, Constants.MIN_MULTIPART_THRESHOLD);\r\n    conf.set(Constants.MULTIPART_SIZE, String.valueOf(MULTIPART_MIN_SIZE));\r\n    conf.set(Constants.MIN_MULTIPART_THRESHOLD, String.valueOf(MULTIPART_MIN_SIZE));\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "validateEncryptionForFileSize",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void validateEncryptionForFileSize(int len) throws IOException\n{\r\n    maybeSkipTest();\r\n    describe(\"Create an encrypted file of size \" + len);\r\n    Path path = writeThenReadFile(getMethodName() + len, len);\r\n    assertEncrypted(path);\r\n    rm(getFileSystem(), path, false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "maybeSkipTest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void maybeSkipTest() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertEncrypted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void assertEncrypted(Path path) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "getSpanCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getSpanCount()\n{\r\n    return spanCount.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "createSpan",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AuditSpanS3A createSpan(final String operation, @Nullable final String path1, @Nullable final String path2)\n{\r\n    spanCount.incrementAndGet();\r\n    return new MemorySpan(createSpanID(), operation);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "getUnbondedSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpanS3A getUnbondedSpan()\n{\r\n    if (unbondedSpan == null) {\r\n        unbondedSpan = new MemorySpan(createSpanID(), \"unbonded\");\r\n    }\r\n    return unbondedSpan;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return String.format(\"%s instance %d span count %d\", super.toString(), getInstanceCount(), getSpanCount());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "noteSpanReferenceLost",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void noteSpanReferenceLost(final long threadId)\n{\r\n    LOG.info(\"Span lost for thread {}\", threadId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "getInstanceCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getInstanceCount()\n{\r\n    return INSTANCE_COUNT.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    oldStdout = System.out;\r\n    oldStderr = System.err;\r\n    stdout = new ByteArrayOutputStream();\r\n    printStdout = new PrintStream(stdout);\r\n    System.setOut(printStdout);\r\n    stderr = new ByteArrayOutputStream();\r\n    printStderr = new PrintStream(stderr);\r\n    System.setErr(printStderr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    System.setOut(oldStdout);\r\n    System.setErr(oldStderr);\r\n    IOUtils.cleanupWithLogger(LOG, printStdout, printStderr);\r\n    super.teardown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "closeAllFilesystems",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void closeAllFilesystems()\n{\r\n    try {\r\n        LOG.info(\"Closing down all filesystems for current user\");\r\n        FileSystem.closeAllForUGI(UserGroupInformation.getCurrentUser());\r\n    } catch (IOException e) {\r\n        LOG.warn(\"UGI.getCurrentUser()\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "createNewConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createNewConfiguration()\n{\r\n    final Configuration conf = new Configuration(getConfiguration());\r\n    removeBaseAndBucketOverrides(conf, HADOOP_SECURITY_CREDENTIAL_PROVIDER_PATH, S3A_SECURITY_CREDENTIAL_PROVIDER_PATH);\r\n    disableFilesystemCaching(conf);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testListMissingJceksFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testListMissingJceksFile() throws Throwable\n{\r\n    final Path dir = path(\"jceks\");\r\n    Path keystore = new Path(dir, \"keystore.jceks\");\r\n    String jceksProvider = toJceksProvider(keystore);\r\n    CredentialShell cs = new CredentialShell();\r\n    cs.setConf(createNewConfiguration());\r\n    run(cs, null, \"list\", \"-provider\", jceksProvider);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testCredentialSuccessfulLifecycle",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testCredentialSuccessfulLifecycle() throws Exception\n{\r\n    final Path dir = path(\"jceks\");\r\n    Path keystore = new Path(dir, \"keystore.jceks\");\r\n    String jceksProvider = toJceksProvider(keystore);\r\n    CredentialShell cs = new CredentialShell();\r\n    cs.setConf(createNewConfiguration());\r\n    run(cs, \"credential1 has been successfully created.\", \"create\", \"credential1\", \"-value\", \"p@ssw0rd\", \"-provider\", jceksProvider);\r\n    assertIsFile(keystore);\r\n    run(cs, \"credential1\", \"list\", \"-provider\", jceksProvider);\r\n    run(cs, \"credential1 has been successfully deleted.\", \"delete\", \"credential1\", \"-f\", \"-provider\", jceksProvider);\r\n    String[] args5 = { \"list\", \"-provider\", jceksProvider };\r\n    String out = run(cs, null, args5);\r\n    Assertions.assertThat(out).describedAs(\"Command result of list\").doesNotContain(\"credential1\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String run(CredentialShell cs, String expected, String... args) throws Exception\n{\r\n    stdout.reset();\r\n    int rc = cs.run(args);\r\n    final String out = stdout.toString(UTF8);\r\n    LOG.error(\"{}\", stderr.toString(UTF8));\r\n    LOG.info(\"{}\", out);\r\n    Assertions.assertThat(rc).describedAs(\"Command result of %s with output %s\", args[0], out).isEqualTo(0);\r\n    if (expected != null) {\r\n        Assertions.assertThat(out).describedAs(\"Command result of %s\", args[0]).contains(expected);\r\n    }\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "toJceksProvider",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toJceksProvider(Path keystore)\n{\r\n    final URI uri = keystore.toUri();\r\n    return String.format(\"jceks://%s@%s%s\", uri.getScheme(), uri.getHost(), uri.getPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileSystem getFileSystem()\n{\r\n    return (S3AFileSystem) super.getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "testRmNonEmptyRootDirNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRmNonEmptyRootDirNonRecursive() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "areZeroByteFilesEncrypted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean areZeroByteFilesEncrypted()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "suitename",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String suitename()\n{\r\n    return \"ITestMagicCommitProtocol\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "getCommitterFactoryName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCommitterFactoryName()\n{\r\n    return CommitConstants.S3A_COMMITTER_FACTORY;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "getCommitterName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCommitterName()\n{\r\n    return CommitConstants.COMMITTER_NAME_MAGIC;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    CommitUtils.verifyIsMagicCommitFS(getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "assertJobAbortCleanedUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertJobAbortCleanedUp(JobData jobData) throws Exception\n{\r\n    Path magicDir = new Path(getOutDir(), MAGIC);\r\n    ContractTestUtils.assertPathDoesNotExist(getFileSystem(), \"magic dir \", magicDir);\r\n    super.assertJobAbortCleanedUp(jobData);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "createCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MagicS3GuardCommitter createCommitter(Path outputPath, TaskAttemptContext context) throws IOException\n{\r\n    return new MagicS3GuardCommitter(outputPath, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "createFailingCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MagicS3GuardCommitter createFailingCommitter(TaskAttemptContext tContext) throws IOException\n{\r\n    return new CommitterWithFailedThenSucceed(getOutDir(), tContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "validateTaskAttemptPathDuringWrite",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void validateTaskAttemptPathDuringWrite(Path p, final long expectedLength) throws IOException\n{\r\n    String pathStr = p.toString();\r\n    assertTrue(\"not magic \" + pathStr, pathStr.contains(MAGIC));\r\n    assertPathDoesNotExist(\"task attempt visible\", p);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "validateTaskAttemptPathAfterWrite",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void validateTaskAttemptPathAfterWrite(Path marker, final long expectedLength) throws IOException\n{\r\n    Path pendingFile = new Path(marker.toString() + PENDING_SUFFIX);\r\n    assertPathExists(\"pending file\", pendingFile);\r\n    S3AFileSystem fs = getFileSystem();\r\n    String name = marker.getName();\r\n    List<LocatedFileStatus> filtered = listAndFilter(fs, marker.getParent(), false, (path) -> path.getName().equals(name));\r\n    Assertions.assertThat(filtered).hasSize(1);\r\n    Assertions.assertThat(filtered.get(0)).matches(lst -> lst.getLen() == 0, \"Listing should return 0 byte length\");\r\n    FileStatus st = fs.getFileStatus(marker);\r\n    assertEquals(\"file length in \" + st, 0, st.getLen());\r\n    Assertions.assertThat(CommitOperations.extractMagicFileLength(fs, marker)).describedAs(\"XAttribute \" + XA_MAGIC_MARKER).isNotEmpty().hasValue(expectedLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "validateTaskAttemptWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void validateTaskAttemptWorkingDirectory(final AbstractS3ACommitter committer, final TaskAttemptContext context) throws IOException\n{\r\n    URI wd = committer.getWorkPath().toUri();\r\n    assertEquals(\"Wrong schema for working dir \" + wd + \" with committer \" + committer, \"s3a\", wd.getScheme());\r\n    assertThat(wd.getPath(), containsString('/' + CommitConstants.MAGIC + '/'));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "testCommittersPathsHaveUUID",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCommittersPathsHaveUUID() throws Throwable\n{\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(getConfiguration(), getTaskAttempt0());\r\n    MagicS3GuardCommitter committer = createCommitter(getOutDir(), tContext);\r\n    String ta0 = getTaskAttempt0().toString();\r\n    Path taskAttemptPath = committer.getTaskAttemptPath(tContext);\r\n    Assertions.assertThat(taskAttemptPath.toString()).describedAs(\"task path of %s\", committer).contains(committer.getUUID()).contains(MAGIC).doesNotContain(TEMP_DATA).endsWith(BASE).contains(ta0);\r\n    Path tempTaskAttemptPath = committer.getTempTaskAttemptPath(tContext);\r\n    Assertions.assertThat(tempTaskAttemptPath.toString()).describedAs(\"Temp task path of %s\", committer).contains(committer.getUUID()).contains(TEMP_DATA).doesNotContain(MAGIC).doesNotContain(BASE).contains(ta0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return SCALE_TEST_TIMEOUT_MILLIS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration newConf = super.createConfiguration();\r\n    newConf.setLong(MULTIPART_SIZE, MULTIPART_SETTING);\r\n    newConf.set(FAST_UPLOAD_BUFFER, FAST_UPLOAD_BUFFER_DISK);\r\n    return newConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "shouldUseDirectWrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldUseDirectWrite()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "testDistCpWithIterator",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testDistCpWithIterator() throws Exception\n{\r\n    final long renames = getRenameOperationCount();\r\n    super.testDistCpWithIterator();\r\n    assertEquals(\"Expected no renames for a direct write distcp\", getRenameOperationCount(), renames);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "testNonDirectWrite",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testNonDirectWrite() throws Exception\n{\r\n    final long renames = getRenameOperationCount();\r\n    super.testNonDirectWrite();\r\n    assertEquals(\"Expected 2 renames for a non-direct write distcp\", 2L, getRenameOperationCount() - renames);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "getRenameOperationCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getRenameOperationCount()\n{\r\n    return getFileSystem().getStorageStatistics().getLong(StorageStatistics.CommonStatisticNames.OP_RENAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "assertFileCount",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void assertFileCount(final String message, final FileSystem fs, final Path path, final long expected) throws IOException\n{\r\n    List<String> files = new ArrayList<>();\r\n    try (DurationInfo ignored = new DurationInfo(LOG, false, \"Counting files in %s\", path)) {\r\n        applyLocatedFiles(fs.listFiles(path, true), (status) -> files.add(status.getPath().toString()));\r\n    }\r\n    long actual = files.size();\r\n    if (actual != expected) {\r\n        String ls = files.stream().collect(Collectors.joining(\"\\n\"));\r\n        Assert.fail(message + \": expected \" + expected + \" files in \" + path + \" but got \" + actual + \"\\n\" + ls);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "assertTextContains",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertTextContains(String text, String contained)\n{\r\n    assertTrue(\"string \\\"\" + contained + \"\\\" not found in \\\"\" + text + \"\\\"\", text != null && text.contains(contained));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "failIf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void failIf(boolean condition, String message, Throwable cause)\n{\r\n    if (condition) {\r\n        ContractTestUtils.fail(message, cause);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "failUnless",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void failUnless(boolean condition, String message, Throwable cause)\n{\r\n    failIf(!condition, message, cause);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "extractCause",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "T extractCause(Class<T> expected, Throwable thrown)\n{\r\n    Throwable cause = thrown.getCause();\r\n    failIf(cause == null, \"No inner cause\", thrown);\r\n    failUnless(cause.getClass().equals(expected), \"Inner cause is of wrong type : expected \" + expected, thrown);\r\n    return (T) cause;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "assertStatusCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertStatusCode(AWSServiceIOException e, int code) throws AWSServiceIOException\n{\r\n    if (e.getStatusCode() != code) {\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "assertCompleteAbort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertCompleteAbort(Abortable.AbortableResult result)\n{\r\n    Assertions.assertThat(result).describedAs(\"Abort operation result %s\", result).matches(r -> !r.alreadyClosed()).matches(r -> r.anyCleanupException() == null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\test",
  "methodName" : "assertNoopAbort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertNoopAbort(Abortable.AbortableResult result)\n{\r\n    Assertions.assertThat(result).describedAs(\"Abort operation result %s\", result).matches(r -> r.alreadyClosed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void init() throws IOException, InterruptedException\n{\r\n    when(mockClient.submitJob(any(JobID.class), any(String.class), any(Credentials.class))).thenAnswer(invocation -> {\r\n        final Object[] args = invocation.getArguments();\r\n        String name = (String) args[1];\r\n        LOG.info(\"Submitted Job {}\", name);\r\n        submittedCredentials = (Credentials) args[2];\r\n        final JobStatus status = new JobStatus();\r\n        status.setState(JobStatus.State.RUNNING);\r\n        status.setSchedulingInfo(NAME);\r\n        status.setTrackingUrl(\"http://localhost:8080/\");\r\n        return status;\r\n    });\r\n    when(mockClient.getNewJobID()).thenReturn(new JobID(trackerId, jobIdCounter++));\r\n    when(mockClient.getQueueAdmins(any(String.class))).thenReturn(new AccessControlList(AccessControlList.WILDCARD_ACL_VALUE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isSuccessful",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSuccessful() throws IOException\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobSubmitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobSubmitter getJobSubmitter(FileSystem fs, ClientProtocol submitClient) throws IOException\n{\r\n    return new JobSubmitter(fs, mockClient);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "connect",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void connect() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    super.connect();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getSubmittedCredentials",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Credentials getSubmittedCredentials()\n{\r\n    return submittedCredentials;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "updateStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void updateStatus() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "LoggingLineRecordWriter<K, V> getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    boolean isCompressed = getCompressOutput(job);\r\n    String keyValueSeparator = conf.get(SEPARATOR, \"\\t\");\r\n    CompressionCodec codec = null;\r\n    String extension = \"\";\r\n    if (isCompressed) {\r\n        Class<? extends CompressionCodec> codecClass = getOutputCompressorClass(job, GzipCodec.class);\r\n        codec = ReflectionUtils.newInstance(codecClass, conf);\r\n        extension = codec.getDefaultExtension();\r\n    }\r\n    Path file = getDefaultWorkFile(job, extension);\r\n    FileSystem fs = file.getFileSystem(conf);\r\n    FSDataOutputStream fileOut = fs.create(file, true);\r\n    LOG.debug(\"Creating LineRecordWriter with destination {}\", file);\r\n    if (isCompressed) {\r\n        return new LoggingLineRecordWriter<>(file, new DataOutputStream(codec.createOutputStream(fileOut)), keyValueSeparator);\r\n    } else {\r\n        return new LoggingLineRecordWriter<>(file, fileOut, keyValueSeparator);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "bind",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void bind(Configuration conf)\n{\r\n    conf.setClass(MRJobConfig.OUTPUT_FORMAT_CLASS_ATTR, LoggingTextOutputFormat.class, OutputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getDestinationFS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getDestinationFS(Path out, Configuration config) throws IOException\n{\r\n    return out.getFileSystem(config);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "commitJob",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void commitJob(JobContext context) throws IOException\n{\r\n    super.commitJob(context);\r\n    Configuration conf = context.getConfiguration();\r\n    try {\r\n        String jobCommitterPath = conf.get(\"mock-results-file\");\r\n        if (jobCommitterPath != null) {\r\n            try (ObjectOutputStream out = new ObjectOutputStream(FileSystem.getLocal(conf).create(new Path(jobCommitterPath), false))) {\r\n                out.writeObject(getResults());\r\n            }\r\n        }\r\n    } catch (Exception e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "maybeCreateSuccessMarker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void maybeCreateSuccessMarker(JobContext context, List<String> filenames, final IOStatisticsSnapshot ioStatistics) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getResults",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ClientResults getResults() throws IOException\n{\r\n    MockS3AFileSystem mockFS = (MockS3AFileSystem) getDestS3AFS();\r\n    return mockFS.getOutcome().getKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getErrors",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ClientErrors getErrors() throws IOException\n{\r\n    MockS3AFileSystem mockFS = (MockS3AFileSystem) getDestS3AFS();\r\n    return mockFS.getOutcome().getValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"MockedStagingCommitter{ \" + super.toString() + \" \";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\fileContext",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    conf = new Configuration();\r\n    fc = S3ATestUtils.createTestFileContext(conf);\r\n    testRootPath = fileContextTestHelper.getTestRootPath(fc, \"test\");\r\n    fc.mkdir(testRootPath, FileContext.DEFAULT_PERM, true);\r\n    FileContext.clearStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\fileContext",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    S3ATestUtils.callQuietly(LOG, () -> fc != null && fc.delete(testRootPath, true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\fileContext",
  "methodName" : "verifyReadBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyReadBytes(FileSystem.Statistics stats)\n{\r\n    Assert.assertEquals(2 * blockSize, stats.getBytesRead());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\fileContext",
  "methodName" : "verifyWrittenBytes",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyWrittenBytes(FileSystem.Statistics stats) throws IOException\n{\r\n    long expectedBlockSize = blockSize;\r\n    if (S3AEncryptionMethods.CSE_KMS.getMethod().equals(getEncryptionAlgorithm(getTestBucketName(conf), conf).getMethod())) {\r\n        String keyId = getS3EncryptionKey(getTestBucketName(conf), conf);\r\n        expectedBlockSize += CSE_PADDING_LENGTH + keyId.getBytes().length + KMS_KEY_GENERATION_REQUEST_PARAMS_BYTES_WRITTEN;\r\n    }\r\n    Assert.assertEquals(\"Mismatch in bytes written\", expectedBlockSize, stats.getBytesWritten());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\fileContext",
  "methodName" : "getFsUri",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI getFsUri()\n{\r\n    return fc.getHomeDirectory().toUri();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "mark",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void mark()\n{\r\n    stats = snapshot();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "compare",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Map<String, Long> compare(Map<String, Long> current)\n{\r\n    Map<String, Long> diff = new HashMap<>(stats.size());\r\n    for (Map.Entry<String, Long> entry : stats.entrySet()) {\r\n        String key = entry.getKey();\r\n        Long latest = current.get(key);\r\n        if (latest != null && !latest.equals(entry.getValue())) {\r\n            diff.put(key, entry.getValue() - latest);\r\n        }\r\n    }\r\n    return diff;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "compareToCurrent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, Long> compareToCurrent()\n{\r\n    return compare(snapshot());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString(Map<String, Long> map)\n{\r\n    return Joiner.on(\"\\n\").withKeyValueSeparator(\"=\").join(map);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "snapshot",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<String, Long> snapshot()\n{\r\n    StatsIterator values = latestValues();\r\n    Map<String, Long> snapshot = new HashMap<>(stats == null ? 0 : stats.size());\r\n    for (StorageStatistics.LongStatistic value : values) {\r\n        snapshot.put(value.getName(), value.getValue());\r\n    }\r\n    return snapshot;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "latestValues",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StatsIterator latestValues()\n{\r\n    return new StatsIterator(fs.getStorageStatistics());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { false }, { true } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    assumeRoleTests();\r\n    basePath = uniquePath();\r\n    readOnlyDir = new Path(basePath, \"readonlyDir\");\r\n    writableDir = new Path(basePath, \"writableDir\");\r\n    readOnlyChild = new Path(readOnlyDir, \"child\");\r\n    noReadDir = new Path(basePath, \"noReadDir\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    fs.delete(basePath, true);\r\n    fs.mkdirs(writableDir);\r\n    assumedRoleConfig = createAssumedRoleConfig();\r\n    bindRolePolicyStatements(assumedRoleConfig, STATEMENT_ALLOW_SSE_KMS_RW, STATEMENT_ALL_BUCKET_READ_ACCESS, new Statement(Effects.Allow).addActions(S3_PATH_RW_OPERATIONS).addResources(directory(writableDir)), new Statement(Effects.Deny).addActions(S3_ALL_GET).addActions(S3_ALL_PUT).addActions(S3_ALL_DELETE).addResources(directory(noReadDir)));\r\n    roleFS = (S3AFileSystem) readOnlyDir.getFileSystem(assumedRoleConfig);\r\n    scaleTest = multiDelete && getTestPropertyBool(getConfiguration(), KEY_SCALE_TESTS_ENABLED, DEFAULT_SCALE_TESTS_ENABLED);\r\n    fileCount = scaleTest ? FILE_COUNT_SCALED : FILE_COUNT_NON_SCALED;\r\n    dirCount = scaleTest ? DIR_COUNT_SCALED : DIR_COUNT;\r\n    dirDepth = scaleTest ? DEPTH_SCALED : DEPTH;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    cleanupWithLogger(LOG, roleFS);\r\n    super.teardown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "assumeRoleTests",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assumeRoleTests()\n{\r\n    assume(\"No ARN for role tests\", !getAssumedRoleARN().isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getAssumedRoleARN",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getAssumedRoleARN()\n{\r\n    return getContract().getConf().getTrimmed(ASSUMED_ROLE_ARN, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "createAssumedRoleConfig",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createAssumedRoleConfig()\n{\r\n    return createAssumedRoleConfig(getAssumedRoleARN());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "createAssumedRoleConfig",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createAssumedRoleConfig(String roleARN)\n{\r\n    Configuration conf = newAssumedRoleConfig(getContract().getConf(), roleARN);\r\n    removeBaseAndBucketOverrides(conf, DELEGATION_TOKEN_BINDING, ENABLE_MULTI_DELETE);\r\n    conf.setBoolean(ENABLE_MULTI_DELETE, multiDelete);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    String bucketName = getTestBucketName(conf);\r\n    removeBucketOverrides(bucketName, conf, MAX_THREADS, MAXIMUM_CONNECTIONS, DIRECTORY_MARKER_POLICY, BULK_DELETE_PAGE_SIZE);\r\n    conf.setInt(MAX_THREADS, EXECUTOR_THREAD_COUNT);\r\n    conf.setInt(MAXIMUM_CONNECTIONS, EXECUTOR_THREAD_COUNT * 2);\r\n    conf.set(DIRECTORY_MARKER_POLICY, DIRECTORY_MARKER_POLICY_KEEP);\r\n    conf.setInt(BULK_DELETE_PAGE_SIZE, 1_000);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "uniquePath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path uniquePath() throws IOException\n{\r\n    long now = System.currentTimeMillis();\r\n    return path(String.format(\"%s-%s-%06d.%03d\", getMethodName(), multiDelete ? \"multi\" : \"single\", now / 1000, now % 1000));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testCannotTouchUnderRODir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCannotTouchUnderRODir() throws Throwable\n{\r\n    forbidden(\"touching the empty child \" + readOnlyChild, \"\", () -> {\r\n        touch(roleFS, readOnlyChild);\r\n        return readOnlyChild;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testCannotReadUnderNoReadDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCannotReadUnderNoReadDir() throws Throwable\n{\r\n    Path path = new Path(noReadDir, \"unreadable.txt\");\r\n    createFile(getFileSystem(), path, true, \"readonly\".getBytes());\r\n    forbidden(\"trying to read \" + path, \"\", () -> readUTF8(roleFS, path, -1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testMultiDeleteOptionPropagated",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testMultiDeleteOptionPropagated() throws Throwable\n{\r\n    describe(\"Verify the test parameter propagates to the store context\");\r\n    StoreContext ctx = roleFS.createStoreContext();\r\n    Assertions.assertThat(ctx.isMultiObjectDeleteEnabled()).as(ctx.toString()).isEqualTo(multiDelete);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testRenameParentPathNotWriteable",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testRenameParentPathNotWriteable() throws Throwable\n{\r\n    describe(\"rename with parent paths not writeable; multi=%s\", multiDelete);\r\n    final Configuration conf = createAssumedRoleConfig();\r\n    bindRolePolicyStatements(conf, STATEMENT_ALLOW_SSE_KMS_RW, STATEMENT_ALL_BUCKET_READ_ACCESS, new Statement(Effects.Allow).addActions(S3_PATH_RW_OPERATIONS).addResources(directory(readOnlyDir)).addResources(directory(writableDir)));\r\n    roleFS = (S3AFileSystem) readOnlyDir.getFileSystem(conf);\r\n    S3AFileSystem fs = getFileSystem();\r\n    roleFS.getFileStatus(ROOT);\r\n    fs.mkdirs(readOnlyDir);\r\n    touch(fs, readOnlyChild);\r\n    fs.delete(writableDir, true);\r\n    assertRenameOutcome(roleFS, readOnlyChild, writableDir, true);\r\n    assertIsFile(writableDir);\r\n    assertIsDirectory(readOnlyDir);\r\n    Path renamedDestPath = new Path(readOnlyDir, writableDir.getName());\r\n    assertRenameOutcome(roleFS, writableDir, readOnlyDir, true);\r\n    assertIsFile(renamedDestPath);\r\n    roleFS.delete(readOnlyDir, true);\r\n    roleFS.delete(writableDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testRenameSingleFileFailsInDelete",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRenameSingleFileFailsInDelete() throws Throwable\n{\r\n    describe(\"rename with source read only; multi=%s\", multiDelete);\r\n    Path readOnlyFile = readOnlyChild;\r\n    S3AFileSystem fs = getFileSystem();\r\n    fs.delete(basePath, true);\r\n    touch(fs, readOnlyFile);\r\n    roleFS.delete(writableDir, true);\r\n    roleFS.mkdirs(writableDir);\r\n    expectRenameForbidden(readOnlyFile, writableDir);\r\n    assertIsFile(readOnlyFile);\r\n    Path renamedFile = new Path(writableDir, readOnlyFile.getName());\r\n    assertIsFile(renamedFile);\r\n    ContractTestUtils.assertDeleted(roleFS, renamedFile, true);\r\n    assertFileCount(\"Empty Dest Dir\", roleFS, writableDir, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testRenameDirFailsInDelete",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testRenameDirFailsInDelete() throws Throwable\n{\r\n    describe(\"rename with source read only; multi=%s\", multiDelete);\r\n    S3AFileSystem fs = getFileSystem();\r\n    roleFS.mkdirs(writableDir);\r\n    List<Path> dirs = new ArrayList<>(dirCount);\r\n    List<Path> createdFiles = createDirsAndFiles(fs, readOnlyDir, dirDepth, fileCount, dirCount, new ArrayList<>(fileCount), dirs);\r\n    int expectedFileCount = createdFiles.size();\r\n    assertFileCount(\"files ready to rename\", roleFS, readOnlyDir, expectedFileCount);\r\n    LOG.info(\"Renaming readonly files {} to {}\", readOnlyDir, writableDir);\r\n    AccessDeniedException deniedException = expectRenameForbidden(readOnlyDir, writableDir);\r\n    if (multiDelete) {\r\n        MultiObjectDeleteException mde = extractCause(MultiObjectDeleteException.class, deniedException);\r\n    }\r\n    LOG.info(\"Result of renaming read-only files is as expected\", deniedException);\r\n    assertFileCount(\"files in the source directory\", roleFS, readOnlyDir, expectedFileCount);\r\n    describe(\"Verify destination directory exists\");\r\n    assertIsDirectory(writableDir);\r\n    assertFileCount(\"files in the dest directory\", roleFS, writableDir, expectedFileCount);\r\n    LOG.info(\"Verifying all directories still exist\");\r\n    for (Path dir : dirs) {\r\n        assertIsDirectory(dir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testRenameFileFailsNoWrite",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRenameFileFailsNoWrite() throws Throwable\n{\r\n    describe(\"Try to rename to a write-only destination fails with src\" + \" & dest unchanged.\");\r\n    roleFS.mkdirs(writableDir);\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path source = new Path(writableDir, \"source\");\r\n    touch(fs, source);\r\n    fs.mkdirs(readOnlyDir);\r\n    Path dest = new Path(readOnlyDir, \"dest\");\r\n    describe(\"Renaming files {} to {}\", writableDir, dest);\r\n    expectRenameForbidden(source, dest);\r\n    assertIsFile(source);\r\n    assertPathDoesNotExist(\"rename destination\", dest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testCopyDirFailsToReadOnlyDir",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCopyDirFailsToReadOnlyDir() throws Throwable\n{\r\n    describe(\"Try to copy to a read-only destination\");\r\n    roleFS.mkdirs(writableDir);\r\n    S3AFileSystem fs = getFileSystem();\r\n    List<Path> files = createFiles(fs, writableDir, dirDepth, fileCount, dirCount);\r\n    fs.mkdirs(readOnlyDir);\r\n    Path dest = new Path(readOnlyDir, \"dest\");\r\n    expectRenameForbidden(writableDir, dest);\r\n    assertPathDoesNotExist(\"rename destination\", dest);\r\n    assertFileCount(\"files in the source directory\", roleFS, writableDir, files.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testCopyFileFailsOnSourceRead",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCopyFileFailsOnSourceRead() throws Throwable\n{\r\n    describe(\"The source file isn't readable, so the COPY fails\");\r\n    Path source = new Path(noReadDir, \"source\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    touch(fs, source);\r\n    fs.mkdirs(writableDir);\r\n    Path dest = new Path(writableDir, \"dest\");\r\n    expectRenameForbidden(source, dest);\r\n    assertIsFile(source);\r\n    assertPathDoesNotExist(\"rename destination\", dest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testCopyDirFailsOnSourceRead",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCopyDirFailsOnSourceRead() throws Throwable\n{\r\n    describe(\"The source file isn't readable, so the COPY fails\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    List<Path> files = createFiles(fs, noReadDir, dirDepth, fileCount, dirCount);\r\n    fs.mkdirs(writableDir);\r\n    Path dest = new Path(writableDir, \"dest\");\r\n    expectRenameForbidden(noReadDir, dest);\r\n    assertFileCount(\"files in the source directory\", fs, noReadDir, files.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testPartialEmptyDirDelete",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testPartialEmptyDirDelete() throws Throwable\n{\r\n    describe(\"delete an empty directory with parent dir r/o\" + \" multidelete=%s\", multiDelete);\r\n    final Path deletableChild = new Path(writableDir, \"deletableChild\");\r\n    roleFS.mkdirs(deletableChild);\r\n    assertPathExists(\"parent dir after create\", writableDir);\r\n    assertPathExists(\"grandparent dir after create\", writableDir.getParent());\r\n    roleFS.delete(deletableChild, true);\r\n    assertPathExists(\"parent dir after deletion\", writableDir);\r\n    assertPathExists(\"grandparent dir after deletion\", writableDir.getParent());\r\n    assertPathDoesNotExist(\"deletable dir after deletion\", deletableChild);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testPartialDirDelete",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testPartialDirDelete() throws Throwable\n{\r\n    describe(\"delete with part of the child tree read only;\" + \" multidelete=%s\", multiDelete);\r\n    S3AFileSystem fs = getFileSystem();\r\n    StoreContext storeContext = fs.createStoreContext();\r\n    List<Path> dirs = new ArrayList<>(dirCount);\r\n    List<Path> readOnlyFiles = createDirsAndFiles(fs, readOnlyDir, dirDepth, fileCount, dirCount, new ArrayList<>(fileCount), dirs);\r\n    List<Path> deletableFiles = createFiles(fs, writableDir, dirDepth, fileCount, dirCount);\r\n    Path head = deletableFiles.remove(0);\r\n    assertTrue(\"delete \" + head + \" failed\", roleFS.delete(head, false));\r\n    MetricDiff rejectionCount = new MetricDiff(roleFS, FILES_DELETE_REJECTED);\r\n    MetricDiff deleteVerbCount = new MetricDiff(roleFS, OBJECT_DELETE_REQUEST);\r\n    MetricDiff bulkDeleteVerbCount = new MetricDiff(roleFS, OBJECT_BULK_DELETE_REQUEST);\r\n    MetricDiff deleteObjectCount = new MetricDiff(roleFS, OBJECT_DELETE_OBJECTS);\r\n    describe(\"Trying to delete read only directory\");\r\n    AccessDeniedException ex = expectDeleteForbidden(readOnlyDir);\r\n    if (multiDelete) {\r\n        extractCause(MultiObjectDeleteException.class, ex);\r\n        deleteVerbCount.assertDiffEquals(\"Wrong delete request count\", 0);\r\n        bulkDeleteVerbCount.assertDiffEquals(\"Wrong bulk delete request count\", 1);\r\n        deleteObjectCount.assertDiffEquals(\"Number of keys in delete request\", readOnlyFiles.size());\r\n        rejectionCount.assertDiffEquals(\"Wrong rejection count\", readOnlyFiles.size());\r\n        reset(rejectionCount, deleteVerbCount, deleteObjectCount, bulkDeleteVerbCount);\r\n    }\r\n    if (!scaleTest) {\r\n        readOnlyFiles.forEach(this::pathMustExist);\r\n    }\r\n    describe(\"Trying to delete upper-level directory\");\r\n    ex = expectDeleteForbidden(basePath);\r\n    String iostats = ioStatisticsSourceToString(roleFS);\r\n    reset(rejectionCount, deleteVerbCount);\r\n    final Set<Path> readOnlyListing = listFilesUnderPath(readOnlyDir, true);\r\n    String directoryList = readOnlyListing.stream().map(Path::toString).collect(Collectors.joining(\", \", \"[\", \"]\"));\r\n    Assertions.assertThat(readOnlyListing).as(\"ReadOnly directory \" + directoryList).containsExactlyInAnyOrderElementsOf(readOnlyFiles);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "expectDeleteForbidden",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AccessDeniedException expectDeleteForbidden(Path path) throws Exception\n{\r\n    try (DurationInfo ignored = new DurationInfo(LOG, true, \"delete %s\", path)) {\r\n        return forbidden(\"Expected an error deleting \" + path, \"\", () -> {\r\n            boolean r = roleFS.delete(path, true);\r\n            return \" delete=\" + r + \" \" + ls(path.getParent());\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "expectRenameForbidden",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AccessDeniedException expectRenameForbidden(Path src, Path dest) throws Exception\n{\r\n    try (DurationInfo ignored = new DurationInfo(LOG, true, \"rename(%s, %s)\", src, dest)) {\r\n        return forbidden(\"Renaming \" + src + \" to \" + dest, \"\", () -> {\r\n            boolean result = roleFS.rename(src, dest);\r\n            LOG.error(\"Rename should have been forbidden but returned {}\", result);\r\n            LOG.error(\"Source directory:\\n{}\", ContractTestUtils.ls(getFileSystem(), src.getParent()));\r\n            LOG.error(\"Destination directory:\\n{}\", ContractTestUtils.ls(getFileSystem(), src.getParent()));\r\n            return \"Rename unexpectedly returned \" + result;\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "pathMustExist",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void pathMustExist(Path p)\n{\r\n    eval(() -> assertPathExists(\"Missing path\", p));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "listFilesUnderPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<Path> listFilesUnderPath(Path path, boolean recursive) throws IOException\n{\r\n    Set<Path> files = new TreeSet<>();\r\n    try (DurationInfo ignore = new DurationInfo(LOG, \"ls -R %s\", path)) {\r\n        applyLocatedFiles(getFileSystem().listFiles(path, recursive), (status) -> files.add(status.getPath()));\r\n    }\r\n    return files;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testRenamePermissionRequirements",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testRenamePermissionRequirements() throws Throwable\n{\r\n    describe(\"Verify rename() only needs s3:DeleteObject permission\");\r\n    IOUtils.cleanupWithLogger(LOG, roleFS);\r\n    Configuration roleConfig = createAssumedRoleConfig();\r\n    bindRolePolicyStatements(roleConfig, STATEMENT_ALLOW_SSE_KMS_RW, STATEMENT_ALL_BUCKET_READ_ACCESS, new Statement(Effects.Allow).addActions(S3_PATH_RW_OPERATIONS).addResources(directory(basePath)), new Statement(Effects.Deny).addActions(S3_DELETE_OBJECT_VERSION).addResources(directory(basePath)));\r\n    roleFS = (S3AFileSystem) basePath.getFileSystem(roleConfig);\r\n    Path srcDir = new Path(basePath, \"src\");\r\n    Path destDir = new Path(basePath, \"dest\");\r\n    roleFS.mkdirs(srcDir);\r\n    List<Path> createdFiles = createFiles(roleFS, srcDir, dirDepth, fileCount, dirCount);\r\n    roleFS.rename(srcDir, destDir);\r\n    roleFS.rename(destDir, srcDir);\r\n    roleFS.delete(srcDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "newJobCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DirectoryStagingCommitter newJobCommitter() throws Exception\n{\r\n    return new DirectoryStagingCommitter(outputPath, createTaskAttemptForJob());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testBadConflictMode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBadConflictMode() throws Throwable\n{\r\n    getJob().getConfiguration().set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, \"merge\");\r\n    intercept(IllegalArgumentException.class, \"MERGE\", \"committer conflict\", this::newJobCommitter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testDefaultConflictResolution",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testDefaultConflictResolution() throws Exception\n{\r\n    getJob().getConfiguration().unset(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE);\r\n    pathIsDirectory(getMockS3A(), outputPath);\r\n    verifyJobSetupAndCommit();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testFailConflictResolution",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testFailConflictResolution() throws Exception\n{\r\n    getJob().getConfiguration().set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, CONFLICT_MODE_FAIL);\r\n    verifyFailureConflictOutcome();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "verifyFailureConflictOutcome",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void verifyFailureConflictOutcome() throws Exception\n{\r\n    pathIsDirectory(getMockS3A(), outputPath);\r\n    final DirectoryStagingCommitter committer = newJobCommitter();\r\n    intercept(PathExistsException.class, InternalCommitterConstants.E_DEST_EXISTS, \"Should throw an exception because the path exists\", () -> committer.setupJob(getJob()));\r\n    committer.preCommitJob(getJob(), AbstractS3ACommitter.ActiveCommit.empty());\r\n    reset(getMockS3A());\r\n    pathDoesNotExist(getMockS3A(), outputPath);\r\n    committer.setupJob(getJob());\r\n    verifyExistenceChecked(getMockS3A(), outputPath);\r\n    verifyMkdirsInvoked(getMockS3A(), outputPath);\r\n    verifyNoMoreInteractions(getMockS3A());\r\n    reset(getMockS3A());\r\n    pathDoesNotExist(getMockS3A(), outputPath);\r\n    committer.commitJob(getJob());\r\n    verifyCompletion(getMockS3A());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testAppendConflictResolution",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAppendConflictResolution() throws Exception\n{\r\n    getJob().getConfiguration().set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, CONFLICT_MODE_APPEND);\r\n    FileSystem mockS3 = getMockS3A();\r\n    pathIsDirectory(mockS3, outputPath);\r\n    verifyJobSetupAndCommit();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "verifyJobSetupAndCommit",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void verifyJobSetupAndCommit() throws Exception\n{\r\n    final DirectoryStagingCommitter committer = newJobCommitter();\r\n    committer.setupJob(getJob());\r\n    FileSystem mockS3 = getMockS3A();\r\n    Mockito.reset(mockS3);\r\n    pathExists(mockS3, outputPath);\r\n    committer.commitJob(getJob());\r\n    verifyCompletion(mockS3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testReplaceConflictResolution",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testReplaceConflictResolution() throws Exception\n{\r\n    FileSystem mockS3 = getMockS3A();\r\n    pathIsDirectory(mockS3, outputPath);\r\n    getJob().getConfiguration().set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, CONFLICT_MODE_REPLACE);\r\n    final DirectoryStagingCommitter committer = newJobCommitter();\r\n    committer.setupJob(getJob());\r\n    Mockito.reset(mockS3);\r\n    pathExists(mockS3, outputPath);\r\n    canDelete(mockS3, outputPath);\r\n    committer.commitJob(getJob());\r\n    verifyDeleted(mockS3, outputPath);\r\n    verifyCompletion(mockS3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testReplaceConflictFailsIfDestIsFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testReplaceConflictFailsIfDestIsFile() throws Exception\n{\r\n    pathIsFile(getMockS3A(), outputPath);\r\n    getJob().getConfiguration().set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, CONFLICT_MODE_REPLACE);\r\n    intercept(PathExistsException.class, InternalCommitterConstants.E_DEST_EXISTS, \"Expected a PathExistsException as the destination\" + \" was a file\", () -> {\r\n        newJobCommitter().setupJob(getJob());\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testAppendConflictFailsIfDestIsFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testAppendConflictFailsIfDestIsFile() throws Exception\n{\r\n    pathIsFile(getMockS3A(), outputPath);\r\n    getJob().getConfiguration().set(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, CONFLICT_MODE_APPEND);\r\n    intercept(PathExistsException.class, InternalCommitterConstants.E_DEST_EXISTS, \"Expected a PathExistsException as a the destination\" + \" is a file\", () -> {\r\n        newJobCommitter().setupJob(getJob());\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "testValidateDefaultConflictMode",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testValidateDefaultConflictMode() throws Throwable\n{\r\n    Configuration baseConf = new Configuration(true);\r\n    String[] sources = baseConf.getPropertySources(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE);\r\n    String sourceStr = Arrays.stream(sources).collect(Collectors.joining(\",\"));\r\n    LOG.info(\"source of conflict mode {}\", sourceStr);\r\n    String baseConfVal = baseConf.getTrimmed(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE);\r\n    assertEquals(\"conflict mode in core config from \" + sourceStr, CONFLICT_MODE_APPEND, baseConfVal);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testDirectoryBecomesEmpty",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testDirectoryBecomesEmpty() throws Exception\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path dir = path(\"testEmptyDir\");\r\n    Path child = path(\"testEmptyDir/dir2\");\r\n    mkdirs(child);\r\n    S3AFileStatus status = getS3AFileStatus(fs, dir);\r\n    assertEmptyDirectory(false, status);\r\n    assertDeleted(child, false);\r\n    status = getS3AFileStatus(fs, dir);\r\n    assertEmptyDirectory(true, status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertEmptyDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertEmptyDirectory(boolean isEmpty, S3AFileStatus s)\n{\r\n    String msg = \"dir is empty\";\r\n    Tristate expected = Tristate.fromBool(isEmpty);\r\n    assertEquals(msg, expected, s.isEmptyDirectory());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testDirectoryBecomesNonEmpty",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testDirectoryBecomesNonEmpty() throws Exception\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path dir = path(\"testEmptyDir\");\r\n    mkdirs(dir);\r\n    S3AFileStatus status = getS3AFileStatus(fs, dir);\r\n    assertEmptyDirectory(true, status);\r\n    ContractTestUtils.touch(fs, path(\"testEmptyDir/file1\"));\r\n    status = getS3AFileStatus(fs, dir);\r\n    assertEmptyDirectory(false, status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getS3AFileStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "S3AFileStatus getS3AFileStatus(S3AFileSystem fs, Path p) throws IOException\n{\r\n    try (AuditSpan span = span()) {\r\n        return fs.innerGetFileStatus(p, true, StatusProbeEnum.ALL);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "lookupToken",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AbstractS3ATokenIdentifier lookupToken(Credentials submittedCredentials, URI uri, Text kind) throws IOException\n{\r\n    final Token<AbstractS3ATokenIdentifier> token = requireNonNull(lookupS3ADelegationToken(submittedCredentials, uri), \"No Token for \" + uri);\r\n    assertEquals(\"Kind of token \" + token, kind, token.getKind());\r\n    AbstractS3ATokenIdentifier tid = token.decodeIdentifier();\r\n    LOG.info(\"Found for URI {}, token {}\", uri, tid);\r\n    return tid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "mkTokens",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Credentials mkTokens(final S3AFileSystem fs) throws IOException\n{\r\n    Credentials cred = new Credentials();\r\n    fs.addDelegationTokens(AbstractDelegationIT.YARN_RM, cred);\r\n    return cred;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "newS3AInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileSystem newS3AInstance(final URI uri, final Configuration conf) throws IOException\n{\r\n    S3AFileSystem fs = new S3AFileSystem();\r\n    fs.initialize(uri, conf);\r\n    return fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "assertBoundToDT",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertBoundToDT(final S3AFileSystem fs, final Text tokenKind)\n{\r\n    final S3ADelegationTokens dtSupport = fs.getDelegationTokens().get();\r\n    assertTrue(\"Expected bound to a delegation token: \" + dtSupport, dtSupport.isBoundToDT());\r\n    assertEquals(\"Wrong token kind\", tokenKind, dtSupport.getBoundDT().get().getKind());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "assertTokenCreationCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertTokenCreationCount(final S3AFileSystem fs, final int expected)\n{\r\n    assertEquals(\"DT creation count from \" + fs.getDelegationTokens().get(), expected, getTokenCreationCount(fs));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getTokenCreationCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTokenCreationCount(final S3AFileSystem fs)\n{\r\n    return fs.getDelegationTokens().map(S3ADelegationTokens::getCreationCount).get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "enableDelegationTokens",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void enableDelegationTokens(Configuration conf, String binding)\n{\r\n    removeBaseAndBucketOverrides(conf, DELEGATION_TOKEN_BINDING);\r\n    LOG.info(\"Enabling delegation token support for {}\", binding);\r\n    conf.set(DELEGATION_TOKEN_BINDING, binding);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "resetUGI",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void resetUGI()\n{\r\n    UserGroupInformation.reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "bindProviderList",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void bindProviderList(String bucket, Configuration config, String... providerClassnames)\n{\r\n    removeBaseAndBucketOverrides(bucket, config, AWS_CREDENTIALS_PROVIDER);\r\n    assertTrue(\"No providers to bind to\", providerClassnames.length > 0);\r\n    config.setStrings(AWS_CREDENTIALS_PROVIDER, providerClassnames);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "saveDT",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void saveDT(final File tokenFile, final Token<?> token) throws IOException\n{\r\n    requireNonNull(token, \"Null token\");\r\n    Credentials cred = new Credentials();\r\n    cred.addToken(token.getService(), token);\r\n    try (DataOutputStream out = new DataOutputStream(new FileOutputStream(tokenFile))) {\r\n        cred.writeTokenStorageToStream(out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "instantiateDTSupport",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "S3ADelegationTokens instantiateDTSupport(Configuration conf) throws IOException\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    S3ADelegationTokens tokens = new S3ADelegationTokens();\r\n    tokens.bindToFileSystem(fs.getCanonicalUri(), fs.createStoreContext(), fs.createDelegationOperations());\r\n    tokens.init(conf);\r\n    return tokens;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getBlockOutputBufferName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockOutputBufferName()\n{\r\n    return FAST_UPLOAD_BYTEBUFFER;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\yarn",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    conf.setBoolean(FS_S3A_COMMITTER_STAGING_UNIQUE_FILENAMES, false);\r\n    conf.set(FS_S3A_COMMITTER_NAME, StagingCommitter.NAME);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\yarn",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    S3AFileSystem fs = getFileSystem();\r\n    Configuration conf = getConfiguration();\r\n    rootPath = path(\"MiniClusterWordCount\");\r\n    Path workingDir = path(\"working\");\r\n    fs.setWorkingDirectory(workingDir);\r\n    fs.mkdirs(new Path(rootPath, \"input/\"));\r\n    yarnCluster = new MiniYARNCluster(\"MiniClusterWordCount\", 1, 1, 1);\r\n    yarnCluster.init(conf);\r\n    yarnCluster.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\yarn",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    if (yarnCluster != null) {\r\n        yarnCluster.stop();\r\n    }\r\n    super.teardown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\yarn",
  "methodName" : "testWithMiniCluster",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testWithMiniCluster() throws Exception\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Configuration conf = getConfiguration();\r\n    Path input = new Path(rootPath, \"input/in.txt\");\r\n    input = input.makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n    Path output = new Path(rootPath, \"output/\");\r\n    output = output.makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n    writeStringToFile(input, \"first line\\nsecond line\\nthird line\");\r\n    Job job = Job.getInstance(conf, \"word count\");\r\n    job.setJarByClass(WordCount.class);\r\n    job.setMapperClass(WordCount.TokenizerMapper.class);\r\n    job.setCombinerClass(WordCount.IntSumReducer.class);\r\n    job.setReducerClass(WordCount.IntSumReducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    FileInputFormat.addInputPath(job, input);\r\n    FileOutputFormat.setOutputPath(job, output);\r\n    int exitCode = (job.waitForCompletion(true) ? 0 : 1);\r\n    assertEquals(\"Returned error code.\", 0, exitCode);\r\n    Path success = new Path(output, _SUCCESS);\r\n    FileStatus status = fs.getFileStatus(success);\r\n    assertTrue(\"0 byte success file - not an S3A committer \" + success, status.getLen() > 0);\r\n    SuccessData successData = SuccessData.load(fs, success);\r\n    String commitDetails = successData.toString();\r\n    LOG.info(\"Committer details \\n{}\", commitDetails);\r\n    String outputAsStr = readStringFromFile(new Path(output, \"part-r-00000\"));\r\n    Map<String, Integer> resAsMap = getResultAsMap(outputAsStr);\r\n    assertEquals(4, resAsMap.size());\r\n    assertEquals(1, (int) resAsMap.get(\"first\"));\r\n    assertEquals(1, (int) resAsMap.get(\"second\"));\r\n    assertEquals(1, (int) resAsMap.get(\"third\"));\r\n    assertEquals(3, (int) resAsMap.get(\"line\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\yarn",
  "methodName" : "getResultAsMap",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<String, Integer> getResultAsMap(String outputAsStr) throws IOException\n{\r\n    Map<String, Integer> result = new HashMap<>();\r\n    for (String line : outputAsStr.split(\"\\n\")) {\r\n        String[] tokens = line.split(\"\\t\");\r\n        assertTrue(\"Not enough tokens in in string \\\" \" + line + \"\\\" from output \\\"\" + outputAsStr + \"\\\"\", tokens.length > 1);\r\n        result.put(tokens[0], Integer.parseInt(tokens[1]));\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\yarn",
  "methodName" : "writeStringToFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeStringToFile(Path path, String string) throws IOException\n{\r\n    Configuration conf = getConfiguration();\r\n    FileContext fc = S3ATestUtils.createTestFileContext(conf);\r\n    try (FSDataOutputStream file = fc.create(path, EnumSet.of(CreateFlag.CREATE))) {\r\n        file.write(string.getBytes());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\yarn",
  "methodName" : "readStringFromFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String readStringFromFile(Path path) throws IOException\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    try (FSDataInputStream in = fs.open(path)) {\r\n        long bytesLen = fs.getFileStatus(path).getLen();\r\n        byte[] buffer = new byte[(int) bytesLen];\r\n        IOUtils.readFully(in, buffer, 0, buffer.length);\r\n        return new String(buffer);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    assumeRoleTests();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    try {\r\n        super.teardown();\r\n    } finally {\r\n        cleanupWithLogger(LOG, readonlyFS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "assumeRoleTests",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assumeRoleTests()\n{\r\n    assume(\"No ARN for role tests\", !getAssumedRoleARN().isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getAssumedRoleARN",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getAssumedRoleARN()\n{\r\n    return getContract().getConf().getTrimmed(ASSUMED_ROLE_ARN, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "createAssumedRoleConfig",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createAssumedRoleConfig()\n{\r\n    return createAssumedRoleConfig(getAssumedRoleARN());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "createAssumedRoleConfig",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createAssumedRoleConfig(String roleARN)\n{\r\n    return newAssumedRoleConfig(getContract().getConf(), roleARN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testNoReadAccess",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testNoReadAccess() throws Throwable\n{\r\n    describe(\"Test failure handling if the client doesn't\" + \" have read access under a path\");\r\n    initNoReadAccess();\r\n    checkBasicFileOperations();\r\n    checkGlobOperations();\r\n    checkSingleThreadedLocatedFileStatus();\r\n    checkLocatedFileStatusFourThreads();\r\n    checkLocatedFileStatusScanFile();\r\n    checkLocatedFileStatusNonexistentPath();\r\n    checkDeleteOperations();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "initNoReadAccess",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void initNoReadAccess() throws Throwable\n{\r\n    describe(\"Setting up filesystem\");\r\n    S3AFileSystem realFS = getFileSystem();\r\n    basePath = methodPath();\r\n    describe(\"Creating test directories and files\");\r\n    noReadDir = new Path(basePath, \"noReadDir\");\r\n    noReadWildcard = new Path(noReadDir, \"*/*.txt\");\r\n    emptyDir = new Path(noReadDir, \"emptyDir\");\r\n    realFS.mkdirs(emptyDir);\r\n    emptyFile = new Path(noReadDir, \"emptyFile.txt\");\r\n    touch(realFS, emptyFile);\r\n    subDir = new Path(noReadDir, \"subDir\");\r\n    subdirFile = new Path(subDir, \"subdirFile.txt\");\r\n    createFile(realFS, subdirFile, true, HELLO);\r\n    subDir2 = new Path(noReadDir, \"subDir2\");\r\n    subdir2File1 = new Path(subDir2, \"subdir2File1.txt\");\r\n    subdir2File2 = new Path(subDir2, \"subdir2File2.docx\");\r\n    createFile(realFS, subdir2File1, true, HELLO);\r\n    createFile(realFS, subdir2File2, true, HELLO);\r\n    roleConfig = createAssumedRoleConfig();\r\n    bindRolePolicyStatements(roleConfig, STATEMENT_ALLOW_SSE_KMS_RW, statement(true, S3_ALL_BUCKETS, S3_ALL_OPERATIONS), new Statement(Effects.Deny).addActions(S3_ALL_GET).addResources(directory(noReadDir)));\r\n    readonlyFS = (S3AFileSystem) basePath.getFileSystem(roleConfig);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "checkBasicFileOperations",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void checkBasicFileOperations() throws Throwable\n{\r\n    readonlyFS.listStatus(basePath);\r\n    lsR(readonlyFS, basePath, true);\r\n    readonlyFS.listStatus(emptyDir);\r\n    lsR(readonlyFS, noReadDir, true);\r\n    readonlyFS.getFileStatus(noReadDir);\r\n    readonlyFS.getFileStatus(emptyDir);\r\n    accessDenied(() -> readonlyFS.getFileStatus(subdirFile));\r\n    accessDenied(() -> ContractTestUtils.readUTF8(readonlyFS, subdirFile, HELLO.length));\r\n    accessDenied(() -> readonlyFS.open(emptyFile));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "checkGlobOperations",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void checkGlobOperations() throws Throwable\n{\r\n    describe(\"Glob Status operations\");\r\n    globFS(getFileSystem(), subdirFile, null, false, 1);\r\n    globFS(readonlyFS, subdirFile, null, true, 1);\r\n    FileStatus[] st = globFS(readonlyFS, emptyDir, null, false, 1);\r\n    st = globFS(readonlyFS, noReadWildcard, null, false, 2);\r\n    globFS(readonlyFS, new Path(noReadDir, \"*/*.docx\"), null, false, 1);\r\n    globFS(readonlyFS, new Path(noReadDir, \"*/*.doc\"), null, false, 0);\r\n    globFS(readonlyFS, noReadDir, EVERYTHING, false, 1);\r\n    FileStatus[] st2 = globFS(readonlyFS, noReadDir, EVERYTHING, false, 1);\r\n    Assertions.assertThat(st2).extracting(FileStatus::getPath).containsExactly(noReadDir);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "checkSingleThreadedLocatedFileStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkSingleThreadedLocatedFileStatus() throws Throwable\n{\r\n    describe(\"LocatedFileStatusFetcher operations\");\r\n    roleConfig.setInt(LIST_STATUS_NUM_THREADS, 1);\r\n    LocatedFileStatusFetcher fetcher = new LocatedFileStatusFetcher(roleConfig, new Path[] { basePath }, true, HIDDEN_FILE_FILTER, true);\r\n    Assertions.assertThat(fetcher.getFileStatuses()).describedAs(\"result of located scan\").flatExtracting(FileStatus::getPath).containsExactlyInAnyOrder(emptyFile, subdirFile, subdir2File1, subdir2File2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "checkLocatedFileStatusFourThreads",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkLocatedFileStatusFourThreads() throws Throwable\n{\r\n    int threads = 4;\r\n    describe(\"LocatedFileStatusFetcher with %d\", threads);\r\n    roleConfig.setInt(LIST_STATUS_NUM_THREADS, threads);\r\n    LocatedFileStatusFetcher fetcher = new LocatedFileStatusFetcher(roleConfig, new Path[] { noReadWildcard }, true, EVERYTHING, true);\r\n    Assertions.assertThat(fetcher.getFileStatuses()).describedAs(\"result of located scan\").isNotNull().flatExtracting(FileStatus::getPath).containsExactlyInAnyOrder(subdirFile, subdir2File1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "checkLocatedFileStatusScanFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkLocatedFileStatusScanFile() throws Throwable\n{\r\n    describe(\"LocatedFileStatusFetcher with file %s\", subdirFile);\r\n    roleConfig.setInt(LIST_STATUS_NUM_THREADS, 16);\r\n    LocatedFileStatusFetcher fetcher = new LocatedFileStatusFetcher(roleConfig, new Path[] { subdirFile }, true, TEXT_FILE, true);\r\n    accessDenied(() -> fetcher.getFileStatuses());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "checkLocatedFileStatusNonexistentPath",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void checkLocatedFileStatusNonexistentPath() throws Throwable\n{\r\n    Path nonexistent = new Path(noReadDir, \"nonexistent\");\r\n    InvalidInputException ex = intercept(InvalidInputException.class, DOES_NOT_EXIST, () -> new LocatedFileStatusFetcher(roleConfig, new Path[] { nonexistent }, true, EVERYTHING, true).getFileStatuses());\r\n    assertExceptionContains(DOES_NOT_EXIST, ex.getCause());\r\n    intercept(InvalidInputException.class, DOES_NOT_EXIST, () -> new LocatedFileStatusFetcher(roleConfig, new Path[] { noReadDir }, true, TEXT_FILE, true).getFileStatuses());\r\n    ex = intercept(InvalidInputException.class, MATCHES_0_FILES, () -> new LocatedFileStatusFetcher(roleConfig, new Path[] { new Path(nonexistent, \"*.txt)\") }, true, TEXT_FILE, true).getFileStatuses());\r\n    assertExceptionContains(MATCHES_0_FILES, ex.getCause());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "checkDeleteOperations",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void checkDeleteOperations() throws Throwable\n{\r\n    describe(\"Testing delete operations\");\r\n    readonlyFS.delete(emptyDir, true);\r\n    accessDenied(() -> readonlyFS.delete(emptyFile, true));\r\n    readonlyFS.delete(subDir, true);\r\n    fileNotFound(() -> readonlyFS.getFileStatus(subDir));\r\n    fileNotFound(() -> readonlyFS.getFileStatus(subdirFile));\r\n    readonlyFS.delete(basePath, true);\r\n    fileNotFound(() -> readonlyFS.getFileStatus(subDir));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "fileNotFound",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileNotFoundException fileNotFound(final Callable<T> eval) throws Exception\n{\r\n    return intercept(FileNotFoundException.class, eval);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "accessDenied",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AccessDeniedException accessDenied(final Callable<T> eval) throws Exception\n{\r\n    return intercept(AccessDeniedException.class, eval);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "assertStatusPathEquals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertStatusPathEquals(final Path expected, final FileStatus[] statuses)\n{\r\n    Assertions.assertThat(statuses).describedAs(\"List of status entries\").isNotNull().hasSize(1);\r\n    Assertions.assertThat(statuses[0].getPath()).describedAs(\"Status entry %s\", statuses[0]).isEqualTo(expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "globFS",
  "errType" : [ "AccessDeniedException", "IOException|RuntimeException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "FileStatus[] globFS(final S3AFileSystem fs, final Path path, final PathFilter filter, boolean expectAuthFailure, final int expectedCount) throws IOException\n{\r\n    LOG.info(\"Glob {}\", path);\r\n    S3ATestUtils.MetricDiff getMetric = new S3ATestUtils.MetricDiff(fs, Statistic.OBJECT_METADATA_REQUESTS);\r\n    S3ATestUtils.MetricDiff listMetric = new S3ATestUtils.MetricDiff(fs, Statistic.OBJECT_LIST_REQUEST);\r\n    FileStatus[] st;\r\n    try {\r\n        st = filter == null ? fs.globStatus(path) : fs.globStatus(path, filter);\r\n        LOG.info(\"Metrics:\\n {},\\n {}\", getMetric, listMetric);\r\n        if (expectAuthFailure) {\r\n            String resultStr;\r\n            if (st == null) {\r\n                resultStr = \"A null array\";\r\n            } else {\r\n                resultStr = StringUtils.join(st, \",\");\r\n            }\r\n            fail(String.format(\"globStatus(%s) should have raised\" + \" an exception, but returned %s\", path, resultStr));\r\n        }\r\n    } catch (AccessDeniedException e) {\r\n        LOG.info(\"Metrics:\\n {},\\n {}\", getMetric, listMetric);\r\n        failif(!expectAuthFailure, \"Access denied in glob of \" + path, e);\r\n        return null;\r\n    } catch (IOException | RuntimeException e) {\r\n        throw new AssertionError(\"Other exception raised in glob:\" + e, e);\r\n    }\r\n    if (expectedCount < 0) {\r\n        Assertions.assertThat(st).describedAs(\"Glob of %s\", path).isNull();\r\n    } else {\r\n        Assertions.assertThat(st).describedAs(\"Glob of %s\", path).isNotNull().hasSize(expectedCount);\r\n    }\r\n    return st;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "head",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int head()\n{\r\n    return head;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "list",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int list()\n{\r\n    return list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "plus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OperationCost plus(OperationCost that)\n{\r\n    return new OperationCost(head + that.head, list + that.list);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"OperationCost{\" + \"head=\" + head + \", list=\" + list + '}';\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "afterClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void afterClass() throws Exception\n{\r\n    ensureDestroyed();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSubmitCallable",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSubmitCallable() throws Exception\n{\r\n    ensureCreated();\r\n    Future<Integer> f = tpe.submit(callableSleeper);\r\n    Integer v = f.get();\r\n    assertEquals(SOME_VALUE, v);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSubmitRunnable",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSubmitRunnable() throws Exception\n{\r\n    ensureCreated();\r\n    verifyQueueSize(tpe, NUM_ACTIVE_TASKS + NUM_WAITING_TASKS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "verifyQueueSize",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyQueueSize(ExecutorService executorService, int expectedQueueSize)\n{\r\n    CountDownLatch latch = new CountDownLatch(1);\r\n    for (int i = 0; i < expectedQueueSize; i++) {\r\n        executorService.submit(new LatchedSleeper(latch));\r\n    }\r\n    StopWatch stopWatch = new StopWatch().start();\r\n    latch.countDown();\r\n    executorService.submit(sleeper);\r\n    assertDidBlock(stopWatch);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testShutdown",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testShutdown() throws Exception\n{\r\n    ensureCreated();\r\n    ensureDestroyed();\r\n    ensureCreated();\r\n    testSubmitRunnable();\r\n    ensureDestroyed();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testChainedQueue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testChainedQueue() throws Throwable\n{\r\n    ensureCreated();\r\n    int size = 2;\r\n    ExecutorService wrapper = new SemaphoredDelegatingExecutor(tpe, size, true);\r\n    verifyQueueSize(wrapper, size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertDidBlock",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertDidBlock(StopWatch sw)\n{\r\n    try {\r\n        if (sw.now(TimeUnit.MILLISECONDS) < BLOCKING_THRESHOLD_MSEC) {\r\n            throw new RuntimeException(\"Blocking call returned too fast.\");\r\n        }\r\n    } finally {\r\n        sw.reset().start();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "ensureCreated",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void ensureCreated() throws Exception\n{\r\n    if (tpe == null) {\r\n        LOG.debug(\"Creating thread pool\");\r\n        tpe = BlockingThreadPoolExecutorService.newInstance(NUM_ACTIVE_TASKS, NUM_WAITING_TASKS, 1, TimeUnit.SECONDS, \"btpetest\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "ensureDestroyed",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void ensureDestroyed() throws Exception\n{\r\n    if (tpe == null) {\r\n        return;\r\n    }\r\n    int shutdownTries = SHUTDOWN_WAIT_TRIES;\r\n    tpe.shutdown();\r\n    if (!tpe.isShutdown()) {\r\n        throw new RuntimeException(\"Shutdown had no effect.\");\r\n    }\r\n    while (!tpe.awaitTermination(SHUTDOWN_WAIT_MSEC, TimeUnit.MILLISECONDS)) {\r\n        LOG.info(\"Waiting for thread pool shutdown.\");\r\n        if (shutdownTries-- <= 0) {\r\n            LOG.error(\"Failed to terminate thread pool gracefully.\");\r\n            break;\r\n        }\r\n    }\r\n    if (!tpe.isTerminated()) {\r\n        tpe.shutdownNow();\r\n        if (!tpe.awaitTermination(SHUTDOWN_WAIT_MSEC, TimeUnit.MILLISECONDS)) {\r\n            throw new RuntimeException(\"Failed to terminate thread pool in timely manner.\");\r\n        }\r\n    }\r\n    tpe = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    basePath = getTestPath();\r\n    basePathDepth = basePath.depth();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testDeepSequentialCreate",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testDeepSequentialCreate() throws Exception\n{\r\n    long numOperations = getOperationCount();\r\n    S3AFileSystem fs = getFileSystem();\r\n    NanoTimer timer = new NanoTimer();\r\n    for (int i = 0; i < numOperations; i++) {\r\n        Path p = getPathIteration(i, PATH_DEPTH);\r\n        OutputStream out = fs.create(p);\r\n        out.write(40);\r\n        out.close();\r\n    }\r\n    timer.end(\"Time to create %d files of depth %d\", getOperationCount(), PATH_DEPTH);\r\n    LOG.info(\"Time per create: {} msec\", timer.nanosPerOperation(numOperations) / 1000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getPathIteration",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Path getPathIteration(long iter, int totalDepth) throws Exception\n{\r\n    assertTrue(\"Test path too long, increase PATH_DEPTH in test.\", totalDepth > basePathDepth);\r\n    int neededDirs = totalDepth - basePathDepth - 1;\r\n    StringBuilder sb = new StringBuilder();\r\n    for (int i = 0; i < neededDirs; i++) {\r\n        sb.append(\"iter-\").append(iter);\r\n        sb.append(\"-dir-\").append(i);\r\n        sb.append(\"/\");\r\n    }\r\n    sb.append(\"file\").append(iter);\r\n    return new Path(basePath, sb.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { DirectoryStagingCommitter.NAME }, { MagicS3GuardCommitter.NAME } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "committerName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String committerName()\n{\r\n    return committerName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    requireScaleTestsEnabled();\r\n    prepareToTerasort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "applyCustomConfigOptions",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void applyCustomConfigOptions(JobConf conf)\n{\r\n    conf.setInt(TeraSortConfigKeys.SAMPLE_SIZE.key(), getSampleSizeForEachPartition());\r\n    conf.setInt(TeraSortConfigKeys.NUM_PARTITIONS.key(), getExpectedPartitionCount());\r\n    conf.setBoolean(TeraSortConfigKeys.USE_SIMPLE_PARTITIONER.key(), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "getExpectedPartitionCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getExpectedPartitionCount()\n{\r\n    return EXPECTED_PARTITION_COUNT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "getSampleSizeForEachPartition",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getSampleSizeForEachPartition()\n{\r\n    return PARTITION_SAMPLE_SIZE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "getRowCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRowCount()\n{\r\n    return ROW_COUNT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "prepareToTerasort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void prepareToTerasort()\n{\r\n    terasortPath = new Path(\"/terasort-\" + committerName).makeQualified(getFileSystem());\r\n    sortInput = new Path(terasortPath, \"sortin\");\r\n    sortOutput = new Path(terasortPath, \"sortout\");\r\n    sortValidate = new Path(terasortPath, \"validate\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "completedStage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void completedStage(final String stage, final DurationInfo d)\n{\r\n    completedStages.put(stage, d);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "requireStage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void requireStage(final String stage)\n{\r\n    Assume.assumeTrue(\"Required stage was not completed: \" + stage, completedStages.get(stage) != null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "executeStage",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void executeStage(final String stage, final JobConf jobConf, final Path dest, final Tool tool, final String[] args, final int minimumFileCount) throws Exception\n{\r\n    int result;\r\n    DurationInfo d = new DurationInfo(LOG, stage);\r\n    try {\r\n        result = ToolRunner.run(jobConf, tool, args);\r\n    } finally {\r\n        d.close();\r\n    }\r\n    dumpOutputTree(dest);\r\n    assertEquals(stage + \"(\" + StringUtils.join(\", \", args) + \")\" + \" failed\", 0, result);\r\n    validateSuccessFile(dest, committerName(), getFileSystem(), stage, minimumFileCount, \"\");\r\n    completedStage(stage, d);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "test_100_terasort_setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void test_100_terasort_setup() throws Throwable\n{\r\n    describe(\"Setting up for a terasort\");\r\n    getFileSystem().delete(terasortPath, true);\r\n    completedStages = new HashMap<>();\r\n    terasortDuration = Optional.of(new DurationInfo(LOG, false, \"Terasort\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "test_110_teragen",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void test_110_teragen() throws Throwable\n{\r\n    describe(\"Teragen to %s\", sortInput);\r\n    getFileSystem().delete(sortInput, true);\r\n    JobConf jobConf = newJobConf();\r\n    patchConfigurationForCommitter(jobConf);\r\n    executeStage(\"teragen\", jobConf, sortInput, new TeraGen(), new String[] { Integer.toString(getRowCount()), sortInput.toString() }, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "test_120_terasort",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void test_120_terasort() throws Throwable\n{\r\n    describe(\"Terasort from %s to %s\", sortInput, sortOutput);\r\n    requireStage(\"teragen\");\r\n    getFileSystem().delete(sortOutput, true);\r\n    loadSuccessFile(getFileSystem(), sortInput, \"previous teragen stage\");\r\n    JobConf jobConf = newJobConf();\r\n    patchConfigurationForCommitter(jobConf);\r\n    executeStage(\"terasort\", jobConf, sortOutput, new TeraSort(), new String[] { sortInput.toString(), sortOutput.toString() }, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "test_130_teravalidate",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void test_130_teravalidate() throws Throwable\n{\r\n    describe(\"TeraValidate from %s to %s\", sortOutput, sortValidate);\r\n    requireStage(\"terasort\");\r\n    getFileSystem().delete(sortValidate, true);\r\n    loadSuccessFile(getFileSystem(), sortOutput, \"previous terasort stage\");\r\n    JobConf jobConf = newJobConf();\r\n    patchConfigurationForCommitter(jobConf);\r\n    executeStage(\"teravalidate\", jobConf, sortValidate, new TeraValidate(), new String[] { sortOutput.toString(), sortValidate.toString() }, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "test_140_teracomplete",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void test_140_teracomplete() throws Throwable\n{\r\n    terasortDuration.ifPresent(d -> {\r\n        d.close();\r\n        completedStage(\"overall\", d);\r\n    });\r\n    final StringBuilder results = new StringBuilder();\r\n    results.append(\"\\\"Operation\\\"\\t\\\"Duration\\\"\\n\");\r\n    Consumer<String> stage = (s) -> {\r\n        DurationInfo duration = completedStages.get(s);\r\n        results.append(String.format(\"\\\"%s\\\"\\t\\\"%s\\\"\\n\", s, duration == null ? \"\" : duration));\r\n    };\r\n    stage.accept(\"teragen\");\r\n    stage.accept(\"terasort\");\r\n    stage.accept(\"teravalidate\");\r\n    stage.accept(\"overall\");\r\n    String text = results.toString();\r\n    File resultsFile = File.createTempFile(\"results\", \".csv\");\r\n    FileUtils.write(resultsFile, text, StandardCharsets.UTF_8);\r\n    LOG.info(\"Results are in {}\\n{}\", resultsFile, text);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "test_150_teracleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_150_teracleanup() throws Throwable\n{\r\n    terasortDuration = Optional.empty();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "test_200_directory_deletion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_200_directory_deletion() throws Throwable\n{\r\n    getFileSystem().delete(terasortPath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\terasort",
  "methodName" : "dumpOutputTree",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void dumpOutputTree(Path path) throws Exception\n{\r\n    LOG.info(\"Files under output directory {}\", path);\r\n    try {\r\n        lsR(getFileSystem(), path, true);\r\n    } catch (FileNotFoundException e) {\r\n        LOG.info(\"Output directory {} not found\", path);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    resetSpan = getManager().getActiveAuditSpan();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "createConfig",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfig()\n{\r\n    return noopAuditConfig();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testStop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testStop() throws Throwable\n{\r\n    getManager().stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testCreateRequestHandlers",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCreateRequestHandlers() throws Throwable\n{\r\n    List<RequestHandler2> handlers = getManager().createRequestHandlers();\r\n    assertThat(handlers).isNotEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testInitialSpanIsInvalid",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testInitialSpanIsInvalid() throws Throwable\n{\r\n    assertThat(resetSpan).matches(f -> !f.isValidSpan(), \"is invalid\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testCreateCloseSpan",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCreateCloseSpan() throws Throwable\n{\r\n    AuditSpan span = getManager().createSpan(\"op\", null, null);\r\n    assertThat(span).matches(AuditSpan::isValidSpan, \"is valid\");\r\n    assertActiveSpan(span);\r\n    span.activate();\r\n    assertActiveSpan(span);\r\n    span.close();\r\n    assertActiveSpan(resetSpan);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testSpanActivation",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSpanActivation() throws Throwable\n{\r\n    AuditSpan span1 = getManager().createSpan(\"op1\", null, null);\r\n    AuditSpan span2 = getManager().createSpan(\"op2\", null, null);\r\n    assertActiveSpan(span2);\r\n    span1.activate();\r\n    assertActiveSpan(span1);\r\n    span2.activate();\r\n    assertActiveSpan(span2);\r\n    span2.close();\r\n    assertActiveSpan(resetSpan);\r\n    span1.close();\r\n    assertActiveSpan(resetSpan);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testSpanDeactivation",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSpanDeactivation() throws Throwable\n{\r\n    AuditSpan span1 = getManager().createSpan(\"op1\", null, null);\r\n    AuditSpan span2 = getManager().createSpan(\"op2\", null, null);\r\n    assertActiveSpan(span2);\r\n    span1.close();\r\n    assertActiveSpan(span2);\r\n    span2.close();\r\n    assertActiveSpan(resetSpan);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testResetSpanCannotBeClosed",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testResetSpanCannotBeClosed() throws Throwable\n{\r\n    assertThat(resetSpan).matches(f -> !f.isValidSpan(), \"is invalid\");\r\n    AuditSpan span1 = getManager().createSpan(\"op1\", null, null);\r\n    resetSpan.activate();\r\n    resetSpan.close();\r\n    assertActiveSpan(resetSpan);\r\n    span1.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getSpanSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuditSpanSource getSpanSource()\n{\r\n    return spanSource;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setSpanSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSpanSource(final AuditSpanSource spanSource)\n{\r\n    this.spanSource = spanSource;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    Thread.currentThread().setName(\"setup\");\r\n    FileSystem.getLocal(new Configuration());\r\n    S3AFileSystem.initializeClass();\r\n    super.setup();\r\n    setSpanSource(getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    Thread.currentThread().setName(\"teardown\");\r\n    maybeAuditTestPath();\r\n    super.teardown();\r\n    if (getFileSystem() != null) {\r\n        FILESYSTEM_IOSTATS.aggregate(getFileSystem().getIOStatistics());\r\n    }\r\n    describe(\"closing file system\");\r\n    IOUtils.closeStream(getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "dumpFileSystemIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void dumpFileSystemIOStatistics()\n{\r\n    LOG.info(\"Aggregate FileSystem Statistics {}\", ioStatisticsToPrettyString(FILESYSTEM_IOSTATS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "maybeAuditTestPath",
  "errType" : [ "FileNotFoundException", "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void maybeAuditTestPath()\n{\r\n    final S3AFileSystem fs = getFileSystem();\r\n    if (fs != null) {\r\n        try {\r\n            boolean audit = getTestPropertyBool(fs.getConf(), DIRECTORY_MARKER_AUDIT, false);\r\n            Path methodPath = methodPath();\r\n            if (audit && !fs.getDirectoryMarkerPolicy().keepDirectoryMarkers(methodPath) && fs.isDirectory(methodPath)) {\r\n                MarkerTool.ScanResult result = MarkerTool.execMarkerTool(new MarkerTool.ScanArgsBuilder().withSourceFS(fs).withPath(methodPath).withDoPurge(true).withMinMarkerCount(0).withMaxMarkerCount(0).withLimit(UNLIMITED_LISTING).withNonAuth(false).build());\r\n                final String resultStr = result.toString();\r\n                assertEquals(\"Audit of \" + methodPath + \" failed: \" + resultStr, 0, result.getExitCode());\r\n                assertEquals(\"Marker Count under \" + methodPath + \" non-zero: \" + resultStr, 0, result.getFilteredMarkerCount());\r\n            }\r\n        } catch (FileNotFoundException ignored) {\r\n        } catch (Exception e) {\r\n            if (!e.toString().contains(E_FS_CLOSED)) {\r\n                LOG.warn(\"Marker Tool Failure\", e);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return S3A_TEST_TIMEOUT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    return S3ATestUtils.prepareTestConfiguration(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    return getContract().getConf();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileSystem getFileSystem()\n{\r\n    return (S3AFileSystem) super.getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "describe",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void describe(String text, Object... args)\n{\r\n    LOG.info(\"\\n\\n{}: {}\\n\", getMethodName(), String.format(text, args));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "writeThenReadFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path writeThenReadFile(String name, int len) throws IOException\n{\r\n    Path path = path(name);\r\n    writeThenReadFile(path, len);\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "writeThenReadFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writeThenReadFile(Path path, int len) throws IOException\n{\r\n    byte[] data = dataset(len, 'a', 'z');\r\n    writeDataset(getFileSystem(), path, data, data.length, 1024 * 1024, true);\r\n    ContractTestUtils.verifyFileContents(getFileSystem(), path, data);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "span",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpan span() throws IOException\n{\r\n    return span(getSpanSource());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "span",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpan span(AuditSpanSource source) throws IOException\n{\r\n    return source.createSpan(getMethodName(), null, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "skipIfClientSideEncryption",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void skipIfClientSideEncryption()\n{\r\n    Assume.assumeTrue(\"Skipping test if CSE is enabled\", !getFileSystem().isCSEEnabled());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testListOperations",
  "errType" : null,
  "containingMethodsNum" : 36,
  "sourceCodeText" : "void testListOperations() throws Throwable\n{\r\n    describe(\"Test recursive list operations\");\r\n    final Path scaleTestDir = path(\"testListOperations\");\r\n    final Path listDir = new Path(scaleTestDir, \"lists\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    int scale = getConf().getInt(KEY_DIRECTORY_COUNT, DEFAULT_DIRECTORY_COUNT);\r\n    int width = scale;\r\n    int depth = scale;\r\n    int files = scale;\r\n    MetricDiff metadataRequests = new MetricDiff(fs, OBJECT_METADATA_REQUESTS);\r\n    MetricDiff listRequests = new MetricDiff(fs, Statistic.OBJECT_LIST_REQUEST);\r\n    MetricDiff listContinueRequests = new MetricDiff(fs, OBJECT_CONTINUE_LIST_REQUESTS);\r\n    MetricDiff listStatusCalls = new MetricDiff(fs, INVOCATION_LIST_FILES);\r\n    MetricDiff getFileStatusCalls = new MetricDiff(fs, INVOCATION_GET_FILE_STATUS);\r\n    NanoTimer createTimer = new NanoTimer();\r\n    TreeScanResults created = createSubdirs(fs, listDir, depth, width, files, 0);\r\n    int emptyDepth = 1 * scale;\r\n    int emptyWidth = 3 * scale;\r\n    created.add(createSubdirs(fs, listDir, emptyDepth, emptyWidth, 0, 0, \"empty\", \"f-\", \"\"));\r\n    createTimer.end(\"Time to create %s\", created);\r\n    LOG.info(\"Time per operation: {}\", toHuman(createTimer.nanosPerOperation(created.totalCount())));\r\n    printThenReset(LOG, metadataRequests, listRequests, listContinueRequests, listStatusCalls, getFileStatusCalls);\r\n    describe(\"Listing files via treewalk\");\r\n    try {\r\n        NanoTimer treeWalkTimer = new NanoTimer();\r\n        TreeScanResults treewalkResults = treeWalk(fs, listDir);\r\n        treeWalkTimer.end(\"List status via treewalk of %s\", created);\r\n        printThenReset(LOG, metadataRequests, listRequests, listContinueRequests, listStatusCalls, getFileStatusCalls);\r\n        assertEquals(\"Files found in listFiles(recursive=true) \" + \" created=\" + created + \" listed=\" + treewalkResults, created.getFileCount(), treewalkResults.getFileCount());\r\n        describe(\"Listing files via listFiles(recursive=true)\");\r\n        NanoTimer listFilesRecursiveTimer = new NanoTimer();\r\n        TreeScanResults listFilesResults = new TreeScanResults(fs.listFiles(listDir, true));\r\n        listFilesRecursiveTimer.end(\"listFiles(recursive=true) of %s\", created);\r\n        assertEquals(\"Files found in listFiles(recursive=true) \" + \" created=\" + created + \" listed=\" + listFilesResults, created.getFileCount(), listFilesResults.getFileCount());\r\n        print(LOG, metadataRequests, listRequests, listContinueRequests, listStatusCalls, getFileStatusCalls);\r\n        assertEquals(listRequests.toString(), 1, listRequests.diff());\r\n        reset(metadataRequests, listRequests, listContinueRequests, listStatusCalls, getFileStatusCalls);\r\n        describe(\"Get content summary for directory\");\r\n        NanoTimer getContentSummaryTimer = new NanoTimer();\r\n        ContentSummary rootPathSummary = fs.getContentSummary(scaleTestDir);\r\n        ContentSummary testPathSummary = fs.getContentSummary(listDir);\r\n        getContentSummaryTimer.end(\"getContentSummary of %s\", created);\r\n        print(LOG, metadataRequests, listRequests, listContinueRequests, listStatusCalls, getFileStatusCalls);\r\n        assertEquals(listRequests.toString(), 2, listRequests.diff());\r\n        reset(metadataRequests, listRequests, listContinueRequests, listStatusCalls, getFileStatusCalls);\r\n        assertTrue(\"Root directory count should be > test path\", rootPathSummary.getDirectoryCount() > testPathSummary.getDirectoryCount());\r\n        assertTrue(\"Root file count should be >= to test path\", rootPathSummary.getFileCount() >= testPathSummary.getFileCount());\r\n        assertEquals(\"Incorrect directory count\", created.getDirCount() + 1, testPathSummary.getDirectoryCount());\r\n        assertEquals(\"Incorrect file count\", created.getFileCount(), testPathSummary.getFileCount());\r\n    } finally {\r\n        describe(\"deletion\");\r\n        NanoTimer deleteTimer = new NanoTimer();\r\n        fs.delete(listDir, true);\r\n        deleteTimer.end(\"Deleting directory tree\");\r\n        printThenReset(LOG, metadataRequests, listRequests, listContinueRequests, listStatusCalls, getFileStatusCalls);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testMultiPagesListingPerformanceAndCorrectness",
  "errType" : null,
  "containingMethodsNum" : 51,
  "sourceCodeText" : "void testMultiPagesListingPerformanceAndCorrectness() throws Throwable\n{\r\n    describe(\"Check performance and correctness for multi page listing \" + \"using different listing api\");\r\n    final Path dir = methodPath();\r\n    final int batchSize = 10;\r\n    final int numOfPutRequests = 1000;\r\n    final int eachFileProcessingTime = 10;\r\n    final int numOfPutThreads = 50;\r\n    Assertions.assertThat(numOfPutRequests % batchSize).describedAs(\"Files put %d must be a multiple of list batch size %d\", numOfPutRequests, batchSize).isEqualTo(0);\r\n    final Configuration conf = getConfigurationWithConfiguredBatchSize(batchSize);\r\n    removeBaseAndBucketOverrides(conf, DIRECTORY_MARKER_POLICY);\r\n    conf.set(DIRECTORY_MARKER_POLICY, DIRECTORY_MARKER_POLICY_KEEP);\r\n    S3AFileSystem fs = (S3AFileSystem) FileSystem.get(dir.toUri(), conf);\r\n    final List<String> originalListOfFiles = new ArrayList<>();\r\n    ExecutorService executorService = Executors.newFixedThreadPool(numOfPutThreads);\r\n    NanoTimer uploadTimer = new NanoTimer();\r\n    try {\r\n        fs.create(dir);\r\n        final AuditSpan span = fs.getAuditSpanSource().createSpan(OBJECT_PUT_REQUESTS.getSymbol(), dir.toString(), null);\r\n        final WriteOperationHelper writeOperationHelper = fs.getWriteOperationHelper();\r\n        final RequestFactory requestFactory = writeOperationHelper.getRequestFactory();\r\n        List<CompletableFuture<PutObjectResult>> futures = new ArrayList<>(numOfPutRequests);\r\n        for (int i = 0; i < numOfPutRequests; i++) {\r\n            Path file = new Path(dir, String.format(\"file-%03d\", i));\r\n            originalListOfFiles.add(file.toString());\r\n            ObjectMetadata om = fs.newObjectMetadata(0L);\r\n            PutObjectRequest put = requestFactory.newPutObjectRequest(fs.pathToKey(file), om, new FailingInputStream());\r\n            futures.add(submit(executorService, () -> writeOperationHelper.putObject(put)));\r\n        }\r\n        LOG.info(\"Waiting for PUTs to complete\");\r\n        waitForCompletion(futures);\r\n        uploadTimer.end(\"uploading %d files with a parallelism of %d\", numOfPutRequests, numOfPutThreads);\r\n        RemoteIterator<LocatedFileStatus> resIterator = fs.listFiles(dir, true);\r\n        List<String> listUsingListFiles = new ArrayList<>();\r\n        NanoTimer timeUsingListFiles = new NanoTimer();\r\n        RemoteIterators.foreach(resIterator, st -> {\r\n            listUsingListFiles.add(st.getPath().toString());\r\n            sleep(eachFileProcessingTime);\r\n        });\r\n        LOG.info(\"Listing Statistics: {}\", ioStatisticsToPrettyString(retrieveIOStatistics(resIterator)));\r\n        timeUsingListFiles.end(\"listing %d files using listFiles() api with \" + \"batch size of %d including %dms of processing time\" + \" for each file\", numOfPutRequests, batchSize, eachFileProcessingTime);\r\n        Assertions.assertThat(listUsingListFiles).describedAs(\"Listing results using listFiles() must\" + \" match with original list of files\").hasSameElementsAs(originalListOfFiles).hasSize(numOfPutRequests);\r\n        List<String> listUsingListStatus = new ArrayList<>();\r\n        NanoTimer timeUsingListStatus = new NanoTimer();\r\n        FileStatus[] fileStatuses = fs.listStatus(dir);\r\n        for (FileStatus fileStatus : fileStatuses) {\r\n            listUsingListStatus.add(fileStatus.getPath().toString());\r\n            sleep(eachFileProcessingTime);\r\n        }\r\n        timeUsingListStatus.end(\"listing %d files using listStatus() api with \" + \"batch size of %d including %dms of processing time\" + \" for each file\", numOfPutRequests, batchSize, eachFileProcessingTime);\r\n        Assertions.assertThat(listUsingListStatus).describedAs(\"Listing results using listStatus() must\" + \"match with original list of files\").hasSameElementsAs(originalListOfFiles).hasSize(numOfPutRequests);\r\n        NanoTimer timeUsingListStatusItr = new NanoTimer();\r\n        List<String> listUsingListStatusItr = new ArrayList<>();\r\n        RemoteIterator<FileStatus> lsItr = fs.listStatusIterator(dir);\r\n        RemoteIterators.foreach(lsItr, st -> {\r\n            listUsingListStatusItr.add(st.getPath().toString());\r\n            sleep(eachFileProcessingTime);\r\n        });\r\n        timeUsingListStatusItr.end(\"listing %d files using \" + \"listStatusIterator() api with batch size of %d \" + \"including %dms of processing time for each file\", numOfPutRequests, batchSize, eachFileProcessingTime);\r\n        Assertions.assertThat(listUsingListStatusItr).describedAs(\"Listing results using listStatusIterator() must\" + \"match with original list of files\").hasSameElementsAs(originalListOfFiles).hasSize(numOfPutRequests);\r\n        IOStatistics lsStats = retrieveIOStatistics(lsItr);\r\n        String statsReport = ioStatisticsToPrettyString(lsStats);\r\n        LOG.info(\"Listing Statistics: {}\", statsReport);\r\n        verifyStatisticCounterValue(lsStats, OBJECT_LIST_REQUEST, 1);\r\n        long continuations = lookupCounterStatistic(lsStats, OBJECT_CONTINUE_LIST_REQUEST);\r\n        int expectedContinuations = numOfPutRequests / batchSize - 1;\r\n        Assertions.assertThat(continuations).describedAs(\"%s in %s\", OBJECT_CONTINUE_LIST_REQUEST, statsReport).isEqualTo(expectedContinuations);\r\n        List<String> listUsingListLocatedStatus = new ArrayList<>();\r\n        RemoteIterator<LocatedFileStatus> it = fs.listLocatedStatus(dir);\r\n        RemoteIterators.foreach(it, st -> {\r\n            listUsingListLocatedStatus.add(st.getPath().toString());\r\n            sleep(eachFileProcessingTime);\r\n        });\r\n        final IOStatistics llsStats = retrieveIOStatistics(it);\r\n        LOG.info(\"Listing Statistics: {}\", ioStatisticsToPrettyString(llsStats));\r\n        verifyStatisticCounterValue(llsStats, OBJECT_CONTINUE_LIST_REQUEST, expectedContinuations);\r\n        Assertions.assertThat(listUsingListLocatedStatus).describedAs(\"Listing results using listLocatedStatus() must\" + \"match with original list of files\").hasSameElementsAs(originalListOfFiles);\r\n        fs.delete(dir, true);\r\n    } finally {\r\n        executorService.shutdown();\r\n        fs.delete(dir, true);\r\n        LOG.info(\"FS statistics {}\", ioStatisticsToPrettyString(fs.getIOStatistics()));\r\n        fs.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "sleep",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sleep(final int eachFileProcessingTime)\n{\r\n    try {\r\n        Thread.sleep(eachFileProcessingTime);\r\n    } catch (InterruptedException ignored) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getConfigurationWithConfiguredBatchSize",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration getConfigurationWithConfiguredBatchSize(int batchSize)\n{\r\n    Configuration conf = new Configuration(getFileSystem().getConf());\r\n    S3ATestUtils.disableFilesystemCaching(conf);\r\n    conf.setInt(Constants.MAX_PAGING_KEYS, batchSize);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testTimeToStatEmptyDirectory",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testTimeToStatEmptyDirectory() throws Throwable\n{\r\n    describe(\"Time to stat an empty directory\");\r\n    Path path = path(\"empty\");\r\n    getFileSystem().mkdirs(path);\r\n    timeToStatPath(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testTimeToStatNonEmptyDirectory",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testTimeToStatNonEmptyDirectory() throws Throwable\n{\r\n    describe(\"Time to stat a non-empty directory\");\r\n    Path path = path(\"dir\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    fs.mkdirs(path);\r\n    touch(fs, new Path(path, \"file\"));\r\n    timeToStatPath(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testTimeToStatFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testTimeToStatFile() throws Throwable\n{\r\n    describe(\"Time to stat a simple file\");\r\n    Path path = path(\"file\");\r\n    touch(getFileSystem(), path);\r\n    timeToStatPath(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testTimeToStatRoot",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testTimeToStatRoot() throws Throwable\n{\r\n    describe(\"Time to stat the root path\");\r\n    timeToStatPath(new Path(\"/\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "timeToStatPath",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void timeToStatPath(Path path) throws IOException\n{\r\n    describe(\"Timing getFileStatus(\\\"%s\\\")\", path);\r\n    S3AFileSystem fs = getFileSystem();\r\n    MetricDiff metadataRequests = new MetricDiff(fs, Statistic.OBJECT_METADATA_REQUESTS);\r\n    MetricDiff listRequests = new MetricDiff(fs, Statistic.OBJECT_LIST_REQUEST);\r\n    long attempts = getOperationCount();\r\n    NanoTimer timer = new NanoTimer();\r\n    for (long l = 0; l < attempts; l++) {\r\n        fs.getFileStatus(path);\r\n    }\r\n    timer.end(\"Time to execute %d getFileStatusCalls\", attempts);\r\n    LOG.info(\"Time per call: {}\", toHuman(timer.nanosPerOperation(attempts)));\r\n    LOG.info(\"metadata: {}\", metadataRequests);\r\n    LOG.info(\"metadata per operation {}\", metadataRequests.diff() / attempts);\r\n    LOG.info(\"listObjects: {}\", listRequests);\r\n    LOG.info(\"listObjects: per operation {}\", listRequests.diff() / attempts);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "data",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> data()\n{\r\n    return Arrays.asList(new Object[][] { { DirectoryPolicy.MarkerPolicy.Delete, FAIL_IF_INVOKED, false, false }, { DirectoryPolicy.MarkerPolicy.Keep, FAIL_IF_INVOKED, true, true }, { DirectoryPolicy.MarkerPolicy.Authoritative, AUTH_PATH_ONLY, false, true } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DirectoryPolicy newPolicy(DirectoryPolicy.MarkerPolicy markerPolicy, Predicate<Path> authoritativeness)\n{\r\n    return new DirectoryPolicyImpl(markerPolicy, authoritativeness);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "assertMarkerRetention",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertMarkerRetention(Path path, boolean retain)\n{\r\n    Assertions.assertThat(directoryPolicy.keepDirectoryMarkers(path)).describedAs(\"Retention of path %s by %s\", path, directoryPolicy).isEqualTo(retain);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "assertPathCapability",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertPathCapability(Path path, String capability, boolean outcome)\n{\r\n    Assertions.assertThat(directoryPolicy).describedAs(\"%s support for capability %s by path %s\" + \" expected as %s\", directoryPolicy, capability, path, outcome).matches(p -> p.hasPathCapability(path, capability) == outcome, \"pathCapability\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testNonAuthPath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testNonAuthPath() throws Throwable\n{\r\n    assertMarkerRetention(nonAuthPath, expectNonAuthDelete);\r\n    assertPathCapability(nonAuthPath, STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_DELETE, !expectNonAuthDelete);\r\n    assertPathCapability(nonAuthPath, STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_KEEP, expectNonAuthDelete);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testAuthPath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testAuthPath() throws Throwable\n{\r\n    assertMarkerRetention(authPath, expectAuthDelete);\r\n    assertPathCapability(authPath, STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_DELETE, !expectAuthDelete);\r\n    assertPathCapability(authPath, STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_KEEP, expectAuthDelete);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testDeepAuthPath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testDeepAuthPath() throws Throwable\n{\r\n    assertMarkerRetention(deepAuth, expectAuthDelete);\r\n    assertPathCapability(deepAuth, STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_DELETE, !expectAuthDelete);\r\n    assertPathCapability(deepAuth, STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_KEEP, expectAuthDelete);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return S3A_TEST_TIMEOUT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "testRenameDirIntoExistingDir",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRenameDirIntoExistingDir() throws Throwable\n{\r\n    describe(\"S3A rename into an existing directory returns false\");\r\n    FileSystem fs = getFileSystem();\r\n    String sourceSubdir = \"source\";\r\n    Path srcDir = path(sourceSubdir);\r\n    Path srcFilePath = new Path(srcDir, \"source-256.txt\");\r\n    byte[] srcDataset = dataset(256, 'a', 'z');\r\n    writeDataset(fs, srcFilePath, srcDataset, srcDataset.length, 1024, false);\r\n    Path destDir = path(\"dest\");\r\n    Path destFilePath = new Path(destDir, \"dest-512.txt\");\r\n    byte[] destDataset = dataset(512, 'A', 'Z');\r\n    writeDataset(fs, destFilePath, destDataset, destDataset.length, 1024, false);\r\n    assertIsFile(destFilePath);\r\n    boolean rename = fs.rename(srcDir, destDir);\r\n    assertFalse(\"s3a doesn't support rename to non-empty directory\", rename);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "testRenamePopulatesFileAncestors2",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testRenamePopulatesFileAncestors2() throws Exception\n{\r\n    final S3AFileSystem fs = (S3AFileSystem) getFileSystem();\r\n    Path base = path(\"testRenamePopulatesFileAncestors2\");\r\n    final Path src = new Path(base, \"src\");\r\n    Path dest = new Path(base, \"dest\");\r\n    fs.mkdirs(src);\r\n    final String nestedFile = \"/dir1/dir2/dir3/fileA\";\r\n    int filesize = 16 * 1024;\r\n    byte[] srcDataset = dataset(filesize, 'a', 'z');\r\n    Path srcFile = path(src + nestedFile);\r\n    Path destFile = path(dest + nestedFile);\r\n    writeDataset(fs, srcFile, srcDataset, srcDataset.length, 1024, false);\r\n    S3ATestUtils.MetricDiff fileCopyDiff = new S3ATestUtils.MetricDiff(fs, Statistic.FILES_COPIED);\r\n    S3ATestUtils.MetricDiff fileCopyBytes = new S3ATestUtils.MetricDiff(fs, Statistic.FILES_COPIED_BYTES);\r\n    rename(src, dest);\r\n    describe(\"Rename has completed, examining data under \" + base);\r\n    fileCopyDiff.assertDiffEquals(\"Number of files copied\", 1);\r\n    fileCopyBytes.assertDiffEquals(\"Number of bytes copied\", filesize);\r\n    S3ATestUtils.lsR(fs, base, true);\r\n    verifyFileContents(fs, destFile, srcDataset);\r\n    describe(\"validating results\");\r\n    validateAncestorsMoved(src, dest, nestedFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "testRenameFileUnderFileSubdir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRenameFileUnderFileSubdir() throws Exception\n{\r\n    skip(\"Rename deep paths under files is allowed\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    S3ATestUtils.disableFilesystemCaching(conf);\r\n    conf.set(Constants.S3_ENCRYPTION_KEY, \"\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getSSEAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AEncryptionMethods getSSEAlgorithm()\n{\r\n    return S3AEncryptionMethods.SSE_S3;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    auditor = (LoggingAuditor) getManager().getAuditor();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "createConfig",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfig()\n{\r\n    return loggingAuditConfig();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testToStringRobustness",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testToStringRobustness() throws Throwable\n{\r\n    LOG.info(getManager().toString());\r\n    LOG.info(auditor.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testLoggingSpan",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testLoggingSpan() throws Throwable\n{\r\n    long executionCount = 0;\r\n    long failureCount = 0;\r\n    AuditSpan span = span();\r\n    assertActiveSpan(span);\r\n    verifyAuditExecutionCount(0);\r\n    head();\r\n    verifyAuditExecutionCount(++executionCount);\r\n    span.deactivate();\r\n    verifyAuditFailureCount(failureCount);\r\n    assertHeadUnaudited();\r\n    verifyAuditFailureCount(++failureCount);\r\n    verifyAuditExecutionCount(++executionCount);\r\n    span.activate();\r\n    head();\r\n    verifyAuditExecutionCount(++executionCount);\r\n    span.activate();\r\n    assertActiveSpan(span);\r\n    span.close();\r\n    assertHeadUnaudited();\r\n    verifyAuditFailureCount(++failureCount);\r\n    verifyAuditExecutionCount(++executionCount);\r\n    span.deactivate();\r\n    span.deactivate();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testCopyOutsideSpanAllowed",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCopyOutsideSpanAllowed() throws Throwable\n{\r\n    getManager().beforeExecution(new CopyPartRequest());\r\n    getManager().beforeExecution(new CompleteMultipartUploadRequest());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testTransferStateListenerOutsideSpan",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testTransferStateListenerOutsideSpan() throws Throwable\n{\r\n    TransferStateChangeListener listener = getManager().createStateChangeListener();\r\n    listener.transferStateChanged(null, null);\r\n    assertHeadUnaudited();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testTransferStateListenerInSpan",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testTransferStateListenerInSpan() throws Throwable\n{\r\n    assertHeadUnaudited();\r\n    AuditSpan span = span();\r\n    TransferStateChangeListener listener = getManager().createStateChangeListener();\r\n    span.deactivate();\r\n    assertHeadUnaudited();\r\n    listener.transferStateChanged(null, null);\r\n    assertActiveSpan(span);\r\n    head();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testUnbondedSpanWillNotDeactivate",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testUnbondedSpanWillNotDeactivate() throws Throwable\n{\r\n    AuditSpan span = activeSpan();\r\n    assertUnbondedSpan(span);\r\n    span.deactivate();\r\n    assertActiveSpan(span);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "testSpanIdsAreDifferent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSpanIdsAreDifferent() throws Throwable\n{\r\n    AuditSpan s1 = span();\r\n    AuditSpan s2 = span();\r\n    assertThat(s1.getSpanId()).doesNotMatch(s2.getSpanId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testSplitPathEmpty",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSplitPathEmpty() throws Throwable\n{\r\n    intercept(IllegalArgumentException.class, () -> splitPathToElements(new Path(\"\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testSplitPathDoubleBackslash",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSplitPathDoubleBackslash()\n{\r\n    assertPathSplits(\"//\", EMPTY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testSplitRootPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSplitRootPath()\n{\r\n    assertPathSplits(\"/\", EMPTY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testSplitBasic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSplitBasic()\n{\r\n    assertPathSplits(\"/a/b/c\", new String[] { \"a\", \"b\", \"c\" });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testSplitTrailingSlash",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSplitTrailingSlash()\n{\r\n    assertPathSplits(\"/a/b/c/\", new String[] { \"a\", \"b\", \"c\" });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testSplitShortPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSplitShortPath()\n{\r\n    assertPathSplits(\"/a\", new String[] { \"a\" });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testSplitShortPathTrailingSlash",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSplitShortPathTrailingSlash()\n{\r\n    assertPathSplits(\"/a/\", new String[] { \"a\" });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testParentsMagicRoot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testParentsMagicRoot()\n{\r\n    assertParents(EMPTY, MAGIC_AT_ROOT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testChildrenMagicRoot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testChildrenMagicRoot()\n{\r\n    assertChildren(EMPTY, MAGIC_AT_ROOT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testParentsMagicRootWithChild",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testParentsMagicRootWithChild()\n{\r\n    assertParents(EMPTY, MAGIC_AT_ROOT_WITH_CHILD);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testChildMagicRootWithChild",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testChildMagicRootWithChild()\n{\r\n    assertChildren(a(\"child\"), MAGIC_AT_ROOT_WITH_CHILD);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testChildrenMagicWithoutChild",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testChildrenMagicWithoutChild()\n{\r\n    assertChildren(EMPTY, MAGIC_AT_WITHOUT_CHILD);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testChildMagicWithChild",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testChildMagicWithChild()\n{\r\n    assertChildren(a(\"child\"), MAGIC_WITH_CHILD);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testParentMagicWithChild",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testParentMagicWithChild()\n{\r\n    assertParents(a(\"parent\"), MAGIC_WITH_CHILD);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testParentDeepMagic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testParentDeepMagic()\n{\r\n    assertParents(a(\"parent1\", \"parent2\"), DEEP_MAGIC);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testChildrenDeepMagic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testChildrenDeepMagic()\n{\r\n    assertChildren(a(\"child1\", \"child2\"), DEEP_MAGIC);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testLastElementEmpty",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLastElementEmpty() throws Throwable\n{\r\n    intercept(IllegalArgumentException.class, () -> lastElement(new ArrayList<>(0)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testLastElementSingle",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLastElementSingle()\n{\r\n    assertEquals(\"first\", lastElement(l(\"first\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testLastElementDouble",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLastElementDouble()\n{\r\n    assertEquals(\"2\", lastElement(l(\"first\", \"2\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFinalDestinationNoMagic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFinalDestinationNoMagic()\n{\r\n    assertEquals(l(\"first\", \"2\"), finalDestination(l(\"first\", \"2\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFinalDestinationMagic1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFinalDestinationMagic1()\n{\r\n    assertEquals(l(\"first\", \"2\"), finalDestination(l(\"first\", MAGIC, \"2\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFinalDestinationMagic2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFinalDestinationMagic2()\n{\r\n    assertEquals(l(\"first\", \"3.txt\"), finalDestination(l(\"first\", MAGIC, \"2\", \"3.txt\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFinalDestinationRootMagic2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFinalDestinationRootMagic2()\n{\r\n    assertEquals(l(\"3.txt\"), finalDestination(l(MAGIC, \"2\", \"3.txt\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFinalDestinationMagicNoChild",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFinalDestinationMagicNoChild()\n{\r\n    finalDestination(l(MAGIC));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFinalDestinationBaseDirectChild",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFinalDestinationBaseDirectChild()\n{\r\n    finalDestination(l(MAGIC, BASE, \"3.txt\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFinalDestinationBaseNoChild",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFinalDestinationBaseNoChild()\n{\r\n    assertEquals(l(), finalDestination(l(MAGIC, BASE)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFinalDestinationBaseSubdirsChild",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFinalDestinationBaseSubdirsChild()\n{\r\n    assertEquals(l(\"2\", \"3.txt\"), finalDestination(l(MAGIC, \"4\", BASE, \"2\", \"3.txt\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testFinalDestinationIgnoresBaseBeforeMagic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFinalDestinationIgnoresBaseBeforeMagic()\n{\r\n    assertEquals(l(BASE, \"home\", \"3.txt\"), finalDestination(l(BASE, \"home\", MAGIC, \"2\", \"3.txt\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "a",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] a(String... str)\n{\r\n    return str;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "l",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> l(String... str)\n{\r\n    return Arrays.asList(str);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "list",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> list(String... args)\n{\r\n    return Lists.newArrayList(args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "assertParents",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertParents(String[] expected, List<String> elements)\n{\r\n    assertListEquals(expected, magicPathParents(elements));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "assertChildren",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertChildren(String[] expected, List<String> elements)\n{\r\n    assertListEquals(expected, magicPathChildren(elements));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "assertPathSplits",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertPathSplits(String pathString, String[] expected)\n{\r\n    Path path = new Path(pathString);\r\n    assertArrayEquals(\"From path \" + path, expected, splitPathToElements(path).toArray());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "assertListEquals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertListEquals(String[] expected, List<String> actual)\n{\r\n    assertArrayEquals(expected, actual.toArray());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    S3ATestUtils.disableFilesystemCaching(conf);\r\n    conf.setInt(Constants.MAX_PAGING_KEYS, LIST_BATCH_SIZE);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testListMultipartUploads",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testListMultipartUploads() throws Exception\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Set<MultipartTestUtils.IdKey> keySet = new HashSet<>();\r\n    try (AuditSpan span = span()) {\r\n        for (int i = 0; i < NUM_KEYS; i++) {\r\n            Path filePath = getPartFilename(i);\r\n            String key = fs.pathToKey(filePath);\r\n            describe(\"creating upload part with key %s\", key);\r\n            MultipartTestUtils.IdKey idKey = MultipartTestUtils.createPartUpload(fs, key, UPLOAD_LEN, 1);\r\n            keySet.add(idKey);\r\n        }\r\n        describe(\"Verifying upload list by prefix\");\r\n        MultipartUtils.UploadIterator uploads = fs.listUploads(getPartPrefix(fs));\r\n        assertUploadsPresent(uploads, keySet);\r\n        describe(\"Verifying list all uploads\");\r\n        uploads = fs.listUploads(null);\r\n        assertUploadsPresent(uploads, keySet);\r\n    } finally {\r\n        MultipartTestUtils.cleanupParts(fs, keySet);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertUploadsPresent",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void assertUploadsPresent(MultipartUtils.UploadIterator list, Set<MultipartTestUtils.IdKey> ourUploads) throws IOException\n{\r\n    Set<MultipartTestUtils.IdKey> uploads = new HashSet<>(ourUploads);\r\n    while (list.hasNext()) {\r\n        MultipartTestUtils.IdKey listing = toIdKey(list.next());\r\n        if (uploads.contains(listing)) {\r\n            LOG.debug(\"Matched: {},{}\", listing.getKey(), listing.getUploadId());\r\n            uploads.remove(listing);\r\n        } else {\r\n            LOG.debug(\"Not our upload {},{}\", listing.getKey(), listing.getUploadId());\r\n        }\r\n    }\r\n    assertTrue(\"Not all our uploads were listed\", uploads.isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toIdKey",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MultipartTestUtils.IdKey toIdKey(MultipartUpload mu)\n{\r\n    return new MultipartTestUtils.IdKey(mu.getKey(), mu.getUploadId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getPartFilename",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getPartFilename(int index) throws IOException\n{\r\n    return path(String.format(\"%s-%d\", PART_FILENAME_BASE, index));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getPartPrefix",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getPartPrefix(S3AFileSystem fs) throws IOException\n{\r\n    return fs.pathToKey(path(\"blah\").getParent());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void add(ContractTestUtils.NanoTimer duration)\n{\r\n    add(duration.elapsedTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void add(long x)\n{\r\n    count++;\r\n    sum += x;\r\n    double delta = x - mean;\r\n    mean += delta / count;\r\n    double delta2 = x - mean;\r\n    m2 += delta * delta2;\r\n    if (min < 0 || x < min) {\r\n        min = x;\r\n    }\r\n    if (x > max) {\r\n        max = x;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reset()\n{\r\n    count = 0;\r\n    sum = 0;\r\n    sum = 0;\r\n    min = -1;\r\n    max = 0;\r\n    mean = 0;\r\n    m2 = 0;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getCount()\n{\r\n    return count;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getSum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getSum()\n{\r\n    return sum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getArithmeticMean",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getArithmeticMean()\n{\r\n    return mean;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getVariance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getVariance()\n{\r\n    return count > 0 ? (m2 / (count - 1)) : Double.NaN;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getDeviation",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "double getDeviation()\n{\r\n    double variance = getVariance();\r\n    return (!Double.isNaN(variance) && variance > 0) ? Math.sqrt(variance) : 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "toSeconds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double toSeconds(double nano)\n{\r\n    return nano / ONE_NS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return String.format(\"%s count=%d total=%.3fs mean=%.3fs stddev=%.3fs min=%.3fs max=%.3fs\", operation, count, toSeconds(sum), toSeconds(mean), getDeviation() / ONE_NS, toSeconds(min), toSeconds(max));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getOperation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getOperation()\n{\r\n    return operation;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getMin",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getMin()\n{\r\n    return min;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getMax",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getMax()\n{\r\n    return max;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getMean",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getMean()\n{\r\n    return mean;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "setup",
  "errType" : [ "AccessDeniedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    try {\r\n        super.setup();\r\n        skipIfEncryptionTestsDisabled(getConfiguration());\r\n    } catch (AccessDeniedException e) {\r\n        skip(\"Bucket does not allow \" + S3AEncryptionMethods.SSE_C + \" encryption method\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "createScaleConfiguration",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Configuration createScaleConfiguration()\n{\r\n    Configuration conf = super.createScaleConfiguration();\r\n    removeBaseAndBucketOverrides(conf, S3_ENCRYPTION_KEY, S3_ENCRYPTION_ALGORITHM, SERVER_SIDE_ENCRYPTION_ALGORITHM, SERVER_SIDE_ENCRYPTION_KEY);\r\n    S3ATestUtils.disableFilesystemCaching(conf);\r\n    conf.set(Constants.S3_ENCRYPTION_ALGORITHM, getSSEAlgorithm().getMethod());\r\n    conf.set(Constants.S3_ENCRYPTION_KEY, KEY_1);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getSSEAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AEncryptionMethods getSSEAlgorithm()\n{\r\n    return S3AEncryptionMethods.SSE_C;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\fileContext",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws IOException, Exception\n{\r\n    Configuration conf = new Configuration();\r\n    fc = S3ATestUtils.createTestFileContext(conf);\r\n    super.setUp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    bindCommitter(conf, CommitConstants.S3A_COMMITTER_FACTORY, CommitConstants.COMMITTER_NAME_MAGIC);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    FileSystem.closeAll();\r\n    super.setup();\r\n    verifyIsMagicCommitFS(getFileSystem());\r\n    progress = new ProgressCounter();\r\n    progress.assertCount(\"progress\", 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testCreateTrackerNormalPath",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCreateTrackerNormalPath() throws Throwable\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    MagicCommitIntegration integration = new MagicCommitIntegration(fs, true);\r\n    String filename = \"notdelayed.txt\";\r\n    Path destFile = methodPath(filename);\r\n    String origKey = fs.pathToKey(destFile);\r\n    PutTracker tracker = integration.createTracker(destFile, origKey);\r\n    assertFalse(\"wrong type: \" + tracker + \" for \" + destFile, tracker instanceof MagicCommitTracker);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testCreateTrackerMagicPath",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testCreateTrackerMagicPath() throws Throwable\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    MagicCommitIntegration integration = new MagicCommitIntegration(fs, true);\r\n    String filename = \"delayed.txt\";\r\n    Path destFile = methodPath(filename);\r\n    String origKey = fs.pathToKey(destFile);\r\n    Path pendingPath = makeMagic(destFile);\r\n    verifyIsMagicCommitPath(fs, pendingPath);\r\n    String pendingPathKey = fs.pathToKey(pendingPath);\r\n    assertTrue(\"wrong path of \" + pendingPathKey, pendingPathKey.endsWith(filename));\r\n    final List<String> elements = splitPathToElements(pendingPath);\r\n    assertEquals(\"splitPathToElements()\", filename, lastElement(elements));\r\n    List<String> finalDestination = finalDestination(elements);\r\n    assertEquals(\"finalDestination()\", filename, lastElement(finalDestination));\r\n    final String destKey = elementsToKey(finalDestination);\r\n    assertEquals(\"destination key\", origKey, destKey);\r\n    PutTracker tracker = integration.createTracker(pendingPath, pendingPathKey);\r\n    assertTrue(\"wrong type: \" + tracker + \" for \" + pendingPathKey, tracker instanceof MagicCommitTracker);\r\n    assertEquals(\"tracker destination key\", origKey, tracker.getDestKey());\r\n    Path pendingSuffixedPath = new Path(pendingPath, \"part-0000\" + PENDING_SUFFIX);\r\n    assertFalse(\"still a delayed complete path \" + pendingSuffixedPath, fs.isMagicCommitPath(pendingSuffixedPath));\r\n    Path pendingSet = new Path(pendingPath, \"part-0000\" + PENDINGSET_SUFFIX);\r\n    assertFalse(\"still a delayed complete path \" + pendingSet, fs.isMagicCommitPath(pendingSet));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testCreateAbortEmptyFile",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testCreateAbortEmptyFile() throws Throwable\n{\r\n    describe(\"create then abort an empty file; throttled\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    String filename = \"empty-abort.txt\";\r\n    Path destFile = methodPath(filename);\r\n    Path pendingFilePath = makeMagic(destFile);\r\n    touch(fs, pendingFilePath);\r\n    validateIntermediateAndFinalPaths(pendingFilePath, destFile);\r\n    Path pendingDataPath = validatePendingCommitData(filename, pendingFilePath);\r\n    CommitOperations actions = newCommitOperations();\r\n    LOG.info(\"Abort call\");\r\n    actions.abortAllSinglePendingCommits(pendingDataPath.getParent(), true).maybeRethrow();\r\n    assertPathDoesNotExist(\"pending file not deleted\", pendingDataPath);\r\n    assertPathDoesNotExist(\"dest file was created\", destFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "newCommitOperations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CommitOperations newCommitOperations() throws IOException\n{\r\n    return new CommitOperations(getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "makeMagic",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path makeMagic(Path destFile)\n{\r\n    return new Path(destFile.getParent(), MAGIC + '/' + destFile.getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testCommitEmptyFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCommitEmptyFile() throws Throwable\n{\r\n    describe(\"create then commit an empty magic file\");\r\n    createCommitAndVerify(\"empty-commit.txt\", new byte[0]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testCommitSmallFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCommitSmallFile() throws Throwable\n{\r\n    describe(\"create then commit a small magic file\");\r\n    createCommitAndVerify(\"small-commit.txt\", DATASET);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testAbortNonexistentDir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testAbortNonexistentDir() throws Throwable\n{\r\n    describe(\"Attempt to abort a directory that does not exist\");\r\n    Path destFile = methodPath(\"testAbortNonexistentPath\");\r\n    newCommitOperations().abortAllSinglePendingCommits(destFile, true).maybeRethrow();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testCommitterFactoryDefault",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCommitterFactoryDefault() throws Throwable\n{\r\n    Configuration conf = new Configuration();\r\n    Path dest = methodPath();\r\n    conf.set(COMMITTER_FACTORY_CLASS, MagicS3GuardCommitterFactory.CLASSNAME);\r\n    PathOutputCommitterFactory factory = getCommitterFactory(dest, conf);\r\n    PathOutputCommitter committer = factory.createOutputCommitter(methodPath(), new TaskAttemptContextImpl(getConfiguration(), new TaskAttemptID(new TaskID(), 1)));\r\n    assertEquals(\"Wrong committer\", MagicS3GuardCommitter.class, committer.getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testCommitterFactorySchema",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCommitterFactorySchema() throws Throwable\n{\r\n    Configuration conf = new Configuration();\r\n    Path dest = methodPath();\r\n    conf.set(S3A_FACTORY_KEY, MagicS3GuardCommitterFactory.CLASSNAME);\r\n    PathOutputCommitterFactory factory = getCommitterFactory(dest, conf);\r\n    MagicS3GuardCommitter s3a = (MagicS3GuardCommitter) factory.createOutputCommitter(methodPath(), new TaskAttemptContextImpl(getConfiguration(), new TaskAttemptID(new TaskID(), 1)));\r\n    assertNotNull(s3a);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testBaseRelativePath",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testBaseRelativePath() throws Throwable\n{\r\n    describe(\"Test creating file with a __base marker and verify that it ends\" + \" up in where expected\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path destDir = methodPath(\"testBaseRelativePath\");\r\n    fs.delete(destDir, true);\r\n    Path pendingBaseDir = new Path(destDir, MAGIC + \"/child/\" + BASE);\r\n    String child = \"subdir/child.txt\";\r\n    Path pendingChildPath = new Path(pendingBaseDir, child);\r\n    Path expectedDestPath = new Path(destDir, child);\r\n    assertPathDoesNotExist(\"dest file was found before upload\", expectedDestPath);\r\n    createFile(fs, pendingChildPath, true, DATASET);\r\n    commit(\"child.txt\", pendingChildPath, expectedDestPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testMarkerFileRename",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testMarkerFileRename() throws Exception\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path destFile = methodPath();\r\n    Path destDir = destFile.getParent();\r\n    fs.delete(destDir, true);\r\n    Path magicDest = makeMagic(destFile);\r\n    Path magicDir = magicDest.getParent();\r\n    fs.mkdirs(magicDir);\r\n    try (FSDataOutputStream stream = fs.createFile(magicDest).overwrite(true).recursive().build()) {\r\n        assertIsMagicStream(stream);\r\n        stream.write(DATASET);\r\n    }\r\n    Path magic2 = new Path(magicDir, \"magic2\");\r\n    fs.rename(magicDest, magic2);\r\n    Assertions.assertThat(extractMagicFileLength(fs, magic2)).describedAs(\"XAttribute \" + XA_MAGIC_MARKER + \" of \" + magic2).isEmpty();\r\n    Assertions.assertThat(newCommitOperations().abortPendingUploadsUnderPath(destDir)).describedAs(\"Aborting all pending uploads under %s\", destDir).isGreaterThanOrEqualTo(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "assertIsMagicStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertIsMagicStream(final FSDataOutputStream stream)\n{\r\n    Assertions.assertThat(stream.hasCapability(STREAM_CAPABILITY_MAGIC_OUTPUT)).describedAs(\"Stream capability %s in stream %s\", STREAM_CAPABILITY_MAGIC_OUTPUT, stream).isTrue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "createCommitAndVerify",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void createCommitAndVerify(String filename, byte[] data) throws Exception\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path destFile = methodPath(filename);\r\n    fs.delete(destFile.getParent(), true);\r\n    Path magicDest = makeMagic(destFile);\r\n    assertPathDoesNotExist(\"Magic file should not exist\", magicDest);\r\n    long dataSize = data != null ? data.length : 0;\r\n    try (FSDataOutputStream stream = fs.create(magicDest, true)) {\r\n        assertIsMagicStream(stream);\r\n        if (dataSize > 0) {\r\n            stream.write(data);\r\n        }\r\n        stream.close();\r\n    }\r\n    FileStatus status = fs.getFileStatus(magicDest);\r\n    assertEquals(\"Magic marker file is not zero bytes: \" + status, 0, 0);\r\n    Assertions.assertThat(extractMagicFileLength(fs, magicDest)).describedAs(\"XAttribute \" + XA_MAGIC_MARKER + \" of \" + magicDest).isNotEmpty().hasValue(dataSize);\r\n    commit(filename, destFile);\r\n    verifyFileContents(fs, destFile, data);\r\n    Assertions.assertThat(extractMagicFileLength(fs, destFile)).describedAs(\"XAttribute \" + XA_MAGIC_MARKER + \" of \" + destFile).isEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "commit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commit(String filename, Path destFile) throws Exception\n{\r\n    commit(filename, makeMagic(destFile), destFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "commit",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void commit(String filename, Path magicFile, Path destFile) throws IOException\n{\r\n    validateIntermediateAndFinalPaths(magicFile, destFile);\r\n    SinglePendingCommit commit = SinglePendingCommit.load(getFileSystem(), validatePendingCommitData(filename, magicFile));\r\n    commitOrFail(destFile, commit, newCommitOperations());\r\n    verifyCommitExists(commit);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "commitOrFail",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void commitOrFail(final Path destFile, final SinglePendingCommit commit, final CommitOperations actions) throws IOException\n{\r\n    try (CommitOperations.CommitContext commitContext = actions.initiateCommitOperation(destFile)) {\r\n        commitContext.commitOrFail(commit);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "validateIntermediateAndFinalPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validateIntermediateAndFinalPaths(Path magicFilePath, Path destFile) throws IOException\n{\r\n    assertPathDoesNotExist(\"dest file was created\", destFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "verifyCommitExists",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyCommitExists(SinglePendingCommit commit) throws FileNotFoundException, ValidationFailure, IOException\n{\r\n    commit.validate();\r\n    Path path = getFileSystem().keyToQualifiedPath(commit.getDestinationKey());\r\n    FileStatus status = getFileSystem().getFileStatus(path);\r\n    LOG.debug(\"Destination entry: {}\", status);\r\n    if (!status.isFile()) {\r\n        throw new PathCommitException(path, \"Not a file: \" + status);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "validatePendingCommitData",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "Path validatePendingCommitData(String filename, Path magicFile) throws IOException\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path pendingDataPath = new Path(magicFile.getParent(), filename + PENDING_SUFFIX);\r\n    FileStatus fileStatus = verifyPathExists(fs, \"no pending file\", pendingDataPath);\r\n    assertTrue(\"No data in \" + fileStatus, fileStatus.getLen() > 0);\r\n    String data = read(fs, pendingDataPath);\r\n    LOG.info(\"Contents of {}: \\n{}\", pendingDataPath, data);\r\n    SinglePendingCommit persisted = SinglePendingCommit.serializer().load(fs, pendingDataPath);\r\n    persisted.validate();\r\n    assertTrue(\"created timestamp wrong in \" + persisted, persisted.getCreated() > 0);\r\n    assertTrue(\"saved timestamp wrong in \" + persisted, persisted.getSaved() > 0);\r\n    List<String> etags = persisted.getEtags();\r\n    assertEquals(\"etag list \" + persisted, 1, etags.size());\r\n    List<PartETag> partList = CommitOperations.toPartEtags(etags);\r\n    assertEquals(\"part list \" + persisted, 1, partList.size());\r\n    return pendingDataPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "methodPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path methodPath(String filename) throws IOException\n{\r\n    return new Path(methodPath(), filename);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "methodPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path methodPath() throws IOException\n{\r\n    return path(getMethodName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testUploadEmptyFile",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testUploadEmptyFile() throws Throwable\n{\r\n    File tempFile = File.createTempFile(\"commit\", \".txt\");\r\n    CommitOperations actions = newCommitOperations();\r\n    Path dest = methodPath(\"testUploadEmptyFile\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    fs.delete(dest, false);\r\n    SinglePendingCommit pendingCommit = actions.uploadFileToPendingCommit(tempFile, dest, null, DEFAULT_MULTIPART_SIZE, progress);\r\n    assertPathDoesNotExist(\"pending commit\", dest);\r\n    commitOrFail(dest, pendingCommit, actions);\r\n    FileStatus status = verifyPathExists(fs, \"uploaded file commit\", dest);\r\n    progress.assertCount(\"Progress counter should be 1.\", 1);\r\n    assertEquals(\"File length in \" + status, 0, status.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testUploadSmallFile",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testUploadSmallFile() throws Throwable\n{\r\n    File tempFile = File.createTempFile(\"commit\", \".txt\");\r\n    String text = \"hello, world\";\r\n    FileUtils.write(tempFile, text, \"UTF-8\");\r\n    CommitOperations actions = newCommitOperations();\r\n    Path dest = methodPath(\"testUploadSmallFile\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    fs.delete(dest, true);\r\n    assertPathDoesNotExist(\"test setup\", dest);\r\n    SinglePendingCommit pendingCommit = actions.uploadFileToPendingCommit(tempFile, dest, null, DEFAULT_MULTIPART_SIZE, progress);\r\n    assertPathDoesNotExist(\"pending commit\", dest);\r\n    LOG.debug(\"Postcommit validation\");\r\n    commitOrFail(dest, pendingCommit, actions);\r\n    String s = readUTF8(fs, dest, -1);\r\n    assertEquals(text, s);\r\n    progress.assertCount(\"Progress counter should be 1.\", 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testUploadMissingFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testUploadMissingFile() throws Throwable\n{\r\n    File tempFile = File.createTempFile(\"commit\", \".txt\");\r\n    tempFile.delete();\r\n    CommitOperations actions = newCommitOperations();\r\n    Path dest = methodPath(\"testUploadMissingile\");\r\n    actions.uploadFileToPendingCommit(tempFile, dest, null, DEFAULT_MULTIPART_SIZE, progress);\r\n    progress.assertCount(\"Progress counter should be 1.\", 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testRevertCommit",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRevertCommit() throws Throwable\n{\r\n    Path destFile = methodPath(\"part-0000\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    touch(fs, destFile);\r\n    CommitOperations actions = newCommitOperations();\r\n    SinglePendingCommit commit = new SinglePendingCommit();\r\n    commit.setDestinationKey(fs.pathToKey(destFile));\r\n    actions.revertCommit(commit);\r\n    assertPathExists(\"parent of reverted commit\", destFile.getParent());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testRevertMissingCommit",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRevertMissingCommit() throws Throwable\n{\r\n    Path destFile = methodPath(\"part-0000\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    fs.delete(destFile, false);\r\n    CommitOperations actions = newCommitOperations();\r\n    SinglePendingCommit commit = new SinglePendingCommit();\r\n    commit.setDestinationKey(fs.pathToKey(destFile));\r\n    actions.revertCommit(commit);\r\n    assertPathExists(\"parent of reverted (nonexistent) commit\", destFile.getParent());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testWriteNormalStream",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testWriteNormalStream() throws Throwable\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    assumeMagicCommitEnabled(fs);\r\n    Path destFile = path(\"normal\");\r\n    try (FSDataOutputStream out = fs.create(destFile, true)) {\r\n        out.writeChars(\"data\");\r\n        assertFalse(\"stream has magic output: \" + out, out.hasCapability(STREAM_CAPABILITY_MAGIC_OUTPUT));\r\n        out.close();\r\n    }\r\n    FileStatus status = fs.getFileStatus(destFile);\r\n    assertTrue(\"Empty marker file: \" + status, status.getLen() > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "testBulkCommitFiles",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testBulkCommitFiles() throws Throwable\n{\r\n    describe(\"verify bulk commit\");\r\n    File localFile = File.createTempFile(\"commit\", \".txt\");\r\n    CommitOperations actions = newCommitOperations();\r\n    Path destDir = methodPath(\"out\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    fs.delete(destDir, false);\r\n    Path destFile1 = new Path(destDir, \"file1\");\r\n    Path subdir = new Path(destDir, \"subdir\");\r\n    Path destFile2 = new Path(subdir, \"file2\");\r\n    Path destFile3 = new Path(subdir, \"file3 with space\");\r\n    List<Path> destinations = Lists.newArrayList(destFile1, destFile2, destFile3);\r\n    List<SinglePendingCommit> commits = new ArrayList<>(3);\r\n    for (Path destination : destinations) {\r\n        SinglePendingCommit commit1 = actions.uploadFileToPendingCommit(localFile, destination, null, DEFAULT_MULTIPART_SIZE, progress);\r\n        commits.add(commit1);\r\n    }\r\n    assertPathDoesNotExist(\"destination dir\", destDir);\r\n    assertPathDoesNotExist(\"subdirectory\", subdir);\r\n    LOG.info(\"Initiating commit operations\");\r\n    try (CommitOperations.CommitContext commitContext = actions.initiateCommitOperation(destDir)) {\r\n        LOG.info(\"Commit #1\");\r\n        commitContext.commitOrFail(commits.get(0));\r\n        final String firstCommitContextString = commitContext.toString();\r\n        LOG.info(\"First Commit state {}\", firstCommitContextString);\r\n        assertPathExists(\"destFile1\", destFile1);\r\n        assertPathExists(\"destination dir\", destDir);\r\n        LOG.info(\"Commit #2\");\r\n        commitContext.commitOrFail(commits.get(1));\r\n        assertPathExists(\"subdirectory\", subdir);\r\n        assertPathExists(\"destFile2\", destFile2);\r\n        final String secondCommitContextString = commitContext.toString();\r\n        LOG.info(\"Second Commit state {}\", secondCommitContextString);\r\n        LOG.info(\"Commit #3\");\r\n        commitContext.commitOrFail(commits.get(2));\r\n        assertPathExists(\"destFile3\", destFile3);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSSECNoKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSSECNoKey() throws Throwable\n{\r\n    assertGetAlgorithmFails(SSE_C_NO_KEY_ERROR, SSE_C.getMethod(), null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSSECBlankKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSSECBlankKey() throws Throwable\n{\r\n    assertGetAlgorithmFails(SSE_C_NO_KEY_ERROR, SSE_C.getMethod(), \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSSECGoodKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSSECGoodKey() throws Throwable\n{\r\n    assertEquals(SSE_C, getAlgorithm(SSE_C, \"sseckey\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testKMSGoodKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testKMSGoodKey() throws Throwable\n{\r\n    assertEquals(SSE_KMS, getAlgorithm(SSE_KMS, \"kmskey\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testAESKeySet",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAESKeySet() throws Throwable\n{\r\n    assertGetAlgorithmFails(SSE_S3_WITH_KEY_ERROR, SSE_S3.getMethod(), \"setkey\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSSEEmptyKey",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSSEEmptyKey()\n{\r\n    Configuration c = buildConf(SSE_C.getMethod(), \"\");\r\n    assertEquals(\"\", getS3EncryptionKey(BUCKET, c));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSSEKeyNull",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSSEKeyNull() throws Throwable\n{\r\n    final Configuration c = buildConf(SSE_C.getMethod(), null);\r\n    assertEquals(\"\", getS3EncryptionKey(BUCKET, c));\r\n    intercept(IOException.class, SSE_C_NO_KEY_ERROR, () -> getEncryptionAlgorithm(BUCKET, c));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSSEKeyFromCredentialProvider",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSSEKeyFromCredentialProvider() throws Exception\n{\r\n    final Configuration conf = confWithProvider();\r\n    String key = \"provisioned\";\r\n    setProviderOption(conf, Constants.S3_ENCRYPTION_KEY, key);\r\n    conf.set(Constants.S3_ENCRYPTION_KEY, \"keyInConfObject\");\r\n    String sseKey = getS3EncryptionKey(BUCKET, conf);\r\n    assertNotNull(\"Proxy password should not retrun null.\", sseKey);\r\n    assertEquals(\"Proxy password override did NOT work.\", key, sseKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "addFileProvider",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addFileProvider(Configuration conf) throws Exception\n{\r\n    final File file = tempDir.newFile(\"test.jks\");\r\n    final URI jks = ProviderUtils.nestURIForLocalJavaKeyStoreProvider(file.toURI());\r\n    conf.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, jks.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setProviderOption",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setProviderOption(final Configuration conf, String option, String value) throws Exception\n{\r\n    final CredentialProvider provider = CredentialProviderFactory.getProviders(conf).get(0);\r\n    provider.createCredentialEntry(option, value.toCharArray());\r\n    provider.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertGetAlgorithmFails",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertGetAlgorithmFails(String expected, final String alg, final String key) throws Exception\n{\r\n    intercept(IOException.class, expected, () -> getAlgorithm(alg, key));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AEncryptionMethods getAlgorithm(S3AEncryptionMethods algorithm, String key) throws IOException\n{\r\n    return getAlgorithm(algorithm.getMethod(), key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AEncryptionMethods getAlgorithm(String algorithm, String key) throws IOException\n{\r\n    return getEncryptionAlgorithm(BUCKET, buildConf(algorithm, key));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "buildConf",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Configuration buildConf(String algorithm, String key)\n{\r\n    Configuration conf = emptyConf();\r\n    if (algorithm != null) {\r\n        conf.set(Constants.S3_ENCRYPTION_ALGORITHM, algorithm);\r\n    } else {\r\n        conf.unset(SERVER_SIDE_ENCRYPTION_ALGORITHM);\r\n        conf.unset(Constants.S3_ENCRYPTION_ALGORITHM);\r\n    }\r\n    if (key != null) {\r\n        conf.set(Constants.S3_ENCRYPTION_KEY, key);\r\n    } else {\r\n        conf.unset(SERVER_SIDE_ENCRYPTION_KEY);\r\n        conf.unset(Constants.S3_ENCRYPTION_KEY);\r\n    }\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "emptyConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration emptyConf()\n{\r\n    return new Configuration(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "confWithProvider",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration confWithProvider() throws Exception\n{\r\n    final Configuration conf = emptyConf();\r\n    addFileProvider(conf);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testGetPasswordFromConf",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetPasswordFromConf() throws Throwable\n{\r\n    final Configuration conf = emptyConf();\r\n    conf.set(SECRET_KEY, SECRET);\r\n    assertEquals(SECRET, lookupPassword(conf, SECRET_KEY, \"\"));\r\n    assertEquals(SECRET, lookupPassword(conf, SECRET_KEY, \"defVal\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testGetPasswordFromProvider",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testGetPasswordFromProvider() throws Throwable\n{\r\n    final Configuration conf = confWithProvider();\r\n    setProviderOption(conf, SECRET_KEY, SECRET);\r\n    assertEquals(SECRET, lookupPassword(conf, SECRET_KEY, \"\"));\r\n    assertSecretKeyEquals(conf, null, SECRET, \"\");\r\n    assertSecretKeyEquals(conf, null, \"overidden\", \"overidden\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testGetBucketPasswordFromProvider",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetBucketPasswordFromProvider() throws Throwable\n{\r\n    final Configuration conf = confWithProvider();\r\n    URI bucketURI = new URI(\"s3a://\" + BUCKET + \"/\");\r\n    setProviderOption(conf, SECRET_KEY, \"unbucketed\");\r\n    String bucketedKey = String.format(BUCKET_PATTERN, BUCKET, SECRET_KEY);\r\n    setProviderOption(conf, bucketedKey, SECRET);\r\n    String overrideVal;\r\n    overrideVal = \"\";\r\n    assertSecretKeyEquals(conf, BUCKET, SECRET, overrideVal);\r\n    assertSecretKeyEquals(conf, bucketURI.getHost(), SECRET, \"\");\r\n    assertSecretKeyEquals(conf, bucketURI.getHost(), \"overidden\", \"overidden\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertSecretKeyEquals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertSecretKeyEquals(Configuration conf, String bucket, String expected, String overrideVal) throws IOException\n{\r\n    assertEquals(expected, S3AUtils.lookupPassword(bucket, conf, SECRET_KEY, overrideVal, null));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testGetBucketPasswordFromProviderShort",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetBucketPasswordFromProviderShort() throws Throwable\n{\r\n    final Configuration conf = confWithProvider();\r\n    URI bucketURI = new URI(\"s3a://\" + BUCKET + \"/\");\r\n    setProviderOption(conf, SECRET_KEY, \"unbucketed\");\r\n    String bucketedKey = String.format(BUCKET_PATTERN, BUCKET, \"secret.key\");\r\n    setProviderOption(conf, bucketedKey, SECRET);\r\n    assertSecretKeyEquals(conf, BUCKET, SECRET, \"\");\r\n    assertSecretKeyEquals(conf, bucketURI.getHost(), SECRET, \"\");\r\n    assertSecretKeyEquals(conf, bucketURI.getHost(), \"overidden\", \"overidden\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testUnknownEncryptionMethod",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUnknownEncryptionMethod() throws Throwable\n{\r\n    intercept(IOException.class, UNKNOWN_ALGORITHM, () -> S3AEncryptionMethods.getMethod(\"SSE-ROT13\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testClientEncryptionMethod",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testClientEncryptionMethod() throws Throwable\n{\r\n    S3AEncryptionMethods method = getMethod(\"CSE-KMS\");\r\n    assertEquals(CSE_KMS, method);\r\n    assertFalse(\"shouldn't be server side \" + method, method.isServerSide());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCSEKMSEncryptionMethod",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCSEKMSEncryptionMethod() throws Throwable\n{\r\n    S3AEncryptionMethods method = getMethod(\"CSE-CUSTOM\");\r\n    assertEquals(CSE_CUSTOM, method);\r\n    assertFalse(\"shouldn't be server side \" + method, method.isServerSide());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testNoEncryptionMethod",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNoEncryptionMethod() throws Throwable\n{\r\n    assertEquals(NONE, getMethod(\" \"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "createStandardCsvFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void createStandardCsvFile(final FileSystem fs, final Path path, final boolean header, final long quoteHeaderPolicy, final long quoteRowPolicy, final int rows, final String separator, final String eol, final String quote, final Consumer<CsvFile> footer) throws IOException\n{\r\n    try (CsvFile csv = new CsvFile(fs, path, true, separator, eol, quote)) {\r\n        if (header) {\r\n            writeStandardHeader(csv, quoteHeaderPolicy);\r\n        }\r\n        DateTimeFormatter formatter = DateTimeFormatter.ISO_OFFSET_DATE_TIME;\r\n        ZonedDateTime timestamp = ZonedDateTime.now();\r\n        Duration duration = Duration.ofHours(20);\r\n        for (int i = 1; i <= rows; i++) {\r\n            boolean odd = (i & 1) == 1;\r\n            timestamp = timestamp.minus(duration);\r\n            csv.row(quoteRowPolicy, i, timestamp.format(formatter), timestamp.toEpochSecond(), String.format(\"entry-%04d\", i), odd ? \"TRUE\" : \"FALSE\", odd ? 1 : 0, odd ? 1 : -1);\r\n        }\r\n        footer.accept(csv);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "writeStandardHeader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CsvFile writeStandardHeader(final CsvFile csv, final long quoteHeaderPolicy) throws IOException\n{\r\n    return csv.row(quoteHeaderPolicy, \"id\", \"date\", \"timestamp\", \"name\", \"odd\", \"oddint\", \"oddrange\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "verifyErrorCode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AWSServiceIOException verifyErrorCode(final String code, final AWSServiceIOException ex)\n{\r\n    logIntercepted(ex);\r\n    if (!code.equals(ex.getErrorCode())) {\r\n        throw new AssertionError(\"Expected Error code\" + code + \" actual \" + ex.getErrorCode(), ex);\r\n    }\r\n    return ex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "isSelectAvailable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSelectAvailable(final FileSystem filesystem)\n{\r\n    return filesystem instanceof StreamCapabilities && ((StreamCapabilities) filesystem).hasCapability(S3_SELECT_CAPABILITY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    Assume.assumeTrue(\"S3 Select is not enabled on \" + getFileSystem().getUri(), isSelectAvailable(getFileSystem()));\r\n    Configuration conf = getConfiguration();\r\n    landsatGZ = getLandsatCSVPath(conf);\r\n    landsatFS = (S3AFileSystem) landsatGZ.getFileSystem(conf);\r\n    Assume.assumeTrue(\"S3 Select is not enabled on \" + landsatFS.getUri(), isSelectAvailable(landsatFS));\r\n    jobId = AbstractCommitITest.randomJobId();\r\n    attempt0 = \"attempt_\" + jobId + \"_m_000000_0\";\r\n    taskAttempt0 = TaskAttemptID.forName(attempt0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "sql",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String sql(final String template, final Object... args)\n{\r\n    return args.length > 0 ? String.format(template, args) : template;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "q",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String q(String c)\n{\r\n    return '\\'' + c + '\\'';\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "select",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "FSDataInputStream select(final FileSystem fileSystem, final Path source, final Configuration conf, final String sql, final Object... args) throws IOException\n{\r\n    String expression = sql(sql, args);\r\n    describe(\"Execution Select call: %s\", expression);\r\n    FutureDataInputStreamBuilder builder = fileSystem.openFile(source).must(SELECT_SQL, expression);\r\n    for (String key : InternalSelectConstants.SELECT_OPTIONS) {\r\n        String value = conf.get(key);\r\n        if (value != null) {\r\n            builder.must(key, value);\r\n        }\r\n    }\r\n    return awaitFuture(builder.build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "select",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "FSDataInputStream select(final FileContext fc, final Path source, final Configuration conf, final String sql, final Object... args) throws IOException\n{\r\n    String expression = sql(sql, args);\r\n    describe(\"Execution Select call: %s\", expression);\r\n    FutureDataInputStreamBuilder builder = fc.openFile(source).must(SELECT_SQL, expression);\r\n    InternalSelectConstants.SELECT_OPTIONS.forEach((key) -> Optional.ofNullable(conf.get(key)).map((v) -> builder.must(key, v)));\r\n    return awaitFuture(builder.build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "parseToLines",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> parseToLines(final FSDataInputStream selection) throws IOException\n{\r\n    return parseToLines(selection, getMaxLines());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "enablePassthroughCodec",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void enablePassthroughCodec(final Configuration conf, final String extension)\n{\r\n    conf.set(CommonConfigurationKeys.IO_COMPRESSION_CODECS_KEY, PassthroughCodec.CLASSNAME);\r\n    conf.set(PassthroughCodec.OPT_EXTENSION, extension);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getMaxLines",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxLines()\n{\r\n    return 100;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "parseToLines",
  "errType" : [ "NoSuchElementException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<String> parseToLines(final FSDataInputStream selection, int maxLines) throws IOException\n{\r\n    List<String> result = new ArrayList<>();\r\n    String stats;\r\n    try (Scanner scanner = new Scanner(new BufferedReader(new InputStreamReader(selection)))) {\r\n        scanner.useDelimiter(CSV_INPUT_RECORD_DELIMITER_DEFAULT);\r\n        while (maxLines > 0) {\r\n            try {\r\n                String l = scanner.nextLine();\r\n                LOG.info(\"{}\", l);\r\n                result.add(l);\r\n                maxLines--;\r\n            } catch (NoSuchElementException e) {\r\n                break;\r\n            }\r\n        }\r\n        stats = selection.toString();\r\n        describe(\"Result line count: %s\\nStatistics\\n%s\", result.size(), stats);\r\n        IOException ioe = scanner.ioException();\r\n        if (ioe != null && !(ioe instanceof EOFException)) {\r\n            throw ioe;\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "verifySelectionCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> verifySelectionCount(final int expected, final String expression, final List<String> selection)\n{\r\n    return verifySelectionCount(expected, expected, expression, selection);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "verifySelectionCount",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<String> verifySelectionCount(final int min, final int max, final String expression, final List<String> selection)\n{\r\n    int size = selection.size();\r\n    if (size < min || (max > -1 && size > max)) {\r\n        String listing = prepareToPrint(selection);\r\n        LOG.error(\"\\n{} => \\n{}\", expression, listing);\r\n        fail(\"row count from select call \" + expression + \" is out of range \" + min + \" to \" + max + \": \" + size + \" \\n\" + listing);\r\n    }\r\n    return selection;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "prepareToPrint",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String prepareToPrint(final List<String> selection)\n{\r\n    return String.join(\"\\n\", selection);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "createStandardCsvFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createStandardCsvFile(final FileSystem fs, final Path path, final long quoteRowPolicy) throws IOException\n{\r\n    createStandardCsvFile(fs, path, true, ALL_QUOTES, quoteRowPolicy, ALL_ROWS_COUNT, \",\", \"\\n\", \"\\\"\", c -> {\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "inputOpt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void inputOpt(Configuration conf, String key, String val)\n{\r\n    conf.set(MRJobConfig.INPUT_FILE_OPTION_PREFIX + key, val);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "inputMust",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void inputMust(Configuration conf, String key, String val)\n{\r\n    conf.set(MRJobConfig.INPUT_FILE_MANDATORY_PREFIX + key, val);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "readRecords",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<String> readRecords(JobConf conf, Path path, String sql, RecordReader<?, ?> reader, int initialCapacity) throws Exception\n{\r\n    inputMust(conf, SELECT_SQL, sql);\r\n    List<String> lines = new ArrayList<>(initialCapacity);\r\n    try {\r\n        reader.initialize(createSplit(conf, path), createTaskAttemptContext(conf));\r\n        while (reader.nextKeyValue()) {\r\n            lines.add(reader.getCurrentValue().toString());\r\n        }\r\n    } finally {\r\n        reader.close();\r\n    }\r\n    return lines;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "readRecordsV1",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<String> readRecordsV1(JobConf conf, org.apache.hadoop.mapred.RecordReader<K, V> reader, K key, V value, int initialCapacity) throws Exception\n{\r\n    List<String> lines = new ArrayList<>(initialCapacity);\r\n    try {\r\n        while (reader.next(key, value)) {\r\n            lines.add(value.toString());\r\n        }\r\n    } finally {\r\n        reader.close();\r\n    }\r\n    return lines;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "createTaskAttemptContext",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskAttemptContext createTaskAttemptContext(final JobConf conf) throws Exception\n{\r\n    String id = AbstractCommitITest.randomJobId();\r\n    return new TaskAttemptContextImpl(conf, TaskAttemptID.forName(\"attempt_\" + id + \"_m_000000_0\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "createSplit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FileSplit createSplit(final JobConf conf, final Path path) throws IOException\n{\r\n    FileSystem fs = path.getFileSystem(conf);\r\n    FileStatus status = fs.getFileStatus(path);\r\n    return new FileSplit(path, 0, status.getLen(), new String[] { \"localhost\" });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "createSplitV1",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "org.apache.hadoop.mapred.FileSplit createSplitV1(final JobConf conf, final Path path) throws IOException\n{\r\n    FileSystem fs = path.getFileSystem(conf);\r\n    FileStatus status = fs.getFileStatus(path);\r\n    return new org.apache.hadoop.mapred.FileSplit(path, 0, status.getLen(), new String[] { \"localhost\" });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "createLineRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<LongWritable, Text> createLineRecordReader()\n{\r\n    return new LineRecordReader(new byte[] { '\\n' });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "createLineRecordReaderV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "org.apache.hadoop.mapred.RecordReader<LongWritable, Text> createLineRecordReaderV1(final JobConf conf, final Path path) throws IOException\n{\r\n    return new org.apache.hadoop.mapred.LineRecordReader(conf, createSplitV1(conf, path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getLandsatGZ",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getLandsatGZ()\n{\r\n    return landsatGZ;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getLandsatFS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AFileSystem getLandsatFS()\n{\r\n    return landsatFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "seek",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void seek(final FSDataInputStream stream, final long target) throws IOException\n{\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"Seek to %d\", target)) {\r\n        stream.seek(target);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "expectSeekEOF",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EOFException expectSeekEOF(final FSDataInputStream seekStream, final int newpos) throws Exception\n{\r\n    return intercept(EOFException.class, () -> {\r\n        seek(seekStream, newpos);\r\n        return \"Stream after seek to \" + newpos + \": \" + seekStream;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getAttempt0",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAttempt0()\n{\r\n    return attempt0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getTaskAttempt0",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getTaskAttempt0()\n{\r\n    return taskAttempt0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobId()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "logIntercepted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T logIntercepted(T ex)\n{\r\n    LOG.info(\"Intercepted Exception is \", ex);\r\n    return ex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    conf.set(Constants.S3_ENCRYPTION_KEY, \"\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getSSEAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AEncryptionMethods getSSEAlgorithm()\n{\r\n    return S3AEncryptionMethods.SSE_KMS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertEncrypted",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertEncrypted(Path path) throws IOException\n{\r\n    ObjectMetadata md = getFileSystem().getObjectMetadata(path);\r\n    assertEquals(\"SSE Algorithm\", EncryptionTestUtils.AWS_KMS_SSE_ALGORITHM, md.getSSEAlgorithm());\r\n    assertThat(md.getSSEAwsKmsKeyId(), containsString(\"arn:aws:kms:\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "progress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void progress()\n{\r\n    count.incrementAndGet();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCount()\n{\r\n    return count.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "assertCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertCount(String message, int expected)\n{\r\n    assertEquals(message, expected, getCount());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    selectConf = new JobConf(false);\r\n    selectConf.set(SELECT_INPUT_COMPRESSION, COMPRESSION_OPT_GZIP);\r\n    selectConf.set(CSV_INPUT_HEADER, CSV_HEADER_OPT_USE);\r\n    selectConf.setBoolean(SELECT_ERRORS_INCLUDE_SQL, true);\r\n    inputMust(selectConf, CSV_INPUT_HEADER, CSV_HEADER_OPT_USE);\r\n    inputMust(selectConf, SELECT_INPUT_FORMAT, SELECT_FORMAT_CSV);\r\n    inputMust(selectConf, SELECT_OUTPUT_FORMAT, SELECT_FORMAT_CSV);\r\n    inputMust(selectConf, SELECT_INPUT_COMPRESSION, COMPRESSION_OPT_GZIP);\r\n    enablePassthroughCodec(selectConf, \".gz\");\r\n    ChangeDetectionPolicy changeDetectionPolicy = getLandsatFS().getChangeDetectionPolicy();\r\n    Assume.assumeFalse(\"the standard landsat bucket doesn't have versioning\", changeDetectionPolicy.getSource() == Source.VersionId && changeDetectionPolicy.isRequireVersion());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getMaxLines",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxLines()\n{\r\n    return SELECT_LIMIT * 2;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectCloudcoverIgnoreHeader",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSelectCloudcoverIgnoreHeader() throws Throwable\n{\r\n    describe(\"select ignoring the header\");\r\n    selectConf.set(CSV_INPUT_HEADER, CSV_HEADER_OPT_IGNORE);\r\n    String sql = \"SELECT\\n\" + \"* from\\n\" + \"S3OBJECT s WHERE\\n\" + \"s._3 = '0.0'\\n\" + LIMITED;\r\n    List<String> list = selectLandsatFile(selectConf, sql);\r\n    LOG.info(\"Line count: {}\", list.size());\r\n    verifySelectionCount(1, SELECT_LIMIT, sql, list);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectCloudcoverUseHeader",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSelectCloudcoverUseHeader() throws Throwable\n{\r\n    describe(\"select 100% cover using the header, \" + \"+ verify projection and incrementing select statistics\");\r\n    S3ATestUtils.MetricDiff selectCount = new S3ATestUtils.MetricDiff(getLandsatFS(), Statistic.OBJECT_SELECT_REQUESTS);\r\n    List<String> list = selectLandsatFile(selectConf, SELECT_ENTITY_ID_ALL_CLOUDS);\r\n    LOG.info(\"Line count: {}\", list.size());\r\n    verifySelectionCount(1, SELECT_LIMIT, SELECT_ENTITY_ID_ALL_CLOUDS, list);\r\n    String line1 = list.get(0);\r\n    assertThat(\"no column filtering from \" + SELECT_ENTITY_ID_ALL_CLOUDS, line1, not(containsString(\"100.0\")));\r\n    selectCount.assertDiffEquals(\"select count\", 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testFileContextIntegration",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testFileContextIntegration() throws Throwable\n{\r\n    describe(\"Test that select works through FileContext\");\r\n    FileContext fc = S3ATestUtils.createTestFileContext(getConfiguration());\r\n    List<String> list = parseToLines(select(fc, getLandsatGZ(), selectConf, SELECT_ENTITY_ID_ALL_CLOUDS), SELECT_LIMIT * 2);\r\n    LOG.info(\"Line count: {}\", list.size());\r\n    verifySelectionCount(1, SELECT_LIMIT, SELECT_ENTITY_ID_ALL_CLOUDS, list);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testReadLandsatRecords",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testReadLandsatRecords() throws Throwable\n{\r\n    describe(\"Use a record reader to read the records\");\r\n    inputMust(selectConf, CSV_OUTPUT_FIELD_DELIMITER, \"\\\\t\");\r\n    inputMust(selectConf, CSV_OUTPUT_QUOTE_CHARACTER, \"'\");\r\n    inputMust(selectConf, CSV_OUTPUT_QUOTE_FIELDS, CSV_OUTPUT_QUOTE_FIELDS_AS_NEEEDED);\r\n    inputMust(selectConf, CSV_OUTPUT_RECORD_DELIMITER, \"\\n\");\r\n    List<String> records = readRecords(selectConf, getLandsatGZ(), SELECT_ENTITY_ID_ALL_CLOUDS, createLineRecordReader(), SELECT_LIMIT);\r\n    verifySelectionCount(1, SELECT_LIMIT, SELECT_ENTITY_ID_ALL_CLOUDS, records);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testReadLandsatRecordsNoMatch",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testReadLandsatRecordsNoMatch() throws Throwable\n{\r\n    describe(\"Verify the v2 record reader does not fail\" + \" when there are no results\");\r\n    verifySelectionCount(0, 0, SELECT_NOTHING, readRecords(selectConf, getLandsatGZ(), SELECT_NOTHING, createLineRecordReader(), SELECT_LIMIT));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testReadLandsatRecordsGZipEnabled",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testReadLandsatRecordsGZipEnabled() throws Throwable\n{\r\n    describe(\"Verify that by default, the gzip codec is connected to .gz\" + \" files, and so fails\");\r\n    selectConf.unset(CommonConfigurationKeys.IO_COMPRESSION_CODECS_KEY);\r\n    intercept(IOException.class, \"gzip\", () -> readRecords(selectConf, getLandsatGZ(), SELECT_ENTITY_ID_ALL_CLOUDS, createLineRecordReader(), SELECT_LIMIT));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testReadLandsatRecordsV1",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testReadLandsatRecordsV1() throws Throwable\n{\r\n    describe(\"Use a record reader to read the records\");\r\n    verifySelectionCount(1, SELECT_LIMIT, SELECT_ENTITY_ID_ALL_CLOUDS, readRecords(selectConf, getLandsatGZ(), SELECT_ENTITY_ID_ALL_CLOUDS, createLineRecordReader(), SELECT_LIMIT));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testReadLandsatRecordsV1NoResults",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testReadLandsatRecordsV1NoResults() throws Throwable\n{\r\n    describe(\"verify that a select with no results is not an error\");\r\n    verifySelectionCount(0, 0, SELECT_NOTHING, readRecords(selectConf, getLandsatGZ(), SELECT_NOTHING, createLineRecordReader(), SELECT_LIMIT));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "selectLandsatFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> selectLandsatFile(final Configuration conf, final String sql, final Object... args) throws Exception\n{\r\n    return parseToLines(select(getLandsatFS(), getLandsatGZ(), conf, sql, args));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "testSelectSeekFullLandsat",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testSelectSeekFullLandsat() throws Throwable\n{\r\n    describe(\"Verify forward seeks work, not others\");\r\n    boolean enabled = getTestPropertyBool(getConfiguration(), KEY_SCALE_TESTS_ENABLED, DEFAULT_SCALE_TESTS_ENABLED);\r\n    assume(\"Scale test disabled\", enabled);\r\n    final Path path = getLandsatGZ();\r\n    S3AFileSystem fs = getLandsatFS();\r\n    int len = (int) fs.getFileStatus(path).getLen();\r\n    byte[] dataset = new byte[4 * _1MB];\r\n    int actualLen;\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"Initial read of %s\", path);\r\n        FSDataInputStream sourceStream = select(fs, path, selectConf, SELECT_EVERYTHING)) {\r\n        actualLen = IOUtils.read(sourceStream, dataset);\r\n    }\r\n    int seekRange = 16 * _1KB;\r\n    try (FSDataInputStream seekStream = select(fs, path, selectConf, SELECT_EVERYTHING)) {\r\n        SelectInputStream sis = (SelectInputStream) seekStream.getWrappedStream();\r\n        S3AInputStreamStatistics streamStats = sis.getS3AStreamStatistics();\r\n        seekStream.seek(0);\r\n        assertEquals(\"first byte read\", dataset[0], seekStream.read());\r\n        seekStream.seek(1);\r\n        seekStream.seek(1);\r\n        intercept(PathIOException.class, SelectInputStream.SEEK_UNSUPPORTED, () -> seekStream.seek(0));\r\n        byte[] buffer = new byte[1];\r\n        seekStream.readFully(seekStream.getPos(), buffer);\r\n        intercept(PathIOException.class, SelectInputStream.SEEK_UNSUPPORTED, () -> seekStream.readFully(0, buffer));\r\n        long target = seekStream.getPos() + seekRange;\r\n        seek(seekStream, target);\r\n        assertEquals(\"Seek position in \" + seekStream, target, seekStream.getPos());\r\n        assertEquals(\"byte at seek position\", dataset[(int) seekStream.getPos()], seekStream.read());\r\n        assertEquals(\"Seek bytes skipped in \" + streamStats, seekRange, streamStats.getBytesSkippedOnSeek());\r\n        long offset;\r\n        long increment = 64 * _1KB;\r\n        for (offset = 32 * _1KB; offset < actualLen; offset += increment) {\r\n            seek(seekStream, offset);\r\n            assertEquals(\"Seek position in \" + seekStream, offset, seekStream.getPos());\r\n            assertEquals(\"byte at seek position\", dataset[(int) seekStream.getPos()], seekStream.read());\r\n        }\r\n        for (; offset < len; offset += _1MB) {\r\n            seek(seekStream, offset);\r\n            assertEquals(\"Seek position in \" + seekStream, offset, seekStream.getPos());\r\n        }\r\n        LOG.info(\"Seek statistics {}\", streamStats);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testXAttrRoot",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testXAttrRoot() throws Throwable\n{\r\n    describe(\"Test xattr on root\");\r\n    Path root = new Path(\"/\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Map<String, byte[]> xAttrs = verifyMetrics(() -> fs.getXAttrs(root), with(INVOCATION_XATTR_GET_MAP, GET_METADATA_ON_OBJECT));\r\n    logXAttrs(xAttrs);\r\n    List<String> headerList = verifyMetrics(() -> fs.listXAttrs(root), with(INVOCATION_OP_XATTR_LIST, GET_METADATA_ON_OBJECT));\r\n    Assertions.assertThat(headerList).describedAs(\"Headers on root object\").containsOnly(XA_CONTENT_LENGTH, XA_CONTENT_TYPE);\r\n    assertHeaderEntry(xAttrs, XA_CONTENT_TYPE).isEqualTo(CONTENT_TYPE_APPLICATION_XML);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "logXAttrs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void logXAttrs(final Map<String, byte[]> xAttrs)\n{\r\n    xAttrs.forEach((k, v) -> LOG.info(\"{} has bytes[{}] => \\\"{}\\\"\", k, v.length, decodeBytes(v)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testXAttrFile",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testXAttrFile() throws Throwable\n{\r\n    describe(\"Test xattr on a file\");\r\n    Path testFile = methodPath();\r\n    create(testFile, true, CREATE_FILE_OVERWRITE);\r\n    S3AFileSystem fs = getFileSystem();\r\n    Map<String, byte[]> xAttrs = verifyMetrics(() -> fs.getXAttrs(testFile), with(INVOCATION_XATTR_GET_MAP, GET_METADATA_ON_OBJECT));\r\n    logXAttrs(xAttrs);\r\n    assertHeaderEntry(xAttrs, XA_CONTENT_LENGTH).isEqualTo(\"0\");\r\n    List<String> headerList = verifyMetrics(() -> fs.listXAttrs(testFile), with(INVOCATION_OP_XATTR_LIST, GET_METADATA_ON_OBJECT));\r\n    Assertions.assertThat(headerList).describedAs(\"Supported headers\").containsAnyElementsOf(Arrays.asList(XA_STANDARD_HEADERS));\r\n    byte[] bytes = verifyMetrics(() -> fs.getXAttr(testFile, XA_CONTENT_LENGTH), with(INVOCATION_XATTR_GET_NAMED, GET_METADATA_ON_OBJECT));\r\n    assertHeader(XA_CONTENT_LENGTH, bytes).isEqualTo(\"0\");\r\n    assertHeaderEntry(xAttrs, XA_CONTENT_TYPE).isEqualTo(CONTENT_TYPE_OCTET_STREAM);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testXAttrDir",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testXAttrDir() throws Throwable\n{\r\n    describe(\"Test xattr on a dir\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path dir = methodPath();\r\n    fs.mkdirs(dir);\r\n    Map<String, byte[]> xAttrs = verifyMetrics(() -> fs.getXAttrs(dir), with(INVOCATION_XATTR_GET_MAP, GET_METADATA_ON_DIR));\r\n    logXAttrs(xAttrs);\r\n    assertHeaderEntry(xAttrs, XA_CONTENT_LENGTH).isEqualTo(\"0\");\r\n    List<String> headerList = verifyMetrics(() -> fs.listXAttrs(dir), with(INVOCATION_OP_XATTR_LIST, GET_METADATA_ON_DIR));\r\n    Assertions.assertThat(headerList).describedAs(\"Supported headers\").containsAnyElementsOf(Arrays.asList(XA_STANDARD_HEADERS));\r\n    byte[] bytes = verifyMetrics(() -> fs.getXAttr(dir, XA_CONTENT_LENGTH), with(INVOCATION_XATTR_GET_NAMED, GET_METADATA_ON_DIR));\r\n    assertHeader(XA_CONTENT_LENGTH, bytes).isEqualTo(\"0\");\r\n    assertHeaderEntry(xAttrs, XA_CONTENT_TYPE).isEqualTo(CONTENT_TYPE_X_DIRECTORY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testXAttrMissingFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testXAttrMissingFile() throws Throwable\n{\r\n    describe(\"Test xattr on a missing path\");\r\n    Path testFile = methodPath();\r\n    S3AFileSystem fs = getFileSystem();\r\n    int getMetadataOnMissingFile = GET_METADATA_ON_DIR;\r\n    verifyMetricsIntercepting(FileNotFoundException.class, \"\", () -> fs.getXAttrs(testFile), with(INVOCATION_XATTR_GET_MAP, getMetadataOnMissingFile));\r\n    verifyMetricsIntercepting(FileNotFoundException.class, \"\", () -> fs.getXAttr(testFile, XA_CONTENT_LENGTH), with(INVOCATION_XATTR_GET_NAMED, getMetadataOnMissingFile));\r\n    verifyMetricsIntercepting(FileNotFoundException.class, \"\", () -> fs.listXAttrs(testFile), with(INVOCATION_OP_XATTR_LIST, getMetadataOnMissingFile));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "assertHeaderEntry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractStringAssert<?> assertHeaderEntry(Map<String, byte[]> xAttrs, String key)\n{\r\n    return assertHeader(key, xAttrs.get(key));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "assertHeader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AbstractStringAssert<?> assertHeader(final String key, final byte[] bytes)\n{\r\n    String decoded = decodeBytes(bytes);\r\n    return Assertions.assertThat(decoded).describedAs(\"xattr %s decoded to: %s\", key, decoded).isNotNull().isNotEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getTestTimeoutSeconds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestTimeoutSeconds()\n{\r\n    return 16 * 60;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    auxFs = getNormalFileSystem();\r\n    testRoot = path(\"/ITestS3AConcurrentOps\");\r\n    testRoot = S3ATestUtils.createTestPath(testRoot);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getNormalFileSystem",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "S3AFileSystem getNormalFileSystem() throws Exception\n{\r\n    S3AFileSystem s3a = new S3AFileSystem();\r\n    Configuration conf = createScaleConfiguration();\r\n    URI rootURI = new URI(conf.get(TEST_FS_S3A_NAME));\r\n    s3a.initialize(rootURI, conf);\r\n    return s3a;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    super.teardown();\r\n    if (auxFs != null) {\r\n        auxFs.delete(testRoot, true);\r\n        auxFs.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "parallelRenames",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void parallelRenames(int concurrentRenames, final S3AFileSystem fs, String sourceNameBase, String targetNameBase) throws ExecutionException, InterruptedException, IOException\n{\r\n    Path[] source = new Path[concurrentRenames];\r\n    Path[] target = new Path[concurrentRenames];\r\n    for (int i = 0; i < concurrentRenames; i++) {\r\n        source[i] = new Path(testRoot, sourceNameBase + i);\r\n        target[i] = new Path(testRoot, targetNameBase + i);\r\n    }\r\n    LOG.info(\"Generating data...\");\r\n    auxFs.mkdirs(testRoot);\r\n    byte[] zeroes = ContractTestUtils.dataset(fileSize, 0, Integer.MAX_VALUE);\r\n    for (Path aSource : source) {\r\n        try (FSDataOutputStream out = auxFs.create(aSource)) {\r\n            for (int mb = 0; mb < 20; mb++) {\r\n                LOG.debug(\"{}: Block {}...\", aSource, mb);\r\n                out.write(zeroes);\r\n            }\r\n        }\r\n    }\r\n    LOG.info(\"Data generated...\");\r\n    ExecutorService executor = Executors.newFixedThreadPool(concurrentRenames, new ThreadFactory() {\r\n\r\n        private AtomicInteger count = new AtomicInteger(0);\r\n\r\n        public Thread newThread(Runnable r) {\r\n            return new Thread(r, \"testParallelRename\" + count.getAndIncrement());\r\n        }\r\n    });\r\n    try {\r\n        ((ThreadPoolExecutor) executor).prestartAllCoreThreads();\r\n        Future<Boolean>[] futures = new Future[concurrentRenames];\r\n        for (int i = 0; i < concurrentRenames; i++) {\r\n            final int index = i;\r\n            futures[i] = executor.submit(new Callable<Boolean>() {\r\n\r\n                @Override\r\n                public Boolean call() throws Exception {\r\n                    NanoTimer timer = new NanoTimer();\r\n                    boolean result = fs.rename(source[index], target[index]);\r\n                    timer.end(\"parallel rename %d\", index);\r\n                    LOG.info(\"Rename {} ran from {} to {}\", index, timer.getStartTime(), timer.getEndTime());\r\n                    return result;\r\n                }\r\n            });\r\n        }\r\n        LOG.info(\"Waiting for tasks to complete...\");\r\n        LOG.info(\"Deadlock may have occurred if nothing else is logged\" + \" or the test times out\");\r\n        for (int i = 0; i < concurrentRenames; i++) {\r\n            assertTrue(\"No future \" + i, futures[i].get());\r\n            assertPathExists(\"target path\", target[i]);\r\n            assertPathDoesNotExist(\"source path\", source[i]);\r\n        }\r\n        LOG.info(\"All tasks have completed successfully\");\r\n    } finally {\r\n        executor.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testParallelRename",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testParallelRename() throws InterruptedException, ExecutionException, IOException\n{\r\n    Configuration conf = getConfiguration();\r\n    conf.setInt(MAX_THREADS, 2);\r\n    conf.setInt(MAX_TOTAL_TASKS, 1);\r\n    conf.set(MIN_MULTIPART_THRESHOLD, \"10K\");\r\n    conf.set(MULTIPART_SIZE, \"5K\");\r\n    try (S3AFileSystem tinyThreadPoolFs = new S3AFileSystem()) {\r\n        tinyThreadPoolFs.initialize(auxFs.getUri(), conf);\r\n        parallelRenames(concurrentRenames, tinyThreadPoolFs, \"testParallelRename-source\", \"testParallelRename-target\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testThreadPoolCoolDown",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testThreadPoolCoolDown() throws InterruptedException, ExecutionException, IOException\n{\r\n    int hotThreads = 0;\r\n    int coldThreads = 0;\r\n    parallelRenames(concurrentRenames, auxFs, \"testThreadPoolCoolDown-source\", \"testThreadPoolCoolDown-target\");\r\n    for (Thread t : Thread.getAllStackTraces().keySet()) {\r\n        if (t.getName().startsWith(\"s3a-transfer\")) {\r\n            hotThreads++;\r\n        }\r\n    }\r\n    int timeoutMs = Constants.DEFAULT_KEEPALIVE_TIME * 1000;\r\n    Thread.sleep((int) (1.1 * timeoutMs));\r\n    for (Thread t : Thread.getAllStackTraces().keySet()) {\r\n        if (t.getName().startsWith(\"s3a-transfer\")) {\r\n            coldThreads++;\r\n        }\r\n    }\r\n    assertNotEquals(\"Failed to find threads in active FS - test is flawed\", hotThreads, 0);\r\n    assertTrue(\"s3a-transfer threads went from \" + hotThreads + \" to \" + coldThreads + \", should have gone to 0\", 0 == coldThreads);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getDelegationBinding",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDelegationBinding()\n{\r\n    return DELEGATION_TOKEN_SESSION_BINDING;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getTokenKind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getTokenKind()\n{\r\n    return SESSION_TOKEN_KIND;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    enableDelegationTokens(conf, getDelegationBinding());\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    assumeSessionTestsEnabled(getConfiguration());\r\n    resetUGI();\r\n    delegationTokens = instantiateDTSupport(getConfiguration());\r\n    delegationTokens.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    IOUtils.cleanupWithLogger(LOG, delegationTokens);\r\n    resetUGI();\r\n    super.teardown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testCanonicalization",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCanonicalization() throws Throwable\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    assertEquals(\"Default port has changed\", 0, fs.getDefaultPort());\r\n    URI uri = fs.getCanonicalUri();\r\n    String service = fs.getCanonicalServiceName();\r\n    assertEquals(\"canonical URI and service name mismatch\", uri, new URI(service));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testSaveLoadTokens",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testSaveLoadTokens() throws Throwable\n{\r\n    File tokenFile = File.createTempFile(\"token\", \"bin\");\r\n    EncryptionSecrets encryptionSecrets = new EncryptionSecrets(S3AEncryptionMethods.SSE_KMS, KMS_KEY);\r\n    Token<AbstractS3ATokenIdentifier> dt = delegationTokens.createDelegationToken(encryptionSecrets, null);\r\n    final SessionTokenIdentifier origIdentifier = (SessionTokenIdentifier) dt.decodeIdentifier();\r\n    assertEquals(\"kind in \" + dt, getTokenKind(), dt.getKind());\r\n    Configuration conf = getConfiguration();\r\n    saveDT(tokenFile, dt);\r\n    assertTrue(\"Empty token file\", tokenFile.length() > 0);\r\n    Credentials creds = Credentials.readTokenStorageFile(tokenFile, conf);\r\n    Text serviceId = delegationTokens.getService();\r\n    Token<? extends TokenIdentifier> token = requireNonNull(creds.getToken(serviceId), () -> \"No token for \\\"\" + serviceId + \"\\\" in: \" + creds.getAllTokens());\r\n    SessionTokenIdentifier decoded = (SessionTokenIdentifier) token.decodeIdentifier();\r\n    decoded.validate();\r\n    assertEquals(\"token identifier \", origIdentifier, decoded);\r\n    assertEquals(\"Origin in \" + decoded, origIdentifier.getOrigin(), decoded.getOrigin());\r\n    assertEquals(\"Expiry time\", origIdentifier.getExpiryTime(), decoded.getExpiryTime());\r\n    assertEquals(\"Encryption Secrets\", encryptionSecrets, decoded.getEncryptionSecrets());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testCreateAndUseDT",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testCreateAndUseDT() throws Throwable\n{\r\n    describe(\"Create a Delegation Token, round trip then reuse\");\r\n    final S3AFileSystem fs = getFileSystem();\r\n    final Configuration conf = fs.getConf();\r\n    assertNull(\"Current User has delegation token\", delegationTokens.selectTokenFromFSOwner());\r\n    EncryptionSecrets secrets = new EncryptionSecrets(S3AEncryptionMethods.SSE_KMS, KMS_KEY);\r\n    Token<AbstractS3ATokenIdentifier> originalDT = delegationTokens.createDelegationToken(secrets, null);\r\n    assertEquals(\"Token kind mismatch\", getTokenKind(), originalDT.getKind());\r\n    SessionTokenIdentifier issued = requireNonNull((SessionTokenIdentifier) originalDT.decodeIdentifier(), () -> \"no identifier in \" + originalDT);\r\n    issued.validate();\r\n    final MarshalledCredentials creds;\r\n    try (S3ADelegationTokens dt2 = instantiateDTSupport(getConfiguration())) {\r\n        dt2.start();\r\n        dt2.resetTokenBindingToDT(originalDT);\r\n        final AWSSessionCredentials awsSessionCreds = verifySessionCredentials(dt2.getCredentialProviders().getCredentials());\r\n        final MarshalledCredentials origCreds = fromAWSCredentials(awsSessionCreds);\r\n        Token<AbstractS3ATokenIdentifier> boundDT = dt2.getBoundOrNewDT(secrets, null);\r\n        assertEquals(\"Delegation Tokens\", originalDT, boundDT);\r\n        creds = roundTrip(origCreds, conf);\r\n        SessionTokenIdentifier reissued = (SessionTokenIdentifier) dt2.createDelegationToken(secrets, null).decodeIdentifier();\r\n        reissued.validate();\r\n        String userAgentField = dt2.getUserAgentField();\r\n        assertThat(\"UA field does not contain UUID\", userAgentField, Matchers.containsString(issued.getUuid()));\r\n    }\r\n    verifyCredentialPropagation(fs, creds, new Configuration(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testCreateWithRenewer",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testCreateWithRenewer() throws Throwable\n{\r\n    describe(\"Create a Delegation Token, round trip then reuse\");\r\n    final S3AFileSystem fs = getFileSystem();\r\n    final Configuration conf = fs.getConf();\r\n    final Text renewer = new Text(\"yarn\");\r\n    assertNull(\"Current User has delegation token\", delegationTokens.selectTokenFromFSOwner());\r\n    EncryptionSecrets secrets = new EncryptionSecrets(S3AEncryptionMethods.SSE_KMS, KMS_KEY);\r\n    Token<AbstractS3ATokenIdentifier> dt = delegationTokens.createDelegationToken(secrets, renewer);\r\n    assertEquals(\"Token kind mismatch\", getTokenKind(), dt.getKind());\r\n    SessionTokenIdentifier issued = requireNonNull((SessionTokenIdentifier) dt.decodeIdentifier(), () -> \"no identifier in \" + dt);\r\n    issued.validate();\r\n    assertEquals(\"Token renewer mismatch\", renewer, issued.getRenewer());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "verifyCredentialPropagation",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "AbstractS3ATokenIdentifier verifyCredentialPropagation(final S3AFileSystem fs, final MarshalledCredentials session, final Configuration conf) throws Exception\n{\r\n    describe(\"Verify Token Propagation\");\r\n    unsetHadoopCredentialProviders(conf);\r\n    conf.set(DELEGATION_TOKEN_CREDENTIALS_PROVIDER, TemporaryAWSCredentialsProvider.NAME);\r\n    session.setSecretsInConfiguration(conf);\r\n    try (S3ADelegationTokens delegationTokens2 = new S3ADelegationTokens()) {\r\n        delegationTokens2.bindToFileSystem(fs.getCanonicalUri(), fs.createStoreContext(), fs.createDelegationOperations());\r\n        delegationTokens2.init(conf);\r\n        delegationTokens2.start();\r\n        final Token<AbstractS3ATokenIdentifier> newDT = delegationTokens2.getBoundOrNewDT(new EncryptionSecrets(), null);\r\n        delegationTokens2.resetTokenBindingToDT(newDT);\r\n        final AbstractS3ATokenIdentifier boundId = delegationTokens2.getDecodedIdentifier().get();\r\n        LOG.info(\"Regenerated DT is {}\", newDT);\r\n        final MarshalledCredentials creds2 = fromAWSCredentials(verifySessionCredentials(delegationTokens2.getCredentialProviders().getCredentials()));\r\n        assertEquals(\"Credentials\", session, creds2);\r\n        assertTrue(\"Origin in \" + boundId, boundId.getOrigin().contains(CREDENTIALS_CONVERTED_TO_DELEGATION_TOKEN));\r\n        return boundId;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "verifySessionCredentials",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AWSSessionCredentials verifySessionCredentials(final AWSCredentials creds)\n{\r\n    AWSSessionCredentials session = (AWSSessionCredentials) creds;\r\n    assertNotNull(\"access key\", session.getAWSAccessKeyId());\r\n    assertNotNull(\"secret key\", session.getAWSSecretKey());\r\n    assertNotNull(\"session token\", session.getSessionToken());\r\n    return session;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testDBindingReentrancyLock",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testDBindingReentrancyLock() throws Throwable\n{\r\n    describe(\"Verify that S3ADelegationTokens cannot be bound twice when there\" + \" is no token\");\r\n    S3ADelegationTokens delegation = instantiateDTSupport(getConfiguration());\r\n    delegation.start();\r\n    assertFalse(\"Delegation is bound to a DT: \" + delegation, delegation.isBoundToDT());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { \"keep-markers\", true }, { \"delete-markers\", false } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    disableFilesystemCaching(conf);\r\n    String bucketName = getTestBucketName(conf);\r\n    removeBaseAndBucketOverrides(bucketName, conf, DIRECTORY_MARKER_POLICY, ETAG_CHECKSUM_ENABLED, S3_ENCRYPTION_ALGORITHM, S3_ENCRYPTION_KEY, SERVER_SIDE_ENCRYPTION_ALGORITHM, SERVER_SIDE_ENCRYPTION_KEY);\r\n    conf.set(DIRECTORY_MARKER_POLICY, keepMarkers ? DIRECTORY_MARKER_POLICY_KEEP : DIRECTORY_MARKER_POLICY_DELETE);\r\n    conf.set(S3_ENCRYPTION_ALGORITHM, getSSEAlgorithm().getMethod());\r\n    conf.set(S3_ENCRYPTION_KEY, KEY_1);\r\n    conf.setBoolean(ETAG_CHECKSUM_ENABLED, true);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    assumeEnabled();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    super.teardown();\r\n    IOUtils.closeStream(fsKeyB);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCreateFileAndReadWithDifferentEncryptionKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCreateFileAndReadWithDifferentEncryptionKey() throws Exception\n{\r\n    intercept(AccessDeniedException.class, SERVICE_AMAZON_S3_STATUS_CODE_403, () -> {\r\n        int len = TEST_FILE_LEN;\r\n        describe(\"Create an encrypted file of size \" + len);\r\n        Path src = path(\"testCreateFileAndReadWithDifferentEncryptionKey\");\r\n        writeThenReadFile(src, len);\r\n        fsKeyB = createNewFileSystemWithSSECKey(\"kX7SdwVc/1VXJr76kfKnkQ3ONYhxianyL2+C3rPVT9s=\");\r\n        byte[] data = dataset(len, 'a', 'z');\r\n        ContractTestUtils.verifyFileContents(fsKeyB, src, data);\r\n        return fsKeyB.getFileStatus(src);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCreateSubdirWithDifferentKey",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCreateSubdirWithDifferentKey() throws Exception\n{\r\n    Path base = path(\"testCreateSubdirWithDifferentKey\");\r\n    Path nestedDirectory = new Path(base, \"nestedDir\");\r\n    fsKeyB = createNewFileSystemWithSSECKey(KEY_2);\r\n    getFileSystem().mkdirs(base);\r\n    fsKeyB.mkdirs(nestedDirectory);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCreateFileThenMoveWithDifferentSSECKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCreateFileThenMoveWithDifferentSSECKey() throws Exception\n{\r\n    intercept(AccessDeniedException.class, SERVICE_AMAZON_S3_STATUS_CODE_403, () -> {\r\n        int len = TEST_FILE_LEN;\r\n        Path src = path(createFilename(len));\r\n        writeThenReadFile(src, len);\r\n        fsKeyB = createNewFileSystemWithSSECKey(KEY_3);\r\n        Path dest = path(createFilename(\"different-path.txt\"));\r\n        getFileSystem().mkdirs(dest.getParent());\r\n        return fsKeyB.rename(src, dest);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRenameFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testRenameFile() throws Exception\n{\r\n    Path src = path(\"original-path.txt\");\r\n    writeThenReadFile(src, TEST_FILE_LEN);\r\n    Path newPath = path(\"different-path.txt\");\r\n    getFileSystem().rename(src, newPath);\r\n    byte[] data = dataset(TEST_FILE_LEN, 'a', 'z');\r\n    ContractTestUtils.verifyFileContents(getFileSystem(), newPath, data);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testListEncryptedDir",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testListEncryptedDir() throws Exception\n{\r\n    Path pathABC = path(\"testListEncryptedDir/a/b/c/\");\r\n    Path pathAB = pathABC.getParent();\r\n    Path pathA = pathAB.getParent();\r\n    Path nestedDirectory = createTestPath(pathABC);\r\n    assertTrue(getFileSystem().mkdirs(nestedDirectory));\r\n    fsKeyB = createNewFileSystemWithSSECKey(KEY_4);\r\n    fsKeyB.listFiles(pathA, true);\r\n    fsKeyB.listFiles(pathAB, true);\r\n    fsKeyB.listFiles(pathABC, false);\r\n    Configuration conf = this.createConfiguration();\r\n    conf.unset(S3_ENCRYPTION_ALGORITHM);\r\n    conf.unset(S3_ENCRYPTION_KEY);\r\n    S3AContract contract = (S3AContract) createContract(conf);\r\n    contract.init();\r\n    FileSystem unencryptedFileSystem = contract.getTestFileSystem();\r\n    unencryptedFileSystem.listFiles(pathA, true);\r\n    unencryptedFileSystem.listFiles(pathAB, true);\r\n    unencryptedFileSystem.listFiles(pathABC, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testListStatusEncryptedDir",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testListStatusEncryptedDir() throws Exception\n{\r\n    Path pathABC = path(\"testListStatusEncryptedDir/a/b/c/\");\r\n    Path pathAB = pathABC.getParent();\r\n    Path pathA = pathAB.getParent();\r\n    assertTrue(getFileSystem().mkdirs(pathABC));\r\n    fsKeyB = createNewFileSystemWithSSECKey(KEY_4);\r\n    fsKeyB.listStatus(pathA);\r\n    fsKeyB.listStatus(pathAB);\r\n    fsKeyB.listStatus(pathABC);\r\n    Configuration conf = createConfiguration();\r\n    conf.unset(S3_ENCRYPTION_ALGORITHM);\r\n    conf.unset(S3_ENCRYPTION_KEY);\r\n    S3AContract contract = (S3AContract) createContract(conf);\r\n    contract.init();\r\n    FileSystem unencryptedFileSystem = contract.getTestFileSystem();\r\n    unencryptedFileSystem.listStatus(pathA);\r\n    unencryptedFileSystem.listStatus(pathAB);\r\n    unencryptedFileSystem.listStatus(pathABC);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testListStatusEncryptedFile",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testListStatusEncryptedFile() throws Exception\n{\r\n    Path pathABC = path(\"testListStatusEncryptedFile/a/b/c/\");\r\n    assertTrue(\"mkdirs failed\", getFileSystem().mkdirs(pathABC));\r\n    Path fileToStat = new Path(pathABC, \"fileToStat.txt\");\r\n    writeThenReadFile(fileToStat, TEST_FILE_LEN);\r\n    fsKeyB = createNewFileSystemWithSSECKey(KEY_4);\r\n    if (statusProbesCheckS3(fsKeyB, fileToStat)) {\r\n        intercept(AccessDeniedException.class, SERVICE_AMAZON_S3_STATUS_CODE_403, () -> fsKeyB.listStatus(fileToStat));\r\n    } else {\r\n        fsKeyB.listStatus(fileToStat);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "statusProbesCheckS3",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean statusProbesCheckS3(S3AFileSystem fs, Path path)\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testDeleteEncryptedObjectWithDifferentKey",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testDeleteEncryptedObjectWithDifferentKey() throws Exception\n{\r\n    Path pathABC = path(\"testDeleteEncryptedObjectWithDifferentKey/a/b/c/\");\r\n    Path pathAB = pathABC.getParent();\r\n    Path pathA = pathAB.getParent();\r\n    assertTrue(getFileSystem().mkdirs(pathABC));\r\n    Path fileToDelete = new Path(pathABC, \"filetobedeleted.txt\");\r\n    writeThenReadFile(fileToDelete, TEST_FILE_LEN);\r\n    fsKeyB = createNewFileSystemWithSSECKey(KEY_4);\r\n    if (statusProbesCheckS3(fsKeyB, fileToDelete)) {\r\n        intercept(AccessDeniedException.class, SERVICE_AMAZON_S3_STATUS_CODE_403, () -> fsKeyB.delete(fileToDelete, false));\r\n    } else {\r\n        fsKeyB.delete(fileToDelete, false);\r\n    }\r\n    fsKeyB.delete(pathABC, true);\r\n    fsKeyB.delete(pathAB, true);\r\n    fsKeyB.delete(pathA, true);\r\n    assertPathDoesNotExist(\"expected recursive delete\", fileToDelete);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testChecksumRequiresReadAccess",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testChecksumRequiresReadAccess() throws Throwable\n{\r\n    Path path = path(\"tagged-file\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    touch(fs, path);\r\n    Assertions.assertThat(fs.getFileChecksum(path)).isNotNull();\r\n    fsKeyB = createNewFileSystemWithSSECKey(KEY_4);\r\n    intercept(AccessDeniedException.class, SERVICE_AMAZON_S3_STATUS_CODE_403, () -> fsKeyB.getFileChecksum(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createNewFileSystemWithSSECKey",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "S3AFileSystem createNewFileSystemWithSSECKey(String sseCKey) throws IOException\n{\r\n    Configuration conf = this.createConfiguration();\r\n    conf.set(S3_ENCRYPTION_KEY, sseCKey);\r\n    S3AContract contract = (S3AContract) createContract(conf);\r\n    contract.init();\r\n    FileSystem fileSystem = contract.getTestFileSystem();\r\n    return (S3AFileSystem) fileSystem;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getSSEAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AEncryptionMethods getSSEAlgorithm()\n{\r\n    return S3AEncryptionMethods.SSE_C;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Csvout write(Object o) throws IOException\n{\r\n    if (isStartOfLine) {\r\n        isStartOfLine = false;\r\n    } else {\r\n        out.write(separator);\r\n    }\r\n    out.write(o.toString());\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "newline",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Csvout newline() throws IOException\n{\r\n    out.write(eol);\r\n    isStartOfLine = true;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Csvout write(Object... objects) throws IOException\n{\r\n    for (Object object : objects) {\r\n        write(object);\r\n    }\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "setAccessAllowed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAccessAllowed(final boolean accessAllowed)\n{\r\n    this.accessAllowed = accessAllowed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "checkAccess",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean checkAccess(final Path path, final S3AFileStatus status, final FsAction mode) throws IOException\n{\r\n    return accessAllowed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "suitename",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String suitename()\n{\r\n    return \"ITestStagingCommitProtocol\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    conf.setInt(FS_S3A_COMMITTER_THREADS, 1);\r\n    conf.setBoolean(FS_S3A_COMMITTER_STAGING_UNIQUE_FILENAMES, false);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    Configuration conf = getConfiguration();\r\n    String uuid = UUID.randomUUID().toString();\r\n    conf.set(InternalCommitterConstants.SPARK_WRITE_UUID, uuid);\r\n    Pair<String, AbstractS3ACommitter.JobUUIDSource> t3 = AbstractS3ACommitter.buildJobUUID(conf, JobID.forName(\"job_\" + getJobId()));\r\n    assertEquals(\"Job UUID\", uuid, t3.getLeft());\r\n    assertEquals(\"Job UUID source: \" + t3, AbstractS3ACommitter.JobUUIDSource.SparkWriteUUID, t3.getRight());\r\n    Path tempDir = Paths.getLocalTaskAttemptTempDir(conf, uuid, getTaskAttempt0());\r\n    rmdir(tempDir, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "getCommitterName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCommitterName()\n{\r\n    return InternalCommitterConstants.COMMITTER_NAME_STAGING;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "createCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractS3ACommitter createCommitter(Path outputPath, TaskAttemptContext context) throws IOException\n{\r\n    return new StagingCommitter(outputPath, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "createFailingCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractS3ACommitter createFailingCommitter(TaskAttemptContext tContext) throws IOException\n{\r\n    return new CommitterWithFailedThenSucceed(getOutDir(), tContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "shouldExpectSuccessMarker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldExpectSuccessMarker()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "expectJobCommitToFail",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectJobCommitToFail(JobContext jContext, AbstractS3ACommitter committer) throws Exception\n{\r\n    expectJobCommitFailure(jContext, committer, IOException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "validateTaskAttemptPathDuringWrite",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validateTaskAttemptPathDuringWrite(Path p, final long expectedLength) throws IOException\n{\r\n    ContractTestUtils.assertPathExists(getLocalFS(), \"task attempt\", p);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "validateTaskAttemptPathAfterWrite",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void validateTaskAttemptPathAfterWrite(Path p, final long expectedLength) throws IOException\n{\r\n    FileSystem localFS = getLocalFS();\r\n    ContractTestUtils.assertPathExists(localFS, \"task attempt\", p);\r\n    FileStatus st = localFS.getFileStatus(p);\r\n    assertEquals(\"file length in \" + st, expectedLength, st.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "getLocalFS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getLocalFS() throws IOException\n{\r\n    return FileSystem.getLocal(getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "validateTaskAttemptWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void validateTaskAttemptWorkingDirectory(final AbstractS3ACommitter committer, final TaskAttemptContext context) throws IOException\n{\r\n    Path wd = context.getWorkingDirectory();\r\n    assertEquals(\"file\", wd.toUri().getScheme());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration c = new Configuration();\r\n    String kmsKey = S3AUtils.getS3EncryptionKey(getTestBucketName(c), c);\r\n    if (StringUtils.isBlank(kmsKey)) {\r\n        skip(S3_ENCRYPTION_KEY + \" is not set for \" + SSE_KMS.getMethod());\r\n    }\r\n    Configuration conf = super.createConfiguration();\r\n    conf.set(S3_ENCRYPTION_KEY, kmsKey);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getSSEAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AEncryptionMethods getSSEAlgorithm()\n{\r\n    return SSE_KMS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { INPUT_FADV_SEQUENTIAL, Default_JSSE }, { INPUT_FADV_RANDOM, OpenSSL }, { INPUT_FADV_NORMAL, Default_JSSE_with_GCM } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "createConfiguration",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    try {\r\n        URI bucketURI = new URI(checkNotNull(conf.get(\"fs.contract.test.fs.s3a\")));\r\n        S3ATestUtils.removeBucketOverrides(bucketURI.getHost(), conf, READAHEAD_RANGE, INPUT_FADVISE, SSL_CHANNEL_MODE);\r\n    } catch (URISyntaxException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n    S3ATestUtils.disableFilesystemCaching(conf);\r\n    conf.setInt(READAHEAD_RANGE, READAHEAD);\r\n    conf.set(INPUT_FADVISE, seekPolicy);\r\n    conf.set(SSL_CHANNEL_MODE, sslChannelMode.name());\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    super.teardown();\r\n    S3AFileSystem fs = getFileSystem();\r\n    if (fs != null && fs.getConf().getBoolean(FS_S3A_IMPL_DISABLE_CACHE, false)) {\r\n        fs.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "path",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path path(final String filepath) throws IOException\n{\r\n    return super.path(filepath + \"-\" + seekPolicy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "readAtEndAndReturn",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "byte readAtEndAndReturn(final FSDataInputStream in) throws IOException\n{\r\n    long pos = in.getPos();\r\n    in.seek(DATASET_LEN - 1);\r\n    in.readByte();\r\n    in.seek(pos);\r\n    return in.readByte();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "assertDatasetEquals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertDatasetEquals(final int readOffset, final String operation, final byte[] data, int length)\n{\r\n    for (int i = 0; i < length; i++) {\r\n        int o = readOffset + i;\r\n        assertEquals(operation + \" with seek policy \" + seekPolicy + \"and read offset \" + readOffset + \": data[\" + i + \"] != DATASET[\" + o + \"]\", DATASET[o], data[i]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileSystem getFileSystem()\n{\r\n    return (S3AFileSystem) super.getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "validateSSLChannelMode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validateSSLChannelMode()\n{\r\n    if (this.sslChannelMode == OpenSSL) {\r\n        assumeTrue(NativeCodeLoader.isNativeCodeLoaded() && NativeCodeLoader.buildSupportsOpenssl());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "testReadPolicyInFS",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testReadPolicyInFS() throws Throwable\n{\r\n    describe(\"Verify the read policy is being consistently set\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    assertEquals(S3AInputPolicy.getPolicy(seekPolicy), fs.getInputPolicy());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "testReadAcrossReadahead",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testReadAcrossReadahead() throws Throwable\n{\r\n    describe(\"Sets up a read which will span the active readahead\" + \" and the rest of the file.\");\r\n    Path path = path(\"testReadAcrossReadahead\");\r\n    writeTestDataset(path);\r\n    FileSystem fs = getFileSystem();\r\n    try (FSDataInputStream in = fs.open(path)) {\r\n        final byte[] temp = new byte[5];\r\n        in.readByte();\r\n        int offset = READAHEAD - 1;\r\n        in.readFully(offset, temp);\r\n        assertDatasetEquals(offset, \"read spanning boundary\", temp, temp.length);\r\n    }\r\n    try (FSDataInputStream in = fs.open(path)) {\r\n        final byte[] temp = new byte[5];\r\n        readAtEndAndReturn(in);\r\n        assertEquals(\"current position\", 1, (int) (in.getPos()));\r\n        in.readFully(READAHEAD, temp);\r\n        assertDatasetEquals(READAHEAD, \"read exactly on boundary\", temp, temp.length);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "testReadSingleByteAcrossReadahead",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testReadSingleByteAcrossReadahead() throws Throwable\n{\r\n    describe(\"Read over boundary using read()/readByte() calls.\");\r\n    Path path = path(\"testReadSingleByteAcrossReadahead\");\r\n    writeTestDataset(path);\r\n    FileSystem fs = getFileSystem();\r\n    try (FSDataInputStream in = fs.open(path)) {\r\n        final byte[] b0 = new byte[1];\r\n        readAtEndAndReturn(in);\r\n        in.seek(READAHEAD - 1);\r\n        b0[0] = in.readByte();\r\n        assertDatasetEquals(READAHEAD - 1, \"read before end of boundary\", b0, b0.length);\r\n        b0[0] = in.readByte();\r\n        assertDatasetEquals(READAHEAD, \"read at end of boundary\", b0, b0.length);\r\n        b0[0] = in.readByte();\r\n        assertDatasetEquals(READAHEAD + 1, \"read after end of boundary\", b0, b0.length);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "testSeekToReadaheadAndRead",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSeekToReadaheadAndRead() throws Throwable\n{\r\n    describe(\"Seek to just before readahead limit and call\" + \" InputStream.read(byte[])\");\r\n    Path path = path(\"testSeekToReadaheadAndRead\");\r\n    FileSystem fs = getFileSystem();\r\n    writeTestDataset(path);\r\n    try (FSDataInputStream in = fs.open(path)) {\r\n        readAtEndAndReturn(in);\r\n        final byte[] temp = new byte[5];\r\n        int offset = READAHEAD - 1;\r\n        in.seek(offset);\r\n        int l = in.read(temp);\r\n        assertTrue(\"Reading in temp data\", l > 0);\r\n        LOG.info(\"Read of byte array at offset {} returned {} bytes\", offset, l);\r\n        assertDatasetEquals(offset, \"read at end of boundary\", temp, l);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "testSeekToReadaheadExactlyAndRead",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSeekToReadaheadExactlyAndRead() throws Throwable\n{\r\n    describe(\"Seek to exactly the readahead limit and call\" + \" InputStream.read(byte[])\");\r\n    Path path = path(\"testSeekToReadaheadExactlyAndRead\");\r\n    FileSystem fs = getFileSystem();\r\n    writeTestDataset(path);\r\n    try (FSDataInputStream in = fs.open(path)) {\r\n        readAtEndAndReturn(in);\r\n        final byte[] temp = new byte[5];\r\n        int offset = READAHEAD;\r\n        in.seek(offset);\r\n        int l = in.read(temp);\r\n        LOG.info(\"Read of byte array at offset {} returned {} bytes\", offset, l);\r\n        assertTrue(\"Reading in temp data\", l > 0);\r\n        assertDatasetEquals(offset, \"read at end of boundary\", temp, l);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "testSeekToReadaheadExactlyAndReadByte",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSeekToReadaheadExactlyAndReadByte() throws Throwable\n{\r\n    describe(\"Seek to exactly the readahead limit and call\" + \" readByte()\");\r\n    Path path = path(\"testSeekToReadaheadExactlyAndReadByte\");\r\n    FileSystem fs = getFileSystem();\r\n    writeTestDataset(path);\r\n    try (FSDataInputStream in = fs.open(path)) {\r\n        readAtEndAndReturn(in);\r\n        final byte[] temp = new byte[1];\r\n        int offset = READAHEAD;\r\n        in.seek(offset);\r\n        temp[0] = in.readByte();\r\n        assertDatasetEquals(READAHEAD, \"read at end of boundary\", temp, 1);\r\n        LOG.info(\"Read of byte at offset {} returned expected value\", offset);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "writeTestDataset",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void writeTestDataset(final Path path) throws IOException\n{\r\n    ContractTestUtils.writeDataset(getFileSystem(), path, DATASET, DATASET_LEN, READAHEAD, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "convertKeyToMd5",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String convertKeyToMd5(FileSystem fs)\n{\r\n    String base64Key = fs.getConf().getTrimmed(S3_ENCRYPTION_KEY);\r\n    byte[] key = Base64.decodeBase64(base64Key);\r\n    byte[] md5 = DigestUtils.md5(key);\r\n    return Base64.encodeBase64String(md5).trim();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertEncrypted",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void assertEncrypted(S3AFileSystem fs, final Path path, final S3AEncryptionMethods algorithm, final String kmsKeyArn) throws IOException\n{\r\n    ObjectMetadata md = fs.getObjectMetadata(path);\r\n    String details = String.format(\"file %s with encryption algorithm %s and key %s\", path, md.getSSEAlgorithm(), md.getSSEAwsKmsKeyId());\r\n    switch(algorithm) {\r\n        case SSE_C:\r\n            assertNull(\"Metadata algorithm should have been null in \" + details, md.getSSEAlgorithm());\r\n            assertEquals(\"Wrong SSE-C algorithm in \" + details, SSE_C_ALGORITHM, md.getSSECustomerAlgorithm());\r\n            String md5Key = convertKeyToMd5(fs);\r\n            assertEquals(\"getSSECustomerKeyMd5() wrong in \" + details, md5Key, md.getSSECustomerKeyMd5());\r\n            break;\r\n        case SSE_KMS:\r\n            assertEquals(\"Wrong algorithm in \" + details, AWS_KMS_SSE_ALGORITHM, md.getSSEAlgorithm());\r\n            assertEquals(\"Wrong KMS key in \" + details, kmsKeyArn, md.getSSEAwsKmsKeyId());\r\n            break;\r\n        default:\r\n            assertEquals(\"AES256\", md.getSSEAlgorithm());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testWithRegionConfig",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testWithRegionConfig()\n{\r\n    getFileSystem().getConf().set(AWS_REGION, AWS_REGION_TEST);\r\n    AwsClientBuilder.EndpointConfiguration epr = createEpr(AWS_ENDPOINT_TEST, getFileSystem().getConf().getTrimmed(AWS_REGION));\r\n    Assertions.assertThat(epr.getSigningRegion()).describedAs(\"There is a region mismatch\").isEqualTo(getFileSystem().getConf().get(AWS_REGION));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testWithoutRegionConfig",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testWithoutRegionConfig()\n{\r\n    getFileSystem().getConf().unset(AWS_REGION);\r\n    AwsClientBuilder.EndpointConfiguration eprRandom = createEpr(AWS_ENDPOINT_TEST_WITH_REGION, getFileSystem().getConf().getTrimmed(AWS_REGION));\r\n    String regionFromEndpoint = AwsHostNameUtils.parseRegionFromAwsPartitionPattern(AWS_ENDPOINT_TEST_WITH_REGION);\r\n    Assertions.assertThat(eprRandom.getSigningRegion()).describedAs(\"There is a region mismatch\").isNotEqualTo(getFileSystem().getConf().get(AWS_REGION)).isEqualTo(regionFromEndpoint);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createEpr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AwsClientBuilder.EndpointConfiguration createEpr(String endpoint, String awsRegion)\n{\r\n    return DefaultS3ClientFactory.createEndpointConfiguration(endpoint, new ClientConfiguration(), awsRegion);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testInvalidRegionDefaultEndpoint",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testInvalidRegionDefaultEndpoint() throws Throwable\n{\r\n    describe(\"Create a client with an invalid region and the default endpoint\");\r\n    Configuration conf = getConfiguration();\r\n    conf.set(AWS_REGION, MARS_NORTH_2);\r\n    createMarsNorth2Client(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testUnsetRegionDefaultEndpoint",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testUnsetRegionDefaultEndpoint() throws Throwable\n{\r\n    describe(\"Create a client with no region and the default endpoint\");\r\n    Configuration conf = getConfiguration();\r\n    conf.unset(AWS_REGION);\r\n    createS3Client(conf, DEFAULT_ENDPOINT, AWS_S3_CENTRAL_REGION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testBlankRegionTriggersSDKResolution",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testBlankRegionTriggersSDKResolution() throws Throwable\n{\r\n    describe(\"Create a client with a blank region and the default endpoint.\" + \" This will trigger the SDK Resolution chain\");\r\n    Configuration conf = getConfiguration();\r\n    conf.set(AWS_REGION, \"\");\r\n    System.setProperty(AWS_REGION_SYSPROP, MARS_NORTH_2);\r\n    try {\r\n        createMarsNorth2Client(conf);\r\n    } finally {\r\n        System.clearProperty(AWS_REGION_SYSPROP);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createMarsNorth2Client",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createMarsNorth2Client(Configuration conf) throws Exception\n{\r\n    AmazonS3 client = createS3Client(conf, DEFAULT_ENDPOINT, MARS_NORTH_2);\r\n    intercept(IllegalArgumentException.class, MARS_NORTH_2, client::getRegion);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createS3Client",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AmazonS3 createS3Client(Configuration conf, String endpoint, String expectedRegion) throws URISyntaxException, IOException\n{\r\n    DefaultS3ClientFactory factory = new DefaultS3ClientFactory();\r\n    factory.setConf(conf);\r\n    S3ClientFactory.S3ClientCreationParameters parameters = new S3ClientFactory.S3ClientCreationParameters().withCredentialSet(new AnonymousAWSCredentialsProvider()).withEndpoint(endpoint).withMetrics(new EmptyS3AStatisticsContext().newStatisticsFromAwsSdk());\r\n    AmazonS3 client = factory.createS3Client(new URI(\"s3a://localhost/\"), parameters);\r\n    Assertions.assertThat(client.getRegionName()).describedAs(\"Client region name\").isEqualTo(expectedRegion);\r\n    return client;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    S3ATestUtils.disableFilesystemCaching(conf);\r\n    conf.setBoolean(Constants.ENABLE_MULTI_DELETE, true);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertIsEOF",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertIsEOF(String operation, int readResult)\n{\r\n    assertEquals(\"Expected EOF from \" + operation + \"; got char \" + (char) readResult, -1, readResult);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testMultiObjectDeleteNoFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMultiObjectDeleteNoFile() throws Throwable\n{\r\n    describe(\"Deleting a missing object\");\r\n    removeKeys(getFileSystem(), \"ITestS3AFailureHandling/missingFile\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testMultiObjectDeleteLargeNumKeys",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testMultiObjectDeleteLargeNumKeys() throws Exception\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path path = path(\"largeDir\");\r\n    mkdirs(path);\r\n    createFiles(fs, path, 1, 1005, 0);\r\n    RemoteIterator<LocatedFileStatus> locatedFileStatusRemoteIterator = fs.listFiles(path, false);\r\n    List<String> keys = toList(mappingRemoteIterator(locatedFileStatusRemoteIterator, locatedFileStatus -> fs.pathToKey(locatedFileStatus.getPath())));\r\n    Long bulkDeleteReqBefore = getNumberOfBulkDeleteRequestsMadeTillNow(fs);\r\n    try (AuditSpan span = span()) {\r\n        fs.removeKeys(buildDeleteRequest(keys.toArray(new String[0])), false);\r\n    }\r\n    Long bulkDeleteReqAfter = getNumberOfBulkDeleteRequestsMadeTillNow(fs);\r\n    Assertions.assertThat(bulkDeleteReqAfter - bulkDeleteReqBefore).describedAs(\"Number of batched bulk delete requests\").isEqualTo(5);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getNumberOfBulkDeleteRequestsMadeTillNow",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Long getNumberOfBulkDeleteRequestsMadeTillNow(S3AFileSystem fs)\n{\r\n    return fs.getIOStatistics().counters().get(StoreStatisticNames.OBJECT_BULK_DELETE_REQUEST);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "removeKeys",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void removeKeys(S3AFileSystem fileSystem, String... keys) throws IOException\n{\r\n    try (AuditSpan span = span()) {\r\n        fileSystem.removeKeys(buildDeleteRequest(keys), false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "buildDeleteRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<DeleteObjectsRequest.KeyVersion> buildDeleteRequest(final String[] keys)\n{\r\n    List<DeleteObjectsRequest.KeyVersion> request = new ArrayList<>(keys.length);\r\n    for (String key : keys) {\r\n        request.add(new DeleteObjectsRequest.KeyVersion(key));\r\n    }\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testMultiObjectDeleteSomeFiles",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testMultiObjectDeleteSomeFiles() throws Throwable\n{\r\n    Path valid = path(\"ITestS3AFailureHandling/validFile\");\r\n    touch(getFileSystem(), valid);\r\n    NanoTimer timer = new NanoTimer();\r\n    removeKeys(getFileSystem(), getFileSystem().pathToKey(valid), \"ITestS3AFailureHandling/missingFile\");\r\n    timer.end(\"removeKeys\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "maybeGetCsvPath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path maybeGetCsvPath()\n{\r\n    Configuration conf = getConfiguration();\r\n    String csvFile = conf.getTrimmed(KEY_CSVTEST_FILE, DEFAULT_CSVTEST_FILE);\r\n    Assume.assumeTrue(\"CSV test file is not the default\", DEFAULT_CSVTEST_FILE.equals(csvFile));\r\n    return new Path(csvFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testMultiObjectDeleteNoPermissions",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMultiObjectDeleteNoPermissions() throws Throwable\n{\r\n    describe(\"Delete the landsat CSV file and expect it to fail\");\r\n    Path csvPath = maybeGetCsvPath();\r\n    S3AFileSystem fs = (S3AFileSystem) csvPath.getFileSystem(getConfiguration());\r\n    fs.getAuditSpanSource().createSpan(StoreStatisticNames.OP_DELETE, csvPath.toString(), null);\r\n    List<DeleteObjectsRequest.KeyVersion> keys = buildDeleteRequest(new String[] { fs.pathToKey(csvPath), \"missing-key.csv\" });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSingleObjectDeleteNoPermissionsTranslated",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSingleObjectDeleteNoPermissionsTranslated() throws Throwable\n{\r\n    describe(\"Delete the landsat CSV file and expect it to fail\");\r\n    Path csvPath = maybeGetCsvPath();\r\n    S3AFileSystem fs = (S3AFileSystem) csvPath.getFileSystem(getConfiguration());\r\n    AccessDeniedException aex = intercept(AccessDeniedException.class, () -> fs.delete(csvPath, false));\r\n    Throwable cause = aex.getCause();\r\n    failIf(cause == null, \"no nested exception\", aex);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "clear",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clear()\n{\r\n    System.clearProperty(KEY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testGetTestProperty",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetTestProperty() throws Throwable\n{\r\n    Configuration conf = new Configuration(false);\r\n    assertEquals(\"a\", getTestProperty(conf, KEY, \"a\"));\r\n    conf.set(KEY, \"\\t b \\n\");\r\n    assertEquals(\"b\", getTestProperty(conf, KEY, \"a\"));\r\n    System.setProperty(KEY, \"c\");\r\n    assertEquals(\"c\", getTestProperty(conf, KEY, \"a\"));\r\n    unsetSysprop();\r\n    assertEquals(\"b\", getTestProperty(conf, KEY, \"a\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testGetTestPropertyLong",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testGetTestPropertyLong() throws Throwable\n{\r\n    Configuration conf = new Configuration(false);\r\n    assertEquals(1, getTestPropertyLong(conf, KEY, 1));\r\n    conf.setInt(KEY, 2);\r\n    assertEquals(2, getTestPropertyLong(conf, KEY, 1));\r\n    System.setProperty(KEY, \"3\");\r\n    assertEquals(3, getTestPropertyLong(conf, KEY, 1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testGetTestPropertyInt",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGetTestPropertyInt() throws Throwable\n{\r\n    Configuration conf = new Configuration(false);\r\n    assertEquals(1, getTestPropertyInt(conf, KEY, 1));\r\n    conf.setInt(KEY, 2);\r\n    assertEquals(2, getTestPropertyInt(conf, KEY, 1));\r\n    System.setProperty(KEY, \"3\");\r\n    assertEquals(3, getTestPropertyInt(conf, KEY, 1));\r\n    conf.unset(KEY);\r\n    assertEquals(3, getTestPropertyInt(conf, KEY, 1));\r\n    unsetSysprop();\r\n    assertEquals(5, getTestPropertyInt(conf, KEY, 5));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testGetTestPropertyBool",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGetTestPropertyBool() throws Throwable\n{\r\n    Configuration conf = new Configuration(false);\r\n    assertTrue(getTestPropertyBool(conf, KEY, true));\r\n    conf.set(KEY, \"\\tfalse \\n\");\r\n    assertFalse(getTestPropertyBool(conf, KEY, true));\r\n    System.setProperty(KEY, \"true\");\r\n    assertTrue(getTestPropertyBool(conf, KEY, true));\r\n    unsetSysprop();\r\n    assertEquals(\"false\", getTestProperty(conf, KEY, \"true\"));\r\n    conf.unset(KEY);\r\n    assertTrue(getTestPropertyBool(conf, KEY, true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "unsetSysprop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void unsetSysprop()\n{\r\n    System.setProperty(KEY, UNSET_PROPERTY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics",
  "methodName" : "testSaveStatisticsLocal",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSaveStatisticsLocal() throws Throwable\n{\r\n    IOStatisticsSnapshot iostats = FILESYSTEM_IOSTATS;\r\n    iostats.aggregate(getFileSystem().getIOStatistics());\r\n    JsonSerialization<IOStatisticsSnapshot> serializer = IOStatisticsSnapshot.serializer();\r\n    File outputDir = createOutputDir();\r\n    File file = new File(outputDir, outputFilename());\r\n    serializer.save(file, iostats);\r\n    IOStatisticsSnapshot loaded = serializer.load(file);\r\n    String s = serializer.toString(loaded);\r\n    LOG.info(\"Deserialized statistics in {}\\n{}\", file, s);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics",
  "methodName" : "testSaveStatisticsS3",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSaveStatisticsS3() throws Throwable\n{\r\n    IOStatisticsSnapshot iostats = FILESYSTEM_IOSTATS;\r\n    JsonSerialization<IOStatisticsSnapshot> serializer = IOStatisticsSnapshot.serializer();\r\n    Path path = methodPath();\r\n    serializer.save(getFileSystem(), path, iostats, true);\r\n    IOStatisticsSnapshot loaded = serializer.load(getFileSystem(), path);\r\n    String s = serializer.toString(loaded);\r\n    LOG.info(\"Deserialized statistics in {}\\n{}\", path, s);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics",
  "methodName" : "createOutputDir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "File createOutputDir()\n{\r\n    String target = System.getProperty(\"test.build.dir\", \"target\");\r\n    File buildDir = new File(target, this.getClass().getSimpleName()).getAbsoluteFile();\r\n    buildDir.mkdirs();\r\n    return buildDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics",
  "methodName" : "outputFilename",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String outputFilename()\n{\r\n    LocalDateTime now = LocalDateTime.now();\r\n    DateTimeFormatter formatter = new DateTimeFormatterBuilder().parseCaseInsensitive().append(ISO_LOCAL_DATE).appendLiteral('-').appendValue(HOUR_OF_DAY, 2).appendLiteral('.').appendValue(MINUTE_OF_HOUR, 2).optionalStart().appendLiteral('.').appendValue(SECOND_OF_MINUTE, 2).optionalStart().appendFraction(NANO_OF_SECOND, 0, 9, true).toFormatter();\r\n    return String.format(\"iostats-%s.json\", now.format(formatter));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testLocalFilesOnly",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testLocalFilesOnly() throws Throwable\n{\r\n    describe(\"Copying into other file systems must fail\");\r\n    Path dest = fileToPath(createTempDirectory(\"someDir\"));\r\n    intercept(IllegalArgumentException.class, () -> getFileSystem().copyFromLocalFile(false, true, dest, dest));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testOnlyFromLocal",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testOnlyFromLocal() throws Throwable\n{\r\n    describe(\"Copying must be from a local file system\");\r\n    File source = createTempFile(\"someFile\");\r\n    Path dest = copyFromLocal(source, true);\r\n    intercept(IllegalArgumentException.class, () -> getFileSystem().copyFromLocalFile(true, true, dest, dest));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    assumeSessionTestsEnabled(getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    S3AUtils.closeAutocloseables(LOG, credentials);\r\n    super.teardown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    conf.set(DELEGATION_TOKEN_BINDING, DELEGATION_TOKEN_SESSION_BINDING);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSTS",
  "errType" : [ "AWSS3IOException|AWSBadRequestException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testSTS() throws IOException\n{\r\n    Configuration conf = getContract().getConf();\r\n    S3AFileSystem testFS = getFileSystem();\r\n    credentials = testFS.shareCredentials(\"testSTS\");\r\n    String bucket = testFS.getBucket();\r\n    AWSSecurityTokenServiceClientBuilder builder = STSClientFactory.builder(conf, bucket, credentials, getStsEndpoint(conf), getStsRegion(conf));\r\n    STSClientFactory.STSClient clientConnection = STSClientFactory.createClientConnection(builder.build(), new Invoker(new S3ARetryPolicy(conf), Invoker.LOG_EVENT));\r\n    Credentials sessionCreds = clientConnection.requestSessionCredentials(TEST_SESSION_TOKEN_DURATION_SECONDS, TimeUnit.SECONDS);\r\n    Configuration conf2 = new Configuration(conf);\r\n    S3AUtils.clearBucketOption(conf2, bucket, AWS_CREDENTIALS_PROVIDER);\r\n    S3AUtils.clearBucketOption(conf2, bucket, ACCESS_KEY);\r\n    S3AUtils.clearBucketOption(conf2, bucket, SECRET_KEY);\r\n    S3AUtils.clearBucketOption(conf2, bucket, SESSION_TOKEN);\r\n    MarshalledCredentials mc = fromSTSCredentials(sessionCreds);\r\n    updateConfigWithSessionCreds(conf2, mc);\r\n    conf2.set(AWS_CREDENTIALS_PROVIDER, TEMPORARY_AWS_CREDENTIALS);\r\n    try (S3AFileSystem fs = S3ATestUtils.createTestFileSystem(conf2)) {\r\n        createAndVerifyFile(fs, path(\"testSTS\"), TEST_FILE_SIZE);\r\n    }\r\n    conf2.set(SESSION_TOKEN, \"invalid-\" + sessionCreds.getSessionToken());\r\n    try (S3AFileSystem fs = S3ATestUtils.createTestFileSystem(conf2)) {\r\n        createAndVerifyFile(fs, path(\"testSTSInvalidToken\"), TEST_FILE_SIZE);\r\n        fail(\"Expected an access exception, but file access to \" + fs.getUri() + \" was allowed: \" + fs);\r\n    } catch (AWSS3IOException | AWSBadRequestException ex) {\r\n        LOG.info(\"Expected Exception: {}\", ex.toString());\r\n        LOG.debug(\"Expected Exception: {}\", ex, ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getStsEndpoint",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getStsEndpoint(final Configuration conf)\n{\r\n    return conf.getTrimmed(ASSUMED_ROLE_STS_ENDPOINT, DEFAULT_ASSUMED_ROLE_STS_ENDPOINT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getStsRegion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getStsRegion(final Configuration conf)\n{\r\n    return conf.getTrimmed(ASSUMED_ROLE_STS_ENDPOINT_REGION, ASSUMED_ROLE_STS_ENDPOINT_REGION_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testTemporaryCredentialValidation",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testTemporaryCredentialValidation() throws Throwable\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(ACCESS_KEY, \"accesskey\");\r\n    conf.set(SECRET_KEY, \"secretkey\");\r\n    conf.set(SESSION_TOKEN, \"\");\r\n    LambdaTestUtils.intercept(CredentialInitializationException.class, () -> new TemporaryAWSCredentialsProvider(conf).getCredentials());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSessionTokenPropagation",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSessionTokenPropagation() throws Exception\n{\r\n    Configuration conf = new Configuration(getContract().getConf());\r\n    MarshalledCredentials sc = requestSessionCredentials(conf, getFileSystem().getBucket());\r\n    updateConfigWithSessionCreds(conf, sc);\r\n    conf.set(AWS_CREDENTIALS_PROVIDER, TEMPORARY_AWS_CREDENTIALS);\r\n    try (S3AFileSystem fs = S3ATestUtils.createTestFileSystem(conf)) {\r\n        createAndVerifyFile(fs, path(\"testSTS\"), TEST_FILE_SIZE);\r\n        SessionTokenIdentifier identifier = (SessionTokenIdentifier) fs.getDelegationToken(\"\").decodeIdentifier();\r\n        String ids = identifier.toString();\r\n        assertThat(\"origin in \" + ids, identifier.getOrigin(), containsString(CREDENTIALS_CONVERTED_TO_DELEGATION_TOKEN));\r\n        assertCredentialsEqual(\"Reissued credentials in \" + ids, sc, identifier.getMarshalledCredentials());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSessionTokenExpiry",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSessionTokenExpiry() throws Exception\n{\r\n    Configuration conf = new Configuration(getContract().getConf());\r\n    MarshalledCredentials sc = requestSessionCredentials(conf, getFileSystem().getBucket());\r\n    long permittedExpiryOffset = 60;\r\n    OffsetDateTime expirationTimestamp = sc.getExpirationDateTime().get();\r\n    OffsetDateTime localTimestamp = OffsetDateTime.now();\r\n    assertTrue(\"local time of \" + localTimestamp + \" is after expiry time of \" + expirationTimestamp, localTimestamp.isBefore(expirationTimestamp));\r\n    Duration actualDuration = Duration.between(localTimestamp, expirationTimestamp);\r\n    Duration offset = actualDuration.minus(TEST_SESSION_TOKEN_DURATION);\r\n    assertThat(\"Duration of session \" + actualDuration + \" out of expected range of with \" + offset + \" this host's clock may be wrong.\", offset.getSeconds(), Matchers.lessThanOrEqualTo(permittedExpiryOffset));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "updateConfigWithSessionCreds",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateConfigWithSessionCreds(final Configuration conf, final MarshalledCredentials sc)\n{\r\n    unsetHadoopCredentialProviders(conf);\r\n    sc.setSecretsInConfiguration(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testInvalidSTSBinding",
  "errType" : [ "AWSBadRequestException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testInvalidSTSBinding() throws Exception\n{\r\n    Configuration conf = new Configuration(getContract().getConf());\r\n    MarshalledCredentials sc = requestSessionCredentials(conf, getFileSystem().getBucket());\r\n    toAWSCredentials(sc, MarshalledCredentials.CredentialTypeRequired.AnyNonEmpty, \"\");\r\n    updateConfigWithSessionCreds(conf, sc);\r\n    conf.set(AWS_CREDENTIALS_PROVIDER, TEMPORARY_AWS_CREDENTIALS);\r\n    conf.set(SESSION_TOKEN, \"invalid-\" + sc.getSessionToken());\r\n    S3AFileSystem fs = null;\r\n    try {\r\n        fs = S3ATestUtils.createTestFileSystem(conf);\r\n        Path path = path(\"testSTSInvalidToken\");\r\n        createAndVerifyFile(fs, path, TEST_FILE_SIZE);\r\n        fail(\"request to create a file should have failed\");\r\n    } catch (AWSBadRequestException expected) {\r\n    } finally {\r\n        IOUtils.closeStream(fs);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSessionCredentialsBadRegion",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSessionCredentialsBadRegion() throws Throwable\n{\r\n    describe(\"Create a session with a bad region and expect failure\");\r\n    expectedSessionRequestFailure(IllegalArgumentException.class, DEFAULT_DELEGATION_TOKEN_ENDPOINT, \"us-west-12\", \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSessionCredentialsWrongRegion",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSessionCredentialsWrongRegion() throws Throwable\n{\r\n    describe(\"Create a session with the wrong region and expect failure\");\r\n    expectedSessionRequestFailure(AccessDeniedException.class, STS_LONDON, EU_IRELAND, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSessionCredentialsWrongCentralRegion",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSessionCredentialsWrongCentralRegion() throws Throwable\n{\r\n    describe(\"Create a session sts.amazonaws.com; region='us-west-1'\");\r\n    expectedSessionRequestFailure(IllegalArgumentException.class, \"sts.amazonaws.com\", \"us-west-1\", \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSessionCredentialsRegionNoEndpoint",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSessionCredentialsRegionNoEndpoint() throws Throwable\n{\r\n    describe(\"Create a session with a bad region and expect fast failure\");\r\n    expectedSessionRequestFailure(IllegalArgumentException.class, \"\", EU_IRELAND, EU_IRELAND);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSessionCredentialsRegionBadEndpoint",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSessionCredentialsRegionBadEndpoint() throws Throwable\n{\r\n    describe(\"Create a session with a bad region and expect fast failure\");\r\n    IllegalArgumentException ex = expectedSessionRequestFailure(IllegalArgumentException.class, \" \", EU_IRELAND, \"\");\r\n    LOG.info(\"Outcome: \", ex);\r\n    if (!(ex.getCause() instanceof URISyntaxException)) {\r\n        throw ex;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSessionCredentialsEndpointNoRegion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSessionCredentialsEndpointNoRegion() throws Throwable\n{\r\n    expectedSessionRequestFailure(IllegalArgumentException.class, STS_LONDON, \"\", STS_LONDON);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "expectedSessionRequestFailure",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "E expectedSessionRequestFailure(final Class<E> clazz, final String endpoint, final String region, final String exceptionText) throws Exception\n{\r\n    try (AWSCredentialProviderList parentCreds = getFileSystem().shareCredentials(\"test\");\r\n        DurationInfo ignored = new DurationInfo(LOG, \"requesting credentials\")) {\r\n        Configuration conf = new Configuration(getContract().getConf());\r\n        ClientConfiguration awsConf = S3AUtils.createAwsConf(conf, null, AWS_SERVICE_IDENTIFIER_STS);\r\n        return intercept(clazz, exceptionText, () -> {\r\n            AWSSecurityTokenService tokenService = STSClientFactory.builder(parentCreds, awsConf, endpoint, region).build();\r\n            Invoker invoker = new Invoker(new S3ARetryPolicy(conf), LOG_AT_ERROR);\r\n            STSClientFactory.STSClient stsClient = STSClientFactory.createClientConnection(tokenService, invoker);\r\n            return stsClient.requestSessionCredentials(30, TimeUnit.MINUTES);\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testTemporaryCredentialValidationOnLoad",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testTemporaryCredentialValidationOnLoad() throws Throwable\n{\r\n    Configuration conf = new Configuration();\r\n    unsetHadoopCredentialProviders(conf);\r\n    conf.set(ACCESS_KEY, \"aaa\");\r\n    conf.set(SECRET_KEY, \"bbb\");\r\n    conf.set(SESSION_TOKEN, \"\");\r\n    final MarshalledCredentials sc = MarshalledCredentialBinding.fromFileSystem(null, conf);\r\n    intercept(IOException.class, MarshalledCredentials.INVALID_CREDENTIALS, () -> {\r\n        sc.validate(\"\", MarshalledCredentials.CredentialTypeRequired.SessionOnly);\r\n        return sc.toString();\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEmptyTemporaryCredentialValidation",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testEmptyTemporaryCredentialValidation() throws Throwable\n{\r\n    Configuration conf = new Configuration();\r\n    unsetHadoopCredentialProviders(conf);\r\n    conf.set(ACCESS_KEY, \"\");\r\n    conf.set(SECRET_KEY, \"\");\r\n    conf.set(SESSION_TOKEN, \"\");\r\n    final MarshalledCredentials sc = MarshalledCredentialBinding.fromFileSystem(null, conf);\r\n    intercept(IOException.class, MarshalledCredentialBinding.NO_AWS_CREDENTIALS, () -> {\r\n        sc.validate(\"\", MarshalledCredentials.CredentialTypeRequired.SessionOnly);\r\n        return sc.toString();\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSessionRequestExceptionTranslation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSessionRequestExceptionTranslation() throws Exception\n{\r\n    intercept(IOException.class, () -> requestSessionCredentials(getConfiguration(), getFileSystem().getBucket(), 10));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "parseAccessPointFromArn",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void parseAccessPointFromArn() throws IllegalArgumentException\n{\r\n    describe(\"Parse AccessPoint ArnResource from arn string\");\r\n    String accessPoint = \"testAp\";\r\n    String[][] regionPartitionEndpoints = new String[][] { { Regions.EU_WEST_1.getName(), \"aws\" }, { Regions.US_GOV_EAST_1.getName(), \"aws-us-gov\" }, { Regions.CN_NORTH_1.getName(), \"aws-cn\" } };\r\n    for (String[] testPair : regionPartitionEndpoints) {\r\n        String region = testPair[0];\r\n        String partition = testPair[1];\r\n        ArnResource resource = getArnResourceFrom(partition, region, MOCK_ACCOUNT, accessPoint);\r\n        assertEquals(\"Access Point name does not match\", accessPoint, resource.getName());\r\n        assertEquals(\"Account Id does not match\", MOCK_ACCOUNT, resource.getOwnerAccountId());\r\n        assertEquals(\"Region does not match\", region, resource.getRegion());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "makeSureEndpointHasTheCorrectFormat",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void makeSureEndpointHasTheCorrectFormat()\n{\r\n    ArnResource accessPoint = getArnResourceFrom(\"aws\", \"eu-west-1\", MOCK_ACCOUNT, \"test\");\r\n    String expected = \"s3-accesspoint.eu-west-1.amazonaws.com\";\r\n    Assertions.assertThat(accessPoint.getEndpoint()).describedAs(\"Endpoint has invalid format. Access Point requests will not work\").isEqualTo(expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "invalidARNsMustThrow",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void invalidARNsMustThrow() throws Exception\n{\r\n    describe(\"Using an invalid ARN format must throw when initializing an ArnResource.\");\r\n    intercept(IllegalArgumentException.class, () -> ArnResource.accessPointFromArn(\"invalid:arn:resource\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getArnResourceFrom",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ArnResource getArnResourceFrom(String partition, String region, String accountId, String resourceName)\n{\r\n    String arn = String.format(\"arn:%s:s3:%s:%s:accesspoint/%s\", partition, region, accountId, resourceName);\r\n    return ArnResource.accessPointFromArn(arn);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "describe",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void describe(String message)\n{\r\n    LOG.info(message);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "initOutput",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initOutput(Path out) throws IOException\n{\r\n    super.initOutput(out);\r\n    setOutputPath(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getDestinationFS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getDestinationFS(Path out, Configuration config) throws IOException\n{\r\n    return out.getFileSystem(config);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testInputStreamReadRetryForException",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testInputStreamReadRetryForException() throws IOException\n{\r\n    S3AInputStream s3AInputStream = getMockedS3AInputStream();\r\n    assertEquals(\"'a' from the test input stream 'ab' should be the first \" + \"character being read\", INPUT.charAt(0), s3AInputStream.read());\r\n    assertEquals(\"'b' from the test input stream 'ab' should be the second \" + \"character being read\", INPUT.charAt(1), s3AInputStream.read());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testInputStreamReadLengthRetryForException",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testInputStreamReadLengthRetryForException() throws IOException\n{\r\n    byte[] result = new byte[INPUT.length()];\r\n    S3AInputStream s3AInputStream = getMockedS3AInputStream();\r\n    s3AInputStream.read(result, 0, INPUT.length());\r\n    assertArrayEquals(\"The read result should equals to the test input stream content\", INPUT.getBytes(), result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testInputStreamReadFullyRetryForException",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testInputStreamReadFullyRetryForException() throws IOException\n{\r\n    byte[] result = new byte[INPUT.length()];\r\n    S3AInputStream s3AInputStream = getMockedS3AInputStream();\r\n    s3AInputStream.readFully(0, result);\r\n    assertArrayEquals(\"The read result should equals to the test input stream content\", INPUT.getBytes(), result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getMockedS3AInputStream",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "S3AInputStream getMockedS3AInputStream()\n{\r\n    Path path = new Path(\"test-path\");\r\n    String eTag = \"test-etag\";\r\n    String versionId = \"test-version-id\";\r\n    String owner = \"test-owner\";\r\n    S3AFileStatus s3AFileStatus = new S3AFileStatus(INPUT.length(), 0, path, INPUT.length(), owner, eTag, versionId);\r\n    S3ObjectAttributes s3ObjectAttributes = new S3ObjectAttributes(fs.getBucket(), path, fs.pathToKey(path), fs.getS3EncryptionAlgorithm(), new EncryptionSecrets().getEncryptionKey(), eTag, versionId, INPUT.length());\r\n    S3AReadOpContext s3AReadOpContext = fs.createReadContext(s3AFileStatus, S3AInputPolicy.Normal, ChangeDetectionPolicy.getPolicy(fs.getConf()), 100, NoopSpan.INSTANCE);\r\n    return new S3AInputStream(s3AReadOpContext, s3ObjectAttributes, getMockedInputStreamCallback());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getMockedInputStreamCallback",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "S3AInputStream.InputStreamCallbacks getMockedInputStreamCallback()\n{\r\n    return new S3AInputStream.InputStreamCallbacks() {\r\n\r\n        private final S3Object mockedS3Object = getMockedS3Object();\r\n\r\n        private Integer mockedS3ObjectIndex = 0;\r\n\r\n        @Override\r\n        public S3Object getObject(GetObjectRequest request) {\r\n            mockedS3ObjectIndex++;\r\n            if (mockedS3ObjectIndex == 3) {\r\n                throw new SdkClientException(\"Failed to get S3Object\");\r\n            }\r\n            return mockedS3Object;\r\n        }\r\n\r\n        @Override\r\n        public GetObjectRequest newGetRequest(String key) {\r\n            return new GetObjectRequest(fs.getBucket(), key);\r\n        }\r\n\r\n        @Override\r\n        public void close() {\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getMockedS3Object",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "S3Object getMockedS3Object()\n{\r\n    S3ObjectInputStream objectInputStreamBad1 = getMockedInputStream(true);\r\n    S3ObjectInputStream objectInputStreamBad2 = getMockedInputStream(true);\r\n    S3ObjectInputStream objectInputStreamGood = getMockedInputStream(false);\r\n    return new S3Object() {\r\n\r\n        private final S3ObjectInputStream[] inputStreams = { objectInputStreamBad1, objectInputStreamBad2, objectInputStreamGood };\r\n\r\n        private Integer inputStreamIndex = 0;\r\n\r\n        @Override\r\n        public S3ObjectInputStream getObjectContent() {\r\n            inputStreamIndex++;\r\n            return inputStreams[min(inputStreamIndex, inputStreams.length) - 1];\r\n        }\r\n\r\n        @Override\r\n        public ObjectMetadata getObjectMetadata() {\r\n            ObjectMetadata metadata = new ObjectMetadata();\r\n            metadata.setHeader(\"ETag\", \"test-etag\");\r\n            return metadata;\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getMockedInputStream",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "S3ObjectInputStream getMockedInputStream(boolean triggerFailure)\n{\r\n    return new S3ObjectInputStream(IOUtils.toInputStream(INPUT, StandardCharsets.UTF_8), null) {\r\n\r\n        private final IOException exception = new SSLException(new SocketException(\"Connection reset\"));\r\n\r\n        @Override\r\n        public int read() throws IOException {\r\n            int result = super.read();\r\n            if (triggerFailure) {\r\n                throw exception;\r\n            }\r\n            return result;\r\n        }\r\n\r\n        @Override\r\n        public int read(byte[] b, int off, int len) throws IOException {\r\n            int result = super.read(b, off, len);\r\n            if (triggerFailure) {\r\n                throw exception;\r\n            }\r\n            return result;\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createScaleConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration createScaleConfiguration()\n{\r\n    Configuration conf = super.createScaleConfiguration();\r\n    conf.set(DELEGATION_TOKEN_BINDING, getDelegationBinding());\r\n    conf.setInt(Constants.MAXIMUM_CONNECTIONS, Math.max(THREADS, Constants.DEFAULT_MAXIMUM_CONNECTIONS));\r\n    conf.setInt(Constants.MAX_ERROR_RETRIES, 0);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getDelegationBinding",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDelegationBinding()\n{\r\n    return DELEGATION_TOKEN_SESSION_BINDING;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    assumeSessionTestsEnabled(getConfiguration());\r\n    S3AFileSystem fileSystem = getFileSystem();\r\n    assertNotNull(\"No delegation tokens in FS\", fileSystem.getCanonicalServiceName());\r\n    dataDir = GenericTestUtils.getTestDir(\"kerberos\");\r\n    dataDir.mkdirs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getFilePrefix",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getFilePrefix()\n{\r\n    return \"session\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testCreate10Tokens",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCreate10Tokens() throws Throwable\n{\r\n    File file = fetchTokens(10);\r\n    String csv = FileUtils.readFileToString(file, \"UTF-8\");\r\n    LOG.info(\"CSV data\\n{}\", csv);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testCreateManyTokens",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCreateManyTokens() throws Throwable\n{\r\n    fetchTokens(50000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "fetchTokens",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "File fetchTokens(final int tokens) throws Exception\n{\r\n    File filename = new File(dataDir, getFilePrefix() + \"-\" + tokens + \".csv\");\r\n    fetchTokens(tokens, filename);\r\n    return filename;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "fetchTokens",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void fetchTokens(final int tokens, final File csvFile) throws Exception\n{\r\n    describe(\"Fetching %d tokens, saving log to %s\", tokens, csvFile);\r\n    final FileWriter out = new FileWriter(csvFile);\r\n    Csvout csvout = new Csvout(out, \"\\t\", \"\\n\");\r\n    Outcome.writeSchema(csvout);\r\n    final S3AFileSystem fileSystem = getFileSystem();\r\n    final ContractTestUtils.NanoTimer jobTimer = new ContractTestUtils.NanoTimer();\r\n    for (int i = 0; i < tokens; i++) {\r\n        final int id = i;\r\n        completionService.submit(() -> {\r\n            final long startTime = System.currentTimeMillis();\r\n            final ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n            Exception ex = null;\r\n            try {\r\n                fileSystem.getDelegationToken(\"Count \");\r\n            } catch (IOException e) {\r\n                ex = e;\r\n            }\r\n            timer.end(\"Request\");\r\n            return new Outcome(id, startTime, timer, ex);\r\n        });\r\n    }\r\n    NanoTimerStats stats = new NanoTimerStats(\"Overall\");\r\n    NanoTimerStats success = new NanoTimerStats(\"Successful\");\r\n    NanoTimerStats throttled = new NanoTimerStats(\"Throttled\");\r\n    List<Outcome> throttledEvents = new ArrayList<>();\r\n    for (int i = 0; i < tokens; i++) {\r\n        Outcome outcome = completionService.take().get();\r\n        ContractTestUtils.NanoTimer timer = outcome.timer;\r\n        Exception ex = outcome.exception;\r\n        outcome.writeln(csvout);\r\n        stats.add(timer);\r\n        if (ex != null) {\r\n            LOG.info(\"Throttled at event {}\", i, ex);\r\n            throttled.add(timer);\r\n            throttledEvents.add(outcome);\r\n        } else {\r\n            success.add(timer);\r\n        }\r\n    }\r\n    csvout.close();\r\n    jobTimer.end(\"Execution of fetch calls\");\r\n    LOG.info(\"Summary file is \" + csvFile);\r\n    LOG.info(\"Fetched {} tokens with {} throttle events\\n: {}\\n{}\\n{}\", tokens, throttled.getCount(), stats, throttled, success);\r\n    double duration = jobTimer.duration();\r\n    double iops = tokens * 1.0e9 / duration;\r\n    LOG.info(String.format(\"Effective IO rate is %3f operations/second\", iops));\r\n    if (LOG.isDebugEnabled()) {\r\n        throttledEvents.stream().forEach((outcome -> {\r\n            LOG.debug(\"{}: duration: {}\", outcome.id, outcome.timer.elapsedTimeMs());\r\n        }));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    S3ATestUtils.disableFilesystemCaching(conf);\r\n    patchConfigurationEncryptionSettings(conf);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "patchConfigurationEncryptionSettings",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void patchConfigurationEncryptionSettings(final Configuration conf)\n{\r\n    removeBaseAndBucketOverrides(conf, S3_ENCRYPTION_ALGORITHM, S3_ENCRYPTION_KEY, SERVER_SIDE_ENCRYPTION_ALGORITHM, SERVER_SIDE_ENCRYPTION_KEY);\r\n    conf.set(S3_ENCRYPTION_ALGORITHM, getSSEAlgorithm().getMethod());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "requireEncryptedFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void requireEncryptedFileSystem()\n{\r\n    skipIfEncryptionTestsDisabled(getFileSystem().getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setup",
  "errType" : [ "AccessDeniedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    try {\r\n        super.setup();\r\n        requireEncryptedFileSystem();\r\n    } catch (AccessDeniedException e) {\r\n        skip(\"Bucket does not allow \" + getSSEAlgorithm() + \" encryption method\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEncryptionSettingPropagation",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testEncryptionSettingPropagation() throws Throwable\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    S3AEncryptionMethods algorithm = getEncryptionAlgorithm(fs.getBucket(), fs.getConf());\r\n    assertEquals(\"Configuration has wrong encryption algorithm\", getSSEAlgorithm(), algorithm);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEncryption",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testEncryption() throws Throwable\n{\r\n    requireEncryptedFileSystem();\r\n    validateEncryptionSecrets(getFileSystem().getEncryptionSecrets());\r\n    for (int size : SIZES) {\r\n        validateEncryptionForFilesize(size);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEncryptionOverRename",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testEncryptionOverRename() throws Throwable\n{\r\n    Path src = path(createFilename(1024));\r\n    byte[] data = dataset(1024, 'a', 'z');\r\n    S3AFileSystem fs = getFileSystem();\r\n    EncryptionSecrets secrets = fs.getEncryptionSecrets();\r\n    validateEncryptionSecrets(secrets);\r\n    writeDataset(fs, src, data, data.length, 1024 * 1024, true);\r\n    ContractTestUtils.verifyFileContents(fs, src, data);\r\n    assertEncrypted(src);\r\n    Path targetDir = path(\"target\");\r\n    mkdirs(targetDir);\r\n    fs.rename(src, targetDir);\r\n    Path renamedFile = new Path(targetDir, src.getName());\r\n    ContractTestUtils.verifyFileContents(fs, renamedFile, data);\r\n    assertEncrypted(renamedFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "validateEncryptionSecrets",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void validateEncryptionSecrets(final EncryptionSecrets secrets)\n{\r\n    assertNotNull(\"No encryption secrets for filesystem\", secrets);\r\n    S3AEncryptionMethods sseAlgorithm = getSSEAlgorithm();\r\n    assertEquals(\"Filesystem has wrong encryption algorithm\", sseAlgorithm, secrets.getEncryptionMethod());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "validateEncryptionForFilesize",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void validateEncryptionForFilesize(int len) throws IOException\n{\r\n    describe(\"Create an encrypted file of size \" + len);\r\n    String src = createFilename(len);\r\n    Path path = writeThenReadFile(src, len);\r\n    assertEncrypted(path);\r\n    rm(getFileSystem(), path, false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createFilename",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String createFilename(int len)\n{\r\n    return String.format(\"%s-%04x\", methodName.getMethodName(), len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createFilename",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String createFilename(String name)\n{\r\n    return String.format(\"%s-%s\", methodName.getMethodName(), name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertEncrypted",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertEncrypted(Path path) throws IOException\n{\r\n    String kmsKeyArn = getS3EncryptionKey(getTestBucketName(getConfiguration()), getConfiguration());\r\n    S3AEncryptionMethods algorithm = getSSEAlgorithm();\r\n    EncryptionTestUtils.assertEncrypted(getFileSystem(), path, algorithm, kmsKeyArn);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getSSEAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AEncryptionMethods getSSEAlgorithm()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    S3AFileSystem fs = getFileSystem();\r\n    basePath = methodPath();\r\n    describe(\"Creating test directories and files\");\r\n    emptyDir = new Path(basePath, \"emptyDir\");\r\n    fs.mkdirs(emptyDir);\r\n    emptyFile = new Path(basePath, \"emptyFile.txt\");\r\n    touch(fs, emptyFile);\r\n    subDir = new Path(basePath, \"subDir\");\r\n    subdirFile = new Path(subDir, \"subdirFile.txt\");\r\n    createFile(fs, subdirFile, true, HELLO);\r\n    subDir2 = new Path(subDir, \"subDir2\");\r\n    subdir2File1 = new Path(subDir2, \"subdir2File1.txt\");\r\n    subdir2File2 = new Path(subDir2, \"subdir2File2.txt\");\r\n    createFile(fs, subdir2File1, true, HELLO);\r\n    createFile(fs, subdir2File2, true, HELLO);\r\n    listConfig = new Configuration(getConfiguration());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertListCount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertListCount(final LocatedFileStatusFetcher fetcher, final int expectedListCount)\n{\r\n    IOStatistics iostats = extractStatistics(fetcher);\r\n    LOG.info(\"Statistics of fetcher: {}\", iostats);\r\n    assertThatStatisticCounter(iostats, OBJECT_LIST_REQUEST).describedAs(\"stats of %s\", iostats).isEqualTo(expectedListCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSingleThreadedLocatedFileStatus",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSingleThreadedLocatedFileStatus() throws Throwable\n{\r\n    describe(\"LocatedFileStatusFetcher operations\");\r\n    listConfig.setInt(LIST_STATUS_NUM_THREADS, 1);\r\n    LocatedFileStatusFetcher fetcher = new LocatedFileStatusFetcher(listConfig, new Path[] { basePath }, true, HIDDEN_FILE_FILTER, true);\r\n    Iterable<FileStatus> stats = fetcher.getFileStatuses();\r\n    Assertions.assertThat(stats).describedAs(\"result of located scan\").flatExtracting(FileStatus::getPath).containsExactlyInAnyOrder(emptyFile, subdirFile, subdir2File1, subdir2File2);\r\n    assertListCount(fetcher, EXPECTED_LIST_COUNT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testLocatedFileStatusFourThreads",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testLocatedFileStatusFourThreads() throws Throwable\n{\r\n    int threads = 4;\r\n    describe(\"LocatedFileStatusFetcher with %d\", threads);\r\n    listConfig.setInt(LIST_STATUS_NUM_THREADS, threads);\r\n    LocatedFileStatusFetcher fetcher = new LocatedFileStatusFetcher(listConfig, new Path[] { basePath }, true, EVERYTHING, true);\r\n    Iterable<FileStatus> stats = fetcher.getFileStatuses();\r\n    IOStatistics iostats = extractStatistics(fetcher);\r\n    LOG.info(\"Statistics of fetcher: {}\", iostats);\r\n    Assertions.assertThat(stats).describedAs(\"result of located scan\").isNotNull().flatExtracting(FileStatus::getPath).containsExactlyInAnyOrder(emptyFile, subdirFile, subdir2File1, subdir2File2);\r\n    assertListCount(fetcher, EXPECTED_LIST_COUNT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testLocatedFileStatusScanFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testLocatedFileStatusScanFile() throws Throwable\n{\r\n    describe(\"LocatedFileStatusFetcher with file %s\", subdirFile);\r\n    listConfig.setInt(LIST_STATUS_NUM_THREADS, 16);\r\n    LocatedFileStatusFetcher fetcher = new LocatedFileStatusFetcher(listConfig, new Path[] { subdirFile }, true, TEXT_FILE, true);\r\n    Iterable<FileStatus> stats = fetcher.getFileStatuses();\r\n    Assertions.assertThat(stats).describedAs(\"result of located scan\").isNotNull().flatExtracting(FileStatus::getPath).containsExactly(subdirFile);\r\n    IOStatistics ioStatistics = fetcher.getIOStatistics();\r\n    Assertions.assertThat(ioStatistics).describedAs(\"IO statistics of %s\", fetcher).isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "setupCluster",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setupCluster() throws Exception\n{\r\n    cluster = new MiniKerberizedHadoopCluster();\r\n    cluster.init(new Configuration());\r\n    cluster.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws Exception\n{\r\n    ServiceOperations.stopQuietly(LOG, cluster);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getCluster",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MiniKerberizedHadoopCluster getCluster()\n{\r\n    return cluster;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getDelegationBinding",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDelegationBinding()\n{\r\n    return DELEGATION_TOKEN_SESSION_BINDING;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getTokenKind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getTokenKind()\n{\r\n    return SESSION_TOKEN_KIND;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createConfiguration",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    assumeSessionTestsEnabled(conf);\r\n    disableFilesystemCaching(conf);\r\n    String s3EncryptionMethod;\r\n    try {\r\n        s3EncryptionMethod = getEncryptionAlgorithm(getTestBucketName(conf), conf).getMethod();\r\n    } catch (IOException e) {\r\n        throw new UncheckedIOException(\"Failed to lookup encryption algorithm.\", e);\r\n    }\r\n    String s3EncryptionKey = getS3EncryptionKey(getTestBucketName(conf), conf);\r\n    removeBaseAndBucketOverrides(conf, DELEGATION_TOKEN_BINDING, Constants.S3_ENCRYPTION_ALGORITHM, Constants.S3_ENCRYPTION_KEY, SERVER_SIDE_ENCRYPTION_ALGORITHM, SERVER_SIDE_ENCRYPTION_KEY);\r\n    conf.set(HADOOP_SECURITY_AUTHENTICATION, UserGroupInformation.AuthenticationMethod.KERBEROS.name());\r\n    enableDelegationTokens(conf, getDelegationBinding());\r\n    conf.set(AWS_CREDENTIALS_PROVIDER, \" \");\r\n    if (conf.getBoolean(KEY_ENCRYPTION_TESTS, true)) {\r\n        conf.set(Constants.S3_ENCRYPTION_ALGORITHM, s3EncryptionMethod);\r\n        conf.set(Constants.S3_ENCRYPTION_KEY, s3EncryptionKey);\r\n    }\r\n    conf.set(YarnConfiguration.RM_PRINCIPAL, YARN_RM);\r\n    conf.set(CANNED_ACL, LOG_DELIVERY_WRITE);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    resetUGI();\r\n    UserGroupInformation.setConfiguration(createConfiguration());\r\n    aliceUser = cluster.createAliceUser();\r\n    bobUser = cluster.createBobUser();\r\n    UserGroupInformation.setLoginUser(aliceUser);\r\n    assertSecurityEnabled();\r\n    super.setup();\r\n    S3AFileSystem fs = getFileSystem();\r\n    assertNull(\"Unexpectedly found an S3A token\", lookupS3ADelegationToken(UserGroupInformation.getCurrentUser().getCredentials(), fs.getUri()));\r\n    delegationTokens = instantiateDTSupport(getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    super.teardown();\r\n    ServiceOperations.stopQuietly(LOG, delegationTokens);\r\n    FileSystem.closeAllForUGI(UserGroupInformation.getCurrentUser());\r\n    MiniKerberizedHadoopCluster.closeUserFileSystems(aliceUser);\r\n    MiniKerberizedHadoopCluster.closeUserFileSystems(bobUser);\r\n    cluster.resetUGI();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "encryptionTestEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean encryptionTestEnabled()\n{\r\n    return getConfiguration().getBoolean(KEY_ENCRYPTION_TESTS, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testGetDTfromFileSystem",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testGetDTfromFileSystem() throws Throwable\n{\r\n    describe(\"Enable delegation tokens and request one\");\r\n    delegationTokens.start();\r\n    S3AFileSystem fs = getFileSystem();\r\n    assertNotNull(\"No tokens from \" + fs, fs.getCanonicalServiceName());\r\n    S3ATestUtils.MetricDiff invocationDiff = new S3ATestUtils.MetricDiff(fs, Statistic.INVOCATION_GET_DELEGATION_TOKEN);\r\n    S3ATestUtils.MetricDiff issueDiff = new S3ATestUtils.MetricDiff(fs, Statistic.DELEGATION_TOKENS_ISSUED);\r\n    Token<AbstractS3ATokenIdentifier> token = requireNonNull(fs.getDelegationToken(\"\"), \"no token from filesystem \" + fs);\r\n    assertEquals(\"token kind\", getTokenKind(), token.getKind());\r\n    assertTokenCreationCount(fs, 1);\r\n    final String fsInfo = fs.toString();\r\n    invocationDiff.assertDiffEquals(\"getDelegationToken() in \" + fsInfo, 1);\r\n    issueDiff.assertDiffEquals(\"DTs issued in \" + delegationTokens, 1);\r\n    Text service = delegationTokens.getService();\r\n    assertEquals(\"service name\", service, token.getService());\r\n    Credentials creds = new Credentials();\r\n    creds.addToken(service, token);\r\n    assertEquals(\"retrieve token from \" + creds, token, creds.getToken(service));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testAddTokensFromFileSystem",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testAddTokensFromFileSystem() throws Throwable\n{\r\n    describe(\"verify FileSystem.addDelegationTokens() collects tokens\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    Credentials cred = new Credentials();\r\n    Token<?>[] tokens = fs.addDelegationTokens(YARN_RM, cred);\r\n    assertEquals(\"Number of tokens\", 1, tokens.length);\r\n    Token<?> token = requireNonNull(tokens[0], \"token\");\r\n    LOG.info(\"FS token is {}\", token);\r\n    Text service = delegationTokens.getService();\r\n    Token<? extends TokenIdentifier> retrieved = requireNonNull(cred.getToken(service), \"retrieved token with key \" + service + \"; expected \" + token);\r\n    delegationTokens.start();\r\n    delegationTokens.resetTokenBindingToDT((Token<AbstractS3ATokenIdentifier>) retrieved);\r\n    assertTrue(\"bind to existing DT failed\", delegationTokens.isBoundToDT());\r\n    AWSCredentialProviderList providerList = requireNonNull(delegationTokens.getCredentialProviders(), \"providers\");\r\n    providerList.getCredentials();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testCanRetrieveTokenFromCurrentUserCreds",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testCanRetrieveTokenFromCurrentUserCreds() throws Throwable\n{\r\n    describe(\"Create a DT, add it to the current UGI credentials,\" + \" then retrieve\");\r\n    delegationTokens.start();\r\n    Credentials cred = createDelegationTokens();\r\n    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n    ugi.addCredentials(cred);\r\n    Token<?>[] tokens = cred.getAllTokens().toArray(new Token<?>[0]);\r\n    Token<?> token0 = tokens[0];\r\n    Text service = token0.getService();\r\n    LOG.info(\"Token = \" + token0);\r\n    Token<?> token1 = requireNonNull(ugi.getCredentials().getToken(service), \"Token from \" + service);\r\n    assertEquals(\"retrieved token\", token0, token1);\r\n    assertNotNull(\"token identifier of \" + token1, token1.getIdentifier());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testDTCredentialProviderFromCurrentUserCreds",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testDTCredentialProviderFromCurrentUserCreds() throws Throwable\n{\r\n    describe(\"Add credentials to the current user, \" + \"then verify that they can be found when S3ADelegationTokens binds\");\r\n    Credentials cred = createDelegationTokens();\r\n    assertThat(\"Token size\", cred.getAllTokens(), hasSize(1));\r\n    UserGroupInformation.getCurrentUser().addCredentials(cred);\r\n    delegationTokens.start();\r\n    assertTrue(\"bind to existing DT failed\", delegationTokens.isBoundToDT());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createDelegationTokens",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Credentials createDelegationTokens() throws IOException\n{\r\n    return mkTokens(getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testDelegatedFileSystem",
  "errType" : null,
  "containingMethodsNum" : 40,
  "sourceCodeText" : "void testDelegatedFileSystem() throws Throwable\n{\r\n    describe(\"Delegation tokens can be passed to a new filesystem;\" + \" if role restricted, permissions are tightened.\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    fs.getObjectMetadata(new Path(\"/\"));\r\n    readLandsatMetadata(fs);\r\n    URI uri = fs.getUri();\r\n    Credentials creds = createDelegationTokens();\r\n    final Text tokenKind = getTokenKind();\r\n    AbstractS3ATokenIdentifier origTokenId = requireNonNull(lookupToken(creds, uri, tokenKind), \"original\");\r\n    final UserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\r\n    currentUser.addCredentials(creds);\r\n    requireNonNull(lookupToken(currentUser.getCredentials(), uri, tokenKind), \"user credentials\");\r\n    Configuration conf = new Configuration(getConfiguration());\r\n    String bucket = fs.getBucket();\r\n    disableFilesystemCaching(conf);\r\n    unsetHadoopCredentialProviders(conf);\r\n    removeBaseAndBucketOverrides(bucket, conf, ACCESS_KEY, SECRET_KEY, SESSION_TOKEN, Constants.S3_ENCRYPTION_ALGORITHM, Constants.S3_ENCRYPTION_KEY, SERVER_SIDE_ENCRYPTION_ALGORITHM, SERVER_SIDE_ENCRYPTION_KEY, DELEGATION_TOKEN_ROLE_ARN, DELEGATION_TOKEN_ENDPOINT);\r\n    conf.set(DELEGATION_TOKEN_ENDPOINT, \"http://localhost:8080/\");\r\n    bindProviderList(bucket, conf, CountInvocationsProvider.NAME);\r\n    long originalCount = CountInvocationsProvider.getInvocationCount();\r\n    Path testPath = path(\"testDTFileSystemClient\");\r\n    try (S3AFileSystem delegatedFS = newS3AInstance(uri, conf)) {\r\n        LOG.info(\"Delegated filesystem is: {}\", delegatedFS);\r\n        assertBoundToDT(delegatedFS, tokenKind);\r\n        if (encryptionTestEnabled()) {\r\n            assertNotNull(\"Encryption propagation failed\", delegatedFS.getS3EncryptionAlgorithm());\r\n            assertEquals(\"Encryption propagation failed\", fs.getS3EncryptionAlgorithm(), delegatedFS.getS3EncryptionAlgorithm());\r\n        }\r\n        verifyRestrictedPermissions(delegatedFS);\r\n        executeDelegatedFSOperations(delegatedFS, testPath);\r\n        delegatedFS.mkdirs(testPath);\r\n        S3ATestUtils.MetricDiff issueDiff = new S3ATestUtils.MetricDiff(delegatedFS, Statistic.DELEGATION_TOKENS_ISSUED);\r\n        AbstractS3ATokenIdentifier tokenFromDelegatedFS = requireNonNull(delegatedFS.getDelegationToken(\"\"), \"New token\").decodeIdentifier();\r\n        assertEquals(\"Newly issued token != old one\", origTokenId, tokenFromDelegatedFS);\r\n        issueDiff.assertDiffEquals(\"DTs issued in \" + delegatedFS, 0);\r\n    }\r\n    assertEquals(\"invocation count\", originalCount, CountInvocationsProvider.getInvocationCount());\r\n    try (S3AFileSystem secondDelegate = newS3AInstance(uri, conf)) {\r\n        assertBoundToDT(secondDelegate, tokenKind);\r\n        if (encryptionTestEnabled()) {\r\n            assertNotNull(\"Encryption propagation failed\", secondDelegate.getS3EncryptionAlgorithm());\r\n            assertEquals(\"Encryption propagation failed\", fs.getS3EncryptionAlgorithm(), secondDelegate.getS3EncryptionAlgorithm());\r\n        }\r\n        ContractTestUtils.assertDeleted(secondDelegate, testPath, true);\r\n        assertNotNull(\"unbounded DT\", secondDelegate.getDelegationToken(\"\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "executeDelegatedFSOperations",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void executeDelegatedFSOperations(final S3AFileSystem delegatedFS, final Path testPath) throws Exception\n{\r\n    ContractTestUtils.assertIsDirectory(delegatedFS, new Path(\"/\"));\r\n    ContractTestUtils.touch(delegatedFS, testPath);\r\n    ContractTestUtils.assertDeleted(delegatedFS, testPath, false);\r\n    delegatedFS.mkdirs(testPath);\r\n    ContractTestUtils.assertIsDirectory(delegatedFS, testPath);\r\n    Path srcFile = new Path(testPath, \"src.txt\");\r\n    Path destFile = new Path(testPath, \"dest.txt\");\r\n    ContractTestUtils.touch(delegatedFS, srcFile);\r\n    ContractTestUtils.rename(delegatedFS, srcFile, destFile);\r\n    ContractTestUtils.assertIsFile(delegatedFS, destFile);\r\n    ContractTestUtils.assertDeleted(delegatedFS, testPath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "verifyRestrictedPermissions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyRestrictedPermissions(final S3AFileSystem delegatedFS) throws Exception\n{\r\n    readLandsatMetadata(delegatedFS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testDelegationBindingMismatch1",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testDelegationBindingMismatch1() throws Throwable\n{\r\n    describe(\"Verify that when the DT client and remote bindings are different,\" + \" the failure is meaningful\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    URI uri = fs.getUri();\r\n    UserGroupInformation.getCurrentUser().addCredentials(createDelegationTokens());\r\n    Configuration conf = new Configuration(getConfiguration());\r\n    String bucket = fs.getBucket();\r\n    removeBaseAndBucketOverrides(bucket, conf, ACCESS_KEY, SECRET_KEY, SESSION_TOKEN);\r\n    conf.set(ACCESS_KEY, \"aaaaa\");\r\n    conf.set(SECRET_KEY, \"bbbb\");\r\n    bindProviderList(bucket, conf, CountInvocationsProvider.NAME);\r\n    conf.set(DELEGATION_TOKEN_BINDING, DELEGATION_TOKEN_FULL_CREDENTIALS_BINDING);\r\n    ServiceStateException e = intercept(ServiceStateException.class, TOKEN_MISMATCH, () -> {\r\n        S3AFileSystem remote = newS3AInstance(uri, conf);\r\n        String s = remote.toString();\r\n        remote.close();\r\n        return s;\r\n    });\r\n    if (!(e.getCause() instanceof DelegationTokenIOException)) {\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testDelegationBindingMismatch2",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testDelegationBindingMismatch2() throws Throwable\n{\r\n    describe(\"assert mismatch reported when client DT is a \" + \"subclass of the remote one\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    URI uri = fs.getUri();\r\n    Configuration conf = new Configuration(getConfiguration());\r\n    String bucket = fs.getBucket();\r\n    enableDelegationTokens(conf, DELEGATION_TOKEN_FULL_CREDENTIALS_BINDING);\r\n    Credentials fullTokens;\r\n    Token<AbstractS3ATokenIdentifier> firstDT;\r\n    try (S3AFileSystem fullFS = newS3AInstance(uri, conf)) {\r\n        fullTokens = mkTokens(fullFS);\r\n        assertTokenCreationCount(fullFS, 1);\r\n        firstDT = fullFS.getDelegationToken(\"first\");\r\n        assertTokenCreationCount(fullFS, 2);\r\n        Token<AbstractS3ATokenIdentifier> secondDT = fullFS.getDelegationToken(\"second\");\r\n        assertTokenCreationCount(fullFS, 3);\r\n        assertNotEquals(\"DT identifiers\", firstDT.getIdentifier(), secondDT.getIdentifier());\r\n    }\r\n    AbstractS3ATokenIdentifier origTokenId = requireNonNull(lookupToken(fullTokens, uri, FULL_TOKEN_KIND), \"token from credentials\");\r\n    UserGroupInformation.getCurrentUser().addCredentials(fullTokens);\r\n    try (S3AFileSystem delegatedFS = newS3AInstance(uri, conf)) {\r\n        assertBoundToDT(delegatedFS, FULL_TOKEN_KIND);\r\n        delegatedFS.getFileStatus(new Path(\"/\"));\r\n        SessionTokenIdentifier tokenFromDelegatedFS = (SessionTokenIdentifier) requireNonNull(delegatedFS.getDelegationToken(\"\"), \"New token\").decodeIdentifier();\r\n        assertTokenCreationCount(delegatedFS, 0);\r\n        assertEquals(\"Newly issued token != old one\", origTokenId, tokenFromDelegatedFS);\r\n    }\r\n    Configuration conf2 = new Configuration(getConfiguration());\r\n    removeBaseAndBucketOverrides(bucket, conf2, ACCESS_KEY, SECRET_KEY, SESSION_TOKEN);\r\n    conf.set(DELEGATION_TOKEN_BINDING, getDelegationBinding());\r\n    ServiceStateException e = intercept(ServiceStateException.class, TOKEN_MISMATCH, () -> {\r\n        S3AFileSystem remote = newS3AInstance(uri, conf);\r\n        String s = remote.toString();\r\n        remote.close();\r\n        return s;\r\n    });\r\n    if (!(e.getCause() instanceof DelegationTokenIOException)) {\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "readLandsatMetadata",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ObjectMetadata readLandsatMetadata(final S3AFileSystem delegatedFS) throws Exception\n{\r\n    AWSCredentialProviderList testingCreds = delegatedFS.shareCredentials(\"testing\");\r\n    URI landsat = new URI(DEFAULT_CSVTEST_FILE);\r\n    DefaultS3ClientFactory factory = new DefaultS3ClientFactory();\r\n    factory.setConf(new Configuration(delegatedFS.getConf()));\r\n    String host = landsat.getHost();\r\n    S3ClientFactory.S3ClientCreationParameters parameters = null;\r\n    parameters = new S3ClientFactory.S3ClientCreationParameters().withCredentialSet(testingCreds).withEndpoint(DEFAULT_ENDPOINT).withMetrics(new EmptyS3AStatisticsContext().newStatisticsFromAwsSdk()).withUserAgentSuffix(\"ITestSessionDelegationInFileystem\");\r\n    AmazonS3 s3 = factory.createS3Client(landsat, parameters);\r\n    return Invoker.once(\"HEAD\", host, () -> s3.getObjectMetadata(host, landsat.getPath().substring(1)));\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testYarnCredentialPickup",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testYarnCredentialPickup() throws Throwable\n{\r\n    describe(\"Verify tokens are picked up by the YARN\" + \" TokenCache.obtainTokensForNamenodes() API Call\");\r\n    Credentials cred = new Credentials();\r\n    Path yarnPath = path(\"testYarnCredentialPickup\");\r\n    Path[] paths = new Path[] { yarnPath };\r\n    Configuration conf = getConfiguration();\r\n    S3AFileSystem fs = getFileSystem();\r\n    TokenCache.obtainTokensForNamenodes(cred, paths, conf);\r\n    assertNotNull(\"No Token in credentials file\", lookupToken(cred, fs.getUri(), getTokenKind()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testHDFSFetchDTCommand",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testHDFSFetchDTCommand() throws Throwable\n{\r\n    describe(\"Use the HDFS fetchdt CLI to fetch a token\");\r\n    ExitUtil.disableSystemExit();\r\n    S3AFileSystem fs = getFileSystem();\r\n    Configuration conf = fs.getConf();\r\n    URI fsUri = fs.getUri();\r\n    String fsurl = fsUri.toString();\r\n    File tokenfile = createTempTokenFile();\r\n    String tokenFilePath = tokenfile.getAbsolutePath();\r\n    doAs(bobUser, () -> DelegationTokenFetcher.main(conf, args(\"--webservice\", fsurl, tokenFilePath)));\r\n    assertTrue(\"token file was not created: \" + tokenfile, tokenfile.exists());\r\n    String s = DelegationTokenFetcher.printTokensToString(conf, new Path(tokenfile.toURI()), false);\r\n    LOG.info(\"Tokens: {}\", s);\r\n    DelegationTokenFetcher.main(conf, args(\"--print\", tokenFilePath));\r\n    DelegationTokenFetcher.main(conf, args(\"--print\", \"--verbose\", tokenFilePath));\r\n    Credentials creds = Credentials.readTokenStorageFile(tokenfile, conf);\r\n    AbstractS3ATokenIdentifier identifier = requireNonNull(lookupToken(creds, fsUri, getTokenKind()), \"Token lookup\");\r\n    assertEquals(\"encryption secrets\", fs.getEncryptionSecrets(), identifier.getEncryptionSecrets());\r\n    assertEquals(\"Username of decoded token\", bobUser.getUserName(), identifier.getUser().getUserName());\r\n    DelegationTokenFetcher.main(conf, args(\"--renew\", tokenFilePath));\r\n    DelegationTokenFetcher.main(conf, args(\"--cancel\", tokenFilePath));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createTempTokenFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "File createTempTokenFile() throws IOException\n{\r\n    File tokenfile = File.createTempFile(\"tokens\", \".bin\", cluster.getWorkDir());\r\n    tokenfile.delete();\r\n    return tokenfile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "args",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] args(String... args)\n{\r\n    return args;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testFileSystemBoundToCreator",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testFileSystemBoundToCreator() throws Throwable\n{\r\n    describe(\"Run tests to verify the DT Setup is bound to the creator\");\r\n    assertNotEquals(\"Alice and Bob logins\", aliceUser.getUserName(), bobUser.getUserName());\r\n    final S3AFileSystem fs = getFileSystem();\r\n    assertEquals(\"FS username in doAs()\", ALICE, doAs(bobUser, () -> fs.getUsername()));\r\n    UserGroupInformation fsOwner = doAs(bobUser, () -> fs.getDelegationTokens().get().getOwner());\r\n    assertEquals(\"username mismatch\", aliceUser.getUserName(), fsOwner.getUserName());\r\n    Token<AbstractS3ATokenIdentifier> dt = fs.getDelegationToken(ALICE);\r\n    AbstractS3ATokenIdentifier identifier = dt.decodeIdentifier();\r\n    UserGroupInformation user = identifier.getUser();\r\n    assertEquals(\"User in DT\", aliceUser.getUserName(), user.getUserName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "dtutil",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String dtutil(int expected, String... args) throws Exception\n{\r\n    final ByteArrayOutputStream dtUtilContent = new ByteArrayOutputStream();\r\n    DtUtilShell dt = new DtUtilShell();\r\n    dt.setOut(new PrintStream(dtUtilContent));\r\n    dtUtilContent.reset();\r\n    int r = doAs(aliceUser, () -> ToolRunner.run(getConfiguration(), dt, args));\r\n    String s = dtUtilContent.toString();\r\n    LOG.info(\"\\n{}\", s);\r\n    assertEquals(expected, r);\r\n    return s;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "testDTUtilShell",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDTUtilShell() throws Throwable\n{\r\n    describe(\"Verify the dtutil shell command can fetch tokens\");\r\n    File tokenfile = createTempTokenFile();\r\n    String tfs = tokenfile.toString();\r\n    String fsURI = getFileSystem().getCanonicalUri().toString();\r\n    dtutil(0, \"get\", fsURI, \"-format\", \"protobuf\", tfs);\r\n    assertTrue(\"not created: \" + tokenfile, tokenfile.exists());\r\n    assertTrue(\"File is empty\" + tokenfile, tokenfile.length() > 0);\r\n    assertTrue(\"File only contains header\" + tokenfile, tokenfile.length() > 6);\r\n    String printed = dtutil(0, \"print\", tfs);\r\n    assertThat(printed, containsString(fsURI));\r\n    assertThat(printed, containsString(getTokenKind().toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    final S3AFileSystem fs = getFileSystem();\r\n    regionName = determineRegion(fs.getBucket());\r\n    LOG.info(\"Determined region name to be [{}] for bucket [{}]\", regionName, fs.getBucket());\r\n    endpoint = fs.getConf().get(Constants.ENDPOINT, Constants.CENTRAL_ENDPOINT);\r\n    LOG.info(\"Test endpoint is {}\", endpoint);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testCustomSignerAndInitializer",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testCustomSignerAndInitializer() throws IOException, InterruptedException\n{\r\n    final Path basePath = path(getMethodName());\r\n    UserGroupInformation ugi1 = UserGroupInformation.createRemoteUser(\"user1\");\r\n    FileSystem fs1 = runMkDirAndVerify(ugi1, new Path(basePath, \"customsignerpath1\"), \"id1\");\r\n    UserGroupInformation ugi2 = UserGroupInformation.createRemoteUser(\"user2\");\r\n    FileSystem fs2 = runMkDirAndVerify(ugi2, new Path(basePath, \"customsignerpath2\"), \"id2\");\r\n    Assertions.assertThat(CustomSignerInitializer.knownStores.size()).as(\"Num registered stores mismatch\").isEqualTo(2);\r\n    fs1.close();\r\n    Assertions.assertThat(CustomSignerInitializer.knownStores.size()).as(\"Num registered stores mismatch\").isEqualTo(1);\r\n    fs2.close();\r\n    Assertions.assertThat(CustomSignerInitializer.knownStores.size()).as(\"Num registered stores mismatch\").isEqualTo(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "runMkDirAndVerify",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileSystem runMkDirAndVerify(UserGroupInformation ugi, Path finalPath, String identifier) throws IOException, InterruptedException\n{\r\n    Configuration conf = createTestConfig(identifier);\r\n    return ugi.doAs((PrivilegedExceptionAction<FileSystem>) () -> {\r\n        int instantiationCount = CustomSigner.getInstantiationCount();\r\n        int invocationCount = CustomSigner.getInvocationCount();\r\n        FileSystem fs = finalPath.getFileSystem(conf);\r\n        fs.mkdirs(finalPath);\r\n        Assertions.assertThat(CustomSigner.getInstantiationCount()).as(\"CustomSigner Instantiation count lower than expected\").isGreaterThan(instantiationCount);\r\n        Assertions.assertThat(CustomSigner.getInvocationCount()).as(\"CustomSigner Invocation count lower than expected\").isGreaterThan(invocationCount);\r\n        Assertions.assertThat(CustomSigner.lastStoreValue).as(\"Store value should not be null\").isNotNull();\r\n        Assertions.assertThat(CustomSigner.lastStoreValue.conf).as(\"Configuration should not be null\").isNotNull();\r\n        Assertions.assertThat(CustomSigner.lastStoreValue.conf.get(TEST_ID_KEY)).as(\"Configuration TEST_KEY mismatch\").isEqualTo(identifier);\r\n        return fs;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "createTestConfig",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Configuration createTestConfig(String identifier)\n{\r\n    Configuration conf = createConfiguration();\r\n    conf.set(CUSTOM_SIGNERS, \"CustomS3Signer:\" + CustomSigner.class.getName() + \":\" + CustomSignerInitializer.class.getName());\r\n    conf.set(SIGNING_ALGORITHM_S3, \"CustomS3Signer\");\r\n    conf.set(TEST_ID_KEY, identifier);\r\n    conf.set(TEST_REGION_KEY, regionName);\r\n    disableFilesystemCaching(conf);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "determineRegion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String determineRegion(String bucketName) throws IOException\n{\r\n    return getFileSystem().getBucketLocation(bucketName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "noopAuditor",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OperationAuditor noopAuditor(Configuration conf)\n{\r\n    return NoopAuditor.createAndStartNoopAuditor(conf, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "noopAuditConfig",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration noopAuditConfig()\n{\r\n    final Configuration conf = new Configuration(false);\r\n    conf.set(AUDIT_SERVICE_CLASSNAME, NOOP_AUDIT_SERVICE);\r\n    conf.setBoolean(AUDIT_ENABLED, true);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "loggingAuditConfig",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration loggingAuditConfig()\n{\r\n    return enableLoggingAuditor(new Configuration(false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "enableLoggingAuditor",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration enableLoggingAuditor(final Configuration conf)\n{\r\n    conf.set(AUDIT_SERVICE_CLASSNAME, LOGGING_AUDIT_SERVICE);\r\n    conf.setBoolean(AUDIT_ENABLED, true);\r\n    conf.setBoolean(REJECT_OUT_OF_SPAN_OPERATIONS, true);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "createIOStatisticsStoreForAuditing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOStatisticsStore createIOStatisticsStoreForAuditing()\n{\r\n    return iostatisticsStore().withCounters(AUDIT_ACCESS_CHECK_FAILURE.getSymbol(), AUDIT_FAILURE.getSymbol(), AUDIT_REQUEST_EXECUTION.getSymbol(), AUDIT_SPAN_CREATION.getSymbol()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "resetAuditOptions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration resetAuditOptions(Configuration conf)\n{\r\n    S3ATestUtils.removeBaseAndBucketOverrides(conf, REFERRER_HEADER_ENABLED, REJECT_OUT_OF_SPAN_OPERATIONS, AUDIT_REQUEST_HANDLERS, AUDIT_SERVICE_CLASSNAME, AUDIT_ENABLED);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testUSEast",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUSEast()\n{\r\n    assertRegionFixup(US_EAST_1, US_EAST_1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testUSWest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUSWest()\n{\r\n    assertRegionFixup(US_WEST_2, US_WEST_2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testRegionUStoUSEast",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRegionUStoUSEast()\n{\r\n    assertRegionFixup(\"US\", US_EAST_1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testRegionNullToUSEast",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRegionNullToUSEast()\n{\r\n    assertRegionFixup(null, US_EAST_1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "assertRegionFixup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertRegionFixup(String region, String expected)\n{\r\n    assertThat(fixBucketRegion(region)).describedAs(\"Fixup of %s\", region).isEqualTo(expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testNull",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNull() throws Throwable\n{\r\n    expectEndpoint(\"\", true, \"unused\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testUSEastEndpoint",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUSEastEndpoint() throws Throwable\n{\r\n    expectEndpoint(US_EAST_1, false, US_EAST_1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testUSWestEndpoint",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUSWestEndpoint() throws Throwable\n{\r\n    expectEndpoint(US_WEST_2, false, US_WEST_2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "expectEndpoint",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void expectEndpoint(final String src, final boolean expectNull, final String expectRegion)\n{\r\n    AwsClientBuilder.EndpointConfiguration epr = createEndpointConfiguration(src, new ClientConfiguration(), src);\r\n    String eprStr = epr == null ? \"(empty)\" : (\"(\" + epr.getServiceEndpoint() + \" \" + epr.getSigningRegion());\r\n    if (expectNull) {\r\n        assertThat(epr).describedAs(\"Endpoint configuration of %s =\", src, eprStr).isNull();\r\n    } else {\r\n        assertThat(epr).describedAs(\"Endpoint configuration of %s =\", src, eprStr).hasFieldOrPropertyWithValue(\"serviceEndpoint\", src).hasFieldOrPropertyWithValue(\"signingRegion\", expectRegion);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "getBlockOutputBufferName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockOutputBufferName()\n{\r\n    return Constants.FAST_UPLOAD_BUFFER_ARRAY;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    removeBaseAndBucketOverrides(conf, CONTENT_ENCODING);\r\n    conf.set(CONTENT_ENCODING, GZIP);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCreatedObjectsHaveEncoding",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCreatedObjectsHaveEncoding() throws Throwable\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path dir = methodPath();\r\n    fs.mkdirs(dir);\r\n    Assertions.assertThat(getEncoding(dir)).describedAs(\"Encoding of object %s\", dir).isNull();\r\n    Path path = new Path(dir, \"1\");\r\n    ContractTestUtils.touch(fs, path);\r\n    assertObjectHasEncoding(path);\r\n    Path path2 = new Path(dir, \"2\");\r\n    fs.rename(path, path2);\r\n    assertObjectHasEncoding(path2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertObjectHasEncoding",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertObjectHasEncoding(Path path) throws Throwable\n{\r\n    Assertions.assertThat(getEncoding(path)).describedAs(\"Encoding of object %s\", path).isEqualTo(GZIP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getEncoding",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getEncoding(Path path) throws IOException\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Map<String, byte[]> xAttrs = fs.getXAttrs(path);\r\n    return decodeBytes(xAttrs.get(XA_CONTENT_ENCODING));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "exec",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String exec(S3GuardTool cmd, Object... args) throws Exception\n{\r\n    return expectExecResult(0, cmd, args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "expectExecResult",
  "errType" : [ "AssertionError", "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String expectExecResult(final int expectedResult, final S3GuardTool cmd, final Object... args) throws Exception\n{\r\n    ByteArrayOutputStream buf = new ByteArrayOutputStream();\r\n    try {\r\n        exec(expectedResult, \"\", cmd, buf, args);\r\n        return buf.toString();\r\n    } catch (AssertionError e) {\r\n        throw e;\r\n    } catch (Exception e) {\r\n        LOG.error(\"Command {} failed: \\n{}\", cmd, buf);\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "varargsToString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] varargsToString(final Object[] oargs)\n{\r\n    return Arrays.stream(oargs).map(Object::toString).toArray(String[]::new);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "exec",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void exec(final int expectedResult, final String errorText, final S3GuardTool cmd, final ByteArrayOutputStream buf, final Object... oargs) throws Exception\n{\r\n    final String[] args = varargsToString(oargs);\r\n    LOG.info(\"exec {}\", (Object) args);\r\n    int r;\r\n    try (PrintStream out = new PrintStream(buf)) {\r\n        r = cmd.run(args, out);\r\n        out.flush();\r\n    } catch (Exception ex) {\r\n        if (ex instanceof ExitCodeProvider) {\r\n            final ExitCodeProvider ec = (ExitCodeProvider) ex;\r\n            if (ec.getExitCode() == expectedResult) {\r\n                return;\r\n            }\r\n        }\r\n        throw ex;\r\n    }\r\n    if (expectedResult != r) {\r\n        String message = errorText.isEmpty() ? \"\" : (errorText + \": \") + \"Command \" + cmd + \" failed\\n\" + buf;\r\n        assertEquals(message, expectedResult, r);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "runS3GuardCommand",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int runS3GuardCommand(Configuration conf, Object... args) throws Exception\n{\r\n    return S3GuardTool.run(conf, varargsToString(args));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "runS3GuardCommandToFailure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void runS3GuardCommandToFailure(Configuration conf, int status, Object... args) throws Exception\n{\r\n    ExitUtil.ExitException ex = intercept(ExitUtil.ExitException.class, () -> {\r\n        int ec = runS3GuardCommand(conf, args);\r\n        if (ec != 0) {\r\n            throw new ExitUtil.ExitException(ec, \"exit code \" + ec);\r\n        }\r\n    });\r\n    if (ex.status != status) {\r\n        throw ex;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    root = getFileSystem().makeQualified(new Path(\"/\"));\r\n    getFileSystem().close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void teardown()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "checkForThreadLeakage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkForThreadLeakage()\n{\r\n    Assertions.assertThat(getCurrentThreadNames()).describedAs(\"The threads at the end of the test run\").isSubsetOf(THREAD_SET);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testClosedGetFileStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testClosedGetFileStatus() throws Exception\n{\r\n    intercept(IOException.class, E_FS_CLOSED, () -> getFileSystem().getFileStatus(root));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testClosedListStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testClosedListStatus() throws Exception\n{\r\n    intercept(IOException.class, E_FS_CLOSED, () -> getFileSystem().listStatus(root));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testClosedListFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testClosedListFile() throws Exception\n{\r\n    intercept(IOException.class, E_FS_CLOSED, () -> getFileSystem().listFiles(root, false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testClosedListLocatedStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testClosedListLocatedStatus() throws Exception\n{\r\n    intercept(IOException.class, E_FS_CLOSED, () -> getFileSystem().listLocatedStatus(root));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testClosedCreate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testClosedCreate() throws Exception\n{\r\n    intercept(IOException.class, E_FS_CLOSED, () -> getFileSystem().create(path(\"to-create\")).close());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testClosedDelete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testClosedDelete() throws Exception\n{\r\n    intercept(IOException.class, E_FS_CLOSED, () -> getFileSystem().delete(path(\"to-delete\"), false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testClosedOpen",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testClosedOpen() throws Exception\n{\r\n    intercept(IOException.class, E_FS_CLOSED, () -> getFileSystem().open(path(\"to-open\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setup",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    ClassLoader loader = this.getClass().getClassLoader();\r\n    try {\r\n        loader.loadClass(\"org.wildfly.openssl.OpenSSLProvider\");\r\n        hasWildfly = true;\r\n    } catch (ClassNotFoundException e) {\r\n        hasWildfly = false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testUnknownMode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testUnknownMode() throws Throwable\n{\r\n    DelegatingSSLSocketFactory.resetDefaultFactory();\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(SSL_CHANNEL_MODE, \"no-such-mode \");\r\n    intercept(IllegalArgumentException.class, () -> bindSSLChannelMode(conf, new ClientConfiguration()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testOpenSSLNoWildfly",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testOpenSSLNoWildfly() throws Throwable\n{\r\n    assumeThat(hasWildfly).isFalse();\r\n    intercept(NoClassDefFoundError.class, \"wildfly\", () -> bindSocketFactory(OpenSSL));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testDefaultDowngradesNoWildfly",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testDefaultDowngradesNoWildfly() throws Throwable\n{\r\n    assumeThat(hasWildfly).isFalse();\r\n    expectBound(Default, Default_JSSE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testWildflyOpenSSL",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testWildflyOpenSSL() throws Throwable\n{\r\n    assumeThat(hasWildfly).isTrue();\r\n    assertThat(bindSocketFactory(Default)).describedAs(\"Sockets from mode \" + Default).isIn(OpenSSL, Default_JSSE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testJSSE",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJSSE() throws Throwable\n{\r\n    expectBound(Default_JSSE, Default_JSSE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testGCM",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGCM() throws Throwable\n{\r\n    expectBound(Default_JSSE_with_GCM, Default_JSSE_with_GCM);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "expectBound",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectBound(DelegatingSSLSocketFactory.SSLChannelMode channelMode, DelegatingSSLSocketFactory.SSLChannelMode finalMode) throws Throwable\n{\r\n    assertThat(bindSocketFactory(channelMode)).describedAs(\"Channel mode of socket factory created with mode %s\", channelMode).isEqualTo(finalMode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "bindSocketFactory",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "DelegatingSSLSocketFactory.SSLChannelMode bindSocketFactory(final DelegatingSSLSocketFactory.SSLChannelMode channelMode) throws IOException\n{\r\n    DelegatingSSLSocketFactory.resetDefaultFactory();\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(SSL_CHANNEL_MODE, channelMode.name());\r\n    ClientConfiguration awsConf = new ClientConfiguration();\r\n    awsConf.setProtocol(Protocol.HTTPS);\r\n    bindSSLChannelMode(conf, awsConf);\r\n    return DelegatingSSLSocketFactory.getDefaultFactory().getChannelMode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    conf.set(DIRECTORY_MARKER_POLICY, DIRECTORY_MARKER_POLICY_DELETE);\r\n    removeBaseAndBucketOverrides(getTestBucketName(conf), conf, DIRECTORY_MARKER_POLICY);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testDeleteRenameRaceCondition",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testDeleteRenameRaceCondition() throws Throwable\n{\r\n    describe(\"verify no race between delete and rename\");\r\n    final S3AFileSystem fs = getFileSystem();\r\n    final Path path = path(getMethodName());\r\n    Path srcDir = new Path(path, \"src\");\r\n    Path destDir = new Path(path, \"dest\");\r\n    Path destSubdir1 = new Path(destDir, \"subdir1\");\r\n    Path subfile1 = new Path(destSubdir1, \"subfile1\");\r\n    Path srcSubdir2 = new Path(srcDir, \"subdir2\");\r\n    Path srcSubfile = new Path(srcSubdir2, \"subfile2\");\r\n    Path destSubdir2 = new Path(destDir, \"subdir2\");\r\n    ContractTestUtils.touch(fs, subfile1);\r\n    assertIsDirectory(destDir);\r\n    ContractTestUtils.touch(fs, srcSubfile);\r\n    final BlockingFakeDirMarkerFS blockingFS = new BlockingFakeDirMarkerFS();\r\n    blockingFS.initialize(fs.getUri(), fs.getConf());\r\n    blockingFS.blockFakeDirCreation();\r\n    try {\r\n        final CompletableFuture<Path> future = submit(EXECUTOR, () -> {\r\n            LOG.info(\"deleting {}\", destSubdir1);\r\n            blockingFS.delete(destSubdir1, true);\r\n            return destSubdir1;\r\n        });\r\n        blockingFS.awaitFakeDirCreation();\r\n        try {\r\n            assertPathDoesNotExist(\"should have been implicitly deleted\", destDir);\r\n            LOG.info(\"renaming {} to {}\", srcSubdir2, destSubdir2);\r\n            Assertions.assertThat(fs.rename(srcSubdir2, destSubdir2)).describedAs(\"rename(%s, %s)\", srcSubdir2, destSubdir2).isTrue();\r\n            assertPathExists(\"must now exist\", destDir);\r\n        } finally {\r\n            blockingFS.allowFakeDirCreationToProceed();\r\n        }\r\n        LOG.info(\"Waiting for delete {} to finish\", destSubdir1);\r\n        waitForCompletion(future);\r\n        assertPathExists(\"must now exist\", destDir);\r\n        assertPathExists(\"must now exist\", new Path(destSubdir2, \"subfile2\"));\r\n        assertPathDoesNotExist(\"Src dir deleted\", srcSubdir2);\r\n    } finally {\r\n        cleanupWithLogger(LOG, blockingFS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "suitename",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String suitename()\n{\r\n    return \"ITestPartitionedCommitProtocol\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "getCommitterName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCommitterName()\n{\r\n    return CommitConstants.COMMITTER_NAME_PARTITIONED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "createCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractS3ACommitter createCommitter(Path outputPath, TaskAttemptContext context) throws IOException\n{\r\n    return new PartitionedStagingCommitter(outputPath, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "createFailingCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractS3ACommitter createFailingCommitter(TaskAttemptContext tContext) throws IOException\n{\r\n    return new CommitterWithFailedThenSucceed(getOutDir(), tContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging\\integration",
  "methodName" : "testMapFileOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMapFileOutputCommitter() throws Exception\n{\r\n    skip(\"Partioning committer is not suitable for Map Output\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    dest = path(\"ITestS3AUnbuffer\");\r\n    describe(\"ITestS3AUnbuffer\");\r\n    byte[] data = ContractTestUtils.dataset(FILE_LENGTH, 'a', 26);\r\n    ContractTestUtils.writeDataset(getFileSystem(), dest, data, data.length, 16, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testUnbuffer",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testUnbuffer() throws IOException\n{\r\n    describe(\"testUnbuffer\");\r\n    IOStatisticsSnapshot iostats = new IOStatisticsSnapshot();\r\n    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\r\n        assertTrue(inputStream.getWrappedStream() instanceof S3AInputStream);\r\n        int bytesToRead = 8;\r\n        readAndAssertBytesRead(inputStream, bytesToRead);\r\n        assertTrue(isObjectStreamOpen(inputStream));\r\n        assertTrue(\"No IOstatistics from \" + inputStream, iostats.aggregate(inputStream.getIOStatistics()));\r\n        verifyStatisticCounterValue(iostats, StreamStatisticNames.STREAM_READ_BYTES, bytesToRead);\r\n        verifyStatisticCounterValue(iostats, StoreStatisticNames.ACTION_HTTP_GET_REQUEST, 1);\r\n        inputStream.unbuffer();\r\n        IOStatistics st2 = inputStream.getIOStatistics();\r\n        verifyStatisticCounterValue(st2, StreamStatisticNames.STREAM_READ_UNBUFFERED, 1);\r\n        verifyStatisticCounterValue(st2, StreamStatisticNames.STREAM_READ_BYTES, bytesToRead);\r\n        verifyStatisticCounterValue(st2, StoreStatisticNames.ACTION_HTTP_GET_REQUEST, 1);\r\n        assertFalse(isObjectStreamOpen(inputStream));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testUnbufferStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testUnbufferStreamStatistics() throws IOException\n{\r\n    describe(\"testUnbufferStreamStatistics\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    S3ATestUtils.MetricDiff bytesRead = new S3ATestUtils.MetricDiff(fs, STREAM_READ_BYTES);\r\n    S3ATestUtils.MetricDiff totalBytesRead = new S3ATestUtils.MetricDiff(fs, STREAM_READ_TOTAL_BYTES);\r\n    S3ATestUtils.MetricDiff bytesReadInClose = new S3ATestUtils.MetricDiff(fs, STREAM_READ_BYTES_READ_CLOSE);\r\n    FSDataInputStream inputStream = null;\r\n    int firstBytesToRead = 8;\r\n    int secondBytesToRead = 1;\r\n    long expectedFinalBytesRead;\r\n    long expectedTotalBytesRead;\r\n    Object streamStatsStr;\r\n    try {\r\n        inputStream = fs.open(dest);\r\n        streamStatsStr = demandStringifyIOStatisticsSource(inputStream);\r\n        LOG.info(\"initial stream statistics {}\", streamStatsStr);\r\n        readAndAssertBytesRead(inputStream, firstBytesToRead);\r\n        LOG.info(\"stream statistics after read {}\", streamStatsStr);\r\n        inputStream.unbuffer();\r\n        bytesRead.assertDiffEquals(firstBytesToRead);\r\n        final long bytesInUnbuffer = bytesReadInClose.diff();\r\n        totalBytesRead.assertDiffEquals(firstBytesToRead + bytesInUnbuffer);\r\n        bytesReadInClose.reset();\r\n        bytesRead.reset();\r\n        readAndAssertBytesRead(inputStream, secondBytesToRead);\r\n        inputStream.unbuffer();\r\n        LOG.info(\"stream statistics after second read {}\", streamStatsStr);\r\n        bytesRead.assertDiffEquals(secondBytesToRead);\r\n        final long bytesInClose = bytesReadInClose.diff();\r\n        expectedFinalBytesRead = firstBytesToRead + secondBytesToRead;\r\n        expectedTotalBytesRead = expectedFinalBytesRead + bytesInUnbuffer + bytesInClose;\r\n        totalBytesRead.assertDiffEquals(expectedTotalBytesRead);\r\n    } finally {\r\n        LOG.info(\"Closing stream\");\r\n        IOUtils.closeStream(inputStream);\r\n    }\r\n    LOG.info(\"stream statistics after close {}\", streamStatsStr);\r\n    totalBytesRead.assertDiffEquals(expectedTotalBytesRead);\r\n    S3AInputStreamStatistics streamStatistics = ((S3AInputStream) inputStream.getWrappedStream()).getS3AStreamStatistics();\r\n    Assertions.assertThat(streamStatistics).describedAs(\"Stream statistics %s\", streamStatistics).hasFieldOrPropertyWithValue(\"bytesRead\", expectedFinalBytesRead).hasFieldOrPropertyWithValue(\"totalBytesRead\", expectedTotalBytesRead);\r\n    assertEquals(\"S3AInputStream statistics were not updated properly in \" + streamStatsStr, expectedFinalBytesRead, streamStatistics.getBytesRead());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isObjectStreamOpen",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isObjectStreamOpen(FSDataInputStream inputStream)\n{\r\n    return ((S3AInputStream) inputStream.getWrappedStream()).isObjectStreamOpen();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "readAndAssertBytesRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readAndAssertBytesRead(FSDataInputStream inputStream, int bytesToRead) throws IOException\n{\r\n    assertEquals(\"S3AInputStream#read did not read the correct number of \" + \"bytes\", bytesToRead, inputStream.read(new byte[bytesToRead]));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics",
  "methodName" : "testLandsatStatistics",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testLandsatStatistics() throws Throwable\n{\r\n    final Configuration conf = getConfiguration();\r\n    Path path = getLandsatCSVPath(conf);\r\n    conf.set(ENDPOINT, DEFAULT_ENDPOINT);\r\n    conf.unset(\"fs.s3a.bucket.landsat-pds.endpoint\");\r\n    try (S3AFileSystem fs = (S3AFileSystem) path.getFileSystem(conf)) {\r\n        fs.getObjectMetadata(path);\r\n        IOStatistics iostats = fs.getIOStatistics();\r\n        assertThatStatisticCounter(iostats, STORE_IO_REQUEST.getSymbol()).isGreaterThanOrEqualTo(1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics",
  "methodName" : "testCommonCrawlStatistics",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCommonCrawlStatistics() throws Throwable\n{\r\n    final Configuration conf = getConfiguration();\r\n    getLandsatCSVPath(conf);\r\n    Path path = COMMON_CRAWL_PATH;\r\n    conf.set(ENDPOINT, DEFAULT_ENDPOINT);\r\n    try (S3AFileSystem fs = (S3AFileSystem) path.getFileSystem(conf)) {\r\n        fs.getObjectMetadata(path);\r\n        IOStatistics iostats = fs.getIOStatistics();\r\n        assertThatStatisticCounter(iostats, STORE_IO_REQUEST.getSymbol()).isGreaterThanOrEqualTo(1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testBlockSize",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testBlockSize() throws Exception\n{\r\n    FileSystem fs = getFileSystem();\r\n    long defaultBlockSize = fs.getDefaultBlockSize();\r\n    assertEquals(\"incorrect blocksize\", S3AFileSystem.DEFAULT_BLOCKSIZE, defaultBlockSize);\r\n    long newBlockSize = defaultBlockSize * 2;\r\n    fs.getConf().setLong(Constants.FS_S3A_BLOCK_SIZE, newBlockSize);\r\n    Path dir = path(\"testBlockSize\");\r\n    Path file = new Path(dir, \"file\");\r\n    createFile(fs, file, true, dataset(1024, 'a', 'z' - 'a'));\r\n    FileStatus fileStatus = fs.getFileStatus(file);\r\n    assertEquals(\"Double default block size in stat(): \" + fileStatus, newBlockSize, fileStatus.getBlockSize());\r\n    boolean found = false;\r\n    FileStatus[] listing = fs.listStatus(dir);\r\n    for (FileStatus stat : listing) {\r\n        LOG.info(\"entry: {}\", stat);\r\n        if (file.equals(stat.getPath())) {\r\n            found = true;\r\n            assertEquals(\"Double default block size in ls(): \" + stat, newBlockSize, stat.getBlockSize());\r\n        }\r\n    }\r\n    assertTrue(\"Did not find \" + fileStatsToString(listing, \", \"), found);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRootFileStatusHasBlocksize",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testRootFileStatusHasBlocksize() throws Throwable\n{\r\n    FileSystem fs = getFileSystem();\r\n    FileStatus status = fs.getFileStatus(new Path(\"/\"));\r\n    assertTrue(\"Invalid root blocksize\", status.getBlockSize() >= 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    assumeRoleTests();\r\n    uri = new URI(S3ATestConstants.DEFAULT_CSVTEST_FILE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    cleanupWithLogger(LOG, roleFS);\r\n    super.teardown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "assumeRoleTests",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assumeRoleTests()\n{\r\n    assume(\"No ARN for role tests\", !getAssumedRoleARN().isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getAssumedRoleARN",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getAssumedRoleARN()\n{\r\n    return getContract().getConf().getTrimmed(ASSUMED_ROLE_ARN, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "expectFileSystemCreateFailure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "E expectFileSystemCreateFailure(Configuration conf, Class<E> clazz, String text) throws Exception\n{\r\n    return interceptClosing(clazz, text, () -> new Path(getFileSystem().getUri()).getFileSystem(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testCreateCredentialProvider",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCreateCredentialProvider() throws IOException\n{\r\n    describe(\"Create the credential provider\");\r\n    Configuration conf = createValidRoleConf();\r\n    try (AssumedRoleCredentialProvider provider = new AssumedRoleCredentialProvider(uri, conf)) {\r\n        LOG.info(\"Provider is {}\", provider);\r\n        AWSCredentials credentials = provider.getCredentials();\r\n        assertNotNull(\"Null credentials from \" + provider, credentials);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testCreateCredentialProviderNoURI",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCreateCredentialProviderNoURI() throws IOException\n{\r\n    describe(\"Create the credential provider\");\r\n    Configuration conf = createValidRoleConf();\r\n    try (AssumedRoleCredentialProvider provider = new AssumedRoleCredentialProvider(null, conf)) {\r\n        LOG.info(\"Provider is {}\", provider);\r\n        AWSCredentials credentials = provider.getCredentials();\r\n        assertNotNull(\"Null credentials from \" + provider, credentials);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "createValidRoleConf",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Configuration createValidRoleConf() throws JsonProcessingException\n{\r\n    String roleARN = getAssumedRoleARN();\r\n    Configuration conf = new Configuration(getContract().getConf());\r\n    conf.set(AWS_CREDENTIALS_PROVIDER, AssumedRoleCredentialProvider.NAME);\r\n    conf.set(ASSUMED_ROLE_ARN, roleARN);\r\n    conf.set(ASSUMED_ROLE_SESSION_NAME, \"valid\");\r\n    conf.set(ASSUMED_ROLE_SESSION_DURATION, \"45m\");\r\n    bindRolePolicy(conf, RESTRICTED_POLICY);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumedInvalidRole",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testAssumedInvalidRole() throws Throwable\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(ASSUMED_ROLE_ARN, ROLE_ARN_EXAMPLE);\r\n    interceptClosing(AWSSecurityTokenServiceException.class, \"\", () -> new AssumedRoleCredentialProvider(uri, conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumeRoleFSBadARN",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAssumeRoleFSBadARN() throws Exception\n{\r\n    describe(\"Attemnpt to create the FS with an invalid ARN\");\r\n    Configuration conf = createAssumedRoleConfig();\r\n    conf.set(ASSUMED_ROLE_ARN, ROLE_ARN_EXAMPLE);\r\n    expectFileSystemCreateFailure(conf, AccessDeniedException.class, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumeRoleNoARN",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAssumeRoleNoARN() throws Exception\n{\r\n    describe(\"Attemnpt to create the FS with no ARN\");\r\n    Configuration conf = createAssumedRoleConfig();\r\n    conf.unset(ASSUMED_ROLE_ARN);\r\n    expectFileSystemCreateFailure(conf, IOException.class, AssumedRoleCredentialProvider.E_NO_ROLE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumeRoleFSBadPolicy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAssumeRoleFSBadPolicy() throws Exception\n{\r\n    describe(\"Attemnpt to create the FS with malformed JSON\");\r\n    Configuration conf = createAssumedRoleConfig();\r\n    conf.set(ASSUMED_ROLE_POLICY, \"}\");\r\n    expectFileSystemCreateFailure(conf, AWSBadRequestException.class, \"JSON\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumeRoleFSBadPolicy2",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAssumeRoleFSBadPolicy2() throws Exception\n{\r\n    describe(\"Attempt to create the FS with valid but non-compliant JSON\");\r\n    Configuration conf = createAssumedRoleConfig();\r\n    conf.set(ASSUMED_ROLE_POLICY, \"{'json':'but not what AWS wants}\");\r\n    expectFileSystemCreateFailure(conf, AWSBadRequestException.class, \"Syntax errors in policy\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumeRoleCannotAuthAssumedRole",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testAssumeRoleCannotAuthAssumedRole() throws Exception\n{\r\n    describe(\"Assert that you can't use assumed roles to auth assumed roles\");\r\n    Configuration conf = createAssumedRoleConfig();\r\n    unsetHadoopCredentialProviders(conf);\r\n    conf.set(ASSUMED_ROLE_CREDENTIALS_PROVIDER, AssumedRoleCredentialProvider.NAME);\r\n    expectFileSystemCreateFailure(conf, IOException.class, E_FORBIDDEN_AWS_PROVIDER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumeRoleBadInnerAuth",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testAssumeRoleBadInnerAuth() throws Exception\n{\r\n    describe(\"Try to authenticate with a keypair with spaces\");\r\n    Configuration conf = createAssumedRoleConfig();\r\n    unsetHadoopCredentialProviders(conf);\r\n    conf.set(ASSUMED_ROLE_CREDENTIALS_PROVIDER, SimpleAWSCredentialsProvider.NAME);\r\n    conf.set(ACCESS_KEY, \"not valid\");\r\n    conf.set(SECRET_KEY, \"not secret\");\r\n    expectFileSystemCreateFailure(conf, AWSBadRequestException.class, \"not a valid \" + \"key=value pair (missing equal-sign) in Authorization header\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumeRoleBadInnerAuth2",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testAssumeRoleBadInnerAuth2() throws Exception\n{\r\n    describe(\"Try to authenticate with an invalid keypair\");\r\n    Configuration conf = createAssumedRoleConfig();\r\n    unsetHadoopCredentialProviders(conf);\r\n    conf.set(ASSUMED_ROLE_CREDENTIALS_PROVIDER, SimpleAWSCredentialsProvider.NAME);\r\n    conf.set(ACCESS_KEY, \"notvalid\");\r\n    conf.set(SECRET_KEY, \"notsecret\");\r\n    expectFileSystemCreateFailure(conf, AccessDeniedException.class, \"The security token included in the request is invalid\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumeRoleBadSession",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAssumeRoleBadSession() throws Exception\n{\r\n    describe(\"Try to authenticate with an invalid session\");\r\n    Configuration conf = createAssumedRoleConfig();\r\n    conf.set(ASSUMED_ROLE_SESSION_NAME, \"Session names cannot hava spaces!\");\r\n    expectFileSystemCreateFailure(conf, AWSBadRequestException.class, \"Member must satisfy regular expression pattern\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumeRoleThreeHourSessionDuration",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAssumeRoleThreeHourSessionDuration() throws Exception\n{\r\n    describe(\"Try to authenticate with a long session duration\");\r\n    Configuration conf = createAssumedRoleConfig();\r\n    conf.setInt(ASSUMED_ROLE_SESSION_DURATION, 3 * 60 * 60);\r\n    try {\r\n        new Path(getFileSystem().getUri()).getFileSystem(conf).close();\r\n        LOG.info(\"Successfully created token of a duration >3h\");\r\n    } catch (IOException ioe) {\r\n        assertExceptionContains(VALIDATION_ERROR, ioe);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumeRoleThirtySixHourSessionDuration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAssumeRoleThirtySixHourSessionDuration() throws Exception\n{\r\n    describe(\"Try to authenticate with a long session duration\");\r\n    Configuration conf = createAssumedRoleConfig();\r\n    conf.setInt(ASSUMED_ROLE_SESSION_DURATION, 36 * 60 * 60);\r\n    IOException ioe = expectFileSystemCreateFailure(conf, IOException.class, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "createAssumedRoleConfig",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createAssumedRoleConfig()\n{\r\n    return createAssumedRoleConfig(getAssumedRoleARN());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "createAssumedRoleConfig",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createAssumedRoleConfig(String roleARN)\n{\r\n    return newAssumedRoleConfig(getContract().getConf(), roleARN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumeRoleUndefined",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testAssumeRoleUndefined() throws Throwable\n{\r\n    describe(\"Verify that you cannot instantiate the\" + \" AssumedRoleCredentialProvider without a role ARN\");\r\n    Configuration conf = new Configuration();\r\n    conf.set(ASSUMED_ROLE_ARN, \"\");\r\n    interceptClosing(IOException.class, AssumedRoleCredentialProvider.E_NO_ROLE, () -> new AssumedRoleCredentialProvider(uri, conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumedIllegalDuration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testAssumedIllegalDuration() throws Throwable\n{\r\n    describe(\"Expect the constructor to fail if the session is to short\");\r\n    Configuration conf = new Configuration();\r\n    conf.set(ASSUMED_ROLE_SESSION_DURATION, \"30s\");\r\n    interceptClosing(AWSSecurityTokenServiceException.class, \"\", () -> new AssumedRoleCredentialProvider(uri, conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumeRoleCreateFS",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testAssumeRoleCreateFS() throws IOException\n{\r\n    describe(\"Create an FS client with the role and do some basic IO\");\r\n    String roleARN = getAssumedRoleARN();\r\n    Configuration conf = createAssumedRoleConfig(roleARN);\r\n    Path path = new Path(getFileSystem().getUri());\r\n    LOG.info(\"Creating test FS and user {} with assumed role {}\", conf.get(ACCESS_KEY), roleARN);\r\n    try (FileSystem fs = path.getFileSystem(conf)) {\r\n        fs.getFileStatus(ROOT);\r\n        fs.mkdirs(path(\"testAssumeRoleFS\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumeRoleRestrictedPolicyFS",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testAssumeRoleRestrictedPolicyFS() throws Exception\n{\r\n    describe(\"Restrict the policy for this session; verify that reads fail.\");\r\n    Configuration conf = createAssumedRoleConfig();\r\n    bindRolePolicy(conf, RESTRICTED_POLICY);\r\n    Path path = new Path(getFileSystem().getUri());\r\n    try (FileSystem fs = path.getFileSystem(conf)) {\r\n        forbidden(\"getFileStatus\", () -> fs.getFileStatus(methodPath()));\r\n        forbidden(\"\", () -> fs.listStatus(ROOT));\r\n        forbidden(\"\", () -> fs.listFiles(ROOT, true));\r\n        forbidden(\"\", () -> fs.listLocatedStatus(ROOT));\r\n        forbidden(\"\", () -> fs.mkdirs(path(\"testAssumeRoleFS\")));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumeRolePoliciesOverrideRolePerms",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAssumeRolePoliciesOverrideRolePerms() throws Throwable\n{\r\n    describe(\"extra policies in assumed roles need;\" + \" all required policies stated\");\r\n    Configuration conf = createAssumedRoleConfig();\r\n    bindRolePolicy(conf, policy(statement(false, S3_ALL_BUCKETS, S3_GET_OBJECT_TORRENT), ALLOW_S3_GET_BUCKET_LOCATION, STATEMENT_ALLOW_SSE_KMS_RW));\r\n    Path path = path(\"testAssumeRoleStillIncludesRolePerms\");\r\n    roleFS = (S3AFileSystem) path.getFileSystem(conf);\r\n    assertTouchForbidden(roleFS, path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testReadOnlyOperations",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testReadOnlyOperations() throws Throwable\n{\r\n    describe(\"Restrict role to read only\");\r\n    Configuration conf = createAssumedRoleConfig();\r\n    bindRolePolicy(conf, policy(statement(false, S3_ALL_BUCKETS, S3_PATH_WRITE_OPERATIONS), STATEMENT_ALL_S3, STATEMENT_ALLOW_SSE_KMS_READ));\r\n    Path path = methodPath();\r\n    roleFS = (S3AFileSystem) path.getFileSystem(conf);\r\n    roleFS.listStatus(ROOT);\r\n    assertTouchForbidden(roleFS, path);\r\n    roleFS.delete(path, true);\r\n    getFileSystem().mkdirs(path);\r\n    assertDeleteForbidden(this.roleFS, path);\r\n    int counter = 0;\r\n    MultipartUtils.UploadIterator iterator = roleFS.listUploads(\"/\");\r\n    while (iterator.hasNext()) {\r\n        counter++;\r\n        iterator.next();\r\n    }\r\n    LOG.info(\"Found {} outstanding MPUs\", counter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testRestrictedWriteSubdir",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testRestrictedWriteSubdir() throws Throwable\n{\r\n    describe(\"Attempt writing to paths where a role only has\" + \" write access to a subdir of the bucket\");\r\n    Path restrictedDir = methodPath();\r\n    Path child = new Path(restrictedDir, \"child\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    fs.delete(restrictedDir, true);\r\n    Configuration conf = createAssumedRoleConfig();\r\n    bindRolePolicyStatements(conf, STATEMENT_ALL_BUCKET_READ_ACCESS, STATEMENT_ALLOW_SSE_KMS_RW, new Statement(Effects.Allow).addActions(S3_ALL_OPERATIONS).addResources(directory(restrictedDir)));\r\n    roleFS = (S3AFileSystem) restrictedDir.getFileSystem(conf);\r\n    roleFS.getFileStatus(ROOT);\r\n    roleFS.mkdirs(restrictedDir);\r\n    assertIsDirectory(restrictedDir);\r\n    touch(roleFS, child);\r\n    assertIsFile(child);\r\n    ContractTestUtils.assertDeleted(roleFS, child, true);\r\n    ContractTestUtils.assertDeleted(roleFS, restrictedDir, true);\r\n    roleFS.delete(restrictedDir, false);\r\n    Path sibling = new Path(restrictedDir.toUri() + \"sibling\");\r\n    touch(fs, sibling);\r\n    assertTouchForbidden(roleFS, sibling);\r\n    assertDeleteForbidden(roleFS, sibling);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "methodPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path methodPath() throws IOException\n{\r\n    return path(getMethodName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testAssumedRoleRetryHandler",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAssumedRoleRetryHandler() throws Throwable\n{\r\n    try (AssumedRoleCredentialProvider provider = new AssumedRoleCredentialProvider(getFileSystem().getUri(), createAssumedRoleConfig())) {\r\n        provider.operationRetried(\"retry\", new IOException(\"failure\"), 0, true);\r\n        provider.operationRetried(\"retry\", new IOException(\"failure\"), 1, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testRestrictedCommitActions",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testRestrictedCommitActions() throws Throwable\n{\r\n    describe(\"Attempt commit operations against a path with restricted rights\");\r\n    Configuration conf = createAssumedRoleConfig();\r\n    final int uploadPartSize = 5 * 1024 * 1024;\r\n    ProgressCounter progress = new ProgressCounter();\r\n    progress.assertCount(\"Progress counter should be zero\", 0);\r\n    Path basePath = methodPath();\r\n    Path readOnlyDir = new Path(basePath, \"readOnlyDir\");\r\n    Path writeableDir = new Path(basePath, \"writeableDir\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    fs.delete(basePath, true);\r\n    fs.mkdirs(readOnlyDir);\r\n    bindRolePolicyStatements(conf, STATEMENT_ALLOW_SSE_KMS_RW, STATEMENT_ALL_BUCKET_READ_ACCESS, new Statement(Effects.Allow).addActions(S3_PATH_RW_OPERATIONS).addResources(directory(writeableDir)));\r\n    roleFS = (S3AFileSystem) writeableDir.getFileSystem(conf);\r\n    CommitterStatistics committerStatistics = fs.newCommitterStatistics();\r\n    CommitOperations fullOperations = new CommitOperations(fs, committerStatistics);\r\n    CommitOperations operations = new CommitOperations(roleFS, committerStatistics);\r\n    File localSrc = File.createTempFile(\"source\", \"\");\r\n    writeCSVData(localSrc);\r\n    Path uploadDest = new Path(readOnlyDir, \"restricted.csv\");\r\n    forbidden(\"initiate MultiPartUpload\", () -> {\r\n        return operations.uploadFileToPendingCommit(localSrc, uploadDest, \"\", uploadPartSize, progress);\r\n    });\r\n    progress.assertCount(\"progress counter not expected.\", 0);\r\n    localSrc.delete();\r\n    localSrc.mkdirs();\r\n    int range = 2;\r\n    IntStream.rangeClosed(1, range).parallel().forEach((i) -> eval(() -> {\r\n        String name = \"part-000\" + i;\r\n        File src = new File(localSrc, name);\r\n        Path dest = new Path(readOnlyDir, name);\r\n        writeCSVData(src);\r\n        SinglePendingCommit pending = fullOperations.uploadFileToPendingCommit(src, dest, \"\", uploadPartSize, progress);\r\n        pending.save(fs, new Path(readOnlyDir, name + CommitConstants.PENDING_SUFFIX), true);\r\n        assertTrue(src.delete());\r\n    }));\r\n    progress.assertCount(\"progress counter is not expected\", range);\r\n    try {\r\n        Pair<PendingSet, List<Pair<LocatedFileStatus, IOException>>> pendingCommits = operations.loadSinglePendingCommits(readOnlyDir, true);\r\n        List<SinglePendingCommit> commits = pendingCommits.getLeft().getCommits();\r\n        assertEquals(range, commits.size());\r\n        try (CommitOperations.CommitContext commitContext = operations.initiateCommitOperation(uploadDest)) {\r\n            commits.parallelStream().forEach((c) -> {\r\n                CommitOperations.MaybeIOE maybeIOE = commitContext.commit(c, \"origin\");\r\n                Path path = c.destinationPath();\r\n                assertCommitAccessDenied(path, maybeIOE);\r\n            });\r\n        }\r\n        LOG.info(\"abortAllSinglePendingCommits({})\", readOnlyDir);\r\n        assertCommitAccessDenied(readOnlyDir, operations.abortAllSinglePendingCommits(readOnlyDir, true));\r\n        Path magicDestPath = new Path(readOnlyDir, CommitConstants.MAGIC + \"/\" + \"magic.txt\");\r\n        forbidden(\"\", () -> {\r\n            touch(roleFS, magicDestPath);\r\n            return fs.getFileStatus(magicDestPath);\r\n        });\r\n        forbidden(\"\", () -> operations.abortPendingUploadsUnderPath(readOnlyDir));\r\n    } finally {\r\n        LOG.info(\"Cleanup\");\r\n        fullOperations.abortPendingUploadsUnderPath(readOnlyDir);\r\n        LOG.info(\"Committer statistics {}\", ioStatisticsSourceToString(committerStatistics));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "assertCommitAccessDenied",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertCommitAccessDenied(final Path path, final CommitOperations.MaybeIOE maybeIOE)\n{\r\n    IOException ex = maybeIOE.getException();\r\n    assertNotNull(\"no IOE in \" + maybeIOE + \" for \" + path, ex);\r\n    if (!(ex instanceof AccessDeniedException)) {\r\n        ContractTestUtils.fail(\"Wrong exception class for commit to \" + path, ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "writeCSVData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void writeCSVData(final File localSrc) throws IOException\n{\r\n    try (FileOutputStream fo = new FileOutputStream(localSrc)) {\r\n        fo.write(\"1, true\".getBytes());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testPartialDelete",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testPartialDelete() throws Throwable\n{\r\n    describe(\"delete with part of the child tree read only; multidelete\");\r\n    executePartialDelete(createAssumedRoleConfig(), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testPartialDeleteSingleDelete",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testPartialDeleteSingleDelete() throws Throwable\n{\r\n    describe(\"delete with part of the child tree read only\");\r\n    executePartialDelete(createAssumedRoleConfig(), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "executePartialDelete",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void executePartialDelete(final Configuration conf, final boolean singleDelete) throws Exception\n{\r\n    conf.setBoolean(ENABLE_MULTI_DELETE, !singleDelete);\r\n    Path destDir = methodPath();\r\n    Path readOnlyDir = new Path(destDir, \"readonlyDir\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    fs.delete(destDir, true);\r\n    bindRolePolicyStatements(conf, STATEMENT_ALLOW_SSE_KMS_RW, statement(true, S3_ALL_BUCKETS, S3_ALL_OPERATIONS), new Statement(Effects.Deny).addActions(S3_PATH_WRITE_OPERATIONS).addResources(directory(readOnlyDir)));\r\n    roleFS = (S3AFileSystem) destDir.getFileSystem(conf);\r\n    int range = 10;\r\n    touchFiles(fs, readOnlyDir, range);\r\n    touchFiles(roleFS, destDir, range);\r\n    forbidden(\"\", () -> roleFS.delete(readOnlyDir, true));\r\n    forbidden(\"\", () -> roleFS.delete(destDir, true));\r\n    Path pathWhichDoesntExist = new Path(readOnlyDir, \"no-such-path\");\r\n    assertFalse(\"deleting \" + pathWhichDoesntExist, roleFS.delete(pathWhichDoesntExist, true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "testBucketLocationForbidden",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testBucketLocationForbidden() throws Throwable\n{\r\n    describe(\"Restrict role to read only\");\r\n    Configuration conf = createAssumedRoleConfig();\r\n    bindRolePolicyStatements(conf, STATEMENT_ALLOW_SSE_KMS_RW, statement(true, S3_ALL_BUCKETS, S3_ALL_OPERATIONS), statement(false, S3_ALL_BUCKETS, S3_GET_BUCKET_LOCATION));\r\n    Path path = methodPath();\r\n    roleFS = (S3AFileSystem) path.getFileSystem(conf);\r\n    forbidden(\"\", () -> roleFS.getBucketLocation());\r\n    S3GuardTool.BucketInfo infocmd = new S3GuardTool.BucketInfo(conf);\r\n    URI fsUri = getFileSystem().getUri();\r\n    String info = exec(infocmd, S3GuardTool.BucketInfo.NAME, fsUri.toString());\r\n    Assertions.assertThat(info).contains(S3GuardTool.BucketInfo.LOCATION_UNKNOWN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "beforeExecution",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AmazonWebServiceRequest beforeExecution(final AmazonWebServiceRequest request)\n{\r\n    INVOCATIONS.incrementAndGet();\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "getInvocationCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getInvocationCount()\n{\r\n    return INVOCATIONS.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "setFaults",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFaults(Faults... faults)\n{\r\n    this.faults = new HashSet<>(faults.length);\r\n    Collections.addAll(this.faults, faults);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "maybeFail",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void maybeFail(Faults condition) throws Failure\n{\r\n    if (faults.contains(condition)) {\r\n        if (resetOnFailure) {\r\n            faults.remove(condition);\r\n        }\r\n        throw new Failure();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getWorkPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getWorkPath() throws IOException\n{\r\n    maybeFail(Faults.getWorkPath);\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getOutputPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getOutputPath()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "setupJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupJob(JobContext jobContext) throws IOException\n{\r\n    maybeFail(Faults.setupJob);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "setupTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupTask(TaskAttemptContext taskContext) throws IOException\n{\r\n    maybeFail(Faults.setupTask);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "needsTaskCommit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean needsTaskCommit(TaskAttemptContext taskContext) throws IOException\n{\r\n    maybeFail(Faults.needsTaskCommit);\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "commitTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commitTask(TaskAttemptContext taskContext) throws IOException\n{\r\n    maybeFail(Faults.commitTask);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "abortTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortTask(TaskAttemptContext taskContext) throws IOException\n{\r\n    maybeFail(Faults.abortTask);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "commitJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commitJob(JobContext jobContext) throws IOException\n{\r\n    maybeFail(Faults.commitJob);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "abortJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortJob(JobContext jobContext, JobStatus.State state) throws IOException\n{\r\n    maybeFail(Faults.abortJob);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    fsUri = new URI(BASE + \"/\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "chooseStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration chooseStore(String classname)\n{\r\n    Configuration conf = new Configuration(false);\r\n    if (classname != null) {\r\n        conf.set(S3_METADATA_STORE_IMPL, classname, \"code\");\r\n    }\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testNoClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNoClass() throws Throwable\n{\r\n    checkOutcome(null, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testNullClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNullClass() throws Throwable\n{\r\n    checkOutcome(NULL_METADATA_STORE, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testLocalStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLocalStore() throws Throwable\n{\r\n    checkOutcome(S3GUARD_METASTORE_LOCAL, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testDDBStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDDBStore() throws Throwable\n{\r\n    intercept(PathIOException.class, () -> checkOutcome(S3GUARD_METASTORE_DYNAMO, false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testUnknownStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUnknownStore() throws Throwable\n{\r\n    intercept(PathIOException.class, \"unknownStore\", () -> checkOutcome(\"unknownStore\", false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "checkOutcome",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkOutcome(final String classname, final boolean outcome) throws PathIOException\n{\r\n    Configuration conf = chooseStore(classname);\r\n    Assertions.assertThat(checkNoS3Guard(fsUri, conf)).describedAs(\"check with classname %s\", classname).isEqualTo(outcome);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    getLogger().info(\"FS details {}\", getFileSystem());\r\n    super.teardown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    S3ATestUtils.disableFilesystemCaching(conf);\r\n    conf.setInt(Constants.MAX_PAGING_KEYS, 2);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return S3ATestConstants.S3A_TEST_TIMEOUT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    String bucketName = getTestBucketName(conf);\r\n    removeBucketOverrides(bucketName, conf, MAGIC_COMMITTER_ENABLED, S3A_COMMITTER_FACTORY_KEY, FS_S3A_COMMITTER_NAME, FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, FS_S3A_COMMITTER_STAGING_UNIQUE_FILENAMES, FAST_UPLOAD_BUFFER);\r\n    conf.setBoolean(MAGIC_COMMITTER_ENABLED, DEFAULT_MAGIC_COMMITTER_ENABLED);\r\n    conf.setLong(MIN_MULTIPART_THRESHOLD, MULTIPART_MIN_SIZE);\r\n    conf.setInt(MULTIPART_SIZE, MULTIPART_MIN_SIZE);\r\n    conf.set(FAST_UPLOAD_BUFFER, FAST_UPLOAD_BUFFER_ARRAY);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "log",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Logger log()\n{\r\n    return LOG;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "bindCommitter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void bindCommitter(Configuration conf, String factory, String committerName)\n{\r\n    conf.set(S3A_COMMITTER_FACTORY_KEY, factory);\r\n    if (StringUtils.isNotEmpty(committerName)) {\r\n        conf.set(FS_S3A_COMMITTER_NAME, committerName);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "rmdir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void rmdir(Path dir, Configuration conf) throws IOException\n{\r\n    if (dir != null) {\r\n        describe(\"deleting %s\", dir);\r\n        FileSystem fs = dir.getFileSystem(conf);\r\n        fs.delete(dir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "randomJobId",
  "errType" : [ "NumberFormatException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String randomJobId() throws Exception\n{\r\n    String testUniqueForkId = System.getProperty(TEST_UNIQUE_FORK_ID, \"0001\");\r\n    int l = testUniqueForkId.length();\r\n    String trailingDigits = testUniqueForkId.substring(l - 4, l);\r\n    try {\r\n        int digitValue = Integer.valueOf(trailingDigits);\r\n        return String.format(\"20070712%04d_%04d\", (long) (Math.random() * 1000), digitValue);\r\n    } catch (NumberFormatException e) {\r\n        throw new Exception(\"Failed to parse \" + trailingDigits, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "abortMultipartUploadsUnderPath",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int abortMultipartUploadsUnderPath(Path path) throws IOException\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    if (fs != null && path != null) {\r\n        String key = fs.pathToKey(path);\r\n        int count = 0;\r\n        try (AuditSpan span = span()) {\r\n            WriteOperationHelper writeOps = fs.getWriteOperationHelper();\r\n            count = writeOps.abortMultipartUploadsUnderPath(key);\r\n            if (count > 0) {\r\n                log().info(\"Multipart uploads deleted: {}\", count);\r\n            }\r\n        }\r\n        return count;\r\n    } else {\r\n        return 0;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "assertMultipartUploadsPending",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertMultipartUploadsPending(Path path) throws IOException\n{\r\n    assertTrue(\"No multipart uploads in progress under \" + path, countMultipartUploads(path) > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "assertNoMultipartUploadsPending",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void assertNoMultipartUploadsPending(Path path) throws IOException\n{\r\n    List<String> uploads = listMultipartUploads(getFileSystem(), pathToPrefix(path));\r\n    if (!uploads.isEmpty()) {\r\n        String result = uploads.stream().collect(Collectors.joining(\"\\n\"));\r\n        fail(\"Multipart uploads in progress under \" + path + \" \\n\" + result);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "countMultipartUploads",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int countMultipartUploads(Path path) throws IOException\n{\r\n    return countMultipartUploads(pathToPrefix(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "countMultipartUploads",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int countMultipartUploads(String prefix) throws IOException\n{\r\n    return listMultipartUploads(getFileSystem(), prefix).size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "pathToPrefix",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String pathToPrefix(Path path)\n{\r\n    return path == null ? \"\" : getFileSystem().pathToKey(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "verifySuccessMarker",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SuccessData verifySuccessMarker(Path dir, String jobId) throws IOException\n{\r\n    return validateSuccessFile(dir, \"\", getFileSystem(), \"query\", 0, jobId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "readFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String readFile(Path path) throws IOException\n{\r\n    return ContractTestUtils.readUTF8(getFileSystem(), path, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "assertSuccessMarkerDoesNotExist",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertSuccessMarkerDoesNotExist(Path dir) throws IOException\n{\r\n    assertPathDoesNotExist(\"Success marker\", new Path(dir, _SUCCESS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "taskAttemptForJob",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "TaskAttemptContext taskAttemptForJob(JobId jobId, JobContext jContext)\n{\r\n    org.apache.hadoop.mapreduce.v2.api.records.TaskId taskID = MRBuilderUtils.newTaskId(jobId, 0, org.apache.hadoop.mapreduce.v2.api.records.TaskType.MAP);\r\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = MRBuilderUtils.newTaskAttemptId(taskID, 0);\r\n    return new TaskAttemptContextImpl(jContext.getConfiguration(), TypeConverter.fromYarn(attemptID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "validateSuccessFile",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "SuccessData validateSuccessFile(final Path outputPath, final String committerName, final S3AFileSystem fs, final String origin, final int minimumFileCount, final String jobId) throws IOException\n{\r\n    SuccessData successData = loadSuccessFile(fs, outputPath, origin);\r\n    String commitDetails = successData.toString();\r\n    LOG.info(\"Committer name \" + committerName + \"\\n{}\", commitDetails);\r\n    LOG.info(\"Committer statistics: \\n{}\", successData.dumpMetrics(\"  \", \" = \", \"\\n\"));\r\n    LOG.info(\"Job IOStatistics: \\n{}\", ioStatisticsToString(successData.getIOStatistics()));\r\n    LOG.info(\"Diagnostics\\n{}\", successData.dumpDiagnostics(\"  \", \" = \", \"\\n\"));\r\n    if (!committerName.isEmpty()) {\r\n        assertEquals(\"Wrong committer in \" + commitDetails, committerName, successData.getCommitter());\r\n    }\r\n    Assertions.assertThat(successData.getFilenames()).describedAs(\"Files committed in \" + commitDetails).hasSizeGreaterThanOrEqualTo(minimumFileCount);\r\n    if (StringUtils.isNotEmpty(jobId)) {\r\n        Assertions.assertThat(successData.getJobId()).describedAs(\"JobID in \" + commitDetails).isEqualTo(jobId);\r\n    }\r\n    return successData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "loadSuccessFile",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "SuccessData loadSuccessFile(final FileSystem fs, final Path outputPath, final String origin) throws IOException\n{\r\n    ContractTestUtils.assertPathExists(fs, \"Output directory \" + outputPath + \" from \" + origin + \" not found: Job may not have executed\", outputPath);\r\n    Path success = new Path(outputPath, _SUCCESS);\r\n    FileStatus status = ContractTestUtils.verifyPathExists(fs, \"job completion marker \" + success + \" from \" + origin + \" not found: Job may have failed\", success);\r\n    assertTrue(\"_SUCCESS outout from \" + origin + \" is not a file \" + status, status.isFile());\r\n    assertTrue(\"0 byte success file \" + success + \" from \" + origin + \"; an S3A committer was not used\", status.getLen() > 0);\r\n    String body = ContractTestUtils.readUTF8(fs, success, -1);\r\n    LOG.info(\"Loading committer success file {}. Actual contents=\\n{}\", success, body);\r\n    return SuccessData.load(fs, success);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "cleanupParts",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void cleanupParts(S3AFileSystem fs, Set<IdKey> keySet)\n{\r\n    boolean anyFailure = false;\r\n    for (IdKey ik : keySet) {\r\n        try (AuditSpan span = fs.createSpan(\"multipart\", ik.key, null)) {\r\n            LOG.debug(\"aborting upload id {}\", ik.getUploadId());\r\n            fs.abortMultipartUpload(ik.getKey(), ik.getUploadId());\r\n        } catch (Exception e) {\r\n            LOG.error(String.format(\"Failure aborting upload %s, continuing.\", ik.getKey()), e);\r\n            anyFailure = true;\r\n        }\r\n    }\r\n    Assert.assertFalse(\"Failure aborting multipart upload(s), see log.\", anyFailure);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createPartUpload",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "IdKey createPartUpload(S3AFileSystem fs, String key, int len, int partNo) throws IOException\n{\r\n    try (AuditSpan span = fs.createSpan(\"multipart\", key, null)) {\r\n        WriteOperationHelper writeHelper = fs.getWriteOperationHelper();\r\n        byte[] data = dataset(len, 'a', 'z');\r\n        InputStream in = new ByteArrayInputStream(data);\r\n        String uploadId = writeHelper.initiateMultiPartUpload(key);\r\n        UploadPartRequest req = writeHelper.newUploadPartRequest(key, uploadId, partNo, len, in, null, 0L);\r\n        PartETag partEtag = writeHelper.uploadPart(req).getPartETag();\r\n        LOG.debug(\"uploaded part etag {}, upid {}\", partEtag.getETag(), uploadId);\r\n        return new IdKey(key, uploadId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "clearAnyUploads",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void clearAnyUploads(S3AFileSystem fs, Path path)\n{\r\n    String key = fs.pathToKey(path);\r\n    AuditSpan span = null;\r\n    try {\r\n        MultipartUtils.UploadIterator uploads = fs.listUploads(key);\r\n        span = fs.createSpan(\"multipart\", path.toString(), null);\r\n        final WriteOperationHelper helper = fs.getWriteOperationHelper();\r\n        while (uploads.hasNext()) {\r\n            MultipartUpload upload = uploads.next();\r\n            LOG.debug(\"Cleaning up upload: {} {}\", upload.getKey(), truncatedUploadId(upload.getUploadId()));\r\n            helper.abortMultipartUpload(upload.getKey(), upload.getUploadId(), true, LOG_EVENT);\r\n        }\r\n    } catch (IOException ioe) {\r\n        LOG.info(\"Ignoring exception: \", ioe);\r\n    } finally {\r\n        IOUtils.closeStream(span);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertNoUploadsAt",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void assertNoUploadsAt(S3AFileSystem fs, Path path) throws Exception\n{\r\n    String key = fs.pathToKey(path);\r\n    MultipartUtils.UploadIterator uploads = fs.listUploads(key);\r\n    while (uploads.hasNext()) {\r\n        MultipartUpload upload = uploads.next();\r\n        Assert.fail(\"Found unexpected upload \" + upload.getKey() + \" \" + truncatedUploadId(upload.getUploadId()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "countUploadsAt",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int countUploadsAt(S3AFileSystem fs, Path path) throws IOException\n{\r\n    String key = fs.pathToKey(path);\r\n    MultipartUtils.UploadIterator uploads = fs.listUploads(key);\r\n    int count = 0;\r\n    while (uploads.hasNext()) {\r\n        MultipartUpload upload = uploads.next();\r\n        count++;\r\n    }\r\n    return count;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listMultipartUploads",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<String> listMultipartUploads(S3AFileSystem fs, String prefix) throws IOException\n{\r\n    try (AuditSpan span = fs.createSpan(\"multipart\", prefix, null)) {\r\n        return fs.listMultipartUploads(prefix).stream().map(upload -> String.format(\"Upload to %s with ID %s; initiated %s\", upload.getKey(), upload.getUploadId(), S3ATestUtils.LISTING_FORMAT.format(upload.getInitiated()))).collect(Collectors.toList());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "truncatedUploadId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String truncatedUploadId(String fullId)\n{\r\n    return fullId.substring(0, 12) + \" ...\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setup()\n{\r\n    resetCounters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "serviceException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AmazonServiceException serviceException(int code, String text)\n{\r\n    AmazonServiceException ex = new AmazonServiceException(text);\r\n    ex.setStatusCode(code);\r\n    return ex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createS3Exception",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AmazonS3Exception createS3Exception(int code)\n{\r\n    return createS3Exception(code, \"\", null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createS3Exception",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AmazonS3Exception createS3Exception(int code, String message, Throwable inner)\n{\r\n    AmazonS3Exception ex = new AmazonS3Exception(message);\r\n    ex.setStatusCode(code);\r\n    ex.initCause(inner);\r\n    return ex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "verifyTranslated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyTranslated(int status, Class<E> expected) throws Exception\n{\r\n    verifyTranslated(expected, createS3Exception(status));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "verifyTranslated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "E verifyTranslated(Class<E> clazz, SdkBaseException exception) throws Exception\n{\r\n    return verifyExceptionClass(clazz, translateException(\"test\", \"/\", exception));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "resetCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void resetCounters()\n{\r\n    retryCount = 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "test503isThrottled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test503isThrottled() throws Exception\n{\r\n    verifyTranslated(503, AWSServiceThrottledException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testS3500isStatus500Exception",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testS3500isStatus500Exception() throws Exception\n{\r\n    verifyTranslated(500, AWSStatus500Exception.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "test500isStatus500Exception",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void test500isStatus500Exception() throws Exception\n{\r\n    AmazonServiceException ex = new AmazonServiceException(\"\");\r\n    ex.setStatusCode(500);\r\n    verifyTranslated(AWSStatus500Exception.class, ex);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testExceptionsWithTranslatableMessage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testExceptionsWithTranslatableMessage() throws Exception\n{\r\n    SdkBaseException xmlParsing = new SdkBaseException(EOF_MESSAGE_IN_XML_PARSER);\r\n    SdkBaseException differentLength = new SdkBaseException(EOF_READ_DIFFERENT_LENGTH);\r\n    verifyTranslated(EOFException.class, xmlParsing);\r\n    verifyTranslated(EOFException.class, differentLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSdkDifferentLengthExceptionIsTranslatable",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSdkDifferentLengthExceptionIsTranslatable() throws Throwable\n{\r\n    final AtomicInteger counter = new AtomicInteger(0);\r\n    invoker.retry(\"test\", null, false, () -> {\r\n        if (counter.incrementAndGet() < ACTIVE_RETRY_LIMIT) {\r\n            throw new SdkClientException(EOF_READ_DIFFERENT_LENGTH);\r\n        }\r\n    });\r\n    assertEquals(ACTIVE_RETRY_LIMIT, counter.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testSdkXmlParsingExceptionIsTranslatable",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSdkXmlParsingExceptionIsTranslatable() throws Throwable\n{\r\n    final AtomicInteger counter = new AtomicInteger(0);\r\n    invoker.retry(\"test\", null, false, () -> {\r\n        if (counter.incrementAndGet() < ACTIVE_RETRY_LIMIT) {\r\n            throw new SdkClientException(EOF_MESSAGE_IN_XML_PARSER);\r\n        }\r\n    });\r\n    assertEquals(ACTIVE_RETRY_LIMIT, counter.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testExtractConnectTimeoutException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testExtractConnectTimeoutException() throws Throwable\n{\r\n    throw extractException(\"\", \"\", new ExecutionException(new AmazonClientException(LOCAL_CONNECTION_TIMEOUT_EX)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testExtractSocketTimeoutException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testExtractSocketTimeoutException() throws Throwable\n{\r\n    throw extractException(\"\", \"\", new ExecutionException(new AmazonClientException(SOCKET_TIMEOUT_EX)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertRetryAction",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertRetryAction(String text, S3ARetryPolicy policy, RetryPolicy.RetryAction expected, Exception ex, int retries, boolean idempotent) throws Exception\n{\r\n    RetryPolicy.RetryAction outcome = policy.shouldRetry(ex, retries, 0, idempotent);\r\n    if (!expected.action.equals(outcome.action)) {\r\n        throw new AssertionError(String.format(\"%s Expected action %s from shouldRetry(%s, %s, %s), but got\" + \" %s\", text, expected, ex.toString(), retries, idempotent, outcome.action), ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRetryThrottled",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testRetryThrottled() throws Throwable\n{\r\n    S3ARetryPolicy policy = RETRY_POLICY;\r\n    IOException ex = translateException(\"GET\", \"/\", newThrottledException());\r\n    assertRetryAction(\"Expected retry on first throttle\", policy, RetryPolicy.RetryAction.RETRY, ex, 0, true);\r\n    int retries = SAFE_RETRY_COUNT;\r\n    assertRetryAction(\"Expected retry on repeated throttle\", policy, RetryPolicy.RetryAction.RETRY, ex, retries, true);\r\n    assertRetryAction(\"Expected retry on non-idempotent throttle\", policy, RetryPolicy.RetryAction.RETRY, ex, retries, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newThrottledException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AmazonServiceException newThrottledException()\n{\r\n    return serviceException(AWSServiceThrottledException.STATUS_CODE, \"throttled\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRetryOnThrottle",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRetryOnThrottle() throws Throwable\n{\r\n    final AtomicInteger counter = new AtomicInteger(0);\r\n    invoker.retry(\"test\", null, false, () -> {\r\n        if (counter.incrementAndGet() < 5) {\r\n            throw newThrottledException();\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testNoRetryOfBadRequestNonIdempotent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNoRetryOfBadRequestNonIdempotent() throws Throwable\n{\r\n    invoker.retry(\"test\", null, false, () -> {\r\n        throw serviceException(400, \"bad request\");\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRetryAWSConnectivity",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRetryAWSConnectivity() throws Throwable\n{\r\n    final AtomicInteger counter = new AtomicInteger(0);\r\n    invoker.retry(\"test\", null, false, () -> {\r\n        if (counter.incrementAndGet() < ACTIVE_RETRY_LIMIT) {\r\n            throw CLIENT_TIMEOUT_EXCEPTION;\r\n        }\r\n    });\r\n    assertEquals(ACTIVE_RETRY_LIMIT, counter.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testRetryBadRequestNotIdempotent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRetryBadRequestNotIdempotent() throws Throwable\n{\r\n    invoker.retry(\"test\", null, false, () -> {\r\n        throw BAD_REQUEST;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testConnectionRetryPolicyIdempotent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testConnectionRetryPolicyIdempotent() throws Throwable\n{\r\n    assertRetryAction(\"Expected retry on connection timeout\", RETRY_POLICY, RetryPolicy.RetryAction.RETRY, HADOOP_CONNECTION_TIMEOUT_EX, 1, true);\r\n    assertRetryAction(\"Expected connection timeout failure\", RETRY_POLICY, RetryPolicy.RetryAction.FAIL, HADOOP_CONNECTION_TIMEOUT_EX, RETRIES_TOO_MANY, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testConnectionRetryPolicyNonIdempotent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testConnectionRetryPolicyNonIdempotent() throws Throwable\n{\r\n    assertRetryAction(\"Expected retry on connection timeout\", RETRY_POLICY, RetryPolicy.RetryAction.RETRY, HADOOP_CONNECTION_TIMEOUT_EX, 1, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testInterruptedIOExceptionRetry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testInterruptedIOExceptionRetry() throws Throwable\n{\r\n    assertRetryAction(\"Expected retry on connection timeout\", RETRY_POLICY, RetryPolicy.RetryAction.FAIL, new InterruptedIOException(\"interrupted\"), 1, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testUnshadedConnectionTimeoutExceptionMatching",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUnshadedConnectionTimeoutExceptionMatching() throws Throwable\n{\r\n    verifyTranslated(ConnectTimeoutException.class, new AmazonClientException(HTTP_CONNECTION_TIMEOUT_EX));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testShadedConnectionTimeoutExceptionMatching",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testShadedConnectionTimeoutExceptionMatching() throws Throwable\n{\r\n    verifyTranslated(ConnectTimeoutException.class, new AmazonClientException(LOCAL_CONNECTION_TIMEOUT_EX));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testShadedConnectionTimeoutExceptionNotMatching",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testShadedConnectionTimeoutExceptionNotMatching() throws Throwable\n{\r\n    InterruptedIOException ex = verifyTranslated(InterruptedIOException.class, new AmazonClientException(new Local.NotAConnectTimeoutException()));\r\n    if (ex instanceof ConnectTimeoutException) {\r\n        throw ex;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testNPEsNotRetried",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testNPEsNotRetried() throws Throwable\n{\r\n    assertRetryAction(\"Expected NPE trigger failure\", RETRY_POLICY, RetryPolicy.RetryAction.FAIL, new NullPointerException(\"oops\"), 1, true);\r\n    assertEquals(\"retry count \", 0, retryCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testQuietlyVoid",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testQuietlyVoid()\n{\r\n    quietlyEval(\"\", \"\", () -> {\r\n        throw HADOOP_CONNECTION_TIMEOUT_EX;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testQuietlyEvalReturnValueSuccess",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testQuietlyEvalReturnValueSuccess()\n{\r\n    assertOptionalEquals(\"quietly\", 3, quietlyEval(\"\", \"\", () -> 3));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testQuietlyEvalReturnValueFail",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testQuietlyEvalReturnValueFail()\n{\r\n    int d = 0;\r\n    assertOptionalUnset(\"quietly\", quietlyEval(\"\", \"\", () -> 3 / d));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEncryptionAlgorithmSetToDES",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testEncryptionAlgorithmSetToDES() throws Throwable\n{\r\n    assumeEnabled();\r\n    intercept(IOException.class, \"Unknown encryption algorithm DES\", () -> {\r\n        Configuration conf = super.createConfiguration();\r\n        conf.set(Constants.S3_ENCRYPTION_ALGORITHM, \"DES\");\r\n        S3AContract contract = (S3AContract) createContract(conf);\r\n        contract.init();\r\n        FileSystem fileSystem = contract.getTestFileSystem();\r\n        assertNotNull(\"null filesystem\", fileSystem);\r\n        URI fsURI = fileSystem.getUri();\r\n        LOG.info(\"Test filesystem = {} implemented by {}\", fsURI, fileSystem);\r\n        assertEquals(\"wrong filesystem of \" + fsURI, contract.getScheme(), fsURI.getScheme());\r\n        fileSystem.initialize(fsURI, conf);\r\n        return fileSystem;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEncryptionAlgorithmSSECWithNoEncryptionKey",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testEncryptionAlgorithmSSECWithNoEncryptionKey() throws Throwable\n{\r\n    assumeEnabled();\r\n    intercept(IllegalArgumentException.class, \"The value of property \" + Constants.S3_ENCRYPTION_KEY + \" must not be null\", () -> {\r\n        Configuration conf = super.createConfiguration();\r\n        conf.set(Constants.S3_ENCRYPTION_ALGORITHM, S3AEncryptionMethods.SSE_C.getMethod());\r\n        conf.set(Constants.S3_ENCRYPTION_KEY, null);\r\n        S3AContract contract = (S3AContract) createContract(conf);\r\n        contract.init();\r\n        FileSystem fileSystem = contract.getTestFileSystem();\r\n        assertNotNull(\"null filesystem\", fileSystem);\r\n        URI fsURI = fileSystem.getUri();\r\n        LOG.info(\"Test filesystem = {} implemented by {}\", fsURI, fileSystem);\r\n        assertEquals(\"wrong filesystem of \" + fsURI, contract.getScheme(), fsURI.getScheme());\r\n        fileSystem.initialize(fsURI, conf);\r\n        return fileSystem;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEncryptionAlgorithmSSECWithBlankEncryptionKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testEncryptionAlgorithmSSECWithBlankEncryptionKey() throws Throwable\n{\r\n    intercept(IOException.class, S3AUtils.SSE_C_NO_KEY_ERROR, () -> {\r\n        Configuration conf = super.createConfiguration();\r\n        conf.set(Constants.S3_ENCRYPTION_ALGORITHM, S3AEncryptionMethods.SSE_C.getMethod());\r\n        conf.set(Constants.S3_ENCRYPTION_KEY, \"\");\r\n        S3AContract contract = (S3AContract) createContract(conf);\r\n        contract.init();\r\n        FileSystem fileSystem = contract.getTestFileSystem();\r\n        assertNotNull(\"null filesystem\", fileSystem);\r\n        URI fsURI = fileSystem.getUri();\r\n        LOG.info(\"Test filesystem = {} implemented by {}\", fsURI, fileSystem);\r\n        assertEquals(\"wrong filesystem of \" + fsURI, contract.getScheme(), fsURI.getScheme());\r\n        fileSystem.initialize(fsURI, conf);\r\n        return fileSystem;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEncryptionAlgorithmSSES3WithEncryptionKey",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testEncryptionAlgorithmSSES3WithEncryptionKey() throws Throwable\n{\r\n    assumeEnabled();\r\n    intercept(IOException.class, S3AUtils.SSE_S3_WITH_KEY_ERROR, () -> {\r\n        Configuration conf = super.createConfiguration();\r\n        conf.set(Constants.S3_ENCRYPTION_ALGORITHM, S3AEncryptionMethods.SSE_S3.getMethod());\r\n        conf.set(Constants.S3_ENCRYPTION_KEY, \"4niV/jPK5VFRHY+KNb6wtqYd4xXyMgdJ9XQJpcQUVbs=\");\r\n        S3AContract contract = (S3AContract) createContract(conf);\r\n        contract.init();\r\n        FileSystem fileSystem = contract.getTestFileSystem();\r\n        assertNotNull(\"null filesystem\", fileSystem);\r\n        URI fsURI = fileSystem.getUri();\r\n        LOG.info(\"Test filesystem = {} implemented by {}\", fsURI, fileSystem);\r\n        assertEquals(\"wrong filesystem of \" + fsURI, contract.getScheme(), fsURI.getScheme());\r\n        fileSystem.initialize(fsURI, conf);\r\n        return fileSystem;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void mkdirs(Path path) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { \"keep-markers\", true }, { \"delete-markers\", false } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    String bucketName = getTestBucketName(conf);\r\n    removeBaseAndBucketOverrides(bucketName, conf, DIRECTORY_MARKER_POLICY);\r\n    conf.set(DIRECTORY_MARKER_POLICY, keepMarkers ? DIRECTORY_MARKER_POLICY_KEEP : DIRECTORY_MARKER_POLICY_DELETE);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    S3AFileSystem fs = getFileSystem();\r\n    s3client = fs.getAmazonS3ClientForTesting(\"markers\");\r\n    bucket = fs.getBucket();\r\n    Path base = new Path(methodPath(), \"base\");\r\n    createTestObjects(base);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    if (s3client != null) {\r\n        deleteObject(markerKey);\r\n        deleteObject(markerKeySlash);\r\n        deleteObject(markerPeerKey);\r\n        deleteObject(fileKeyUnderMarker);\r\n    }\r\n    deleteTestDirInTeardown();\r\n    super.teardown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "createTestObjects",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void createTestObjects(final Path path) throws Exception\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    basePath = path;\r\n    markerDir = new Path(basePath, MARKER);\r\n    markerPeer = new Path(basePath, MARKER_PEER);\r\n    markerPeerKey = fs.pathToKey(markerPeer);\r\n    markerKey = fs.pathToKey(markerDir);\r\n    markerKeySlash = markerKey + \"/\";\r\n    fileKeyUnderMarker = markerKeySlash + FILENAME;\r\n    filePathUnderMarker = new Path(markerDir, FILENAME);\r\n    fs.mkdirs(markerDir);\r\n    touch(fs, markerPeer);\r\n    put(fileKeyUnderMarker, HELLO);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testMarkerExists",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testMarkerExists() throws Throwable\n{\r\n    describe(\"Verify the marker exists\");\r\n    head(markerKeySlash);\r\n    assertIsDirectory(markerDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testObjectUnderMarker",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testObjectUnderMarker() throws Throwable\n{\r\n    describe(\"verify the file under the marker dir exists\");\r\n    assertIsFile(filePathUnderMarker);\r\n    head(fileKeyUnderMarker);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testListStatusMarkerDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testListStatusMarkerDir() throws Throwable\n{\r\n    describe(\"list the marker directory and expect to see the file\");\r\n    assertContainsFileUnderMarkerOnly(toList(getFileSystem().listStatus(markerDir)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testListFilesMarkerDirFlat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testListFilesMarkerDirFlat() throws Throwable\n{\r\n    assertContainsFileUnderMarkerOnly(toList(getFileSystem().listFiles(markerDir, false)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testListFilesMarkerDirRecursive",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testListFilesMarkerDirRecursive() throws Throwable\n{\r\n    List<FileStatus> statuses = toList(getFileSystem().listFiles(markerDir, true));\r\n    assertContainsFileUnderMarkerOnly(statuses);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testListStatusBaseDirRecursive",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testListStatusBaseDirRecursive() throws Throwable\n{\r\n    List<FileStatus> statuses = toList(getFileSystem().listFiles(basePath, true));\r\n    assertContainsExactlyStatusOfPaths(statuses, filePathUnderMarker, markerPeer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testGlobStatusBaseDirRecursive",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGlobStatusBaseDirRecursive() throws Throwable\n{\r\n    Path escapedPath = new Path(escape(basePath.toUri().getPath()));\r\n    List<FileStatus> statuses = exec(\"glob\", () -> toList(getFileSystem().globStatus(new Path(escapedPath, \"*\"))));\r\n    assertContainsExactlyStatusOfPaths(statuses, markerDir, markerPeer);\r\n    assertIsFileAtPath(markerPeer, statuses.get(1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testGlobStatusMarkerDir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGlobStatusMarkerDir() throws Throwable\n{\r\n    Path escapedPath = new Path(escape(markerDir.toUri().getPath()));\r\n    List<FileStatus> statuses = exec(\"glob\", () -> toList(getFileSystem().globStatus(new Path(escapedPath, \"*\"))));\r\n    assertContainsFileUnderMarkerOnly(statuses);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testListLocatedStatusBaseDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testListLocatedStatusBaseDir() throws Throwable\n{\r\n    List<FileStatus> statuses = exec(\"listLocatedStatus\", () -> toList(getFileSystem().listLocatedStatus(basePath)));\r\n    assertContainsExactlyStatusOfPaths(statuses, markerPeer, markerDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testListLocatedStatusMarkerDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testListLocatedStatusMarkerDir() throws Throwable\n{\r\n    List<FileStatus> statuses = exec(\"listLocatedStatus\", () -> toList(getFileSystem().listLocatedStatus(markerDir)));\r\n    assertContainsFileUnderMarkerOnly(statuses);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testCreateNoOverwriteMarkerDir",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCreateNoOverwriteMarkerDir() throws Throwable\n{\r\n    describe(\"create no-overwrite over the marker dir fails\");\r\n    head(markerKeySlash);\r\n    intercept(FileAlreadyExistsException.class, () -> exec(\"create\", () -> getFileSystem().create(markerDir, false)));\r\n    head(markerKeySlash);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testCreateNoOverwriteFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCreateNoOverwriteFile() throws Throwable\n{\r\n    describe(\"create-no-overwrite on the file fails\");\r\n    head(fileKeyUnderMarker);\r\n    intercept(FileAlreadyExistsException.class, () -> exec(\"create\", () -> getFileSystem().create(filePathUnderMarker, false)));\r\n    assertTestObjectsExist();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testCreateFileNoOverwrite",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCreateFileNoOverwrite() throws Throwable\n{\r\n    describe(\"verify the createFile() API also fails\");\r\n    head(fileKeyUnderMarker);\r\n    intercept(FileAlreadyExistsException.class, () -> exec(\"create\", () -> getFileSystem().createFile(filePathUnderMarker).overwrite(false).build()));\r\n    assertTestObjectsExist();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testDelete",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testDelete() throws Throwable\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    intercept(PathIsNotEmptyDirectoryException.class, () -> fs.delete(markerDir, false));\r\n    head(fileKeyUnderMarker);\r\n    fs.delete(markerDir, true);\r\n    head404(fileKeyUnderMarker);\r\n    head404(markerKeySlash);\r\n    fs.delete(basePath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testRenameBase",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRenameBase() throws Throwable\n{\r\n    describe(\"rename base directory\");\r\n    Path src = basePath;\r\n    Path dest = new Path(methodPath(), \"dest\");\r\n    assertRenamed(src, dest);\r\n    assertPathDoesNotExist(\"source\", src);\r\n    assertPathDoesNotExist(\"source\", filePathUnderMarker);\r\n    assertPathExists(\"dest not found\", dest);\r\n    Path destMarkerDir = new Path(dest, MARKER);\r\n    Path destMarkerPeer = new Path(dest, MARKER_PEER);\r\n    String destMarkerKey = toKey(destMarkerDir);\r\n    String destMarkerKeySlash = destMarkerKey + \"/\";\r\n    String destFileKeyUnderMarker = destMarkerKeySlash + FILENAME;\r\n    Path destFilePathUnderMarker = new Path(destMarkerDir, FILENAME);\r\n    assertIsFile(destFilePathUnderMarker);\r\n    assertIsFile(destMarkerPeer);\r\n    head(destFileKeyUnderMarker);\r\n    if (RENAME_COPIES_MARKERS) {\r\n        head(destMarkerKeySlash);\r\n    } else {\r\n        head404(destMarkerKeySlash);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testRenameUnderMarkerDir",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testRenameUnderMarkerDir() throws Throwable\n{\r\n    describe(\"directory rename under an existing marker\");\r\n    String file = \"sourceFile\";\r\n    Path srcDir = new Path(basePath, \"srcdir\");\r\n    mkdirs(srcDir);\r\n    Path src = new Path(srcDir, file);\r\n    String srcKey = toKey(src);\r\n    put(srcKey, file);\r\n    head(srcKey);\r\n    Path dest = markerDir;\r\n    assertRenamed(src, dest);\r\n    assertIsFile(new Path(dest, file));\r\n    assertIsDirectory(srcDir);\r\n    if (isDeletingMarkers) {\r\n        head404(markerKeySlash);\r\n    } else {\r\n        head(markerKeySlash);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testRenameUnderMarkerWithPath",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRenameUnderMarkerWithPath() throws Throwable\n{\r\n    describe(\"directory rename under an existing marker\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    String file = \"sourceFile\";\r\n    Path srcDir = new Path(basePath, \"srcdir\");\r\n    mkdirs(srcDir);\r\n    Path src = new Path(srcDir, file);\r\n    String srcKey = toKey(src);\r\n    put(srcKey, file);\r\n    head(srcKey);\r\n    Path dest = new Path(markerDir, \"destFile\");\r\n    assertRenamed(src, dest);\r\n    assertIsFile(dest);\r\n    assertIsDirectory(srcDir);\r\n    if (isDeletingMarkers) {\r\n        head404(markerKeySlash);\r\n    } else {\r\n        head(markerKeySlash);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "testRenameEmptyDirOverMarker",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRenameEmptyDirOverMarker() throws Throwable\n{\r\n    describe(\"rename an empty directory over the marker\");\r\n    S3AFileSystem fs = getFileSystem();\r\n    String dir = \"sourceDir\";\r\n    Path src = new Path(basePath, dir);\r\n    fs.mkdirs(src);\r\n    assertIsDirectory(src);\r\n    String srcKey = toKey(src) + \"/\";\r\n    head(srcKey);\r\n    Path dest = markerDir;\r\n    assertFalse(\"rename(\" + src + \", \" + dest + \") should have failed\", getFileSystem().rename(src, dest));\r\n    assertIsDirectory(src);\r\n    head(srcKey);\r\n    assertDeleted(src, false);\r\n    assertTestObjectsExist();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "assertTestObjectsExist",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertTestObjectsExist() throws Exception\n{\r\n    head(fileKeyUnderMarker);\r\n    head(markerKeySlash);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "put",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void put(final String key, final String content) throws Exception\n{\r\n    exec(\"PUT \" + key, () -> s3client.putObject(bucket, key, content));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "deleteObject",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deleteObject(final String key) throws Exception\n{\r\n    exec(\"DELETE \" + key, () -> {\r\n        s3client.deleteObject(bucket, key);\r\n        return \"deleted \" + key;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "head",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String head(final String key) throws Exception\n{\r\n    ObjectMetadata md = exec(\"HEAD \" + key, () -> s3client.getObjectMetadata(bucket, key));\r\n    return String.format(\"Object %s of length %d\", key, md.getInstanceLength());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "head404",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void head404(final String key) throws Exception\n{\r\n    intercept(FileNotFoundException.class, \"\", \"Expected 404 of \" + key, () -> head(key));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "exec",
  "errType" : [ "AmazonClientException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "T exec(String op, Callable<T> call) throws Exception\n{\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    try (AuditSpan span = getSpanSource().createSpan(op, null, null)) {\r\n        return call.call();\r\n    } catch (AmazonClientException ex) {\r\n        throw S3AUtils.translateException(op, \"\", ex);\r\n    } finally {\r\n        timer.end(op);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "assertContainsFileUnderMarkerOnly",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertContainsFileUnderMarkerOnly(final List<FileStatus> statuses)\n{\r\n    assertContainsExactlyStatusOfPaths(statuses, filePathUnderMarker);\r\n    assertIsFileUnderMarker(statuses.get(0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "assertContainsExactlyStatusOfPaths",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void assertContainsExactlyStatusOfPaths(List<T> statuses, Path... paths)\n{\r\n    String actual = statuses.stream().map(Object::toString).collect(Collectors.joining(\";\"));\r\n    String expected = Arrays.stream(paths).map(Object::toString).collect(Collectors.joining(\";\"));\r\n    String summary = \"expected [\" + expected + \"]\" + \" actual = [\" + actual + \"]\";\r\n    assertEquals(\"mismatch in size of listing \" + summary, paths.length, statuses.size());\r\n    for (int i = 0; i < statuses.size(); i++) {\r\n        assertEquals(\"Path mismatch at element \" + i + \" in \" + summary, paths[i], statuses.get(i).getPath());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "assertIsFileUnderMarker",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertIsFileUnderMarker(final FileStatus stat)\n{\r\n    assertIsFileAtPath(filePathUnderMarker, stat);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "assertIsFileAtPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertIsFileAtPath(final Path path, final FileStatus stat)\n{\r\n    assertTrue(\"Is not file \" + stat, stat.isFile());\r\n    assertPathEquals(path, stat);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "assertPathEquals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertPathEquals(final Path path, final FileStatus stat)\n{\r\n    assertEquals(\"filename is not the expected path :\" + stat, path, stat.getPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "toList",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<FileStatus> toList(RemoteIterator<T> status) throws IOException\n{\r\n    List<FileStatus> l = new ArrayList<>();\r\n    foreach(status, st -> l.add(st));\r\n    return dump(l);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "toList",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<FileStatus> toList(T[] status) throws IOException\n{\r\n    return dump(Arrays.asList(status));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "dump",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<T> dump(List<T> l)\n{\r\n    int c = 1;\r\n    for (T t : l) {\r\n        LOG.info(\"{}\\t{}\", c++, t);\r\n    }\r\n    return l;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "assertRenamed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertRenamed(final Path src, final Path dest) throws IOException\n{\r\n    assertTrue(\"rename(\" + src + \", \" + dest + \") failed\", getFileSystem().rename(src, dest));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "toKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toKey(final Path path)\n{\r\n    return getFileSystem().pathToKey(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\performance",
  "methodName" : "escape",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String escape(String pathstr)\n{\r\n    StringBuilder r = new StringBuilder();\r\n    for (char c : pathstr.toCharArray()) {\r\n        String ch = Character.toString(c);\r\n        if (\"?*[{\".contains(ch)) {\r\n            r.append(\"\\\\\");\r\n        }\r\n        r.append(ch);\r\n    }\r\n    return r.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    CONTEXT_ACCESSORS.len = FILE_LENGTH;\r\n    CONTEXT_ACCESSORS.userHeaders.put(X_HEADER_MAGIC_MARKER, Long.toString(MAGIC_LEN));\r\n    context = S3ATestUtils.createMockStoreContext(true, CONTEXT_ACCESSORS);\r\n    headerProcessing = new HeaderProcessing(context, CONTEXT_ACCESSORS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testByteRoundTrip",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testByteRoundTrip() throws Throwable\n{\r\n    Assertions.assertThat(decodeBytes(encodeBytes(VALUE))).describedAs(\"encoding of \" + VALUE).isEqualTo(VALUE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testGetMarkerXAttr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetMarkerXAttr() throws Throwable\n{\r\n    assertAttributeHasValue(XA_MAGIC_MARKER, MAGIC_LEN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testGetLengthXAttr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetLengthXAttr() throws Throwable\n{\r\n    assertAttributeHasValue(XA_CONTENT_LENGTH, FILE_LENGTH);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testGetDateXAttr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetDateXAttr() throws Throwable\n{\r\n    Assertions.assertThat(decodeBytes(headerProcessing.getXAttr(MAGIC_PATH, XA_LAST_MODIFIED))).describedAs(\"XAttribute \" + XA_LAST_MODIFIED).isEqualTo(CONTEXT_ACCESSORS.date.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "test404",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test404() throws Throwable\n{\r\n    intercept(FileNotFoundException.class, () -> headerProcessing.getXAttr(new Path(FINAL_FILE), XA_MAGIC_MARKER));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testGetAllXAttrs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetAllXAttrs() throws Throwable\n{\r\n    Map<String, byte[]> xAttrs = headerProcessing.getXAttrs(MAGIC_PATH);\r\n    Assertions.assertThat(xAttrs.keySet()).describedAs(\"Attribute keys\").contains(RETRIEVED_XATTRS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testListXAttrKeys",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testListXAttrKeys() throws Throwable\n{\r\n    List<String> xAttrs = headerProcessing.listXAttrs(MAGIC_PATH);\r\n    Assertions.assertThat(xAttrs).describedAs(\"Attribute keys\").contains(RETRIEVED_XATTRS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testGetFilteredXAttrs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetFilteredXAttrs() throws Throwable\n{\r\n    Map<String, byte[]> xAttrs = headerProcessing.getXAttrs(MAGIC_PATH, Lists.list(XA_MAGIC_MARKER, XA_CONTENT_LENGTH, \"unknown\"));\r\n    Assertions.assertThat(xAttrs.keySet()).describedAs(\"Attribute keys\").containsExactlyInAnyOrder(XA_MAGIC_MARKER, XA_CONTENT_LENGTH);\r\n    assertLongAttributeValue(XA_MAGIC_MARKER, xAttrs.get(XA_MAGIC_MARKER), MAGIC_LEN);\r\n    assertLongAttributeValue(XA_CONTENT_LENGTH, xAttrs.get(XA_CONTENT_LENGTH), FILE_LENGTH);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testFilterEmptyXAttrs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testFilterEmptyXAttrs() throws Throwable\n{\r\n    Map<String, byte[]> xAttrs = headerProcessing.getXAttrs(MAGIC_PATH, Lists.list());\r\n    Assertions.assertThat(xAttrs.keySet()).describedAs(\"Attribute keys\").isEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "testMetadataCopySkipsMagicAttribute",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testMetadataCopySkipsMagicAttribute() throws Throwable\n{\r\n    final String owner = \"x-header-owner\";\r\n    final String root = \"root\";\r\n    CONTEXT_ACCESSORS.userHeaders.put(owner, root);\r\n    final ObjectMetadata source = CONTEXT_ACCESSORS.getObjectMetadata(MAGIC_KEY);\r\n    final Map<String, String> sourceUserMD = source.getUserMetadata();\r\n    Assertions.assertThat(sourceUserMD.get(owner)).describedAs(\"owner header in copied MD\").isEqualTo(root);\r\n    ObjectMetadata dest = new ObjectMetadata();\r\n    headerProcessing.cloneObjectMetadata(source, dest);\r\n    Assertions.assertThat(dest.getUserMetadata().get(X_HEADER_MAGIC_MARKER)).describedAs(\"Magic marker header in copied MD\").isNull();\r\n    Assertions.assertThat(dest.getUserMetadata().get(owner)).describedAs(\"owner header in copied MD\").isEqualTo(root);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "assertLongAttributeValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertLongAttributeValue(final String key, final byte[] bytes, final long expected)\n{\r\n    Assertions.assertThat(extractXAttrLongValue(bytes)).describedAs(\"XAttribute \" + key).isNotEmpty().hasValue(expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "assertAttributeHasValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertAttributeHasValue(final String key, final long expected) throws IOException\n{\r\n    assertLongAttributeValue(key, headerProcessing.getXAttr(MAGIC_PATH, key), expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\s3a",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new S3AContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics",
  "methodName" : "testBytesReadWithStream",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testBytesReadWithStream() throws IOException\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path filePath = path(getMethodName());\r\n    byte[] oneKbBuf = new byte[ONE_KB];\r\n    try (FSDataOutputStream out = fs.create(filePath)) {\r\n        out.write(oneKbBuf);\r\n        IOStatisticAssertions.assertThatStatisticCounter(out.getIOStatistics(), StreamStatisticNames.STREAM_WRITE_BYTES).describedAs(\"Bytes written by OutputStream \" + \"should match the actual bytes\").isEqualTo(ONE_KB);\r\n    }\r\n    try (FSDataInputStream in = fs.open(filePath, ONE_KB)) {\r\n        in.readFully(0, oneKbBuf);\r\n    }\r\n    try (FSDataInputStream in2 = fs.open(filePath, ONE_KB)) {\r\n        in2.readFully(0, oneKbBuf);\r\n    }\r\n    FileSystem.Statistics fsStats = fs.getFsStatistics();\r\n    assertEquals(\"Mismatch in number of FS bytes read by InputStreams\", TWO_KB, fsStats.getBytesRead());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "createScaleConfiguration",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Configuration createScaleConfiguration()\n{\r\n    Configuration conf = super.createScaleConfiguration();\r\n    S3ATestUtils.disableFilesystemCaching(conf);\r\n    S3ATestUtils.removeBaseAndBucketOverrides(conf, EXPERIMENTAL_AWS_INTERNAL_THROTTLING, BULK_DELETE_PAGE_SIZE, USER_AGENT_PREFIX);\r\n    conf.setBoolean(EXPERIMENTAL_AWS_INTERNAL_THROTTLING, false);\r\n    conf.setInt(BULK_DELETE_PAGE_SIZE, DELETE_PAGE_SIZE);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\scale",
  "methodName" : "testBulkRenameAndDelete",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testBulkRenameAndDelete() throws Throwable\n{\r\n    final int count = getConf().getInt(KEY_FILE_COUNT, DEFAULT_FILE_COUNT);\r\n    describe(\"Testing bulk rename and delete of %d files\", count);\r\n    final Path scaleTestDir = path(\"testBulkRenameAndDelete\");\r\n    final Path srcParentDir = new Path(scaleTestDir, \"srcParent\");\r\n    final Path srcDir = new Path(srcParentDir, \"src\");\r\n    final Path finalParentDir = new Path(scaleTestDir, \"finalParent\");\r\n    final Path finalDir = new Path(finalParentDir, \"final\");\r\n    final S3AFileSystem fs = getFileSystem();\r\n    rm(fs, scaleTestDir, true, false);\r\n    fs.mkdirs(srcDir);\r\n    fs.mkdirs(finalParentDir);\r\n    createFiles(fs, srcDir, 1, count, 0);\r\n    FileStatus[] statuses = fs.listStatus(srcDir);\r\n    int nSrcFiles = statuses.length;\r\n    long sourceSize = Arrays.stream(statuses).mapToLong(FileStatus::getLen).sum();\r\n    assertEquals(\"Source file Count\", count, nSrcFiles);\r\n    ContractTestUtils.NanoTimer renameTimer = new ContractTestUtils.NanoTimer();\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"Rename %s to %s\", srcDir, finalDir)) {\r\n        rename(srcDir, finalDir);\r\n    }\r\n    renameTimer.end();\r\n    LOG.info(\"Effective rename bandwidth {} MB/s\", renameTimer.bandwidthDescription(sourceSize));\r\n    LOG.info(String.format(\"Time to rename a file: %,03f milliseconds\", (renameTimer.nanosPerOperation(count) * 1.0f) / 1.0e6));\r\n    Assertions.assertThat(lsR(fs, srcParentDir, true)).describedAs(\"Recursive listing of source dir %s\", srcParentDir).isEqualTo(0);\r\n    assertPathDoesNotExist(\"not deleted after rename\", new Path(srcDir, filenameOfIndex(0)));\r\n    assertPathDoesNotExist(\"not deleted after rename\", new Path(srcDir, filenameOfIndex(count / 2)));\r\n    assertPathDoesNotExist(\"not deleted after rename\", new Path(srcDir, filenameOfIndex(count - 1)));\r\n    Assertions.assertThat(lsR(fs, finalDir, true)).describedAs(\"size of recursive destination listFiles(%s)\", finalDir).isEqualTo(count);\r\n    Assertions.assertThat(fs.listStatus(finalDir)).describedAs(\"size of destination listStatus(%s)\", finalDir).hasSize(count);\r\n    assertPathExists(\"not renamed to dest dir\", new Path(finalDir, filenameOfIndex(0)));\r\n    assertPathExists(\"not renamed to dest dir\", new Path(finalDir, filenameOfIndex(count / 2)));\r\n    assertPathExists(\"not renamed to dest dir\", new Path(finalDir, filenameOfIndex(count - 1)));\r\n    ContractTestUtils.NanoTimer deleteTimer = new ContractTestUtils.NanoTimer();\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"Delete subtree %s\", finalDir)) {\r\n        assertDeleted(finalDir, true);\r\n    }\r\n    deleteTimer.end();\r\n    LOG.info(String.format(\"Time to delete an object %,03f milliseconds\", (deleteTimer.nanosPerOperation(count) * 1.0f) / 1.0e6));\r\n    Assertions.assertThat(lsR(fs, finalParentDir, true)).describedAs(\"Recursive listing of deleted rename destination %s\", finalParentDir).isEqualTo(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createFileSystem",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileSystem createFileSystem() throws Exception\n{\r\n    contract = new S3AContract(new Configuration());\r\n    contract.init();\r\n    return contract.getTestFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (contract.getTestFileSystem() != null) {\r\n        super.tearDown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testListStatusThrowsExceptionForUnreadableDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testListStatusThrowsExceptionForUnreadableDir()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testGlobStatusThrowsExceptionForUnreadableDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testGlobStatusThrowsExceptionForUnreadableDir()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testCopyToLocalWithUseRawLocalFileSystemOption",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testCopyToLocalWithUseRawLocalFileSystemOption() throws Exception\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    fsUri = new URI(BASE + \"/\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "authPathsConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration authPathsConf(String... paths)\n{\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(AUTHORITATIVE_PATH, String.join(\",\", paths));\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testResolution",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testResolution() throws Throwable\n{\r\n    assertAuthPaths(l(\"/one\"), \"/one/\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testResolutionWithFQP",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testResolutionWithFQP() throws Throwable\n{\r\n    assertAuthPaths(l(\"/one/\", BASE + \"/two/\"), \"/one/\", \"/two/\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testOtherBucket",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testOtherBucket() throws Throwable\n{\r\n    assertAuthPaths(l(\"/one/\", \"s3a://landsat-pds/\", BASE + \"/two/\"), \"/one/\", \"/two/\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "testOtherScheme",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testOtherScheme() throws Throwable\n{\r\n    assertAuthPaths(l(\"/one/\", \"s3a://landsat-pds/\", \"http://bucket/two/\"), \"/one/\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "getAuthoritativePaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<String> getAuthoritativePaths(Configuration conf)\n{\r\n    return S3Guard.getAuthoritativePaths(fsUri, conf, p -> {\r\n        Path q = p.makeQualified(fsUri, root);\r\n        assertThat(q.toUri().getAuthority()).describedAs(\"Path %s\", q).isEqualTo(fsUri.getAuthority());\r\n        return S3AUtils.maybeAddTrailingSlash(q.toString());\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "l",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] l(String... s)\n{\r\n    return s;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "assertAuthPaths",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void assertAuthPaths(String[] src, String... expected)\n{\r\n    Configuration conf = authPathsConf(src);\r\n    List<String> collect = Arrays.stream(expected).map(s -> BASE + s).collect(Collectors.toList());\r\n    Collection<String> paths = getAuthoritativePaths(conf);\r\n    assertThat(paths).containsExactlyInAnyOrderElementsOf(collect);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    S3AFileSystem fs = getFileSystem();\r\n    Configuration c = fs.getConf();\r\n    skipIfEncryptionNotSet(c, getSSEAlgorithm());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "patchConfigurationEncryptionSettings",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void patchConfigurationEncryptionSettings(final Configuration conf)\n{\r\n    removeBaseAndBucketOverrides(conf, S3_ENCRYPTION_ALGORITHM, SERVER_SIDE_ENCRYPTION_ALGORITHM);\r\n    conf.set(S3_ENCRYPTION_ALGORITHM, getSSEAlgorithm().getMethod());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getSSEAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AEncryptionMethods getSSEAlgorithm()\n{\r\n    return S3AEncryptionMethods.NONE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "assertEncrypted",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void assertEncrypted(Path path) throws IOException\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Configuration c = fs.getConf();\r\n    String kmsKey = getS3EncryptionKey(getTestBucketName(c), c);\r\n    EncryptionTestUtils.assertEncrypted(fs, path, SSE_KMS, kmsKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEncryptionSettingPropagation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testEncryptionSettingPropagation() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEncryption",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testEncryption() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEncryptionOverRename",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testEncryptionOverRename() throws Throwable\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path path = path(getMethodName() + \"find-encryption-algo\");\r\n    ContractTestUtils.touch(fs, path);\r\n    String sseAlgorithm = fs.getObjectMetadata(path).getSSEAlgorithm();\r\n    if (StringUtils.isBlank(sseAlgorithm) || !sseAlgorithm.equals(AWS_KMS_SSE_ALGORITHM)) {\r\n        skip(\"Test bucket is not configured with \" + AWS_KMS_SSE_ALGORITHM);\r\n    }\r\n    super.testEncryptionOverRename();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\test\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "testEncryptionOverRename2",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testEncryptionOverRename2() throws Throwable\n{\r\n    S3AFileSystem fs = getFileSystem();\r\n    Path src = path(createFilename(1024));\r\n    byte[] data = dataset(1024, 'a', 'z');\r\n    EncryptionSecrets secrets = fs.getEncryptionSecrets();\r\n    validateEncryptionSecrets(secrets);\r\n    writeDataset(fs, src, data, data.length, 1024 * 1024, true);\r\n    ContractTestUtils.verifyFileContents(fs, src, data);\r\n    Configuration fs2Conf = new Configuration(fs.getConf());\r\n    fs2Conf.set(S3_ENCRYPTION_ALGORITHM, S3AEncryptionMethods.SSE_KMS.getMethod());\r\n    try (FileSystem kmsFS = FileSystem.newInstance(fs.getUri(), fs2Conf)) {\r\n        Path targetDir = path(\"target\");\r\n        kmsFS.mkdirs(targetDir);\r\n        ContractTestUtils.rename(kmsFS, src, targetDir);\r\n        Path renamedFile = new Path(targetDir, src.getName());\r\n        ContractTestUtils.verifyFileContents(fs, renamedFile, data);\r\n        String kmsKey = getS3EncryptionKey(getTestBucketName(fs2Conf), fs2Conf);\r\n        EncryptionTestUtils.assertEncrypted(fs, renamedFile, SSE_KMS, kmsKey);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
} ]
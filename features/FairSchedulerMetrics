[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getMemorySize",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long getMemorySize(Schedulable schedulable, Metric metric)\n{\r\n    if (schedulable != null) {\r\n        switch(metric) {\r\n            case DEMAND:\r\n                return schedulable.getDemand().getMemorySize();\r\n            case USAGE:\r\n                return schedulable.getResourceUsage().getMemorySize();\r\n            case MINSHARE:\r\n                return schedulable.getMinShare().getMemorySize();\r\n            case MAXSHARE:\r\n                return schedulable.getMaxShare().getMemorySize();\r\n            case FAIRSHARE:\r\n                return schedulable.getFairShare().getMemorySize();\r\n            default:\r\n                return 0L;\r\n        }\r\n    }\r\n    return 0L;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getVirtualCores",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int getVirtualCores(Schedulable schedulable, Metric metric)\n{\r\n    if (schedulable != null) {\r\n        switch(metric) {\r\n            case DEMAND:\r\n                return schedulable.getDemand().getVirtualCores();\r\n            case USAGE:\r\n                return schedulable.getResourceUsage().getVirtualCores();\r\n            case MINSHARE:\r\n                return schedulable.getMinShare().getVirtualCores();\r\n            case MAXSHARE:\r\n                return schedulable.getMaxShare().getVirtualCores();\r\n            case FAIRSHARE:\r\n                return schedulable.getFairShare().getVirtualCores();\r\n            default:\r\n                return 0;\r\n        }\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "registerAppMetrics",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void registerAppMetrics(ApplicationId appId, String oldAppId, Metric metric)\n{\r\n    metrics.register(\"variable.app.\" + oldAppId + \".\" + metric.value + \".memory\", new Gauge<Long>() {\r\n\r\n        @Override\r\n        public Long getValue() {\r\n            return getMemorySize((FSAppAttempt) getSchedulerAppAttempt(appId), metric);\r\n        }\r\n    });\r\n    metrics.register(\"variable.app.\" + oldAppId + \".\" + metric.value + \".vcores\", new Gauge<Integer>() {\r\n\r\n        @Override\r\n        public Integer getValue() {\r\n            return getVirtualCores((FSAppAttempt) getSchedulerAppAttempt(appId), metric);\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "trackApp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void trackApp(ApplicationId appId, String oldAppId)\n{\r\n    super.trackApp(appId, oldAppId);\r\n    for (Metric metric : Metric.values()) {\r\n        registerAppMetrics(appId, oldAppId, metric);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "registerQueueMetrics",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void registerQueueMetrics(FSQueue queue, Metric metric)\n{\r\n    metrics.register(\"variable.queue.\" + queue.getName() + \".\" + metric.value + \".memory\", new Gauge<Long>() {\r\n\r\n        @Override\r\n        public Long getValue() {\r\n            return getMemorySize(queue, metric);\r\n        }\r\n    });\r\n    metrics.register(\"variable.queue.\" + queue.getName() + \".\" + metric.value + \".vcores\", new Gauge<Integer>() {\r\n\r\n        @Override\r\n        public Integer getValue() {\r\n            return getVirtualCores(queue, metric);\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "registerQueueMetrics",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void registerQueueMetrics(String queueName)\n{\r\n    super.registerQueueMetrics(queueName);\r\n    FairScheduler fair = (FairScheduler) scheduler;\r\n    final FSQueue queue = fair.getQueueManager().getQueue(queueName);\r\n    registerQueueMetrics(queue, Metric.DEMAND);\r\n    registerQueueMetrics(queue, Metric.USAGE);\r\n    registerQueueMetrics(queue, Metric.MINSHARE);\r\n    registerQueueMetrics(queue, Metric.FAIRSHARE);\r\n    metrics.register(\"variable.queue.\" + queueName + \".maxshare.memory\", new Gauge<Long>() {\r\n\r\n        @Override\r\n        public Long getValue() {\r\n            if (!maxReset && SLSRunner.getSimulateInfoMap().containsKey(\"Number of nodes\") && SLSRunner.getSimulateInfoMap().containsKey(\"Node memory (MB)\") && SLSRunner.getSimulateInfoMap().containsKey(\"Node VCores\")) {\r\n                int numNMs = Integer.parseInt(SLSRunner.getSimulateInfoMap().get(\"Number of nodes\").toString());\r\n                int numMemoryMB = Integer.parseInt(SLSRunner.getSimulateInfoMap().get(\"Node memory (MB)\").toString());\r\n                int numVCores = Integer.parseInt(SLSRunner.getSimulateInfoMap().get(\"Node VCores\").toString());\r\n                totalMemoryMB = numNMs * numMemoryMB;\r\n                totalVCores = numNMs * numVCores;\r\n                maxReset = false;\r\n            }\r\n            return Math.min(queue.getMaxShare().getMemorySize(), totalMemoryMB);\r\n        }\r\n    });\r\n    metrics.register(\"variable.queue.\" + queueName + \".maxshare.vcores\", new Gauge<Integer>() {\r\n\r\n        @Override\r\n        public Integer getValue() {\r\n            return Math.min(queue.getMaxShare().getVirtualCores(), totalVCores);\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void init(Configuration conf) throws ClassNotFoundException\n{\r\n    amMap = new ConcurrentHashMap<>();\r\n    amClassMap = new HashMap<>();\r\n    appIdAMSim = new ConcurrentHashMap<>();\r\n    for (Map.Entry<String, String> e : conf) {\r\n        String key = e.getKey();\r\n        if (key.startsWith(SLSConfiguration.AM_TYPE_PREFIX)) {\r\n            String amType = key.substring(SLSConfiguration.AM_TYPE_PREFIX.length());\r\n            amClassMap.put(amType, Class.forName(conf.get(key)));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "startAM",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void startAM() throws YarnException, IOException\n{\r\n    switch(inputType) {\r\n        case SLS:\r\n            for (String inputTrace : inputTraces) {\r\n                startAMFromSLSTrace(inputTrace);\r\n            }\r\n            break;\r\n        case RUMEN:\r\n            long baselineTimeMS = 0;\r\n            for (String inputTrace : inputTraces) {\r\n                startAMFromRumenTrace(inputTrace, baselineTimeMS);\r\n            }\r\n            break;\r\n        case SYNTH:\r\n            startAMFromSynthGenerator();\r\n            break;\r\n        default:\r\n            throw new YarnException(\"Input configuration not recognized, \" + \"trace type should be SLS, RUMEN, or SYNTH\");\r\n    }\r\n    numAMs = amMap.size();\r\n    remainingApps = numAMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "startAMFromSLSTrace",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void startAMFromSLSTrace(String inputTrace) throws IOException\n{\r\n    JsonFactory jsonF = new JsonFactory();\r\n    ObjectMapper mapper = new ObjectMapper();\r\n    try (Reader input = new InputStreamReader(new FileInputStream(inputTrace), StandardCharsets.UTF_8)) {\r\n        JavaType type = mapper.getTypeFactory().constructMapType(Map.class, String.class, String.class);\r\n        Iterator<Map<String, String>> jobIter = mapper.readValues(jsonF.createParser(input), type);\r\n        while (jobIter.hasNext()) {\r\n            try {\r\n                Map<String, String> jsonJob = jobIter.next();\r\n                AMDefinitionSLS amDef = AMDefinitionFactory.createFromSlsTrace(jsonJob, slsRunner);\r\n                startAMs(amDef);\r\n            } catch (Exception e) {\r\n                LOG.error(\"Failed to create an AM: {}\", e.getMessage());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "startAMFromSynthGenerator",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void startAMFromSynthGenerator() throws YarnException, IOException\n{\r\n    Configuration localConf = new Configuration();\r\n    localConf.set(\"fs.defaultFS\", \"file:///\");\r\n    if (slsRunner.getStjp() == null) {\r\n        slsRunner.setStjp(new SynthTraceJobProducer(conf, new Path(inputTraces[0])));\r\n    }\r\n    SynthJob job;\r\n    while ((job = (SynthJob) slsRunner.getStjp().getNextJob()) != null) {\r\n        ReservationId reservationId = null;\r\n        if (job.hasDeadline()) {\r\n            reservationId = ReservationId.newInstance(rm.getStartTime(), AM_ID);\r\n        }\r\n        AMDefinitionSynth amDef = AMDefinitionFactory.createFromSynth(job, slsRunner);\r\n        startAMs(amDef, reservationId, job.getParams(), job.getDeadline());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "startAMFromRumenTrace",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void startAMFromRumenTrace(String inputTrace, long baselineTimeMS) throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(\"fs.defaultFS\", \"file:///\");\r\n    File fin = new File(inputTrace);\r\n    try (JobTraceReader reader = new JobTraceReader(new Path(fin.getAbsolutePath()), conf)) {\r\n        LoggedJob job = reader.getNext();\r\n        while (job != null) {\r\n            try {\r\n                AMDefinitionRumen amDef = AMDefinitionFactory.createFromRumenTrace(job, baselineTimeMS, slsRunner);\r\n                startAMs(amDef);\r\n            } catch (Exception e) {\r\n                LOG.error(\"Failed to create an AM\", e);\r\n            }\r\n            job = reader.getNext();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "startAMs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void startAMs(AMDefinition amDef)\n{\r\n    for (int i = 0; i < amDef.getJobCount(); i++) {\r\n        JobDefinition jobDef = JobDefinition.Builder.create().withAmDefinition(amDef).withDeadline(-1).withReservationId(null).withParams(null).build();\r\n        runNewAM(jobDef);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "startAMs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void startAMs(AMDefinition amDef, ReservationId reservationId, Map<String, String> params, long deadline)\n{\r\n    for (int i = 0; i < amDef.getJobCount(); i++) {\r\n        JobDefinition jobDef = JobDefinition.Builder.create().withAmDefinition(amDef).withReservationId(reservationId).withParams(params).withDeadline(deadline).build();\r\n        runNewAM(jobDef);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "runNewAM",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void runNewAM(JobDefinition jobDef)\n{\r\n    AMDefinition amDef = jobDef.getAmDefinition();\r\n    String oldJobId = amDef.getOldAppId();\r\n    AMSimulator amSim = createAmSimulator(amDef.getAmType());\r\n    if (amSim != null) {\r\n        int heartbeatInterval = conf.getInt(SLSConfiguration.AM_HEARTBEAT_INTERVAL_MS, SLSConfiguration.AM_HEARTBEAT_INTERVAL_MS_DEFAULT);\r\n        boolean isTracked = trackedApps.contains(oldJobId);\r\n        if (oldJobId == null) {\r\n            oldJobId = Integer.toString(AM_ID);\r\n        }\r\n        AM_ID++;\r\n        amSim.init(amDef, rm, slsRunner, isTracked, runner.getStartTimeMS(), heartbeatInterval, appIdAMSim);\r\n        if (jobDef.getReservationId() != null) {\r\n            UTCClock clock = new UTCClock();\r\n            amSim.initReservation(jobDef.getReservationId(), jobDef.getDeadline(), clock.getTime());\r\n        }\r\n        runner.schedule(amSim);\r\n        maxRuntime = Math.max(maxRuntime, amDef.getJobFinishTime());\r\n        numTasks += amDef.getTaskContainers().size();\r\n        amMap.put(oldJobId, amSim);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "createAmSimulator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AMSimulator createAmSimulator(String jobType)\n{\r\n    return (AMSimulator) ReflectionUtils.newInstance(amClassMap.get(jobType), new Configuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getAMSimulator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AMSimulator getAMSimulator(ApplicationId appId)\n{\r\n    return appIdAMSim.get(appId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setInputType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setInputType(TraceType inputType)\n{\r\n    this.inputType = inputType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setInputTraces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setInputTraces(String[] inputTraces)\n{\r\n    this.inputTraces = inputTraces.clone();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setResourceManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setResourceManager(ResourceManager rm)\n{\r\n    this.rm = rm;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getTrackedApps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<String> getTrackedApps()\n{\r\n    return trackedApps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setTrackedApps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTrackedApps(Set<String> trackApps)\n{\r\n    this.trackedApps = trackApps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getNumAMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumAMs()\n{\r\n    return numAMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getNumTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumTasks()\n{\r\n    return numTasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getMaxRuntime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMaxRuntime()\n{\r\n    return maxRuntime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getAmMap",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, AMSimulator> getAmMap()\n{\r\n    return amMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getQueue()\n{\r\n    return queue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getTaskContainers",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<ContainerSimulator> getTaskContainers(Map<String, ?> jsonJob, SLSRunner slsRunner) throws YarnException\n{\r\n    List<Map<String, String>> tasks = (List) jsonJob.get(SLSConfiguration.JOB_TASKS);\r\n    if (tasks == null || tasks.size() == 0) {\r\n        throw new YarnException(\"No task for the job!\");\r\n    }\r\n    List<ContainerSimulator> containers = new ArrayList<>();\r\n    for (Map<String, String> jsonTask : tasks) {\r\n        TaskContainerDefinition containerDef = TaskContainerDefinition.Builder.create().withCount(jsonTask, SLSConfiguration.COUNT).withHostname(jsonTask.get(SLSConfiguration.TASK_HOST)).withDuration(jsonTask, SLSConfiguration.TASK_DURATION_MS).withDurationLegacy(jsonTask, SLSConfiguration.DURATION_MS).withTaskStart(jsonTask, SLSConfiguration.TASK_START_MS).withTaskFinish(jsonTask, SLSConfiguration.TASK_END_MS).withResource(getResourceForContainer(jsonTask, slsRunner)).withPriority(jsonTask, SLSConfiguration.TASK_PRIORITY).withType(jsonTask, SLSConfiguration.TASK_TYPE).withExecutionType(jsonTask, SLSConfiguration.TASK_EXECUTION_TYPE).withAllocationId(jsonTask, SLSConfiguration.TASK_ALLOCATION_ID).withRequestDelay(jsonTask, SLSConfiguration.TASK_REQUEST_DELAY).build();\r\n        for (int i = 0; i < containerDef.getCount(); i++) {\r\n            containers.add(ContainerSimulator.createFromTaskContainerDefinition(containerDef));\r\n        }\r\n    }\r\n    return containers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getResourceForContainer",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Resource getResourceForContainer(Map<String, String> jsonTask, SLSRunner slsRunner)\n{\r\n    Resource res = slsRunner.getDefaultContainerResource();\r\n    ResourceInformation[] infors = ResourceUtils.getResourceTypesArray();\r\n    for (ResourceInformation info : infors) {\r\n        if (jsonTask.containsKey(SLSConfiguration.TASK_PREFIX + info.getName())) {\r\n            long value = Long.parseLong(jsonTask.get(SLSConfiguration.TASK_PREFIX + info.getName()));\r\n            res.setResourceValue(info.getName(), value);\r\n        }\r\n    }\r\n    return res;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void init(AMDefinition amDef, ResourceManager rm, SLSRunner slsRunner, boolean tracked, long baselineTimeMS, long heartbeatInterval, Map<ApplicationId, AMSimulator> appIdToAMSim)\n{\r\n    super.init(amDef, rm, slsRunner, tracked, baselineTimeMS, heartbeatInterval, appIdToAMSim);\r\n    super.amtype = \"dag\";\r\n    allContainers.addAll(amDef.getTaskContainers());\r\n    pendingContainers.addAll(amDef.getTaskContainers());\r\n    totalContainers = allContainers.size();\r\n    LOG.info(\"Added new job with {} containers\", allContainers.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "firstStep",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void firstStep() throws Exception\n{\r\n    super.firstStep();\r\n    amStartTime = System.currentTimeMillis();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "initReservation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initReservation(ReservationId reservationId, long deadline, long now)\n{\r\n    setReservationRequest(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "notifyAMContainerLaunched",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void notifyAMContainerLaunched(Container masterContainer) throws Exception\n{\r\n    if (null != masterContainer) {\r\n        restart();\r\n        super.notifyAMContainerLaunched(masterContainer);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "processResponseQueue",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void processResponseQueue() throws Exception\n{\r\n    while (!responseQueue.isEmpty()) {\r\n        AllocateResponse response = responseQueue.take();\r\n        if (!response.getCompletedContainersStatuses().isEmpty()) {\r\n            for (ContainerStatus cs : response.getCompletedContainersStatuses()) {\r\n                ContainerId containerId = cs.getContainerId();\r\n                if (cs.getExitStatus() == ContainerExitStatus.SUCCESS) {\r\n                    if (assignedContainers.containsKey(containerId)) {\r\n                        LOG.debug(\"Application {} has one container finished ({}).\", appId, containerId);\r\n                        ContainerSimulator containerSimulator = assignedContainers.remove(containerId);\r\n                        finishedContainers++;\r\n                        completedContainers.add(containerSimulator);\r\n                    } else if (amContainer.getId().equals(containerId)) {\r\n                        isFinished = true;\r\n                        LOG.info(\"Application {} goes to finish.\", appId);\r\n                    }\r\n                    if (finishedContainers >= totalContainers) {\r\n                        lastStep();\r\n                    }\r\n                } else {\r\n                    if (assignedContainers.containsKey(containerId)) {\r\n                        LOG.error(\"Application {} has one container killed ({}).\", appId, containerId);\r\n                        pendingContainers.add(assignedContainers.remove(containerId));\r\n                    } else if (amContainer.getId().equals(containerId)) {\r\n                        LOG.error(\"Application {}'s AM is \" + \"going to be killed. Waiting for rescheduling...\", appId);\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        if (isAMContainerRunning && (finishedContainers >= totalContainers)) {\r\n            isAMContainerRunning = false;\r\n            LOG.info(\"Application {} sends out event to clean up\" + \" its AM container.\", appId);\r\n            isFinished = true;\r\n            break;\r\n        }\r\n        for (Container container : response.getAllocatedContainers()) {\r\n            if (!scheduledContainers.isEmpty()) {\r\n                ContainerSimulator cs = scheduledContainers.remove(0);\r\n                LOG.debug(\"Application {} starts to launch a container ({}).\", appId, container.getId());\r\n                assignedContainers.put(container.getId(), cs);\r\n                se.getNmMap().get(container.getNodeId()).addNewContainer(container, cs.getLifeTime(), appId);\r\n                getRanNodes().add(container.getNodeId());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "sendContainerRequest",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void sendContainerRequest() throws Exception\n{\r\n    if (isFinished) {\r\n        return;\r\n    }\r\n    List<ResourceRequest> ask = null;\r\n    if (finishedContainers != totalContainers) {\r\n        if (!pendingContainers.isEmpty()) {\r\n            List<ContainerSimulator> toBeScheduled = getToBeScheduledContainers(pendingContainers, amStartTime);\r\n            if (toBeScheduled.size() > 0) {\r\n                ask = packageRequests(toBeScheduled, PRIORITY);\r\n                LOG.info(\"Application {} sends out request for {} containers.\", appId, toBeScheduled.size());\r\n                scheduledContainers.addAll(toBeScheduled);\r\n                pendingContainers.removeAll(toBeScheduled);\r\n                toBeScheduled.clear();\r\n            }\r\n        }\r\n    }\r\n    if (ask == null) {\r\n        ask = new ArrayList<>();\r\n    }\r\n    final AllocateRequest request = createAllocateRequest(ask);\r\n    if (totalContainers == 0) {\r\n        request.setProgress(1.0f);\r\n    } else {\r\n        request.setProgress((float) finishedContainers / totalContainers);\r\n    }\r\n    UserGroupInformation ugi = UserGroupInformation.createRemoteUser(appAttemptId.toString());\r\n    Token<AMRMTokenIdentifier> token = rm.getRMContext().getRMApps().get(appAttemptId.getApplicationId()).getRMAppAttempt(appAttemptId).getAMRMToken();\r\n    ugi.addTokenIdentifier(token.decodeIdentifier());\r\n    AllocateResponse response = ugi.doAs((PrivilegedExceptionAction<AllocateResponse>) () -> rm.getApplicationMasterService().allocate(request));\r\n    if (response != null) {\r\n        responseQueue.put(response);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "getToBeScheduledContainers",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<ContainerSimulator> getToBeScheduledContainers(List<ContainerSimulator> containers, long startTime)\n{\r\n    List<ContainerSimulator> toBeScheduled = new LinkedList<>();\r\n    for (ContainerSimulator cs : containers) {\r\n        if (cs.getRequestDelay() + startTime <= System.currentTimeMillis()) {\r\n            toBeScheduled.add(cs);\r\n        }\r\n    }\r\n    return toBeScheduled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "checkStop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkStop()\n{\r\n    if (isFinished) {\r\n        super.setEndTime(System.currentTimeMillis());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "lastStep",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void lastStep() throws Exception\n{\r\n    super.lastStep();\r\n    allContainers.clear();\r\n    pendingContainers.clear();\r\n    scheduledContainers.clear();\r\n    assignedContainers.clear();\r\n    completedContainers.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "restart",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void restart()\n{\r\n    isFinished = false;\r\n    pendingContainers.clear();\r\n    pendingContainers.addAll(allContainers);\r\n    pendingContainers.removeAll(completedContainers);\r\n    amContainer = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void init(String nodeIdStr, Resource nodeResource, int dispatchTime, int heartBeatInterval, ResourceManager pRm, float pResourceUtilizationRatio, Set<NodeLabel> labels) throws IOException, YarnException\n{\r\n    super.init(dispatchTime, dispatchTime + 1000000L * heartBeatInterval, heartBeatInterval);\r\n    String[] rackHostName = SLSUtils.getRackHostName(nodeIdStr);\r\n    this.node = NodeInfo.newNodeInfo(rackHostName[0], rackHostName[1], Resources.clone(nodeResource));\r\n    this.rm = pRm;\r\n    completedContainerList = Collections.synchronizedList(new ArrayList<ContainerId>());\r\n    releasedContainerList = Collections.synchronizedList(new ArrayList<ContainerId>());\r\n    containerQueue = new DelayQueue<ContainerSimulator>();\r\n    amContainerList = Collections.synchronizedList(new ArrayList<ContainerId>());\r\n    runningContainers = new ConcurrentHashMap<ContainerId, ContainerSimulator>();\r\n    RegisterNodeManagerRequest req = Records.newRecord(RegisterNodeManagerRequest.class);\r\n    req.setNodeId(node.getNodeID());\r\n    req.setResource(node.getTotalCapability());\r\n    req.setNodeLabels(labels);\r\n    req.setHttpPort(80);\r\n    RegisterNodeManagerResponse response = this.rm.getResourceTrackerService().registerNodeManager(req);\r\n    masterKey = response.getNMTokenMasterKey();\r\n    this.resourceUtilizationRatio = pResourceUtilizationRatio;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void init(String nodeIdStr, Resource nodeResource, int dispatchTime, int heartBeatInterval, ResourceManager pRm, float pResourceUtilizationRatio) throws IOException, YarnException\n{\r\n    init(nodeIdStr, nodeResource, dispatchTime, heartBeatInterval, pRm, pResourceUtilizationRatio, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "firstStep",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void firstStep()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "middleStep",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void middleStep() throws Exception\n{\r\n    ContainerSimulator cs;\r\n    synchronized (completedContainerList) {\r\n        while ((cs = containerQueue.poll()) != null) {\r\n            runningContainers.remove(cs.getId());\r\n            completedContainerList.add(cs.getId());\r\n            LOG.debug(\"Container {} has completed\", cs.getId());\r\n        }\r\n    }\r\n    NodeHeartbeatRequest beatRequest = Records.newRecord(NodeHeartbeatRequest.class);\r\n    beatRequest.setLastKnownNMTokenMasterKey(masterKey);\r\n    NodeStatus ns = Records.newRecord(NodeStatus.class);\r\n    ns.setContainersStatuses(generateContainerStatusList());\r\n    ns.setNodeId(node.getNodeID());\r\n    ns.setKeepAliveApplications(new ArrayList<ApplicationId>());\r\n    ns.setResponseId(responseId++);\r\n    ns.setNodeHealthStatus(NodeHealthStatus.newInstance(true, \"\", 0));\r\n    if (resourceUtilizationRatio > 0 && resourceUtilizationRatio <= 1) {\r\n        int pMemUsed = Math.round(node.getTotalCapability().getMemorySize() * resourceUtilizationRatio);\r\n        float cpuUsed = node.getTotalCapability().getVirtualCores() * resourceUtilizationRatio;\r\n        ResourceUtilization resourceUtilization = ResourceUtilization.newInstance(pMemUsed, pMemUsed, cpuUsed);\r\n        ns.setContainersUtilization(resourceUtilization);\r\n        ns.setNodeUtilization(resourceUtilization);\r\n    }\r\n    beatRequest.setNodeStatus(ns);\r\n    NodeHeartbeatResponse beatResponse = rm.getResourceTrackerService().nodeHeartbeat(beatRequest);\r\n    if (!beatResponse.getContainersToCleanup().isEmpty()) {\r\n        synchronized (releasedContainerList) {\r\n            for (ContainerId containerId : beatResponse.getContainersToCleanup()) {\r\n                if (amContainerList.contains(containerId)) {\r\n                    synchronized (amContainerList) {\r\n                        amContainerList.remove(containerId);\r\n                    }\r\n                    LOG.debug(\"NodeManager {} releases an AM ({}).\", node.getNodeID(), containerId);\r\n                } else {\r\n                    cs = runningContainers.remove(containerId);\r\n                    containerQueue.remove(cs);\r\n                    releasedContainerList.add(containerId);\r\n                    LOG.debug(\"NodeManager {} releases a container ({}).\", node.getNodeID(), containerId);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    if (beatResponse.getNodeAction() == NodeAction.SHUTDOWN) {\r\n        lastStep();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "lastStep",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void lastStep()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "generateContainerStatusList",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "ArrayList<ContainerStatus> generateContainerStatusList()\n{\r\n    ArrayList<ContainerStatus> csList = new ArrayList<ContainerStatus>();\r\n    for (ContainerSimulator container : runningContainers.values()) {\r\n        csList.add(newContainerStatus(container.getId(), ContainerState.RUNNING, ContainerExitStatus.SUCCESS));\r\n    }\r\n    synchronized (amContainerList) {\r\n        for (ContainerId cId : amContainerList) {\r\n            csList.add(newContainerStatus(cId, ContainerState.RUNNING, ContainerExitStatus.SUCCESS));\r\n        }\r\n    }\r\n    synchronized (completedContainerList) {\r\n        for (ContainerId cId : completedContainerList) {\r\n            LOG.debug(\"NodeManager {} completed container ({}).\", node.getNodeID(), cId);\r\n            csList.add(newContainerStatus(cId, ContainerState.COMPLETE, ContainerExitStatus.SUCCESS));\r\n        }\r\n        completedContainerList.clear();\r\n    }\r\n    synchronized (releasedContainerList) {\r\n        for (ContainerId cId : releasedContainerList) {\r\n            LOG.debug(\"NodeManager {} released container ({}).\", node.getNodeID(), cId);\r\n            csList.add(newContainerStatus(cId, ContainerState.COMPLETE, ContainerExitStatus.ABORTED));\r\n        }\r\n        releasedContainerList.clear();\r\n    }\r\n    return csList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "newContainerStatus",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ContainerStatus newContainerStatus(ContainerId cId, ContainerState state, int exitState)\n{\r\n    ContainerStatus cs = Records.newRecord(ContainerStatus.class);\r\n    cs.setContainerId(cId);\r\n    cs.setState(state);\r\n    cs.setExitStatus(exitState);\r\n    return cs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "getNode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RMNode getNode()\n{\r\n    return node;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "addNewContainer",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void addNewContainer(Container container, long lifeTimeMS, ApplicationId applicationId)\n{\r\n    LOG.debug(\"NodeManager {} launches a new container ({}).\", node.getNodeID(), container.getId());\r\n    if (lifeTimeMS != -1) {\r\n        ContainerSimulator cs = new ContainerSimulator(container.getId(), container.getResource(), lifeTimeMS + System.currentTimeMillis(), lifeTimeMS, container.getAllocationRequestId());\r\n        containerQueue.add(cs);\r\n        runningContainers.put(cs.getId(), cs);\r\n    } else {\r\n        synchronized (amContainerList) {\r\n            amContainerList.add(container.getId());\r\n        }\r\n    }\r\n    if (applicationId != null && !getNode().getRunningApps().contains(applicationId)) {\r\n        getNode().getRunningApps().add(applicationId);\r\n    }\r\n    LOG.debug(\"Adding running app: {} on node: {}. \" + \"Updated runningApps on this node are: {}\", applicationId, getNode().getNodeID(), getNode().getRunningApps());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "cleanupContainer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cleanupContainer(ContainerId containerId)\n{\r\n    synchronized (amContainerList) {\r\n        amContainerList.remove(containerId);\r\n    }\r\n    synchronized (completedContainerList) {\r\n        completedContainerList.add(containerId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "getRunningContainers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<ContainerId, ContainerSimulator> getRunningContainers()\n{\r\n    return runningContainers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "getAMContainers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<ContainerId> getAMContainers()\n{\r\n    return amContainerList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "getCompletedContainers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<ContainerId> getCompletedContainers()\n{\r\n    return completedContainerList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "finishApplication",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void finishApplication(ApplicationId applicationId)\n{\r\n    if (getNode().getRunningApps().contains(applicationId)) {\r\n        getNode().getRunningApps().remove(applicationId);\r\n        LOG.debug(\"Removed running app: {} from node: {}. \" + \"Updated runningApps on this node are: {}\", applicationId, getNode().getNodeID(), getNode().getRunningApps());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void init(AMDefinition amDef, ResourceManager rm, SLSRunner slsRunner, boolean tracked, long baselineTimeMS, long heartbeatInterval, Map<ApplicationId, AMSimulator> appIdToAMSim)\n{\r\n    super.init(amDef, rm, slsRunner, tracked, baselineTimeMS, heartbeatInterval, appIdToAMSim);\r\n    amtype = \"mapreduce\";\r\n    for (ContainerSimulator cs : amDef.getTaskContainers()) {\r\n        if (cs.getType().equals(\"map\")) {\r\n            cs.setPriority(PRIORITY_MAP);\r\n            allMaps.add(cs);\r\n        } else if (cs.getType().equals(\"reduce\")) {\r\n            cs.setPriority(PRIORITY_REDUCE);\r\n            allReduces.add(cs);\r\n        }\r\n    }\r\n    LOG.info(\"Added new job with {} mapper and {} reducers\", allMaps.size(), allReduces.size());\r\n    mapTotal = allMaps.size();\r\n    reduceTotal = allReduces.size();\r\n    totalContainers = mapTotal + reduceTotal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "notifyAMContainerLaunched",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void notifyAMContainerLaunched(Container masterContainer) throws Exception\n{\r\n    if (null != masterContainer) {\r\n        restart();\r\n        super.notifyAMContainerLaunched(masterContainer);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "processResponseQueue",
  "errType" : null,
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void processResponseQueue() throws Exception\n{\r\n    while (!responseQueue.isEmpty()) {\r\n        AllocateResponse response = responseQueue.take();\r\n        if (!response.getCompletedContainersStatuses().isEmpty()) {\r\n            for (ContainerStatus cs : response.getCompletedContainersStatuses()) {\r\n                ContainerId containerId = cs.getContainerId();\r\n                if (cs.getExitStatus() == ContainerExitStatus.SUCCESS) {\r\n                    if (assignedMaps.containsKey(containerId)) {\r\n                        LOG.debug(\"Application {} has one mapper finished ({}).\", appId, containerId);\r\n                        assignedMaps.remove(containerId);\r\n                        mapFinished++;\r\n                        finishedContainers++;\r\n                    } else if (assignedReduces.containsKey(containerId)) {\r\n                        LOG.debug(\"Application {} has one reducer finished ({}).\", appId, containerId);\r\n                        assignedReduces.remove(containerId);\r\n                        reduceFinished++;\r\n                        finishedContainers++;\r\n                    } else if (amContainer.getId().equals(containerId)) {\r\n                        isFinished = true;\r\n                        LOG.info(\"Application {} goes to finish.\", appId);\r\n                    }\r\n                    if (mapFinished >= mapTotal && reduceFinished >= reduceTotal) {\r\n                        lastStep();\r\n                    }\r\n                } else {\r\n                    if (assignedMaps.containsKey(containerId)) {\r\n                        LOG.debug(\"Application {} has one mapper killed ({}).\", appId, containerId);\r\n                        pendingFailedMaps.add(assignedMaps.remove(containerId));\r\n                    } else if (assignedReduces.containsKey(containerId)) {\r\n                        LOG.debug(\"Application {} has one reducer killed ({}).\", appId, containerId);\r\n                        pendingFailedReduces.add(assignedReduces.remove(containerId));\r\n                    } else if (amContainer.getId().equals(containerId)) {\r\n                        LOG.info(\"Application {}'s AM is \" + \"going to be killed. Waiting for rescheduling...\", appId);\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        if (isAMContainerRunning && (mapFinished >= mapTotal) && (reduceFinished >= reduceTotal)) {\r\n            isAMContainerRunning = false;\r\n            LOG.debug(\"Application {} sends out event to clean up\" + \" its AM container.\", appId);\r\n            isFinished = true;\r\n            break;\r\n        }\r\n        for (Container container : response.getAllocatedContainers()) {\r\n            if (!scheduledMaps.isEmpty()) {\r\n                ContainerSimulator cs = scheduledMaps.remove();\r\n                LOG.debug(\"Application {} starts to launch a mapper ({}).\", appId, container.getId());\r\n                assignedMaps.put(container.getId(), cs);\r\n                se.getNmMap().get(container.getNodeId()).addNewContainer(container, cs.getLifeTime(), appId);\r\n                getRanNodes().add(container.getNodeId());\r\n            } else if (!this.scheduledReduces.isEmpty()) {\r\n                ContainerSimulator cs = scheduledReduces.remove();\r\n                LOG.debug(\"Application {} starts to launch a reducer ({}).\", appId, container.getId());\r\n                assignedReduces.put(container.getId(), cs);\r\n                se.getNmMap().get(container.getNodeId()).addNewContainer(container, cs.getLifeTime(), appId);\r\n                getRanNodes().add(container.getNodeId());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "restart",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void restart() throws YarnException, IOException, InterruptedException\n{\r\n    isFinished = false;\r\n    pendingFailedMaps.clear();\r\n    pendingMaps.clear();\r\n    pendingReduces.clear();\r\n    pendingFailedReduces.clear();\r\n    int added = 0;\r\n    for (ContainerSimulator cs : allMaps) {\r\n        if (added >= mapTotal - mapFinished) {\r\n            break;\r\n        }\r\n        pendingMaps.add(cs);\r\n    }\r\n    added = 0;\r\n    for (ContainerSimulator cs : allReduces) {\r\n        if (added >= reduceTotal - reduceFinished) {\r\n            break;\r\n        }\r\n        pendingReduces.add(cs);\r\n    }\r\n    amContainer = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "mergeLists",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<ContainerSimulator> mergeLists(List<ContainerSimulator> left, List<ContainerSimulator> right)\n{\r\n    List<ContainerSimulator> list = new ArrayList<>();\r\n    list.addAll(left);\r\n    list.addAll(right);\r\n    return list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "sendContainerRequest",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void sendContainerRequest() throws YarnException, IOException, InterruptedException\n{\r\n    if (isFinished) {\r\n        return;\r\n    }\r\n    List<ResourceRequest> ask = null;\r\n    if (mapFinished != mapTotal) {\r\n        if (!pendingMaps.isEmpty()) {\r\n            ask = packageRequests(mergeLists(pendingMaps, scheduledMaps), PRIORITY_MAP);\r\n            LOG.debug(\"Application {} sends out request for {} mappers.\", appId, pendingMaps.size());\r\n            scheduledMaps.addAll(pendingMaps);\r\n            pendingMaps.clear();\r\n        } else if (!pendingFailedMaps.isEmpty()) {\r\n            ask = packageRequests(mergeLists(pendingFailedMaps, scheduledMaps), PRIORITY_MAP);\r\n            LOG.debug(\"Application {} sends out requests for {} failed mappers.\", appId, pendingFailedMaps.size());\r\n            scheduledMaps.addAll(pendingFailedMaps);\r\n            pendingFailedMaps.clear();\r\n        }\r\n    } else if (reduceFinished != reduceTotal) {\r\n        if (!pendingReduces.isEmpty()) {\r\n            ask = packageRequests(mergeLists(pendingReduces, scheduledReduces), PRIORITY_REDUCE);\r\n            LOG.debug(\"Application {} sends out requests for {} reducers.\", appId, pendingReduces.size());\r\n            scheduledReduces.addAll(pendingReduces);\r\n            pendingReduces.clear();\r\n        } else if (!pendingFailedReduces.isEmpty()) {\r\n            ask = packageRequests(mergeLists(pendingFailedReduces, scheduledReduces), PRIORITY_REDUCE);\r\n            LOG.debug(\"Application {} sends out request for {} failed reducers.\", appId, pendingFailedReduces.size());\r\n            scheduledReduces.addAll(pendingFailedReduces);\r\n            pendingFailedReduces.clear();\r\n        }\r\n    }\r\n    if (ask == null) {\r\n        ask = new ArrayList<>();\r\n    }\r\n    final AllocateRequest request = createAllocateRequest(ask);\r\n    if (totalContainers == 0) {\r\n        request.setProgress(1.0f);\r\n    } else {\r\n        request.setProgress((float) finishedContainers / totalContainers);\r\n    }\r\n    UserGroupInformation ugi = UserGroupInformation.createRemoteUser(appAttemptId.toString());\r\n    Token<AMRMTokenIdentifier> token = rm.getRMContext().getRMApps().get(appAttemptId.getApplicationId()).getRMAppAttempt(appAttemptId).getAMRMToken();\r\n    ugi.addTokenIdentifier(token.decodeIdentifier());\r\n    AllocateResponse response = ugi.doAs(new PrivilegedExceptionAction<AllocateResponse>() {\r\n\r\n        @Override\r\n        public AllocateResponse run() throws Exception {\r\n            return rm.getApplicationMasterService().allocate(request);\r\n        }\r\n    });\r\n    if (response != null) {\r\n        responseQueue.put(response);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "initReservation",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void initReservation(ReservationId reservationId, long deadline, long now)\n{\r\n    Resource mapRes = getMaxResource(allMaps);\r\n    long mapDur = getMaxDuration(allMaps);\r\n    Resource redRes = getMaxResource(allReduces);\r\n    long redDur = getMaxDuration(allReduces);\r\n    ReservationSubmissionRequest rr = ReservationClientUtil.createMRReservation(reservationId, \"reservation_\" + reservationId.getId(), mapRes, allMaps.size(), mapDur, redRes, allReduces.size(), redDur, now + traceStartTimeMS, now + deadline, queue);\r\n    setReservationRequest(rr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "getMaxResource",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Resource getMaxResource(Collection<ContainerSimulator> containers)\n{\r\n    return containers.parallelStream().map(ContainerSimulator::getResource).reduce(Resource.newInstance(0, 0), Resources::componentwiseMax);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "getMaxDuration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getMaxDuration(Collection<ContainerSimulator> containers)\n{\r\n    return containers.parallelStream().mapToLong(ContainerSimulator::getLifeTime).reduce(0L, Long::max);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "checkStop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkStop()\n{\r\n    if (isFinished) {\r\n        super.setEndTime(System.currentTimeMillis());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "lastStep",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void lastStep() throws Exception\n{\r\n    super.lastStep();\r\n    allMaps.clear();\r\n    allReduces.clear();\r\n    assignedMaps.clear();\r\n    assignedReduces.clear();\r\n    pendingFailedMaps.clear();\r\n    pendingFailedReduces.clear();\r\n    pendingMaps.clear();\r\n    pendingReduces.clear();\r\n    scheduledMaps.clear();\r\n    scheduledReduces.clear();\r\n    responseQueue.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getDuration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDuration()\n{\r\n    return duration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getResource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Resource getResource()\n{\r\n    return resource;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getPriority",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getPriority()\n{\r\n    return priority;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getType()\n{\r\n    return type;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getCount()\n{\r\n    return count;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getExecutionType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ExecutionType getExecutionType()\n{\r\n    return executionType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getAllocationId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getAllocationId()\n{\r\n    return allocationId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getRequestDelay",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getRequestDelay()\n{\r\n    return requestDelay;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getHostname",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHostname()\n{\r\n    return hostname;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.conf = conf;\r\n    super.setConf(conf);\r\n    schedulerCommons.initMetrics(CapacityScheduler.class, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "allocate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Allocation allocate(ApplicationAttemptId attemptId, List<ResourceRequest> resourceRequests, List<SchedulingRequest> schedulingRequests, List<ContainerId> containerIds, List<String> blacklistAdditions, List<String> blacklistRemovals, ContainerUpdates updateRequests)\n{\r\n    return schedulerCommons.allocate(attemptId, resourceRequests, schedulingRequests, containerIds, blacklistAdditions, blacklistRemovals, updateRequests);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "allocatePropagated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Allocation allocatePropagated(ApplicationAttemptId attemptId, List<ResourceRequest> resourceRequests, List<SchedulingRequest> schedulingRequests, List<ContainerId> containerIds, List<String> blacklistAdditions, List<String> blacklistRemovals, ContainerUpdates updateRequests)\n{\r\n    return super.allocate(attemptId, resourceRequests, schedulingRequests, containerIds, blacklistAdditions, blacklistRemovals, updateRequests);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "tryCommit",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "boolean tryCommit(Resource cluster, ResourceCommitRequest r, boolean updatePending)\n{\r\n    if (schedulerCommons.isMetricsON()) {\r\n        boolean isSuccess = false;\r\n        long startTimeNs = System.nanoTime();\r\n        try {\r\n            isSuccess = super.tryCommit(cluster, r, updatePending);\r\n            return isSuccess;\r\n        } finally {\r\n            long elapsedNs = System.nanoTime() - startTimeNs;\r\n            if (isSuccess) {\r\n                getSchedulerMetrics().getSchedulerCommitSuccessTimer().update(elapsedNs, TimeUnit.NANOSECONDS);\r\n                getSchedulerMetrics().increaseSchedulerCommitSuccessCounter();\r\n            } else {\r\n                getSchedulerMetrics().getSchedulerCommitFailureTimer().update(elapsedNs, TimeUnit.NANOSECONDS);\r\n                getSchedulerMetrics().increaseSchedulerCommitFailureCounter();\r\n            }\r\n        }\r\n    } else {\r\n        return super.tryCommit(cluster, r, updatePending);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "handle",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void handle(SchedulerEvent schedulerEvent)\n{\r\n    schedulerCommons.handle(schedulerEvent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "propagatedHandle",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void propagatedHandle(SchedulerEvent schedulerEvent)\n{\r\n    super.handle(schedulerEvent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    schedulerCommons.stopMetrics();\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getRealQueueName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getRealQueueName(String queue) throws YarnException\n{\r\n    if (getQueue(queue) == null) {\r\n        throw new YarnException(\"Can't find the queue by the given name: \" + queue + \"! Please check if queue \" + queue + \" is in the allocation file.\");\r\n    }\r\n    return getQueue(queue).getQueuePath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getSchedulerMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SchedulerMetrics getSchedulerMetrics()\n{\r\n    return schedulerCommons.getSchedulerMetrics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getTracker",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Tracker getTracker()\n{\r\n    return schedulerCommons.getTracker();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "setSLSRunner",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSLSRunner(SLSRunner runner)\n{\r\n    this.runner = runner;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getSLSRunner",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SLSRunner getSLSRunner()\n{\r\n    return this.runner;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "newNodeID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NodeId newNodeID(String host, int port)\n{\r\n    return NodeId.newInstance(host, port);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "newNodeInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RMNode newNodeInfo(String rackName, String hostName, final Resource resource, int port)\n{\r\n    final NodeId nodeId = newNodeID(hostName, port);\r\n    final String nodeAddr = hostName + \":\" + port;\r\n    return new FakeRMNodeImpl(nodeId, nodeAddr, hostName, resource, rackName, \"Me good\", port, hostName, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "newNodeInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RMNode newNodeInfo(String rackName, String hostName, final Resource resource)\n{\r\n    return newNodeInfo(rackName, hostName, resource, NODE_ID++);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getTaskContainers",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "List<ContainerSimulator> getTaskContainers(SynthJob job, SLSRunner slsRunner) throws YarnException\n{\r\n    List<ContainerSimulator> containerList = new ArrayList<>();\r\n    ArrayList<NodeId> keyAsArray = new ArrayList<>(slsRunner.getNmMap().keySet());\r\n    Random rand = new Random(slsRunner.getStjp().getSeed());\r\n    for (SynthJob.SynthTask task : job.getTasks()) {\r\n        RMNode node = getRandomNode(slsRunner, keyAsArray, rand);\r\n        TaskContainerDefinition containerDef = TaskContainerDefinition.Builder.create().withCount(1).withHostname(\"/\" + node.getRackName() + \"/\" + node.getHostName()).withDuration(task.getTime()).withResource(Resource.newInstance((int) task.getMemory(), (int) task.getVcores())).withPriority(task.getPriority()).withType(task.getType()).withExecutionType(task.getExecutionType()).withAllocationId(-1).withRequestDelay(0).build();\r\n        containerList.add(ContainerSimulator.createFromTaskContainerDefinition(containerDef));\r\n    }\r\n    return containerList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getRandomNode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RMNode getRandomNode(SLSRunner slsRunner, ArrayList<NodeId> keyAsArray, Random rand)\n{\r\n    int randomIndex = rand.nextInt(keyAsArray.size());\r\n    return slsRunner.getNmMap().get(keyAsArray.get(randomIndex)).getNode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getType()\n{\r\n    return type;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<SynthTask> getTasks()\n{\r\n    return tasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "hasDeadline",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasDeadline()\n{\r\n    return deadline > 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUser()\n{\r\n    return jobDef.user_name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobID getJobID()\n{\r\n    return new JobID(\"job_mock_\" + name, id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getSubmissionTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSubmissionTime()\n{\r\n    return submitTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getQueueName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getQueueName()\n{\r\n    return queueName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    String res = \"\\nSynthJob [\" + jobDef.class_name + \"]: \\n\" + \"\\tname: \" + getName() + \"\\n\" + \"\\ttype: \" + getType() + \"\\n\" + \"\\tid: \" + id + \"\\n\" + \"\\tqueue: \" + getQueueName() + \"\\n\" + \"\\tsubmission: \" + getSubmissionTime() + \"\\n\" + \"\\tduration: \" + getDuration() + \"\\n\" + \"\\tdeadline: \" + getDeadline() + \"\\n\";\r\n    sb.append(res);\r\n    int taskno = 0;\r\n    for (SynthJob.SynthTask t : getTasks()) {\r\n        sb.append(\"\\t\");\r\n        sb.append(taskno);\r\n        sb.append(\": \\t\");\r\n        sb.append(t.toString());\r\n        taskno++;\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getTotalSlotTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTotalSlotTime()\n{\r\n    return totalSlotTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getDuration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDuration()\n{\r\n    return duration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getDeadline",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDeadline()\n{\r\n    return deadline;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getParams",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, String> getParams()\n{\r\n    return params;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean equals(Object other)\n{\r\n    if (!(other instanceof SynthJob)) {\r\n        return false;\r\n    }\r\n    SynthJob o = (SynthJob) other;\r\n    return tasks.equals(o.tasks) && submitTime == o.submitTime && type.equals(o.type) && queueName.equals(o.queueName) && jobDef.class_name.equals(o.jobDef.class_name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return jobDef.class_name.hashCode() * (int) submitTime * (int) duration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getJobConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobConf getJobConf()\n{\r\n    return new JobConf(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getNumberMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumberMaps()\n{\r\n    return taskCounts.get(MRAMSimulator.MAP_TYPE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getNumberReduces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumberReduces()\n{\r\n    return taskCounts.get(MRAMSimulator.REDUCE_TYPE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getInputSplits",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InputSplit[] getInputSplits()\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getTaskInfo",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "TaskInfo getTaskInfo(TaskType taskType, int taskNumber)\n{\r\n    switch(taskType) {\r\n        case MAP:\r\n            return new TaskInfo(-1, -1, -1, -1, taskMemory.get(MRAMSimulator.MAP_TYPE), taskVcores.get(MRAMSimulator.MAP_TYPE));\r\n        case REDUCE:\r\n            return new TaskInfo(-1, -1, -1, -1, taskMemory.get(MRAMSimulator.REDUCE_TYPE), taskVcores.get(MRAMSimulator.REDUCE_TYPE));\r\n        default:\r\n            break;\r\n    }\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getTaskAttemptInfo",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "TaskAttemptInfo getTaskAttemptInfo(TaskType taskType, int taskNumber, int taskAttemptNumber)\n{\r\n    switch(taskType) {\r\n        case MAP:\r\n            return new MapTaskAttemptInfo(State.SUCCEEDED, getTaskInfo(taskType, taskNumber), taskByType.get(MRAMSimulator.MAP_TYPE).get(taskNumber).time, null);\r\n        case REDUCE:\r\n            return new ReduceTaskAttemptInfo(State.SUCCEEDED, getTaskInfo(taskType, taskNumber), taskByType.get(MRAMSimulator.MAP_TYPE).get(taskNumber).time / 3, taskByType.get(MRAMSimulator.MAP_TYPE).get(taskNumber).time / 3, taskByType.get(MRAMSimulator.MAP_TYPE).get(taskNumber).time / 3, null);\r\n        default:\r\n            break;\r\n    }\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getMapTaskAttemptInfoAdjusted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptInfo getMapTaskAttemptInfoAdjusted(int taskNumber, int taskAttemptNumber, int locality)\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getOutcome",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Values getOutcome()\n{\r\n    return Values.SUCCESS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "createStory",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Queue<StoryParams> createStory()\n{\r\n    Queue<StoryParams> storyQueue = new PriorityQueue<>(this.numJobs.get(), new Comparator<StoryParams>() {\r\n\r\n        @Override\r\n        public int compare(StoryParams o1, StoryParams o2) {\r\n            return Math.toIntExact(o1.actualSubmissionTime - o2.actualSubmissionTime);\r\n        }\r\n    });\r\n    for (int i = 0; i < numJobs.get(); i++) {\r\n        Workload wl = trace.generateWorkload();\r\n        long actualSubmissionTime = wl.generateSubmissionTime();\r\n        String queue = wl.queue_name;\r\n        JobDefinition job = wl.generateJobDefinition();\r\n        storyQueue.add(new StoryParams(actualSubmissionTime, queue, job));\r\n    }\r\n    return storyQueue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getNextJob",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobStory getNextJob() throws IOException\n{\r\n    if (numJobs.decrementAndGet() < 0) {\r\n        return null;\r\n    }\r\n    StoryParams storyParams = listStoryParams.poll();\r\n    return new SynthJob(rand, conf, storyParams.jobDef, storyParams.queue, storyParams.actualSubmissionTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"SynthTraceJobProducer [ conf=\" + conf + \", numJobs=\" + numJobs + \", r=\" + rand + \", totalWeight=\" + totalWeight + \", workloads=\" + trace.workloads + \"]\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getNumJobs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumJobs()\n{\r\n    return trace.num_jobs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "validateJobDef",
  "errType" : [ "JsonMappingException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void validateJobDef(JobDefinition jobDef)\n{\r\n    if (jobDef.tasks == null) {\r\n        LOG.info(\"Detected old JobDefinition format. Converting.\");\r\n        try {\r\n            jobDef.tasks = new ArrayList<>();\r\n            jobDef.type = \"mapreduce\";\r\n            jobDef.deadline_factor = new Sample(jobDef.deadline_factor_avg, jobDef.deadline_factor_stddev);\r\n            jobDef.duration = new Sample(jobDef.dur_avg, jobDef.dur_stddev);\r\n            jobDef.reservation = new Sample(jobDef.chance_of_reservation);\r\n            TaskDefinition map = new TaskDefinition();\r\n            map.type = MRAMSimulator.MAP_TYPE;\r\n            map.count = new Sample(jobDef.mtasks_avg, jobDef.mtasks_stddev);\r\n            map.time = new Sample(jobDef.mtime_avg, jobDef.mtime_stddev);\r\n            map.max_memory = new Sample((double) jobDef.map_max_memory_avg, jobDef.map_max_memory_stddev);\r\n            map.max_vcores = new Sample((double) jobDef.map_max_vcores_avg, jobDef.map_max_vcores_stddev);\r\n            map.priority = DEFAULT_MAPPER_PRIORITY;\r\n            map.executionType = jobDef.map_execution_type;\r\n            jobDef.tasks.add(map);\r\n            TaskDefinition reduce = new TaskDefinition();\r\n            reduce.type = MRAMSimulator.REDUCE_TYPE;\r\n            reduce.count = new Sample(jobDef.rtasks_avg, jobDef.rtasks_stddev);\r\n            reduce.time = new Sample(jobDef.rtime_avg, jobDef.rtime_stddev);\r\n            reduce.max_memory = new Sample((double) jobDef.reduce_max_memory_avg, jobDef.reduce_max_memory_stddev);\r\n            reduce.max_vcores = new Sample((double) jobDef.reduce_max_vcores_avg, jobDef.reduce_max_vcores_stddev);\r\n            reduce.priority = DEFAULT_REDUCER_PRIORITY;\r\n            reduce.executionType = jobDef.reduce_execution_type;\r\n            jobDef.tasks.add(reduce);\r\n        } catch (JsonMappingException e) {\r\n            LOG.warn(\"Error converting old JobDefinition format\", e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getSeed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSeed()\n{\r\n    return seed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getNodesPerRack",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNodesPerRack()\n{\r\n    return trace.nodes_per_rack < 1 ? 1 : trace.nodes_per_rack;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getNumNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumNodes()\n{\r\n    return trace.num_nodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "registerQueueMetrics",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void registerQueueMetrics(String queueName)\n{\r\n    super.registerQueueMetrics(queueName);\r\n    FifoScheduler fifo = (FifoScheduler) scheduler;\r\n    final QueueInfo queue = fifo.getQueueInfo(queueName, false, false);\r\n    metrics.register(\"variable.queue.\" + queueName + \".currentcapacity\", new Gauge<Float>() {\r\n\r\n        @Override\r\n        public Float getValue() {\r\n            return queue.getCurrentCapacity();\r\n        }\r\n    });\r\n    metrics.register(\"variable.queue.\" + queueName + \".\", new Gauge<Float>() {\r\n\r\n        @Override\r\n        public Float getValue() {\r\n            return queue.getCurrentCapacity();\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "setQueueSet",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setQueueSet(Set<String> queues)\n{\r\n    queueSet = queues;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getQueueSet",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<String> getQueueSet()\n{\r\n    return queueSet;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "setTrackedAppSet",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTrackedAppSet(Set<String> apps)\n{\r\n    trackedAppSet = apps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getTrackedAppSet",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<String> getTrackedAppSet()\n{\r\n    return trackedAppSet;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getWeighted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getWeighted(Collection<Double> weights, Random rr)\n{\r\n    double totalWeight = 0;\r\n    for (Double i : weights) {\r\n        totalWeight += i;\r\n    }\r\n    double rand = rr.nextDouble() * totalWeight;\r\n    double cur = 0;\r\n    int ind = 0;\r\n    for (Double i : weights) {\r\n        cur += i;\r\n        if (cur > rand) {\r\n            break;\r\n        }\r\n        ind++;\r\n    }\r\n    return ind;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getNormalDist",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NormalDistribution getNormalDist(JDKRandomGenerator rand, double average, double stdDev)\n{\r\n    if (average <= 0) {\r\n        return null;\r\n    }\r\n    if (stdDev == 0) {\r\n        stdDev = average / 6;\r\n    }\r\n    NormalDistribution ret = new NormalDistribution(average, stdDev, NormalDistribution.DEFAULT_INVERSE_ABSOLUTE_ACCURACY);\r\n    ret.reseedRandomGenerator(rand.nextLong());\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\synthetic",
  "methodName" : "getLogNormalDist",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "LogNormalDistribution getLogNormalDist(JDKRandomGenerator rand, double mean, double stdDev)\n{\r\n    if (mean <= 0) {\r\n        return null;\r\n    }\r\n    if (stdDev == 0) {\r\n        stdDev = mean / 6;\r\n    }\r\n    double var = stdDev * stdDev;\r\n    double sigmasq = Math.log1p(var / (mean * mean));\r\n    double sigma = Math.sqrt(sigmasq);\r\n    double mu = Math.log(mean) - 0.5 * sigmasq;\r\n    LogNormalDistribution ret = new LogNormalDistribution(mu, sigma, LogNormalDistribution.DEFAULT_INVERSE_ABSOLUTE_ACCURACY);\r\n    ret.reseedRandomGenerator(rand.nextLong());\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\conf",
  "methodName" : "getAMContainerResource",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Resource getAMContainerResource(Configuration conf)\n{\r\n    return Resource.newInstance(conf.getLong(AM_CONTAINER_MEMORY, AM_CONTAINER_MEMORY_DEFAULT), conf.getInt(AM_CONTAINER_VCORES, AM_CONTAINER_VCORES_DEFAULT));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "setQueueSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setQueueSize(int threadPoolSize)\n{\r\n    this.threadPoolSize = threadPoolSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "start",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void start()\n{\r\n    if (executor != null && !executor.isTerminated()) {\r\n        throw new IllegalStateException(\"Executor already running\");\r\n    }\r\n    DelayQueue preStartQueue = queue;\r\n    queue = new DelayQueue();\r\n    executor = new ThreadPoolExecutor(threadPoolSize, threadPoolSize, 0, TimeUnit.MILLISECONDS, queue);\r\n    executor.prestartAllCoreThreads();\r\n    startTimeMS = System.currentTimeMillis();\r\n    for (Object d : preStartQueue) {\r\n        schedule((Task) d, startTimeMS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stop() throws InterruptedException\n{\r\n    executor.shutdownNow();\r\n    executor.awaitTermination(20, TimeUnit.SECONDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "schedule",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void schedule(Task task, long timeNow)\n{\r\n    task.timeRebase(timeNow);\r\n    task.setQueue(queue);\r\n    queue.add(task);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "schedule",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void schedule(Task task)\n{\r\n    schedule(task, System.currentTimeMillis());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getStartTimeMS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTimeMS()\n{\r\n    return this.startTimeMS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getAmType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAmType()\n{\r\n    return amType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUser()\n{\r\n    return user;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getOldAppId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getOldAppId()\n{\r\n    return oldAppId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getJobStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getJobStartTime()\n{\r\n    return jobStartTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getJobFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getJobFinishTime()\n{\r\n    return jobFinishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getTaskContainers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<ContainerSimulator> getTaskContainers()\n{\r\n    return taskContainers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getAmResource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Resource getAmResource()\n{\r\n    return amResource;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getLabelExpression",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLabelExpression()\n{\r\n    return labelExpression;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getQueue()\n{\r\n    return queue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getJobCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getJobCount()\n{\r\n    return jobCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "createFromSlsTrace",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AMDefinitionSLS createFromSlsTrace(Map<String, String> jsonJob, SLSRunner slsRunner) throws YarnException\n{\r\n    AMDefinitionSLS amDefinition = AMDefinitionSLS.Builder.create(jsonJob).withAmType(SLSConfiguration.AM_TYPE).withAmResource(getAMContainerResourceSLS(jsonJob, slsRunner)).withTaskContainers(AMDefinitionSLS.getTaskContainers(jsonJob, slsRunner)).withJobStartTime(SLSConfiguration.JOB_START_MS).withJobFinishTime(SLSConfiguration.JOB_END_MS).withLabelExpression(SLSConfiguration.JOB_LABEL_EXPR).withUser(SLSConfiguration.JOB_USER).withQueue(SLSConfiguration.JOB_QUEUE_NAME).withJobId(SLSConfiguration.JOB_ID).withJobCount(SLSConfiguration.JOB_COUNT).build();\r\n    slsRunner.increaseQueueAppNum(amDefinition.getQueue());\r\n    return amDefinition;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "createFromRumenTrace",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AMDefinitionRumen createFromRumenTrace(LoggedJob job, long baselineTimeMs, SLSRunner slsRunner) throws YarnException\n{\r\n    AMDefinitionRumen amDefinition = AMDefinitionRumen.Builder.create().withAmType(DEFAULT_JOB_TYPE).withAmResource(getAMContainerResourceSynthAndRumen(slsRunner)).withTaskContainers(AMDefinitionRumen.getTaskContainers(job, slsRunner)).withJobStartTime(job.getSubmitTime()).withJobFinishTime(job.getFinishTime()).withBaseLineTimeMs(baselineTimeMs).withUser(job.getUser()).withQueue(job.getQueue().getValue()).withJobId(job.getJobID().toString()).build();\r\n    slsRunner.increaseQueueAppNum(amDefinition.getQueue());\r\n    return amDefinition;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "createFromSynth",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AMDefinitionSynth createFromSynth(SynthJob job, SLSRunner slsRunner) throws YarnException\n{\r\n    AMDefinitionSynth amDefinition = AMDefinitionSynth.Builder.create().withAmType(job.getType()).withAmResource(getAMContainerResourceSynthAndRumen(slsRunner)).withTaskContainers(AMDefinitionSynth.getTaskContainers(job, slsRunner)).withUser(job.getUser()).withQueue(job.getQueueName()).withJobId(job.getJobID().toString()).withJobStartTime(job.getSubmissionTime()).withJobFinishTime(job.getSubmissionTime() + job.getDuration()).withBaseLineTimeMs(0).build();\r\n    slsRunner.increaseQueueAppNum(amDefinition.getQueue());\r\n    return amDefinition;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getAMContainerResourceSLS",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Resource getAMContainerResourceSLS(Map<String, String> jsonJob, Configured configured)\n{\r\n    Resource amContainerResource = SLSConfiguration.getAMContainerResource(configured.getConf());\r\n    if (jsonJob == null) {\r\n        return amContainerResource;\r\n    }\r\n    ResourceInformation[] infors = ResourceUtils.getResourceTypesArray();\r\n    for (ResourceInformation info : infors) {\r\n        String key = SLSConfiguration.JOB_AM_PREFIX + info.getName();\r\n        if (jsonJob.containsKey(key)) {\r\n            long value = Long.parseLong(jsonJob.get(key));\r\n            amContainerResource.setResourceValue(info.getName(), value);\r\n        }\r\n    }\r\n    return amContainerResource;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getAMContainerResourceSynthAndRumen",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Resource getAMContainerResourceSynthAndRumen(Configured configured)\n{\r\n    return SLSConfiguration.getAMContainerResource(configured.getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "adjustTimeValuesToBaselineTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void adjustTimeValuesToBaselineTime(AMDefinition amDef, AMDefinition.AmDefinitionBuilder builder, long baselineTimeMs)\n{\r\n    builder.jobStartTime -= baselineTimeMs;\r\n    builder.jobFinishTime -= baselineTimeMs;\r\n    if (builder.jobStartTime < 0) {\r\n        LOG.warn(\"Warning: reset job {} start time to 0.\", amDef.getOldAppId());\r\n        builder.jobFinishTime = builder.jobFinishTime - builder.jobStartTime;\r\n        builder.jobStartTime = 0;\r\n    }\r\n    amDef.jobStartTime = builder.jobStartTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\resourcemanager",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\resourcemanager",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\resourcemanager",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\resourcemanager",
  "methodName" : "setupAMRMToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setupAMRMToken(RMAppAttempt appAttempt)\n{\r\n    Token<AMRMTokenIdentifier> amrmToken = super.context.getAMRMTokenSecretManager().createAndGetAMRMToken(appAttempt.getAppAttemptId());\r\n    ((RMAppAttemptImpl) appAttempt).setAMRMToken(amrmToken);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\resourcemanager",
  "methodName" : "handle",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void handle(AMLauncherEvent event)\n{\r\n    ApplicationId appId = event.getAppAttempt().getAppAttemptId().getApplicationId();\r\n    AMSimulator ams = slsRunner.getAMSimulatorByAppId(appId);\r\n    if (ams == null) {\r\n        throw new YarnRuntimeException(\"Didn't find any AMSimulator for applicationId=\" + appId);\r\n    }\r\n    Container amContainer = event.getAppAttempt().getMasterContainer();\r\n    switch(event.getType()) {\r\n        case LAUNCH:\r\n            try {\r\n                setupAMRMToken(event.getAppAttempt());\r\n                super.context.getDispatcher().getEventHandler().handle(new RMAppAttemptEvent(event.getAppAttempt().getAppAttemptId(), RMAppAttemptEventType.LAUNCHED));\r\n                ams.notifyAMContainerLaunched(event.getAppAttempt().getMasterContainer());\r\n                LOG.info(\"Notify AM launcher launched:\" + amContainer.getId());\r\n                slsRunner.getNmMap().get(amContainer.getNodeId()).addNewContainer(amContainer, -1, appId);\r\n                ams.getRanNodes().add(amContainer.getNodeId());\r\n                return;\r\n            } catch (Exception e) {\r\n                throw new YarnRuntimeException(e);\r\n            }\r\n        case CLEANUP:\r\n            slsRunner.getNmMap().get(amContainer.getNodeId()).cleanupContainer(amContainer.getId());\r\n            break;\r\n        default:\r\n            throw new YarnRuntimeException(\"Didn't find any AMSimulator for applicationId=\" + appId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    super.setConfig(conf);\r\n    schedulerCommons.initMetrics(FairScheduler.class, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "allocate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Allocation allocate(ApplicationAttemptId attemptId, List<ResourceRequest> resourceRequests, List<SchedulingRequest> schedulingRequests, List<ContainerId> containerIds, List<String> blacklistAdditions, List<String> blacklistRemovals, ContainerUpdates updateRequests)\n{\r\n    return schedulerCommons.allocate(attemptId, resourceRequests, schedulingRequests, containerIds, blacklistAdditions, blacklistRemovals, updateRequests);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "handle",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void handle(SchedulerEvent schedulerEvent)\n{\r\n    schedulerCommons.handle(schedulerEvent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "propagatedHandle",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void propagatedHandle(SchedulerEvent schedulerEvent)\n{\r\n    super.handle(schedulerEvent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "allocatePropagated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Allocation allocatePropagated(ApplicationAttemptId attemptId, List<ResourceRequest> resourceRequests, List<SchedulingRequest> schedulingRequests, List<ContainerId> containerIds, List<String> blacklistAdditions, List<String> blacklistRemovals, ContainerUpdates updateRequests)\n{\r\n    return super.allocate(attemptId, resourceRequests, schedulingRequests, containerIds, blacklistAdditions, blacklistRemovals, updateRequests);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    schedulerCommons.stopMetrics();\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getRealQueueName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getRealQueueName(String queue) throws YarnException\n{\r\n    if (!getQueueManager().exists(queue)) {\r\n        throw new YarnException(\"Can't find the queue by the given name: \" + queue + \"! Please check if queue \" + queue + \" is in the allocation file.\");\r\n    }\r\n    return getQueueManager().getQueue(queue).getQueueName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getSchedulerMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SchedulerMetrics getSchedulerMetrics()\n{\r\n    return schedulerCommons.getSchedulerMetrics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getTracker",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Tracker getTracker()\n{\r\n    return schedulerCommons.getTracker();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "setSLSRunner",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSLSRunner(SLSRunner runner)\n{\r\n    this.runner = runner;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getSLSRunner",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SLSRunner getSLSRunner()\n{\r\n    return this.runner;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getNodeID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NodeId getNodeID()\n{\r\n    return node.getNodeID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getHostName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getHostName()\n{\r\n    return node.getHostName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getCommandPort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getCommandPort()\n{\r\n    return node.getCommandPort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getHttpPort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getHttpPort()\n{\r\n    return node.getHttpPort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getNodeAddress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getNodeAddress()\n{\r\n    return node.getNodeAddress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getHttpAddress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getHttpAddress()\n{\r\n    return node.getHttpAddress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getHealthReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getHealthReport()\n{\r\n    return node.getHealthReport();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getLastHealthReportTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLastHealthReportTime()\n{\r\n    return node.getLastHealthReportTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getTotalCapability",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Resource getTotalCapability()\n{\r\n    return node.getTotalCapability();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getAllocatedContainerResource",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Resource getAllocatedContainerResource()\n{\r\n    return node.getAllocatedContainerResource();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getRackName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRackName()\n{\r\n    return node.getRackName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getNode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Node getNode()\n{\r\n    return node.getNode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NodeState getState()\n{\r\n    return node.getState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getContainersToCleanUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<ContainerId> getContainersToCleanUp()\n{\r\n    return node.getContainersToCleanUp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getAppsToCleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<ApplicationId> getAppsToCleanup()\n{\r\n    return node.getAppsToCleanup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getRunningApps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<ApplicationId> getRunningApps()\n{\r\n    return node.getRunningApps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "setAndUpdateNodeHeartbeatResponse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setAndUpdateNodeHeartbeatResponse(NodeHeartbeatResponse nodeHeartbeatResponse)\n{\r\n    node.setAndUpdateNodeHeartbeatResponse(nodeHeartbeatResponse);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getLastNodeHeartBeatResponse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NodeHeartbeatResponse getLastNodeHeartBeatResponse()\n{\r\n    return node.getLastNodeHeartBeatResponse();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "resetLastNodeHeartBeatResponse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void resetLastNodeHeartBeatResponse()\n{\r\n    node.getLastNodeHeartBeatResponse().setResponseId(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "pullContainerUpdates",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<UpdatedContainerInfo> pullContainerUpdates()\n{\r\n    List<UpdatedContainerInfo> list = Collections.emptyList();\r\n    if (!pulled) {\r\n        list = updates;\r\n        pulled = true;\r\n    }\r\n    return list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getContainerUpdates",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<UpdatedContainerInfo> getContainerUpdates()\n{\r\n    return updates;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getNodeManagerVersion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getNodeManagerVersion()\n{\r\n    return node.getNodeManagerVersion();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getNodeLabels",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<String> getNodeLabels()\n{\r\n    return RMNodeLabelsManager.EMPTY_STRING_SET;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "pullNewlyIncreasedContainers",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<Container> pullNewlyIncreasedContainers()\n{\r\n    return Collections.emptyList();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getOpportunisticContainersStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OpportunisticContainersStatus getOpportunisticContainersStatus()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getAggregatedContainersUtilization",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ResourceUtilization getAggregatedContainersUtilization()\n{\r\n    return node.getAggregatedContainersUtilization();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getNodeUtilization",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ResourceUtilization getNodeUtilization()\n{\r\n    return node.getNodeUtilization();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getUntrackedTimeStamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getUntrackedTimeStamp()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "setUntrackedTimeStamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setUntrackedTimeStamp(long timeStamp)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getDecommissioningTimeout",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Integer getDecommissioningTimeout()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getAllocationTagsWithCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, Long> getAllocationTagsWithCount()\n{\r\n    return node.getAllocationTagsWithCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getAllNodeAttributes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<NodeAttribute> getAllNodeAttributes()\n{\r\n    return node.getAllNodeAttributes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getRMContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RMContext getRMContext()\n{\r\n    return node.getRMContext();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getPhysicalResource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Resource getPhysicalResource()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "isUpdatedCapability",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isUpdatedCapability()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "resetUpdatedCapability",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void resetUpdatedCapability()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "calculateHeartBeatInterval",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long calculateHeartBeatInterval(long defaultInterval, long minInterval, long maxInterval, float speedupFactor, float slowdownFactor)\n{\r\n    return defaultInterval;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "initMetrics",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void initMetrics(Class<? extends AbstractYarnScheduler<?, ?>> schedulerClass, Configuration conf)\n{\r\n    metricsON = conf.getBoolean(SLSConfiguration.METRICS_SWITCH, true);\r\n    if (metricsON) {\r\n        try {\r\n            schedulerMetrics = SchedulerMetrics.getInstance(conf, schedulerClass);\r\n            schedulerMetrics.init(scheduler, conf);\r\n        } catch (Exception e) {\r\n            LOG.error(\"Caught exception while initializing schedulerMetrics\", e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "stopMetrics",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stopMetrics()\n{\r\n    try {\r\n        if (metricsON) {\r\n            schedulerMetrics.tearDown();\r\n        }\r\n    } catch (Exception e) {\r\n        LOG.error(\"Caught exception while stopping service\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "allocate",
  "errType" : [ "Exception", "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Allocation allocate(ApplicationAttemptId attemptId, List<ResourceRequest> resourceRequests, List<SchedulingRequest> schedulingRequests, List<ContainerId> containerIds, List<String> blacklistAdditions, List<String> blacklistRemovals, ContainerUpdates updateRequests)\n{\r\n    if (metricsON) {\r\n        final Timer.Context context = schedulerMetrics.getSchedulerAllocateTimer().time();\r\n        Allocation allocation = null;\r\n        try {\r\n            allocation = ((SchedulerWrapper) scheduler).allocatePropagated(attemptId, resourceRequests, schedulingRequests, containerIds, blacklistAdditions, blacklistRemovals, updateRequests);\r\n            return allocation;\r\n        } catch (Exception e) {\r\n            LOG.error(\"Caught exception from allocate\", e);\r\n            throw e;\r\n        } finally {\r\n            context.stop();\r\n            schedulerMetrics.increaseSchedulerAllocationCounter();\r\n            try {\r\n                updateQueueWithAllocateRequest(allocation, attemptId, resourceRequests, containerIds);\r\n            } catch (IOException e) {\r\n                LOG.error(\"Caught exception while executing finally block\", e);\r\n            }\r\n        }\r\n    } else {\r\n        return ((SchedulerWrapper) scheduler).allocatePropagated(attemptId, resourceRequests, schedulingRequests, containerIds, blacklistAdditions, blacklistRemovals, updateRequests);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "updateQueueWithAllocateRequest",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void updateQueueWithAllocateRequest(Allocation allocation, ApplicationAttemptId attemptId, List<ResourceRequest> resourceRequests, List<ContainerId> containerIds) throws IOException\n{\r\n    Resource pendingResource = Resources.createResource(0, 0);\r\n    Resource allocatedResource = Resources.createResource(0, 0);\r\n    String queueName = appQueueMap.get(attemptId);\r\n    for (ResourceRequest request : resourceRequests) {\r\n        if (request.getResourceName().equals(ResourceRequest.ANY)) {\r\n            Resources.addTo(pendingResource, Resources.multiply(request.getCapability(), request.getNumContainers()));\r\n        }\r\n    }\r\n    for (Container container : allocation.getContainers()) {\r\n        Resources.addTo(allocatedResource, container.getResource());\r\n        Resources.subtractFrom(pendingResource, container.getResource());\r\n    }\r\n    SchedulerAppReport report = scheduler.getSchedulerAppInfo(attemptId);\r\n    for (ContainerId containerId : containerIds) {\r\n        Container container = null;\r\n        for (RMContainer c : report.getLiveContainers()) {\r\n            if (c.getContainerId().equals(containerId)) {\r\n                container = c.getContainer();\r\n                break;\r\n            }\r\n        }\r\n        if (container != null) {\r\n            Resources.subtractFrom(allocatedResource, container.getResource());\r\n        } else {\r\n            for (RMContainer c : report.getReservedContainers()) {\r\n                if (c.getContainerId().equals(containerId)) {\r\n                    container = c.getContainer();\r\n                    break;\r\n                }\r\n            }\r\n            if (container != null) {\r\n                Resources.subtractFrom(pendingResource, container.getResource());\r\n            }\r\n        }\r\n    }\r\n    Set<ContainerId> preemptionContainers = new HashSet<>();\r\n    if (allocation.getContainerPreemptions() != null) {\r\n        preemptionContainers.addAll(allocation.getContainerPreemptions());\r\n    }\r\n    if (allocation.getStrictContainerPreemptions() != null) {\r\n        preemptionContainers.addAll(allocation.getStrictContainerPreemptions());\r\n    }\r\n    if (!preemptionContainers.isEmpty()) {\r\n        for (ContainerId containerId : preemptionContainers) {\r\n            if (!preemptionContainerMap.containsKey(containerId)) {\r\n                Container container = null;\r\n                for (RMContainer c : report.getLiveContainers()) {\r\n                    if (c.getContainerId().equals(containerId)) {\r\n                        container = c.getContainer();\r\n                        break;\r\n                    }\r\n                }\r\n                if (container != null) {\r\n                    preemptionContainerMap.put(containerId, container.getResource());\r\n                }\r\n            }\r\n        }\r\n    }\r\n    schedulerMetrics.updateQueueMetrics(pendingResource, allocatedResource, queueName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "handle",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void handle(SchedulerEvent schedulerEvent)\n{\r\n    SchedulerWrapper wrapper = (SchedulerWrapper) scheduler;\r\n    if (!metricsON) {\r\n        ((SchedulerWrapper) scheduler).propagatedHandle(schedulerEvent);\r\n        return;\r\n    }\r\n    if (!schedulerMetrics.isRunning()) {\r\n        schedulerMetrics.setRunning(true);\r\n    }\r\n    Timer.Context handlerTimer = null;\r\n    Timer.Context operationTimer = null;\r\n    NodeUpdateSchedulerEventWrapper eventWrapper;\r\n    try {\r\n        if (schedulerEvent.getType() == SchedulerEventType.NODE_UPDATE && schedulerEvent instanceof NodeUpdateSchedulerEvent) {\r\n            eventWrapper = new NodeUpdateSchedulerEventWrapper((NodeUpdateSchedulerEvent) schedulerEvent);\r\n            schedulerEvent = eventWrapper;\r\n            updateQueueWithNodeUpdate(eventWrapper);\r\n        } else if (schedulerEvent.getType() == SchedulerEventType.APP_ATTEMPT_REMOVED && schedulerEvent instanceof AppAttemptRemovedSchedulerEvent) {\r\n            AppAttemptRemovedSchedulerEvent appRemoveEvent = (AppAttemptRemovedSchedulerEvent) schedulerEvent;\r\n            ApplicationAttemptId appAttemptId = appRemoveEvent.getApplicationAttemptID();\r\n            String queue = appQueueMap.get(appAttemptId);\r\n            SchedulerAppReport app = scheduler.getSchedulerAppInfo(appAttemptId);\r\n            if (!app.getLiveContainers().isEmpty()) {\r\n                RMContainer rmc = app.getLiveContainers().iterator().next();\r\n                schedulerMetrics.updateQueueMetricsByRelease(rmc.getContainer().getResource(), queue);\r\n            }\r\n        }\r\n        handlerTimer = schedulerMetrics.getSchedulerHandleTimer().time();\r\n        operationTimer = schedulerMetrics.getSchedulerHandleTimer(schedulerEvent.getType()).time();\r\n        ((SchedulerWrapper) scheduler).propagatedHandle(schedulerEvent);\r\n    } finally {\r\n        if (handlerTimer != null) {\r\n            handlerTimer.stop();\r\n        }\r\n        if (operationTimer != null) {\r\n            operationTimer.stop();\r\n        }\r\n        schedulerMetrics.increaseSchedulerHandleCounter(schedulerEvent.getType());\r\n        if (schedulerEvent.getType() == SchedulerEventType.APP_ATTEMPT_REMOVED && schedulerEvent instanceof AppAttemptRemovedSchedulerEvent) {\r\n            wrapper.getSLSRunner().decreaseRemainingApps();\r\n            AppAttemptRemovedSchedulerEvent appRemoveEvent = (AppAttemptRemovedSchedulerEvent) schedulerEvent;\r\n            appQueueMap.remove(appRemoveEvent.getApplicationAttemptID());\r\n            if (wrapper.getSLSRunner().getRemainingApps() == 0) {\r\n                try {\r\n                    schedulerMetrics.tearDown();\r\n                    SLSRunner.exitSLSRunner();\r\n                } catch (Exception e) {\r\n                    LOG.error(\"Scheduler Metrics failed to tear down.\", e);\r\n                }\r\n            }\r\n        } else if (schedulerEvent.getType() == SchedulerEventType.APP_ATTEMPT_ADDED && schedulerEvent instanceof AppAttemptAddedSchedulerEvent) {\r\n            AppAttemptAddedSchedulerEvent appAddEvent = (AppAttemptAddedSchedulerEvent) schedulerEvent;\r\n            SchedulerApplication app = scheduler.getSchedulerApplications().get(appAddEvent.getApplicationAttemptId().getApplicationId());\r\n            appQueueMap.put(appAddEvent.getApplicationAttemptId(), app.getQueue().getQueueName());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "updateQueueWithNodeUpdate",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void updateQueueWithNodeUpdate(NodeUpdateSchedulerEventWrapper eventWrapper)\n{\r\n    RMNodeWrapper node = (RMNodeWrapper) eventWrapper.getRMNode();\r\n    List<UpdatedContainerInfo> containerList = node.getContainerUpdates();\r\n    for (UpdatedContainerInfo info : containerList) {\r\n        for (ContainerStatus status : info.getCompletedContainers()) {\r\n            ContainerId containerId = status.getContainerId();\r\n            SchedulerAppReport app = scheduler.getSchedulerAppInfo(containerId.getApplicationAttemptId());\r\n            if (app == null) {\r\n                continue;\r\n            }\r\n            int releasedMemory = 0, releasedVCores = 0;\r\n            if (status.getExitStatus() == ContainerExitStatus.SUCCESS) {\r\n                for (RMContainer rmc : app.getLiveContainers()) {\r\n                    if (rmc.getContainerId() == containerId) {\r\n                        Resource resource = rmc.getContainer().getResource();\r\n                        releasedMemory += resource.getMemorySize();\r\n                        releasedVCores += resource.getVirtualCores();\r\n                        break;\r\n                    }\r\n                }\r\n            } else if (status.getExitStatus() == ContainerExitStatus.ABORTED) {\r\n                if (preemptionContainerMap.containsKey(containerId)) {\r\n                    Resource preResource = preemptionContainerMap.get(containerId);\r\n                    releasedMemory += preResource.getMemorySize();\r\n                    releasedVCores += preResource.getVirtualCores();\r\n                    preemptionContainerMap.remove(containerId);\r\n                }\r\n            }\r\n            String queue = appQueueMap.get(containerId.getApplicationAttemptId());\r\n            schedulerMetrics.updateQueueMetricsByRelease(Resource.newInstance(releasedMemory, releasedVCores), queue);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getSchedulerMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SchedulerMetrics getSchedulerMetrics()\n{\r\n    return schedulerMetrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "isMetricsON",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isMetricsON()\n{\r\n    return metricsON;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getTracker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Tracker getTracker()\n{\r\n    return tracker;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "startNM",
  "errType" : [ "IOException|YarnException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void startNM() throws YarnException, IOException, InterruptedException\n{\r\n    int heartbeatInterval = conf.getInt(SLSConfiguration.NM_HEARTBEAT_INTERVAL_MS, SLSConfiguration.NM_HEARTBEAT_INTERVAL_MS_DEFAULT);\r\n    float resourceUtilizationRatio = conf.getFloat(SLSConfiguration.NM_RESOURCE_UTILIZATION_RATIO, SLSConfiguration.NM_RESOURCE_UTILIZATION_RATIO_DEFAULT);\r\n    Set<SLSRunner.NodeDetails> nodeSet = null;\r\n    if (nodeFile.isEmpty()) {\r\n        for (String inputTrace : inputTraces) {\r\n            switch(inputType) {\r\n                case SLS:\r\n                    nodeSet = SLSUtils.parseNodesFromSLSTrace(inputTrace);\r\n                    break;\r\n                case RUMEN:\r\n                    nodeSet = SLSUtils.parseNodesFromRumenTrace(inputTrace);\r\n                    break;\r\n                case SYNTH:\r\n                    stjp = new SynthTraceJobProducer(conf, new Path(inputTraces[0]));\r\n                    nodeSet = SLSUtils.generateNodes(stjp.getNumNodes(), stjp.getNumNodes() / stjp.getNodesPerRack());\r\n                    break;\r\n                default:\r\n                    throw new YarnException(\"Input configuration not recognized, \" + \"trace type should be SLS, RUMEN, or SYNTH\");\r\n            }\r\n        }\r\n    } else {\r\n        nodeSet = SLSUtils.parseNodesFromNodeFile(nodeFile, nodeManagerResource);\r\n    }\r\n    if (nodeSet == null || nodeSet.isEmpty()) {\r\n        throw new YarnException(\"No node! Please configure nodes.\");\r\n    }\r\n    SLSUtils.generateNodeTableMapping(nodeSet, tableMapping);\r\n    Random random = new Random();\r\n    Set<String> rackSet = ConcurrentHashMap.newKeySet();\r\n    int threadPoolSize = Math.max(this.threadPoolSize, SLSConfiguration.RUNNER_POOL_SIZE_DEFAULT);\r\n    ExecutorService executorService = Executors.newFixedThreadPool(threadPoolSize);\r\n    for (SLSRunner.NodeDetails nodeDetails : nodeSet) {\r\n        executorService.submit(new Runnable() {\r\n\r\n            @Override\r\n            public void run() {\r\n                try {\r\n                    NMSimulator nm = new NMSimulator();\r\n                    Resource nmResource = nodeManagerResource;\r\n                    String hostName = nodeDetails.getHostname();\r\n                    if (nodeDetails.getNodeResource() != null) {\r\n                        nmResource = nodeDetails.getNodeResource();\r\n                    }\r\n                    Set<NodeLabel> nodeLabels = nodeDetails.getLabels();\r\n                    nm.init(hostName, nmResource, random.nextInt(heartbeatInterval), heartbeatInterval, rm, resourceUtilizationRatio, nodeLabels);\r\n                    nmMap.put(nm.getNode().getNodeID(), nm);\r\n                    taskRunner.schedule(nm);\r\n                    rackSet.add(nm.getNode().getRackName());\r\n                } catch (IOException | YarnException e) {\r\n                    LOG.error(\"Got an error while adding node\", e);\r\n                }\r\n            }\r\n        });\r\n    }\r\n    executorService.shutdown();\r\n    executorService.awaitTermination(10, TimeUnit.MINUTES);\r\n    numRacks = rackSet.size();\r\n    numNMs = nmMap.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "waitForNodesRunning",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void waitForNodesRunning() throws InterruptedException\n{\r\n    long startTimeMS = System.currentTimeMillis();\r\n    while (true) {\r\n        int numRunningNodes = 0;\r\n        for (RMNode node : rm.getRMContext().getRMNodes().values()) {\r\n            if (node.getState() == NodeState.RUNNING) {\r\n                numRunningNodes++;\r\n            }\r\n        }\r\n        if (numRunningNodes == numNMs) {\r\n            break;\r\n        }\r\n        LOG.info(\"SLSRunner is waiting for all nodes RUNNING.\" + \" {} of {} NMs initialized.\", numRunningNodes, numNMs);\r\n        Thread.sleep(1000);\r\n    }\r\n    LOG.info(\"SLSRunner takes {} ms to launch all nodes.\", System.currentTimeMillis() - startTimeMS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getNodeManagerResourceFromConf",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Resource getNodeManagerResourceFromConf()\n{\r\n    Resource resource = Resources.createResource(0);\r\n    ResourceInformation[] infors = ResourceUtils.getResourceTypesArray();\r\n    for (ResourceInformation info : infors) {\r\n        long value;\r\n        if (info.getName().equals(ResourceInformation.MEMORY_URI)) {\r\n            value = conf.getInt(SLSConfiguration.NM_MEMORY_MB, SLSConfiguration.NM_MEMORY_MB_DEFAULT);\r\n        } else if (info.getName().equals(ResourceInformation.VCORES_URI)) {\r\n            value = conf.getInt(SLSConfiguration.NM_VCORES, SLSConfiguration.NM_VCORES_DEFAULT);\r\n        } else {\r\n            value = conf.getLong(SLSConfiguration.NM_PREFIX + info.getName(), SLSConfiguration.NM_RESOURCE_DEFAULT);\r\n        }\r\n        resource.setResourceValue(info.getName(), value);\r\n    }\r\n    return resource;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setNodeFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNodeFile(String nodeFile)\n{\r\n    this.nodeFile = nodeFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setInputType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setInputType(TraceType inputType)\n{\r\n    this.inputType = inputType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setInputTraces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setInputTraces(String[] inputTraces)\n{\r\n    this.inputTraces = inputTraces.clone();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getNumNMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumNMs()\n{\r\n    return numNMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getNumRacks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumRacks()\n{\r\n    return numRacks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getNodeManagerResource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Resource getNodeManagerResource()\n{\r\n    return nodeManagerResource;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getNmMap",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<NodeId, NMSimulator> getNmMap()\n{\r\n    return nmMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getStjp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SynthTraceJobProducer getStjp()\n{\r\n    return stjp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setTableMapping",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTableMapping(String tableMapping)\n{\r\n    this.tableMapping = tableMapping;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setRm",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRm(ResourceManager rm)\n{\r\n    this.rm = rm;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\web",
  "methodName" : "readObject",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException\n{\r\n    in.defaultReadObject();\r\n    handleOperTimecostHistogramMap = new HashMap<>();\r\n    queueAllocatedMemoryCounterMap = new HashMap<>();\r\n    queueAllocatedVCoresCounterMap = new HashMap<>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\web",
  "methodName" : "start",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void start() throws Exception\n{\r\n    final ResourceHandler staticHandler = new ResourceHandler();\r\n    staticHandler.setMimeTypes(new MimeTypes());\r\n    String webRootDir = getClass().getClassLoader().getResource(\"html\").toExternalForm();\r\n    staticHandler.setResourceBase(webRootDir);\r\n    Handler handler = new AbstractHandler() {\r\n\r\n        @Override\r\n        public void handle(String target, Request baseRequest, HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException {\r\n            try {\r\n                int timeunit = 1000;\r\n                String timeunitLabel = \"second\";\r\n                if (request.getParameter(\"u\") != null && request.getParameter(\"u\").equalsIgnoreCase(\"m\")) {\r\n                    timeunit = 1000 * 60;\r\n                    timeunitLabel = \"minute\";\r\n                }\r\n                if (target.equals(\"/\")) {\r\n                    printPageIndex(request, response);\r\n                } else if (target.equals(\"/simulate\")) {\r\n                    printPageSimulate(request, response, timeunit, timeunitLabel);\r\n                } else if (target.equals(\"/track\")) {\r\n                    printPageTrack(request, response, timeunit, timeunitLabel);\r\n                } else if (target.startsWith(\"/js\") || target.startsWith(\"/css\")) {\r\n                    response.setCharacterEncoding(\"utf-8\");\r\n                    staticHandler.handle(target, baseRequest, request, response);\r\n                } else if (target.equals(\"/simulateMetrics\")) {\r\n                    printJsonMetrics(request, response);\r\n                } else if (target.equals(\"/trackMetrics\")) {\r\n                    printJsonTrack(request, response);\r\n                }\r\n            } catch (Exception e) {\r\n                LOG.error(\"Caught exception while starting SLSWebApp\", e);\r\n            }\r\n        }\r\n    };\r\n    server = new Server(port);\r\n    server.setHandler(handler);\r\n    server.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\web",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void stop() throws Exception\n{\r\n    if (server != null) {\r\n        server.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\web",
  "methodName" : "printPageIndex",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void printPageIndex(HttpServletRequest request, HttpServletResponse response) throws IOException\n{\r\n    response.setContentType(\"text/html\");\r\n    response.setStatus(HttpServletResponse.SC_OK);\r\n    String simulateInfo;\r\n    if (SLSRunner.getSimulateInfoMap().isEmpty()) {\r\n        String empty = \"<tr><td colspan='2' align='center'>\" + \"No information available</td></tr>\";\r\n        simulateInfo = MessageFormat.format(simulateInfoTemplate, empty);\r\n    } else {\r\n        StringBuilder info = new StringBuilder();\r\n        for (Map.Entry<String, Object> entry : SLSRunner.getSimulateInfoMap().entrySet()) {\r\n            info.append(\"<tr>\");\r\n            info.append(\"<td class='td1'>\").append(entry.getKey()).append(\"</td>\");\r\n            info.append(\"<td class='td2'>\").append(entry.getValue()).append(\"</td>\");\r\n            info.append(\"</tr>\");\r\n        }\r\n        simulateInfo = MessageFormat.format(simulateInfoTemplate, info.toString());\r\n    }\r\n    response.getWriter().println(simulateInfo);\r\n    ((Request) request).setHandled(true);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\web",
  "methodName" : "printPageSimulate",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void printPageSimulate(HttpServletRequest request, HttpServletResponse response, int timeunit, String timeunitLabel) throws IOException\n{\r\n    response.setContentType(\"text/html\");\r\n    response.setStatus(HttpServletResponse.SC_OK);\r\n    Set<String> queues = wrapper.getTracker().getQueueSet();\r\n    StringBuilder queueInfo = new StringBuilder();\r\n    int i = 0;\r\n    for (String queue : queues) {\r\n        queueInfo.append(\"legends[4][\").append(i).append(\"] = 'queue.\").append(queue).append(\".allocated.memory';\");\r\n        queueInfo.append(\"legends[5][\").append(i).append(\"] = 'queue.\").append(queue).append(\".allocated.vcores';\");\r\n        i++;\r\n    }\r\n    String simulateInfo = MessageFormat.format(simulateTemplate, queueInfo.toString(), timeunitLabel, \"\" + timeunit, \"\" + ajaxUpdateTimeMS);\r\n    response.getWriter().println(simulateInfo);\r\n    ((Request) request).setHandled(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\web",
  "methodName" : "printPageTrack",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void printPageTrack(HttpServletRequest request, HttpServletResponse response, int timeunit, String timeunitLabel) throws IOException\n{\r\n    response.setContentType(\"text/html\");\r\n    response.setStatus(HttpServletResponse.SC_OK);\r\n    StringBuilder trackedQueueInfo = new StringBuilder();\r\n    Set<String> trackedQueues = wrapper.getTracker().getQueueSet();\r\n    for (String queue : trackedQueues) {\r\n        trackedQueueInfo.append(\"<option value='Queue \").append(queue).append(\"'>\").append(queue).append(\"</option>\");\r\n    }\r\n    StringBuilder trackedAppInfo = new StringBuilder();\r\n    Set<String> trackedApps = wrapper.getTracker().getTrackedAppSet();\r\n    for (String job : trackedApps) {\r\n        trackedAppInfo.append(\"<option value='Job \").append(job).append(\"'>\").append(job).append(\"</option>\");\r\n    }\r\n    String trackInfo = MessageFormat.format(trackTemplate, trackedQueueInfo.toString(), trackedAppInfo.toString(), timeunitLabel, \"\" + timeunit, \"\" + ajaxUpdateTimeMS);\r\n    response.getWriter().println(trackInfo);\r\n    ((Request) request).setHandled(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\web",
  "methodName" : "printJsonMetrics",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void printJsonMetrics(HttpServletRequest request, HttpServletResponse response) throws IOException\n{\r\n    response.setContentType(\"text/json\");\r\n    response.setStatus(HttpServletResponse.SC_OK);\r\n    response.getWriter().println(generateRealTimeTrackingMetrics());\r\n    ((Request) request).setHandled(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\web",
  "methodName" : "generateRealTimeTrackingMetrics",
  "errType" : null,
  "containingMethodsNum" : 85,
  "sourceCodeText" : "String generateRealTimeTrackingMetrics()\n{\r\n    double jvmFreeMemoryGB, jvmMaxMemoryGB, jvmTotalMemoryGB;\r\n    if (jvmFreeMemoryGauge == null && metrics.getGauges().containsKey(\"variable.jvm.free.memory\")) {\r\n        jvmFreeMemoryGauge = metrics.getGauges().get(\"variable.jvm.free.memory\");\r\n    }\r\n    if (jvmMaxMemoryGauge == null && metrics.getGauges().containsKey(\"variable.jvm.max.memory\")) {\r\n        jvmMaxMemoryGauge = metrics.getGauges().get(\"variable.jvm.max.memory\");\r\n    }\r\n    if (jvmTotalMemoryGauge == null && metrics.getGauges().containsKey(\"variable.jvm.total.memory\")) {\r\n        jvmTotalMemoryGauge = metrics.getGauges().get(\"variable.jvm.total.memory\");\r\n    }\r\n    jvmFreeMemoryGB = jvmFreeMemoryGauge == null ? 0 : Double.parseDouble(jvmFreeMemoryGauge.getValue().toString()) / 1024 / 1024 / 1024;\r\n    jvmMaxMemoryGB = jvmMaxMemoryGauge == null ? 0 : Double.parseDouble(jvmMaxMemoryGauge.getValue().toString()) / 1024 / 1024 / 1024;\r\n    jvmTotalMemoryGB = jvmTotalMemoryGauge == null ? 0 : Double.parseDouble(jvmTotalMemoryGauge.getValue().toString()) / 1024 / 1024 / 1024;\r\n    String numRunningApps, numRunningContainers;\r\n    if (numRunningAppsGauge == null && metrics.getGauges().containsKey(\"variable.running.application\")) {\r\n        numRunningAppsGauge = metrics.getGauges().get(\"variable.running.application\");\r\n    }\r\n    if (numRunningContainersGauge == null && metrics.getGauges().containsKey(\"variable.running.container\")) {\r\n        numRunningContainersGauge = metrics.getGauges().get(\"variable.running.container\");\r\n    }\r\n    numRunningApps = numRunningAppsGauge == null ? \"0\" : numRunningAppsGauge.getValue().toString();\r\n    numRunningContainers = numRunningContainersGauge == null ? \"0\" : numRunningContainersGauge.getValue().toString();\r\n    double allocatedMemoryGB, allocatedVCoresGB, availableMemoryGB, availableVCoresGB;\r\n    if (allocatedMemoryGauge == null && metrics.getGauges().containsKey(\"variable.cluster.allocated.memory\")) {\r\n        allocatedMemoryGauge = metrics.getGauges().get(\"variable.cluster.allocated.memory\");\r\n    }\r\n    if (allocatedVCoresGauge == null && metrics.getGauges().containsKey(\"variable.cluster.allocated.vcores\")) {\r\n        allocatedVCoresGauge = metrics.getGauges().get(\"variable.cluster.allocated.vcores\");\r\n    }\r\n    if (availableMemoryGauge == null && metrics.getGauges().containsKey(\"variable.cluster.available.memory\")) {\r\n        availableMemoryGauge = metrics.getGauges().get(\"variable.cluster.available.memory\");\r\n    }\r\n    if (availableVCoresGauge == null && metrics.getGauges().containsKey(\"variable.cluster.available.vcores\")) {\r\n        availableVCoresGauge = metrics.getGauges().get(\"variable.cluster.available.vcores\");\r\n    }\r\n    allocatedMemoryGB = allocatedMemoryGauge == null ? 0 : Double.parseDouble(allocatedMemoryGauge.getValue().toString()) / 1024;\r\n    allocatedVCoresGB = allocatedVCoresGauge == null ? 0 : Double.parseDouble(allocatedVCoresGauge.getValue().toString());\r\n    availableMemoryGB = availableMemoryGauge == null ? 0 : Double.parseDouble(availableMemoryGauge.getValue().toString()) / 1024;\r\n    availableVCoresGB = availableVCoresGauge == null ? 0 : Double.parseDouble(availableVCoresGauge.getValue().toString());\r\n    double allocateTimecost, commitSuccessTimecost, commitFailureTimecost, handleTimecost;\r\n    if (allocateTimecostHistogram == null && metrics.getHistograms().containsKey(\"sampler.scheduler.operation.allocate.timecost\")) {\r\n        allocateTimecostHistogram = metrics.getHistograms().get(\"sampler.scheduler.operation.allocate.timecost\");\r\n    }\r\n    if (commitSuccessTimecostHistogram == null && metrics.getHistograms().containsKey(\"sampler.scheduler.operation.commit.success.timecost\")) {\r\n        commitSuccessTimecostHistogram = metrics.getHistograms().get(\"sampler.scheduler.operation.commit.success.timecost\");\r\n    }\r\n    if (commitFailureTimecostHistogram == null && metrics.getHistograms().containsKey(\"sampler.scheduler.operation.commit.failure.timecost\")) {\r\n        commitFailureTimecostHistogram = metrics.getHistograms().get(\"sampler.scheduler.operation.commit.failure.timecost\");\r\n    }\r\n    if (handleTimecostHistogram == null && metrics.getHistograms().containsKey(\"sampler.scheduler.operation.handle.timecost\")) {\r\n        handleTimecostHistogram = metrics.getHistograms().get(\"sampler.scheduler.operation.handle.timecost\");\r\n    }\r\n    allocateTimecost = allocateTimecostHistogram == null ? 0.0 : allocateTimecostHistogram.getSnapshot().getMean() / 1000000;\r\n    commitSuccessTimecost = commitSuccessTimecostHistogram == null ? 0.0 : commitSuccessTimecostHistogram.getSnapshot().getMean() / 1000000;\r\n    commitFailureTimecost = commitFailureTimecostHistogram == null ? 0.0 : commitFailureTimecostHistogram.getSnapshot().getMean() / 1000000;\r\n    handleTimecost = handleTimecostHistogram == null ? 0.0 : handleTimecostHistogram.getSnapshot().getMean() / 1000000;\r\n    Map<SchedulerEventType, Double> handleOperTimecostMap = new HashMap<SchedulerEventType, Double>();\r\n    for (SchedulerEventType e : SchedulerEventType.values()) {\r\n        String key = \"sampler.scheduler.operation.handle.\" + e + \".timecost\";\r\n        if (!handleOperTimecostHistogramMap.containsKey(e) && metrics.getHistograms().containsKey(key)) {\r\n            handleOperTimecostHistogramMap.put(e, metrics.getHistograms().get(key));\r\n        }\r\n        double timecost = handleOperTimecostHistogramMap.containsKey(e) ? handleOperTimecostHistogramMap.get(e).getSnapshot().getMean() / 1000000 : 0;\r\n        handleOperTimecostMap.put(e, timecost);\r\n    }\r\n    Map<String, Double> queueAllocatedMemoryMap = new HashMap<String, Double>();\r\n    Map<String, Long> queueAllocatedVCoresMap = new HashMap<String, Long>();\r\n    for (String queue : wrapper.getTracker().getQueueSet()) {\r\n        String key = \"counter.queue.\" + queue + \".allocated.memory\";\r\n        if (!queueAllocatedMemoryCounterMap.containsKey(queue) && metrics.getCounters().containsKey(key)) {\r\n            queueAllocatedMemoryCounterMap.put(queue, metrics.getCounters().get(key));\r\n        }\r\n        double queueAllocatedMemoryGB = queueAllocatedMemoryCounterMap.containsKey(queue) ? queueAllocatedMemoryCounterMap.get(queue).getCount() / 1024.0 : 0;\r\n        queueAllocatedMemoryMap.put(queue, queueAllocatedMemoryGB);\r\n        key = \"counter.queue.\" + queue + \".allocated.cores\";\r\n        if (!queueAllocatedVCoresCounterMap.containsKey(queue) && metrics.getCounters().containsKey(key)) {\r\n            queueAllocatedVCoresCounterMap.put(queue, metrics.getCounters().get(key));\r\n        }\r\n        long queueAllocatedVCores = queueAllocatedVCoresCounterMap.containsKey(queue) ? queueAllocatedVCoresCounterMap.get(queue).getCount() : 0;\r\n        queueAllocatedVCoresMap.put(queue, queueAllocatedVCores);\r\n    }\r\n    if (schedulerCommitSuccessCounter == null && metrics.getCounters().containsKey(\"counter.scheduler.operation.commit.success\")) {\r\n        schedulerCommitSuccessCounter = metrics.getCounters().get(\"counter.scheduler.operation.commit.success\");\r\n    }\r\n    if (schedulerCommitFailureCounter == null && metrics.getCounters().containsKey(\"counter.scheduler.operation.commit.failure\")) {\r\n        schedulerCommitFailureCounter = metrics.getCounters().get(\"counter.scheduler.operation.commit.failure\");\r\n    }\r\n    long schedulerCommitSuccessThroughput = 0;\r\n    long schedulerCommitFailureThroughput = 0;\r\n    if (schedulerCommitSuccessCounter != null && schedulerCommitFailureCounter != null) {\r\n        long currentTrackingTime = System.currentTimeMillis();\r\n        long currentSchedulerCommitSucessCount = schedulerCommitSuccessCounter.getCount();\r\n        long currentSchedulerCommitFailureCount = schedulerCommitFailureCounter.getCount();\r\n        if (lastTrackingTime != null) {\r\n            double intervalSeconds = (double) (currentTrackingTime - lastTrackingTime) / 1000;\r\n            schedulerCommitSuccessThroughput = Math.round((currentSchedulerCommitSucessCount - lastSchedulerCommitSuccessCount) / intervalSeconds);\r\n            schedulerCommitFailureThroughput = Math.round((currentSchedulerCommitFailureCount - lastSchedulerCommitFailureCount) / intervalSeconds);\r\n        }\r\n        lastTrackingTime = currentTrackingTime;\r\n        lastSchedulerCommitSuccessCount = currentSchedulerCommitSucessCount;\r\n        lastSchedulerCommitFailureCount = currentSchedulerCommitFailureCount;\r\n    }\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(\"{\");\r\n    sb.append(\"\\\"time\\\":\").append(System.currentTimeMillis()).append(\",\\\"jvm.free.memory\\\":\").append(jvmFreeMemoryGB).append(\",\\\"jvm.max.memory\\\":\").append(jvmMaxMemoryGB).append(\",\\\"jvm.total.memory\\\":\").append(jvmTotalMemoryGB).append(\",\\\"running.applications\\\":\").append(numRunningApps).append(\",\\\"running.containers\\\":\").append(numRunningContainers).append(\",\\\"cluster.allocated.memory\\\":\").append(allocatedMemoryGB).append(\",\\\"cluster.allocated.vcores\\\":\").append(allocatedVCoresGB).append(\",\\\"cluster.available.memory\\\":\").append(availableMemoryGB).append(\",\\\"cluster.available.vcores\\\":\").append(availableVCoresGB);\r\n    for (String queue : wrapper.getTracker().getQueueSet()) {\r\n        sb.append(\",\\\"queue.\").append(queue).append(\".allocated.memory\\\":\").append(queueAllocatedMemoryMap.get(queue));\r\n        sb.append(\",\\\"queue.\").append(queue).append(\".allocated.vcores\\\":\").append(queueAllocatedVCoresMap.get(queue));\r\n    }\r\n    sb.append(\",\\\"scheduler.allocate.timecost\\\":\").append(allocateTimecost);\r\n    sb.append(\",\\\"scheduler.commit.success.timecost\\\":\").append(commitSuccessTimecost);\r\n    sb.append(\",\\\"scheduler.commit.failure.timecost\\\":\").append(commitFailureTimecost);\r\n    sb.append(\",\\\"scheduler.commit.success.throughput\\\":\").append(schedulerCommitSuccessThroughput);\r\n    sb.append(\",\\\"scheduler.commit.failure.throughput\\\":\").append(schedulerCommitFailureThroughput);\r\n    sb.append(\",\\\"scheduler.handle.timecost\\\":\").append(handleTimecost);\r\n    for (SchedulerEventType e : SchedulerEventType.values()) {\r\n        sb.append(\",\\\"scheduler.handle-\").append(e).append(\".timecost\\\":\").append(handleOperTimecostMap.get(e));\r\n    }\r\n    sb.append(generateNodeUsageMetrics(\"memory\"));\r\n    sb.append(generateNodeUsageMetrics(\"vcores\"));\r\n    sb.append(\"}\");\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\web",
  "methodName" : "generateNodeUsageMetrics",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "String generateNodeUsageMetrics(String resourceType)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    Map<String, Integer> perNodeUsageMap = new HashMap<>();\r\n    for (NodeUsageRanges.Range range : NodeUsageRanges.getRanges()) {\r\n        String metricName = \"nodes.\" + resourceType + \".\" + range.getKeyword();\r\n        if (!perNodeUsageGaugeMap.containsKey(metricName) && metrics.getGauges().containsKey(metricName)) {\r\n            perNodeUsageGaugeMap.put(metricName, metrics.getGauges().get(metricName));\r\n        }\r\n        int perNodeUsageCount = perNodeUsageGaugeMap.containsKey(metricName) ? Integer.parseInt(perNodeUsageGaugeMap.get(metricName).getValue().toString()) : 0;\r\n        perNodeUsageMap.put(metricName, perNodeUsageCount);\r\n    }\r\n    for (NodeUsageRanges.Range range : NodeUsageRanges.getRanges()) {\r\n        String metricName = \"nodes.\" + resourceType + \".\" + range.getKeyword();\r\n        sb.append(\",\\\"\").append(metricName).append(\"\\\":\").append(perNodeUsageMap.get(metricName));\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\web",
  "methodName" : "printJsonTrack",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void printJsonTrack(HttpServletRequest request, HttpServletResponse response) throws IOException\n{\r\n    response.setContentType(\"text/json\");\r\n    response.setStatus(HttpServletResponse.SC_OK);\r\n    StringBuilder sb = new StringBuilder();\r\n    if (schedulerMetrics instanceof FairSchedulerMetrics) {\r\n        String para = request.getParameter(\"t\");\r\n        if (para.startsWith(\"Job \")) {\r\n            String appId = para.substring(\"Job \".length());\r\n            sb.append(\"{\");\r\n            sb.append(\"\\\"time\\\": \").append(System.currentTimeMillis()).append(\",\");\r\n            sb.append(\"\\\"appId\\\": \\\"\").append(appId).append(\"\\\"\");\r\n            for (String metric : this.schedulerMetrics.getAppTrackedMetrics()) {\r\n                String key = \"variable.app.\" + appId + \".\" + metric;\r\n                sb.append(\",\\\"\").append(metric).append(\"\\\": \");\r\n                if (metrics.getGauges().containsKey(key)) {\r\n                    double memoryGB = Double.parseDouble(metrics.getGauges().get(key).getValue().toString()) / 1024;\r\n                    sb.append(memoryGB);\r\n                } else {\r\n                    sb.append(-1);\r\n                }\r\n            }\r\n            sb.append(\"}\");\r\n        } else if (para.startsWith(\"Queue \")) {\r\n            String queueName = para.substring(\"Queue \".length());\r\n            sb.append(\"{\");\r\n            sb.append(\"\\\"time\\\": \").append(System.currentTimeMillis()).append(\",\");\r\n            sb.append(\"\\\"queueName\\\": \\\"\").append(queueName).append(\"\\\"\");\r\n            for (String metric : this.schedulerMetrics.getQueueTrackedMetrics()) {\r\n                String key = \"variable.queue.\" + queueName + \".\" + metric;\r\n                sb.append(\",\\\"\").append(metric).append(\"\\\": \");\r\n                if (metrics.getGauges().containsKey(key)) {\r\n                    double memoryGB = Double.parseDouble(metrics.getGauges().get(key).getValue().toString()) / 1024;\r\n                    sb.append(memoryGB);\r\n                } else {\r\n                    sb.append(-1);\r\n                }\r\n            }\r\n            sb.append(\"}\");\r\n        }\r\n    }\r\n    String output = sb.toString();\r\n    if (output.isEmpty()) {\r\n        output = \"[]\";\r\n    }\r\n    response.getWriter().println(output);\r\n    ((Request) request).setHandled(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getTaskContainers",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "List<ContainerSimulator> getTaskContainers(LoggedJob job, SLSRunner slsRunner) throws YarnException\n{\r\n    List<ContainerSimulator> containerList = new ArrayList<>();\r\n    TaskContainerDefinition.Builder builder = TaskContainerDefinition.Builder.create().withCount(1).withResource(slsRunner.getDefaultContainerResource()).withExecutionType(ExecutionType.GUARANTEED).withAllocationId(-1).withRequestDelay(0);\r\n    for (LoggedTask mapTask : job.getMapTasks()) {\r\n        if (mapTask.getAttempts().size() == 0) {\r\n            throw new YarnException(\"Invalid map task, no attempt for a mapper!\");\r\n        }\r\n        LoggedTaskAttempt taskAttempt = mapTask.getAttempts().get(mapTask.getAttempts().size() - 1);\r\n        TaskContainerDefinition containerDef = builder.withHostname(taskAttempt.getHostName().getValue()).withDuration(taskAttempt.getFinishTime() - taskAttempt.getStartTime()).withPriority(DEFAULT_MAPPER_PRIORITY).withType(\"map\").build();\r\n        containerList.add(ContainerSimulator.createFromTaskContainerDefinition(containerDef));\r\n    }\r\n    for (LoggedTask reduceTask : job.getReduceTasks()) {\r\n        if (reduceTask.getAttempts().size() == 0) {\r\n            throw new YarnException(\"Invalid reduce task, no attempt for a reducer!\");\r\n        }\r\n        LoggedTaskAttempt taskAttempt = reduceTask.getAttempts().get(reduceTask.getAttempts().size() - 1);\r\n        TaskContainerDefinition containerDef = builder.withHostname(taskAttempt.getHostName().getValue()).withDuration(taskAttempt.getFinishTime() - taskAttempt.getStartTime()).withPriority(DEFAULT_REDUCER_PRIORITY).withType(\"reduce\").build();\r\n        containerList.add(ContainerSimulator.createFromTaskContainerDefinition(containerDef));\r\n    }\r\n    return containerList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 40,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    Options options = new Options();\r\n    options.addOption(\"input\", true, \"input rumen json file\");\r\n    options.addOption(\"outputJobs\", true, \"output jobs file\");\r\n    options.addOption(\"outputNodes\", true, \"output nodes file\");\r\n    CommandLineParser parser = new GnuParser();\r\n    CommandLine cmd = parser.parse(options, args);\r\n    if (!cmd.hasOption(\"input\") || !cmd.hasOption(\"outputJobs\") || !cmd.hasOption(\"outputNodes\")) {\r\n        System.err.println();\r\n        System.err.println(\"ERROR: Missing input or output file\");\r\n        System.err.println();\r\n        System.err.println(\"LoadGenerator creates a SLS script \" + \"from a Hadoop Rumen output\");\r\n        System.err.println();\r\n        System.err.println(\"Options: -input FILE -outputJobs FILE \" + \"-outputNodes FILE\");\r\n        System.err.println();\r\n        System.exit(1);\r\n    }\r\n    String inputFile = cmd.getOptionValue(\"input\");\r\n    String outputJsonFile = cmd.getOptionValue(\"outputJobs\");\r\n    String outputNodeFile = cmd.getOptionValue(\"outputNodes\");\r\n    if (!new File(inputFile).exists()) {\r\n        System.err.println();\r\n        System.err.println(\"ERROR: input does not exist\");\r\n        System.exit(1);\r\n    }\r\n    if (new File(outputJsonFile).exists()) {\r\n        System.err.println();\r\n        System.err.println(\"ERROR: output job file is existing\");\r\n        System.exit(1);\r\n    }\r\n    if (new File(outputNodeFile).exists()) {\r\n        System.err.println();\r\n        System.err.println(\"ERROR: output node file is existing\");\r\n        System.exit(1);\r\n    }\r\n    File jsonFile = new File(outputJsonFile);\r\n    if (!jsonFile.getParentFile().exists() && !jsonFile.getParentFile().mkdirs()) {\r\n        System.err.println(\"ERROR: Cannot create output directory in path: \" + jsonFile.getParentFile().getAbsoluteFile());\r\n        System.exit(1);\r\n    }\r\n    File nodeFile = new File(outputNodeFile);\r\n    if (!nodeFile.getParentFile().exists() && !nodeFile.getParentFile().mkdirs()) {\r\n        System.err.println(\"ERROR: Cannot create output directory in path: \" + nodeFile.getParentFile().getAbsoluteFile());\r\n        System.exit(1);\r\n    }\r\n    generateSLSLoadFile(inputFile, outputJsonFile);\r\n    generateSLSNodeFile(outputNodeFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "generateSLSLoadFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void generateSLSLoadFile(String inputFile, String outputFile) throws IOException\n{\r\n    try (Reader input = new InputStreamReader(new FileInputStream(inputFile), StandardCharsets.UTF_8)) {\r\n        try (Writer output = new OutputStreamWriter(new FileOutputStream(outputFile), StandardCharsets.UTF_8)) {\r\n            ObjectMapper mapper = new ObjectMapper();\r\n            ObjectWriter writer = mapper.writerWithDefaultPrettyPrinter();\r\n            Iterator<Map> i = mapper.readValues(new JsonFactory().createParser(input), Map.class);\r\n            while (i.hasNext()) {\r\n                Map m = i.next();\r\n                output.write(writer.writeValueAsString(createSLSJob(m)) + EOL);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "generateSLSNodeFile",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void generateSLSNodeFile(String outputFile) throws IOException\n{\r\n    try (Writer output = new OutputStreamWriter(new FileOutputStream(outputFile), StandardCharsets.UTF_8)) {\r\n        ObjectMapper mapper = new ObjectMapper();\r\n        ObjectWriter writer = mapper.writerWithDefaultPrettyPrinter();\r\n        for (Map.Entry<String, Set<String>> entry : rackNodeMap.entrySet()) {\r\n            Map rack = new LinkedHashMap();\r\n            rack.put(\"rack\", entry.getKey());\r\n            List nodes = new ArrayList();\r\n            for (String name : entry.getValue()) {\r\n                Map node = new LinkedHashMap();\r\n                node.put(\"node\", name);\r\n                nodes.add(node);\r\n            }\r\n            rack.put(\"nodes\", nodes);\r\n            output.write(writer.writeValueAsString(rack) + EOL);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "createSLSJob",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "Map createSLSJob(Map rumenJob)\n{\r\n    Map json = new LinkedHashMap();\r\n    long jobStart = (Long) rumenJob.get(\"submitTime\");\r\n    long jobFinish = (Long) rumenJob.get(\"finishTime\");\r\n    String jobId = rumenJob.get(\"jobID\").toString();\r\n    String queue = rumenJob.get(\"queue\").toString();\r\n    String user = rumenJob.get(\"user\").toString();\r\n    if (baseline == 0) {\r\n        baseline = jobStart;\r\n    }\r\n    jobStart -= baseline;\r\n    jobFinish -= baseline;\r\n    long offset = 0;\r\n    if (jobStart < 0) {\r\n        System.out.println(\"Warning: reset job \" + jobId + \" start time to 0.\");\r\n        offset = -jobStart;\r\n        jobFinish = jobFinish - jobStart;\r\n        jobStart = 0;\r\n    }\r\n    json.put(\"am.type\", \"mapreduce\");\r\n    json.put(\"job.start.ms\", jobStart);\r\n    json.put(\"job.end.ms\", jobFinish);\r\n    json.put(\"job.queue.name\", queue);\r\n    json.put(\"job.id\", jobId);\r\n    json.put(\"job.user\", user);\r\n    List maps = createSLSTasks(\"map\", (List) rumenJob.get(\"mapTasks\"), offset);\r\n    List reduces = createSLSTasks(\"reduce\", (List) rumenJob.get(\"reduceTasks\"), offset);\r\n    List tasks = new ArrayList();\r\n    tasks.addAll(maps);\r\n    tasks.addAll(reduces);\r\n    json.put(\"job.tasks\", tasks);\r\n    return json;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "createSLSTasks",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "List createSLSTasks(String taskType, List rumenTasks, long offset)\n{\r\n    int priority = taskType.equals(\"reduce\") ? 10 : 20;\r\n    List array = new ArrayList();\r\n    for (Object e : rumenTasks) {\r\n        Map rumenTask = (Map) e;\r\n        for (Object ee : (List) rumenTask.get(\"attempts\")) {\r\n            Map rumenAttempt = (Map) ee;\r\n            long taskStart = (Long) rumenAttempt.get(\"startTime\");\r\n            long taskFinish = (Long) rumenAttempt.get(\"finishTime\");\r\n            String hostname = (String) rumenAttempt.get(\"hostName\");\r\n            taskStart = taskStart - baseline + offset;\r\n            taskFinish = taskFinish - baseline + offset;\r\n            Map task = new LinkedHashMap();\r\n            task.put(\"container.host\", hostname);\r\n            task.put(\"container.start.ms\", taskStart);\r\n            task.put(\"container.end.ms\", taskFinish);\r\n            task.put(\"container.priority\", priority);\r\n            task.put(\"container.type\", taskType);\r\n            array.add(task);\r\n            String[] rackHost = SLSUtils.getRackHostName(hostname);\r\n            if (rackNodeMap.containsKey(rackHost[0])) {\r\n                rackNodeMap.get(rackHost[0]).add(rackHost[1]);\r\n            } else {\r\n                Set<String> hosts = new TreeSet<String>();\r\n                hosts.add(rackHost[1]);\r\n                rackNodeMap.put(rackHost[0], hosts);\r\n            }\r\n        }\r\n    }\r\n    return array;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getSchedulerMetricsClass",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Class getSchedulerMetricsClass(Configuration conf, Class schedulerClass) throws ClassNotFoundException\n{\r\n    Class metricClass = null;\r\n    String schedulerMetricsType = conf.get(schedulerClass.getName());\r\n    if (schedulerMetricsType != null) {\r\n        metricClass = Class.forName(schedulerMetricsType);\r\n    }\r\n    if (schedulerClass.equals(FairScheduler.class)) {\r\n        metricClass = FairSchedulerMetrics.class;\r\n    } else if (schedulerClass.equals(CapacityScheduler.class)) {\r\n        metricClass = CapacitySchedulerMetrics.class;\r\n    } else if (schedulerClass.equals(FifoScheduler.class)) {\r\n        metricClass = FifoSchedulerMetrics.class;\r\n    }\r\n    return metricClass;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SchedulerMetrics getInstance(Configuration conf, Class schedulerClass) throws ClassNotFoundException\n{\r\n    Class schedulerMetricClass = getSchedulerMetricsClass(conf, schedulerClass);\r\n    return (SchedulerMetrics) ReflectionUtils.newInstance(schedulerMetricClass, new Configuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void init(ResourceScheduler resourceScheduler, Configuration config) throws Exception\n{\r\n    this.scheduler = resourceScheduler;\r\n    this.conf = config;\r\n    metricsOutputDir = conf.get(SLSConfiguration.METRICS_OUTPUT_DIR);\r\n    registerJvmMetrics();\r\n    registerClusterResourceMetrics();\r\n    registerContainerAppNumMetrics();\r\n    registerSchedulerMetrics();\r\n    registerNodesUsageMetrics(\"memory\");\r\n    registerNodesUsageMetrics(\"vcores\");\r\n    initMetricsCSVOutput();\r\n    int metricsWebAddressPort = conf.getInt(SLSConfiguration.METRICS_WEB_ADDRESS_PORT, SLSConfiguration.METRICS_WEB_ADDRESS_PORT_DEFAULT);\r\n    web = new SLSWebApp((SchedulerWrapper) scheduler, metricsWebAddressPort);\r\n    web.start();\r\n    pool = new HadoopScheduledThreadPoolExecutor(2);\r\n    pool.scheduleAtFixedRate(new HistogramsRunnable(), 0, 1000, TimeUnit.MILLISECONDS);\r\n    pool.scheduleAtFixedRate(new MetricsLogRunnable(), 0, 1000, TimeUnit.MILLISECONDS);\r\n    jobRuntimeLogBW = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(metricsOutputDir + \"/jobruntime.csv\"), StandardCharsets.UTF_8));\r\n    jobRuntimeLogBW.write(\"JobID,real_start_time,real_end_time,\" + \"simulate_start_time,simulate_end_time\" + EOL);\r\n    jobRuntimeLogBW.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MetricRegistry getMetrics()\n{\r\n    return metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getSchedulerAppAttempt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SchedulerApplicationAttempt getSchedulerAppAttempt(ApplicationId appId)\n{\r\n    AbstractYarnScheduler yarnScheduler = (AbstractYarnScheduler) scheduler;\r\n    SchedulerApplication app = (SchedulerApplication) yarnScheduler.getSchedulerApplications().get(appId);\r\n    if (app == null) {\r\n        return null;\r\n    }\r\n    return app.getCurrentAppAttempt();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "trackApp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void trackApp(final ApplicationId appId, String oldAppId)\n{\r\n    metrics.register(\"variable.app.\" + oldAppId + \".live.containers\", new Gauge<Integer>() {\r\n\r\n        @Override\r\n        public Integer getValue() {\r\n            SchedulerApplicationAttempt appAttempt = getSchedulerAppAttempt(appId);\r\n            if (appAttempt != null) {\r\n                return appAttempt.getLiveContainers().size();\r\n            } else {\r\n                return 0;\r\n            }\r\n        }\r\n    });\r\n    metrics.register(\"variable.app.\" + oldAppId + \".reserved.containers\", new Gauge<Integer>() {\r\n\r\n        @Override\r\n        public Integer getValue() {\r\n            SchedulerApplicationAttempt appAttempt = getSchedulerAppAttempt(appId);\r\n            if (appAttempt != null) {\r\n                return appAttempt.getReservedContainers().size();\r\n            } else {\r\n                return 0;\r\n            }\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "untrackApp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void untrackApp(String oldAppId)\n{\r\n    for (String m : appTrackedMetrics) {\r\n        metrics.remove(\"variable.app.\" + oldAppId + \".\" + m);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "trackQueue",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void trackQueue(String queue)\n{\r\n    queueLock.lock();\r\n    try {\r\n        if (!isTracked(queue)) {\r\n            trackedQueues.add(queue);\r\n            registerQueueMetrics(queue);\r\n        }\r\n    } finally {\r\n        queueLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "registerQueueMetrics",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void registerQueueMetrics(String queueName)\n{\r\n    SortedMap<String, Counter> counterMap = metrics.getCounters();\r\n    for (QueueMetric queueMetric : QueueMetric.values()) {\r\n        String metricName = getQueueMetricName(queueName, queueMetric);\r\n        if (!counterMap.containsKey(metricName)) {\r\n            metrics.counter(metricName);\r\n            queueTrackedMetrics.add(metricName);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "isTracked",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isTracked(String queueName)\n{\r\n    return trackedQueues.contains(queueName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getAppTrackedMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<String> getAppTrackedMetrics()\n{\r\n    return appTrackedMetrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getQueueTrackedMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<String> getQueueTrackedMetrics()\n{\r\n    return queueTrackedMetrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "registerJvmMetrics",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void registerJvmMetrics()\n{\r\n    metrics.register(\"variable.jvm.free.memory\", new Gauge<Long>() {\r\n\r\n        @Override\r\n        public Long getValue() {\r\n            return Runtime.getRuntime().freeMemory();\r\n        }\r\n    });\r\n    metrics.register(\"variable.jvm.max.memory\", new Gauge<Long>() {\r\n\r\n        @Override\r\n        public Long getValue() {\r\n            return Runtime.getRuntime().maxMemory();\r\n        }\r\n    });\r\n    metrics.register(\"variable.jvm.total.memory\", new Gauge<Long>() {\r\n\r\n        @Override\r\n        public Long getValue() {\r\n            return Runtime.getRuntime().totalMemory();\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "registerClusterResourceMetrics",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void registerClusterResourceMetrics()\n{\r\n    metrics.register(\"variable.cluster.allocated.memory\", new Gauge<Long>() {\r\n\r\n        @Override\r\n        public Long getValue() {\r\n            if (isMetricsAvailable()) {\r\n                return 0L;\r\n            } else {\r\n                return scheduler.getRootQueueMetrics().getAllocatedMB();\r\n            }\r\n        }\r\n    });\r\n    metrics.register(\"variable.cluster.allocated.vcores\", new Gauge<Integer>() {\r\n\r\n        @Override\r\n        public Integer getValue() {\r\n            if (isMetricsAvailable()) {\r\n                return 0;\r\n            } else {\r\n                return scheduler.getRootQueueMetrics().getAllocatedVirtualCores();\r\n            }\r\n        }\r\n    });\r\n    metrics.register(\"variable.cluster.available.memory\", new Gauge<Long>() {\r\n\r\n        @Override\r\n        public Long getValue() {\r\n            if (isMetricsAvailable()) {\r\n                return 0L;\r\n            } else {\r\n                return scheduler.getRootQueueMetrics().getAvailableMB();\r\n            }\r\n        }\r\n    });\r\n    metrics.register(\"variable.cluster.available.vcores\", new Gauge<Integer>() {\r\n\r\n        @Override\r\n        public Integer getValue() {\r\n            if (isMetricsAvailable()) {\r\n                return 0;\r\n            } else {\r\n                return scheduler.getRootQueueMetrics().getAvailableVirtualCores();\r\n            }\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "isMetricsAvailable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isMetricsAvailable()\n{\r\n    return scheduler.getRootQueueMetrics() == null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "registerContainerAppNumMetrics",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void registerContainerAppNumMetrics()\n{\r\n    metrics.register(\"variable.running.application\", new Gauge<Integer>() {\r\n\r\n        @Override\r\n        public Integer getValue() {\r\n            if (scheduler.getRootQueueMetrics() == null) {\r\n                return 0;\r\n            } else {\r\n                return scheduler.getRootQueueMetrics().getAppsRunning();\r\n            }\r\n        }\r\n    });\r\n    metrics.register(\"variable.running.container\", new Gauge<Integer>() {\r\n\r\n        @Override\r\n        public Integer getValue() {\r\n            if (scheduler.getRootQueueMetrics() == null) {\r\n                return 0;\r\n            } else {\r\n                return scheduler.getRootQueueMetrics().getAllocatedContainers();\r\n            }\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "registerSchedulerMetrics",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void registerSchedulerMetrics()\n{\r\n    samplerLock.lock();\r\n    try {\r\n        schedulerAllocateCounter = metrics.counter(\"counter.scheduler.operation.allocate\");\r\n        schedulerCommitSuccessCounter = metrics.counter(\"counter.scheduler.operation.commit.success\");\r\n        schedulerCommitFailureCounter = metrics.counter(\"counter.scheduler.operation.commit.failure\");\r\n        schedulerHandleCounter = metrics.counter(\"counter.scheduler.operation.handle\");\r\n        schedulerHandleCounterMap = new HashMap<>();\r\n        for (SchedulerEventType e : SchedulerEventType.values()) {\r\n            Counter counter = metrics.counter(\"counter.scheduler.operation.handle.\" + e);\r\n            schedulerHandleCounterMap.put(e, counter);\r\n        }\r\n        int timeWindowSize = conf.getInt(SLSConfiguration.METRICS_TIMER_WINDOW_SIZE, SLSConfiguration.METRICS_TIMER_WINDOW_SIZE_DEFAULT);\r\n        schedulerAllocateTimer = new Timer(new SlidingWindowReservoir(timeWindowSize));\r\n        schedulerCommitSuccessTimer = new Timer(new SlidingWindowReservoir(timeWindowSize));\r\n        schedulerCommitFailureTimer = new Timer(new SlidingWindowReservoir(timeWindowSize));\r\n        schedulerHandleTimer = new Timer(new SlidingWindowReservoir(timeWindowSize));\r\n        schedulerHandleTimerMap = new HashMap<>();\r\n        for (SchedulerEventType e : SchedulerEventType.values()) {\r\n            Timer timer = new Timer(new SlidingWindowReservoir(timeWindowSize));\r\n            schedulerHandleTimerMap.put(e, timer);\r\n        }\r\n        schedulerHistogramList = new ArrayList<>();\r\n        histogramTimerMap = new HashMap<>();\r\n        Histogram schedulerAllocateHistogram = new Histogram(new SlidingWindowReservoir(SAMPLING_SIZE));\r\n        metrics.register(\"sampler.scheduler.operation.allocate.timecost\", schedulerAllocateHistogram);\r\n        schedulerHistogramList.add(schedulerAllocateHistogram);\r\n        histogramTimerMap.put(schedulerAllocateHistogram, schedulerAllocateTimer);\r\n        Histogram schedulerCommitHistogram = new Histogram(new SlidingWindowReservoir(SAMPLING_SIZE));\r\n        metrics.register(\"sampler.scheduler.operation.commit.success.timecost\", schedulerCommitHistogram);\r\n        schedulerHistogramList.add(schedulerCommitHistogram);\r\n        histogramTimerMap.put(schedulerCommitHistogram, schedulerCommitSuccessTimer);\r\n        Histogram schedulerCommitFailureHistogram = new Histogram(new SlidingWindowReservoir(SAMPLING_SIZE));\r\n        metrics.register(\"sampler.scheduler.operation.commit.failure.timecost\", schedulerCommitFailureHistogram);\r\n        schedulerHistogramList.add(schedulerCommitFailureHistogram);\r\n        histogramTimerMap.put(schedulerCommitFailureHistogram, schedulerCommitFailureTimer);\r\n        Histogram schedulerHandleHistogram = new Histogram(new SlidingWindowReservoir(SAMPLING_SIZE));\r\n        metrics.register(\"sampler.scheduler.operation.handle.timecost\", schedulerHandleHistogram);\r\n        schedulerHistogramList.add(schedulerHandleHistogram);\r\n        histogramTimerMap.put(schedulerHandleHistogram, schedulerHandleTimer);\r\n        for (SchedulerEventType e : SchedulerEventType.values()) {\r\n            Histogram histogram = new Histogram(new SlidingWindowReservoir(SAMPLING_SIZE));\r\n            metrics.register(\"sampler.scheduler.operation.handle.\" + e + \".timecost\", histogram);\r\n            schedulerHistogramList.add(histogram);\r\n            histogramTimerMap.put(histogram, schedulerHandleTimerMap.get(e));\r\n        }\r\n    } finally {\r\n        samplerLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "registerNodesUsageMetrics",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void registerNodesUsageMetrics(String resourceType)\n{\r\n    samplerLock.lock();\r\n    try {\r\n        for (NodeUsageRanges.Range range : NodeUsageRanges.getRanges()) {\r\n            String metricName = \"nodes.\" + resourceType + \".\" + range.getKeyword();\r\n            metrics.register(metricName, new Gauge<Integer>() {\r\n\r\n                @Override\r\n                public Integer getValue() {\r\n                    if (!(scheduler instanceof AbstractYarnScheduler)) {\r\n                        return 0;\r\n                    } else {\r\n                        int count = 0;\r\n                        AbstractYarnScheduler sch = (AbstractYarnScheduler) scheduler;\r\n                        for (Object node : sch.getNodeTracker().getAllNodes()) {\r\n                            SchedulerNode sNode = (SchedulerNode) node;\r\n                            long allocated = 0, total = 0;\r\n                            if (resourceType.equals(\"memory\")) {\r\n                                allocated = sNode.getAllocatedResource().getMemorySize();\r\n                                total = sNode.getTotalResource().getMemorySize();\r\n                            } else if (resourceType.equals(\"vcores\")) {\r\n                                allocated = sNode.getAllocatedResource().getVirtualCores();\r\n                                total = sNode.getTotalResource().getVirtualCores();\r\n                            }\r\n                            float usedPct = allocated * 100f / total;\r\n                            if (range.getLowerLimit() <= usedPct && usedPct <= range.getUpperLimit()) {\r\n                                count++;\r\n                            }\r\n                        }\r\n                        return count;\r\n                    }\r\n                }\r\n            });\r\n        }\r\n    } finally {\r\n        samplerLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "initMetricsCSVOutput",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void initMetricsCSVOutput()\n{\r\n    int timeIntervalMS = conf.getInt(SLSConfiguration.METRICS_RECORD_INTERVAL_MS, SLSConfiguration.METRICS_RECORD_INTERVAL_MS_DEFAULT);\r\n    File dir = new File(metricsOutputDir + \"/metrics\");\r\n    if (!dir.exists() && !dir.mkdirs()) {\r\n        LOG.error(\"Cannot create directory {}\", dir.getAbsoluteFile());\r\n    }\r\n    final CsvReporter reporter = CsvReporter.forRegistry(metrics).formatFor(Locale.US).convertRatesTo(TimeUnit.SECONDS).convertDurationsTo(TimeUnit.MILLISECONDS).build(new File(metricsOutputDir + \"/metrics\"));\r\n    reporter.start(timeIntervalMS, TimeUnit.MILLISECONDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "isRunning",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isRunning()\n{\r\n    return running;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "setRunning",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRunning(boolean running)\n{\r\n    this.running = running;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    setRunning(false);\r\n    LOG.info(\"Scheduler Metrics tears down\");\r\n    if (metricsLogBW != null) {\r\n        metricsLogBW.write(\"]\");\r\n        metricsLogBW.close();\r\n        metricsLogBW = null;\r\n    }\r\n    if (web != null) {\r\n        web.stop();\r\n    }\r\n    if (jobRuntimeLogBW != null) {\r\n        jobRuntimeLogBW.close();\r\n    }\r\n    if (pool != null) {\r\n        pool.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "increaseSchedulerAllocationCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void increaseSchedulerAllocationCounter()\n{\r\n    schedulerAllocateCounter.inc();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "increaseSchedulerCommitSuccessCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void increaseSchedulerCommitSuccessCounter()\n{\r\n    schedulerCommitSuccessCounter.inc();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "increaseSchedulerCommitFailureCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void increaseSchedulerCommitFailureCounter()\n{\r\n    schedulerCommitFailureCounter.inc();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "increaseSchedulerHandleCounter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void increaseSchedulerHandleCounter(SchedulerEventType schedulerEventType)\n{\r\n    schedulerHandleCounter.inc();\r\n    schedulerHandleCounterMap.get(schedulerEventType).inc();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getSchedulerAllocateTimer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Timer getSchedulerAllocateTimer()\n{\r\n    return schedulerAllocateTimer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getSchedulerCommitSuccessTimer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Timer getSchedulerCommitSuccessTimer()\n{\r\n    return schedulerCommitSuccessTimer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getSchedulerCommitFailureTimer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Timer getSchedulerCommitFailureTimer()\n{\r\n    return schedulerCommitFailureTimer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getSchedulerHandleTimer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Timer getSchedulerHandleTimer()\n{\r\n    return schedulerHandleTimer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getSchedulerHandleTimer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Timer getSchedulerHandleTimer(SchedulerEventType schedulerEventType)\n{\r\n    return schedulerHandleTimerMap.get(schedulerEventType);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getQueueMetricName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getQueueMetricName(String queue, QueueMetric metric)\n{\r\n    return \"counter.queue.\" + queue + \".\" + metric.value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "updateQueueMetrics",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void updateQueueMetrics(Resource pendingResource, Resource allocatedResource, String queueName)\n{\r\n    trackQueue(queueName);\r\n    SortedMap<String, Counter> counterMap = metrics.getCounters();\r\n    for (QueueMetric metric : QueueMetric.values()) {\r\n        String metricName = getQueueMetricName(queueName, metric);\r\n        if (metric == QueueMetric.PENDING_MEMORY) {\r\n            counterMap.get(metricName).inc(pendingResource.getMemorySize());\r\n        } else if (metric == QueueMetric.PENDING_VCORES) {\r\n            counterMap.get(metricName).inc(pendingResource.getVirtualCores());\r\n        } else if (metric == QueueMetric.ALLOCATED_MEMORY) {\r\n            counterMap.get(metricName).inc(allocatedResource.getMemorySize());\r\n        } else if (metric == QueueMetric.ALLOCATED_VCORES) {\r\n            counterMap.get(metricName).inc(allocatedResource.getVirtualCores());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "updateQueueMetricsByRelease",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void updateQueueMetricsByRelease(Resource releaseResource, String queue)\n{\r\n    SortedMap<String, Counter> counterMap = metrics.getCounters();\r\n    String name = getQueueMetricName(queue, QueueMetric.ALLOCATED_MEMORY);\r\n    if (!counterMap.containsKey(name)) {\r\n        metrics.counter(name);\r\n        counterMap = metrics.getCounters();\r\n    }\r\n    counterMap.get(name).inc(-releaseResource.getMemorySize());\r\n    String vcoreMetric = getQueueMetricName(queue, QueueMetric.ALLOCATED_VCORES);\r\n    if (!counterMap.containsKey(vcoreMetric)) {\r\n        metrics.counter(vcoreMetric);\r\n        counterMap = metrics.getCounters();\r\n    }\r\n    counterMap.get(vcoreMetric).inc(-releaseResource.getVirtualCores());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "addTrackedApp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addTrackedApp(ApplicationId appId, String oldAppId)\n{\r\n    trackApp(appId, oldAppId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "removeTrackedApp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeTrackedApp(String oldAppId)\n{\r\n    untrackApp(oldAppId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "addAMRuntime",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addAMRuntime(ApplicationId appId, long traceStartTimeMS, long traceEndTimeMS, long simulateStartTimeMS, long simulateEndTimeMS)\n{\r\n    try {\r\n        String runtimeInfo = appId + \",\" + traceStartTimeMS + \",\" + traceEndTimeMS + \",\" + simulateStartTimeMS + \",\" + simulateEndTimeMS;\r\n        jobRuntimeLogBW.write(runtimeInfo + EOL);\r\n        jobRuntimeLogBW.flush();\r\n    } catch (IOException e) {\r\n        LOG.info(e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void init(AMDefinition amDef, ResourceManager rm, SLSRunner slsRunner, boolean tracked, long baselineTimeMS, long heartbeatInterval, Map<ApplicationId, AMSimulator> appIdToAMSim)\n{\r\n    super.init(amDef, rm, slsRunner, tracked, baselineTimeMS, heartbeatInterval, appIdToAMSim);\r\n    amtype = \"stream\";\r\n    allStreams.addAll(amDef.getTaskContainers());\r\n    duration = amDef.getJobFinishTime() - amDef.getJobStartTime();\r\n    LOG.info(\"Added new job with {} streams, running for {}\", allStreams.size(), duration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "notifyAMContainerLaunched",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void notifyAMContainerLaunched(Container masterContainer) throws Exception\n{\r\n    if (null != masterContainer) {\r\n        restart();\r\n        super.notifyAMContainerLaunched(masterContainer);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "processResponseQueue",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void processResponseQueue() throws Exception\n{\r\n    while (!responseQueue.isEmpty()) {\r\n        AllocateResponse response = responseQueue.take();\r\n        if (!response.getCompletedContainersStatuses().isEmpty()) {\r\n            for (ContainerStatus cs : response.getCompletedContainersStatuses()) {\r\n                ContainerId containerId = cs.getContainerId();\r\n                if (assignedStreams.containsKey(containerId)) {\r\n                    LOG.debug(\"Application {} has one streamer finished ({}).\", appId, containerId);\r\n                    pendingStreams.add(assignedStreams.remove(containerId));\r\n                } else if (amContainer.getId().equals(containerId)) {\r\n                    if (cs.getExitStatus() == ContainerExitStatus.SUCCESS) {\r\n                        isAMContainerRunning = false;\r\n                        isFinished = true;\r\n                        LOG.info(\"Application {} goes to finish.\", appId);\r\n                    } else {\r\n                        LOG.info(\"Application {}'s AM is \" + \"going to be killed. Waiting for rescheduling...\", appId);\r\n                        isAMContainerRunning = false;\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        if (isAMContainerRunning && (System.currentTimeMillis() - simulateStartTimeMS >= duration)) {\r\n            LOG.debug(\"Application {} sends out event to clean up\" + \" its AM container.\", appId);\r\n            isAMContainerRunning = false;\r\n            isFinished = true;\r\n            break;\r\n        }\r\n        for (Container container : response.getAllocatedContainers()) {\r\n            if (!scheduledStreams.isEmpty()) {\r\n                ContainerSimulator cs = scheduledStreams.remove();\r\n                LOG.debug(\"Application {} starts to launch a stream ({}).\", appId, container.getId());\r\n                assignedStreams.put(container.getId(), cs);\r\n                se.getNmMap().get(container.getNodeId()).addNewContainer(container, cs.getLifeTime(), appId);\r\n                getRanNodes().add(container.getNodeId());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "restart",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void restart() throws YarnException, IOException, InterruptedException\n{\r\n    isFinished = false;\r\n    pendingStreams.clear();\r\n    pendingStreams.addAll(allStreams);\r\n    amContainer = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "mergeLists",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<ContainerSimulator> mergeLists(List<ContainerSimulator> left, List<ContainerSimulator> right)\n{\r\n    List<ContainerSimulator> list = new ArrayList<>();\r\n    list.addAll(left);\r\n    list.addAll(right);\r\n    return list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "sendContainerRequest",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void sendContainerRequest() throws YarnException, IOException, InterruptedException\n{\r\n    List<ResourceRequest> ask = new ArrayList<>();\r\n    List<ContainerId> release = new ArrayList<>();\r\n    if (!isFinished) {\r\n        if (!pendingStreams.isEmpty()) {\r\n            ask = packageRequests(mergeLists(pendingStreams, scheduledStreams), PRIORITY_MAP);\r\n            LOG.debug(\"Application {} sends out request for {} streams.\", appId, pendingStreams.size());\r\n            scheduledStreams.addAll(pendingStreams);\r\n            pendingStreams.clear();\r\n        }\r\n    }\r\n    if (isFinished) {\r\n        release.addAll(assignedStreams.keySet());\r\n        ask.clear();\r\n    }\r\n    final AllocateRequest request = createAllocateRequest(ask, release);\r\n    if (totalContainers == 0) {\r\n        request.setProgress(1.0f);\r\n    } else {\r\n        request.setProgress((float) finishedContainers / totalContainers);\r\n    }\r\n    UserGroupInformation ugi = UserGroupInformation.createRemoteUser(appAttemptId.toString());\r\n    Token<AMRMTokenIdentifier> token = rm.getRMContext().getRMApps().get(appAttemptId.getApplicationId()).getRMAppAttempt(appAttemptId).getAMRMToken();\r\n    ugi.addTokenIdentifier(token.decodeIdentifier());\r\n    AllocateResponse response = ugi.doAs(new PrivilegedExceptionAction<AllocateResponse>() {\r\n\r\n        @Override\r\n        public AllocateResponse run() throws Exception {\r\n            return rm.getApplicationMasterService().allocate(request);\r\n        }\r\n    });\r\n    if (response != null) {\r\n        responseQueue.put(response);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "initReservation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initReservation(ReservationId reservationId, long deadline, long now)\n{\r\n    setReservationRequest(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "checkStop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkStop()\n{\r\n    if (isFinished) {\r\n        super.setEndTime(System.currentTimeMillis());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "lastStep",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void lastStep() throws Exception\n{\r\n    super.lastStep();\r\n    allStreams.clear();\r\n    assignedStreams.clear();\r\n    pendingStreams.clear();\r\n    scheduledStreams.clear();\r\n    responseQueue.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\utils",
  "methodName" : "getRanges",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<Range> getRanges()\n{\r\n    return RANGES;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getAmDefinition",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AMDefinition getAmDefinition()\n{\r\n    return amDefinition;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getReservationId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReservationId getReservationId()\n{\r\n    return reservationId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getDeadline",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDeadline()\n{\r\n    return deadline;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getParams",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, String> getParams()\n{\r\n    return params;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void init(AMDefinition amDef, ResourceManager rm, SLSRunner slsRunner, boolean tracked, long baselineTimeMS, long heartbeatInterval, Map<ApplicationId, AMSimulator> appIdToAMSim)\n{\r\n    long startTime = amDef.getJobStartTime();\r\n    long endTime = startTime + 1000000L * heartbeatInterval;\r\n    super.init(startTime, endTime, heartbeatInterval);\r\n    this.user = amDef.getUser();\r\n    this.queue = amDef.getQueue();\r\n    this.oldAppId = amDef.getOldAppId();\r\n    this.amContainerResource = amDef.getAmResource();\r\n    this.nodeLabelExpression = amDef.getLabelExpression();\r\n    this.traceStartTimeMS = amDef.getJobStartTime();\r\n    this.traceFinishTimeMS = amDef.getJobFinishTime();\r\n    this.rm = rm;\r\n    this.se = slsRunner;\r\n    this.isTracked = tracked;\r\n    this.baselineTimeMS = baselineTimeMS;\r\n    this.appIdToAMSim = appIdToAMSim;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "firstStep",
  "errType" : [ "UndeclaredThrowableException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void firstStep() throws Exception\n{\r\n    simulateStartTimeMS = System.currentTimeMillis() - baselineTimeMS;\r\n    ReservationId reservationId = null;\r\n    try {\r\n        reservationId = submitReservationWhenSpecified();\r\n    } catch (UndeclaredThrowableException y) {\r\n        LOG.warn(\"Unable to place reservation: \" + y.getMessage());\r\n    }\r\n    submitApp(reservationId);\r\n    appIdToAMSim.put(appId, this);\r\n    trackApp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "notifyAMContainerLaunched",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void notifyAMContainerLaunched(Container masterContainer) throws Exception\n{\r\n    this.amContainer = masterContainer;\r\n    this.appAttemptId = masterContainer.getId().getApplicationAttemptId();\r\n    registerAM();\r\n    isAMContainerRunning = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "setReservationRequest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setReservationRequest(ReservationSubmissionRequest rr)\n{\r\n    this.reservationRequest = rr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "submitReservationWhenSpecified",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ReservationId submitReservationWhenSpecified() throws IOException, InterruptedException\n{\r\n    if (reservationRequest != null) {\r\n        UserGroupInformation ugi = UserGroupInformation.createRemoteUser(user);\r\n        ugi.doAs(new PrivilegedExceptionAction<Object>() {\r\n\r\n            @Override\r\n            public Object run() throws YarnException, IOException {\r\n                rm.getClientRMService().submitReservation(reservationRequest);\r\n                LOG.info(\"RESERVATION SUCCESSFULLY SUBMITTED \" + reservationRequest.getReservationId());\r\n                return null;\r\n            }\r\n        });\r\n        return reservationRequest.getReservationId();\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "middleStep",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void middleStep() throws Exception\n{\r\n    if (isAMContainerRunning) {\r\n        processResponseQueue();\r\n        sendContainerRequest();\r\n        checkStop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "lastStep",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void lastStep() throws Exception\n{\r\n    if (simulateFinishTimeMS != FINISH_TIME_NOT_INITIALIZED) {\r\n        LOG.warn(\"Method AMSimulator#lastStep was already called. \" + \"Skipping execution of method for application: {}\", appId);\r\n        return;\r\n    }\r\n    LOG.info(\"Application {} is shutting down.\", appId);\r\n    if (isTracked) {\r\n        untrackApp();\r\n    }\r\n    if (amContainer != null) {\r\n        LOG.info(\"AM container = {} reported to finish\", amContainer.getId());\r\n        se.getNmMap().get(amContainer.getNodeId()).cleanupContainer(amContainer.getId());\r\n    } else {\r\n        LOG.info(\"AM container is null\");\r\n    }\r\n    for (NodeId nodeId : ranNodes) {\r\n        se.getNmMap().get(nodeId).finishApplication(getApplicationId());\r\n    }\r\n    if (null == appAttemptId) {\r\n        return;\r\n    }\r\n    final FinishApplicationMasterRequest finishAMRequest = recordFactory.newRecordInstance(FinishApplicationMasterRequest.class);\r\n    finishAMRequest.setFinalApplicationStatus(FinalApplicationStatus.SUCCEEDED);\r\n    UserGroupInformation ugi = UserGroupInformation.createRemoteUser(appAttemptId.toString());\r\n    Token<AMRMTokenIdentifier> token = rm.getRMContext().getRMApps().get(appId).getRMAppAttempt(appAttemptId).getAMRMToken();\r\n    ugi.addTokenIdentifier(token.decodeIdentifier());\r\n    ugi.doAs(new PrivilegedExceptionAction<Object>() {\r\n\r\n        @Override\r\n        public Object run() throws Exception {\r\n            rm.getApplicationMasterService().finishApplicationMaster(finishAMRequest);\r\n            return null;\r\n        }\r\n    });\r\n    simulateFinishTimeMS = System.currentTimeMillis() - baselineTimeMS;\r\n    SchedulerMetrics schedulerMetrics = ((SchedulerWrapper) rm.getResourceScheduler()).getSchedulerMetrics();\r\n    if (schedulerMetrics != null) {\r\n        schedulerMetrics.addAMRuntime(appId, traceStartTimeMS, traceFinishTimeMS, simulateStartTimeMS, simulateFinishTimeMS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "createResourceRequest",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "ResourceRequest createResourceRequest(Resource resource, ExecutionType executionType, String host, int priority, long allocationId, int numContainers)\n{\r\n    ResourceRequest request = recordFactory.newRecordInstance(ResourceRequest.class);\r\n    request.setCapability(resource);\r\n    request.setResourceName(host);\r\n    request.setNumContainers(numContainers);\r\n    request.setExecutionTypeRequest(ExecutionTypeRequest.newInstance(executionType));\r\n    Priority prio = recordFactory.newRecordInstance(Priority.class);\r\n    prio.setPriority(priority);\r\n    request.setPriority(prio);\r\n    request.setAllocationRequestId(allocationId);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "createAllocateRequest",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AllocateRequest createAllocateRequest(List<ResourceRequest> ask, List<ContainerId> toRelease)\n{\r\n    AllocateRequest allocateRequest = recordFactory.newRecordInstance(AllocateRequest.class);\r\n    allocateRequest.setResponseId(responseId++);\r\n    allocateRequest.setAskList(ask);\r\n    allocateRequest.setReleaseList(toRelease);\r\n    return allocateRequest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "createAllocateRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AllocateRequest createAllocateRequest(List<ResourceRequest> ask)\n{\r\n    return createAllocateRequest(ask, new ArrayList<ContainerId>());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "processResponseQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void processResponseQueue() throws Exception",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "sendContainerRequest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void sendContainerRequest() throws Exception",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "initReservation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initReservation(ReservationId reservationId, long deadline, long now)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "checkStop",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkStop()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "submitApp",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void submitApp(ReservationId reservationId) throws YarnException, InterruptedException, IOException\n{\r\n    GetNewApplicationRequest newAppRequest = Records.newRecord(GetNewApplicationRequest.class);\r\n    GetNewApplicationResponse newAppResponse = rm.getClientRMService().getNewApplication(newAppRequest);\r\n    appId = newAppResponse.getApplicationId();\r\n    final SubmitApplicationRequest subAppRequest = Records.newRecord(SubmitApplicationRequest.class);\r\n    ApplicationSubmissionContext appSubContext = Records.newRecord(ApplicationSubmissionContext.class);\r\n    appSubContext.setApplicationId(appId);\r\n    appSubContext.setMaxAppAttempts(1);\r\n    appSubContext.setQueue(queue);\r\n    appSubContext.setPriority(Priority.newInstance(0));\r\n    ContainerLaunchContext conLauContext = Records.newRecord(ContainerLaunchContext.class);\r\n    conLauContext.setApplicationACLs(new HashMap<>());\r\n    conLauContext.setCommands(new ArrayList<>());\r\n    conLauContext.setEnvironment(new HashMap<>());\r\n    conLauContext.setLocalResources(new HashMap<>());\r\n    conLauContext.setServiceData(new HashMap<>());\r\n    appSubContext.setAMContainerSpec(conLauContext);\r\n    appSubContext.setResource(amContainerResource);\r\n    if (nodeLabelExpression != null) {\r\n        appSubContext.setNodeLabelExpression(nodeLabelExpression);\r\n    }\r\n    if (reservationId != null) {\r\n        appSubContext.setReservationID(reservationId);\r\n    }\r\n    subAppRequest.setApplicationSubmissionContext(appSubContext);\r\n    UserGroupInformation ugi = UserGroupInformation.createRemoteUser(user);\r\n    ugi.doAs(new PrivilegedExceptionAction<Object>() {\r\n\r\n        @Override\r\n        public Object run() throws YarnException, IOException {\r\n            rm.getClientRMService().submitApplication(subAppRequest);\r\n            return null;\r\n        }\r\n    });\r\n    LOG.info(\"Submit a new application {}\", appId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "registerAM",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void registerAM() throws YarnException, IOException, InterruptedException\n{\r\n    final RegisterApplicationMasterRequest amRegisterRequest = Records.newRecord(RegisterApplicationMasterRequest.class);\r\n    amRegisterRequest.setHost(\"localhost\");\r\n    amRegisterRequest.setRpcPort(1000);\r\n    amRegisterRequest.setTrackingUrl(\"localhost:1000\");\r\n    UserGroupInformation ugi = UserGroupInformation.createRemoteUser(appAttemptId.toString());\r\n    Token<AMRMTokenIdentifier> token = rm.getRMContext().getRMApps().get(appId).getRMAppAttempt(appAttemptId).getAMRMToken();\r\n    ugi.addTokenIdentifier(token.decodeIdentifier());\r\n    ugi.doAs(new PrivilegedExceptionAction<RegisterApplicationMasterResponse>() {\r\n\r\n        @Override\r\n        public RegisterApplicationMasterResponse run() throws Exception {\r\n            return rm.getApplicationMasterService().registerApplicationMaster(amRegisterRequest);\r\n        }\r\n    });\r\n    LOG.info(\"Register the application master for application {}\", appId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "trackApp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void trackApp()\n{\r\n    if (isTracked) {\r\n        SchedulerMetrics schedulerMetrics = ((SchedulerWrapper) rm.getResourceScheduler()).getSchedulerMetrics();\r\n        if (schedulerMetrics != null) {\r\n            schedulerMetrics.addTrackedApp(appId, oldAppId);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "untrackApp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void untrackApp()\n{\r\n    if (isTracked) {\r\n        SchedulerMetrics schedulerMetrics = ((SchedulerWrapper) rm.getResourceScheduler()).getSchedulerMetrics();\r\n        if (schedulerMetrics != null) {\r\n            schedulerMetrics.removeTrackedApp(oldAppId);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "packageRequests",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "List<ResourceRequest> packageRequests(List<ContainerSimulator> csList, int priority)\n{\r\n    Map<Long, Map<String, ResourceRequest>> rackLocalRequests = new HashMap<>();\r\n    Map<Long, Map<String, ResourceRequest>> nodeLocalRequests = new HashMap<>();\r\n    Map<Long, ResourceRequest> anyRequests = new HashMap<>();\r\n    for (ContainerSimulator cs : csList) {\r\n        long allocationId = cs.getAllocationId();\r\n        ResourceRequest anyRequest = anyRequests.get(allocationId);\r\n        if (cs.getHostname() != null) {\r\n            Map<String, ResourceRequest> rackLocalRequestMap;\r\n            if (rackLocalRequests.containsKey(allocationId)) {\r\n                rackLocalRequestMap = rackLocalRequests.get(allocationId);\r\n            } else {\r\n                rackLocalRequestMap = new HashMap<>();\r\n                rackLocalRequests.put(allocationId, rackLocalRequestMap);\r\n            }\r\n            String[] rackHostNames = SLSUtils.getRackHostName(cs.getHostname());\r\n            String rackname = \"/\" + rackHostNames[0];\r\n            if (rackLocalRequestMap.containsKey(rackname)) {\r\n                rackLocalRequestMap.get(rackname).setNumContainers(rackLocalRequestMap.get(rackname).getNumContainers() + 1);\r\n            } else {\r\n                ResourceRequest request = createResourceRequest(cs.getResource(), cs.getExecutionType(), rackname, priority, cs.getAllocationId(), 1);\r\n                rackLocalRequestMap.put(rackname, request);\r\n            }\r\n            Map<String, ResourceRequest> nodeLocalRequestMap;\r\n            if (nodeLocalRequests.containsKey(allocationId)) {\r\n                nodeLocalRequestMap = nodeLocalRequests.get(allocationId);\r\n            } else {\r\n                nodeLocalRequestMap = new HashMap<>();\r\n                nodeLocalRequests.put(allocationId, nodeLocalRequestMap);\r\n            }\r\n            String hostname = rackHostNames[1];\r\n            if (nodeLocalRequestMap.containsKey(hostname)) {\r\n                nodeLocalRequestMap.get(hostname).setNumContainers(nodeLocalRequestMap.get(hostname).getNumContainers() + 1);\r\n            } else {\r\n                ResourceRequest request = createResourceRequest(cs.getResource(), cs.getExecutionType(), hostname, priority, cs.getAllocationId(), 1);\r\n                nodeLocalRequestMap.put(hostname, request);\r\n            }\r\n        }\r\n        if (anyRequest == null) {\r\n            anyRequest = createResourceRequest(cs.getResource(), cs.getExecutionType(), ResourceRequest.ANY, priority, cs.getAllocationId(), 1);\r\n            anyRequests.put(allocationId, anyRequest);\r\n        } else {\r\n            anyRequest.setNumContainers(anyRequest.getNumContainers() + 1);\r\n        }\r\n    }\r\n    List<ResourceRequest> ask = new ArrayList<ResourceRequest>();\r\n    for (Map<String, ResourceRequest> nodeLocalRequestMap : nodeLocalRequests.values()) {\r\n        ask.addAll(nodeLocalRequestMap.values());\r\n    }\r\n    for (Map<String, ResourceRequest> rackLocalRequestMap : rackLocalRequests.values()) {\r\n        ask.addAll(rackLocalRequestMap.values());\r\n    }\r\n    ask.addAll(anyRequests.values());\r\n    return ask;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "getQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getQueue()\n{\r\n    return queue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "getAMType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAMType()\n{\r\n    return amtype;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "getDuration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDuration()\n{\r\n    return simulateFinishTimeMS - simulateStartTimeMS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "getNumTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumTasks()\n{\r\n    return totalContainers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "getApplicationId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ApplicationId getApplicationId()\n{\r\n    return appId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "getApplicationAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ApplicationAttemptId getApplicationAttemptId()\n{\r\n    return appAttemptId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "getRanNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<NodeId> getRanNodes()\n{\r\n    return this.ranNodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\utils",
  "methodName" : "getRackHostName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String[] getRackHostName(String hostname)\n{\r\n    NodeBase node = new NodeBase(hostname);\r\n    return new String[] { node.getNetworkLocation().substring(1), node.getName() };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\utils",
  "methodName" : "parseNodesFromRumenTrace",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "Set<NodeDetails> parseNodesFromRumenTrace(String jobTrace) throws IOException\n{\r\n    Set<NodeDetails> nodeSet = new HashSet<>();\r\n    File fin = new File(jobTrace);\r\n    Configuration conf = new Configuration();\r\n    conf.set(\"fs.defaultFS\", \"file:///\");\r\n    JobTraceReader reader = new JobTraceReader(new Path(fin.getAbsolutePath()), conf);\r\n    try {\r\n        LoggedJob job;\r\n        while ((job = reader.getNext()) != null) {\r\n            for (LoggedTask mapTask : job.getMapTasks()) {\r\n                if (mapTask.getAttempts().size() == 0) {\r\n                    continue;\r\n                }\r\n                LoggedTaskAttempt taskAttempt = mapTask.getAttempts().get(mapTask.getAttempts().size() - 1);\r\n                nodeSet.add(new NodeDetails(taskAttempt.getHostName().getValue()));\r\n            }\r\n            for (LoggedTask reduceTask : job.getReduceTasks()) {\r\n                if (reduceTask.getAttempts().size() == 0) {\r\n                    continue;\r\n                }\r\n                LoggedTaskAttempt taskAttempt = reduceTask.getAttempts().get(reduceTask.getAttempts().size() - 1);\r\n                nodeSet.add(new NodeDetails(taskAttempt.getHostName().getValue()));\r\n            }\r\n        }\r\n    } finally {\r\n        reader.close();\r\n    }\r\n    return nodeSet;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\utils",
  "methodName" : "parseNodesFromSLSTrace",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Set<NodeDetails> parseNodesFromSLSTrace(String jobTrace) throws IOException\n{\r\n    Set<NodeDetails> nodeSet = new HashSet<>();\r\n    JsonFactory jsonF = new JsonFactory();\r\n    ObjectMapper mapper = new ObjectMapper();\r\n    Reader input = new InputStreamReader(new FileInputStream(jobTrace), StandardCharsets.UTF_8);\r\n    try {\r\n        Iterator<Map> i = mapper.readValues(jsonF.createParser(input), Map.class);\r\n        while (i.hasNext()) {\r\n            addNodes(nodeSet, i.next());\r\n        }\r\n    } finally {\r\n        input.close();\r\n    }\r\n    return nodeSet;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\utils",
  "methodName" : "addNodes",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void addNodes(Set<NodeDetails> nodeSet, Map jsonEntry)\n{\r\n    if (jsonEntry.containsKey(SLSConfiguration.NUM_NODES)) {\r\n        int numNodes = Integer.parseInt(jsonEntry.get(SLSConfiguration.NUM_NODES).toString());\r\n        int numRacks = 1;\r\n        if (jsonEntry.containsKey(SLSConfiguration.NUM_RACKS)) {\r\n            numRacks = Integer.parseInt(jsonEntry.get(SLSConfiguration.NUM_RACKS).toString());\r\n        }\r\n        nodeSet.addAll(generateNodes(numNodes, numRacks));\r\n    }\r\n    if (jsonEntry.containsKey(SLSConfiguration.JOB_TASKS)) {\r\n        List tasks = (List) jsonEntry.get(SLSConfiguration.JOB_TASKS);\r\n        for (Object o : tasks) {\r\n            Map jsonTask = (Map) o;\r\n            String hostname = (String) jsonTask.get(SLSConfiguration.TASK_HOST);\r\n            if (hostname != null) {\r\n                nodeSet.add(new NodeDetails(hostname));\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\utils",
  "methodName" : "parseNodesFromNodeFile",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "Set<NodeDetails> parseNodesFromNodeFile(String nodeFile, Resource nmDefaultResource) throws IOException\n{\r\n    Set<NodeDetails> nodeSet = new HashSet<>();\r\n    JsonFactory jsonF = new JsonFactory();\r\n    ObjectMapper mapper = new ObjectMapper();\r\n    Reader input = new InputStreamReader(new FileInputStream(nodeFile), StandardCharsets.UTF_8);\r\n    try {\r\n        Iterator<Map> i = mapper.readValues(jsonF.createParser(input), Map.class);\r\n        while (i.hasNext()) {\r\n            Map jsonE = i.next();\r\n            String rack = \"/\" + jsonE.get(\"rack\");\r\n            List tasks = (List) jsonE.get(\"nodes\");\r\n            for (Object o : tasks) {\r\n                Map jsonNode = (Map) o;\r\n                NodeDetails nodeDetails = new NodeDetails(rack + \"/\" + jsonNode.get(\"node\"));\r\n                Resource nodeResource = Resources.clone(nmDefaultResource);\r\n                ResourceInformation[] infors = ResourceUtils.getResourceTypesArray();\r\n                for (ResourceInformation info : infors) {\r\n                    if (jsonNode.get(info.getName()) != null) {\r\n                        nodeResource.setResourceValue(info.getName(), Integer.parseInt(jsonNode.get(info.getName()).toString()));\r\n                    }\r\n                }\r\n                nodeDetails.setNodeResource(nodeResource);\r\n                if (jsonNode.get(\"labels\") != null) {\r\n                    Set<NodeLabel> nodeLabels = new HashSet<>(YarnClientUtils.buildNodeLabelsFromStr(jsonNode.get(\"labels\").toString()));\r\n                    nodeDetails.setLabels(nodeLabels);\r\n                }\r\n                nodeSet.add(nodeDetails);\r\n            }\r\n        }\r\n    } finally {\r\n        input.close();\r\n    }\r\n    return nodeSet;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\utils",
  "methodName" : "generateNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<NodeDetails> generateNodes(int numNodes, int numRacks)\n{\r\n    Set<NodeDetails> nodeSet = new HashSet<>();\r\n    if (numRacks < 1) {\r\n        numRacks = 1;\r\n    }\r\n    if (numRacks > numNodes) {\r\n        numRacks = numNodes;\r\n    }\r\n    for (int i = 0; i < numNodes; i++) {\r\n        nodeSet.add(new NodeDetails(\"/rack\" + i % numRacks + \"/node\" + i));\r\n    }\r\n    return nodeSet;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\utils",
  "methodName" : "generateNodeTableMapping",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void generateNodeTableMapping(Set<NodeDetails> nodeDetails, String filePath) throws IOException\n{\r\n    List<String> entries = new ArrayList<>();\r\n    for (NodeDetails nodeDetail : nodeDetails) {\r\n        if (nodeDetail.getHostname().contains(\"/\")) {\r\n            String hostname = nodeDetail.getHostname();\r\n            int lIndex = hostname.lastIndexOf(\"/\");\r\n            String node = hostname.substring(lIndex + 1);\r\n            String rack = hostname.substring(0, lIndex);\r\n            entries.add(node + \" \" + rack);\r\n        }\r\n    }\r\n    Files.write(Paths.get(filePath), entries, StandardCharsets.UTF_8, StandardOpenOption.CREATE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "createFromTaskContainerDefinition",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ContainerSimulator createFromTaskContainerDefinition(TaskContainerDefinition def)\n{\r\n    return new ContainerSimulator(def.getResource(), def.getDuration(), def.getHostname(), def.getPriority(), def.getType(), def.getExecutionType(), def.getAllocationId(), def.getRequestDelay());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getResource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Resource getResource()\n{\r\n    return resource;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ContainerId getId()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int compareTo(Delayed o)\n{\r\n    if (!(o instanceof ContainerSimulator)) {\r\n        throw new IllegalArgumentException(\"Parameter must be a ContainerSimulator instance\");\r\n    }\r\n    ContainerSimulator other = (ContainerSimulator) o;\r\n    return (int) Math.signum(endTime - other.endTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getDelay",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDelay(TimeUnit unit)\n{\r\n    return unit.convert(endTime - System.currentTimeMillis(), TimeUnit.MILLISECONDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getLifeTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLifeTime()\n{\r\n    return lifeTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getHostname",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHostname()\n{\r\n    return hostname;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getEndTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getEndTime()\n{\r\n    return endTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getPriority",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getPriority()\n{\r\n    return priority;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getType()\n{\r\n    return type;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "setPriority",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setPriority(int p)\n{\r\n    priority = p;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getExecutionType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ExecutionType getExecutionType()\n{\r\n    return executionType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getAllocationId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getAllocationId()\n{\r\n    return allocationId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "getRequestDelay",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getRequestDelay()\n{\r\n    return requestDelay;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "startRM",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void startRM() throws ClassNotFoundException, YarnException\n{\r\n    Configuration rmConf = new YarnConfiguration(conf);\r\n    String schedulerClass = rmConf.get(YarnConfiguration.RM_SCHEDULER);\r\n    if (Class.forName(schedulerClass) == CapacityScheduler.class) {\r\n        rmConf.set(YarnConfiguration.RM_SCHEDULER, SLSCapacityScheduler.class.getName());\r\n        rmConf.setBoolean(YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS, true);\r\n        rmConf.set(YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES, ProportionalCapacityPreemptionPolicy.class.getName());\r\n    } else if (Class.forName(schedulerClass) == FairScheduler.class) {\r\n        rmConf.set(YarnConfiguration.RM_SCHEDULER, SLSFairScheduler.class.getName());\r\n    } else if (Class.forName(schedulerClass) == FifoScheduler.class) {\r\n        throw new YarnException(\"Fifo Scheduler is not supported yet.\");\r\n    }\r\n    rmConf.setClass(CommonConfigurationKeysPublic.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, TableMapping.class, DNSToSwitchMapping.class);\r\n    rmConf.set(CommonConfigurationKeysPublic.NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, tableMapping);\r\n    rmConf.set(SLSConfiguration.METRICS_OUTPUT_DIR, metricsOutputDir);\r\n    rm = new ResourceManager() {\r\n\r\n        @Override\r\n        protected ApplicationMasterLauncher createAMLauncher() {\r\n            return new MockAMLauncher(slsRunner, this.rmContext);\r\n        }\r\n    };\r\n    JvmMetrics jvmMetrics = JvmMetrics.initSingleton(\"ResourceManager\", null);\r\n    jvmMetrics.registerIfNeeded();\r\n    rm.init(rmConf);\r\n    rm.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "increaseQueueAppNum",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void increaseQueueAppNum(String queue) throws YarnException\n{\r\n    SchedulerWrapper wrapper = (SchedulerWrapper) rm.getResourceScheduler();\r\n    String queueName = wrapper.getRealQueueName(queue);\r\n    Integer appNum = queueAppNumMap.get(queueName);\r\n    if (appNum == null) {\r\n        appNum = 1;\r\n    } else {\r\n        appNum = appNum + 1;\r\n    }\r\n    queueAppNumMap.put(queueName, appNum);\r\n    SchedulerMetrics metrics = wrapper.getSchedulerMetrics();\r\n    if (metrics != null) {\r\n        metrics.trackQueue(queueName);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setMetricsOutputDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMetricsOutputDir(String metricsOutputDir)\n{\r\n    this.metricsOutputDir = metricsOutputDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getTableMapping",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTableMapping()\n{\r\n    return tableMapping;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setTableMapping",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTableMapping(String tableMapping)\n{\r\n    this.tableMapping = tableMapping;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void stop()\n{\r\n    rm.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getRm",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ResourceManager getRm()\n{\r\n    return rm;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getQueueAppNumMap",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, Integer> getQueueAppNumMap()\n{\r\n    return queueAppNumMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getRemainingApps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRemainingApps()\n{\r\n    return amRunner.remainingApps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    if (null != conf) {\r\n        conf.addResource(\"sls-runner.xml\");\r\n    }\r\n    super.setConf(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void init(Configuration tempConf) throws ClassNotFoundException, YarnException\n{\r\n    setConf(tempConf);\r\n    int poolSize = tempConf.getInt(SLSConfiguration.RUNNER_POOL_SIZE, SLSConfiguration.RUNNER_POOL_SIZE_DEFAULT);\r\n    SLSRunner.runner.setQueueSize(poolSize);\r\n    rmRunner = new RMRunner(getConf(), this);\r\n    nmRunner = new NMRunner(runner, getConf(), rmRunner.getRm(), rmRunner.getTableMapping(), poolSize);\r\n    amRunner = new AMRunner(runner, this);\r\n    amRunner.init(tempConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getSynthJobTraceProducer",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "SynthTraceJobProducer getSynthJobTraceProducer() throws YarnException\n{\r\n    if (nmRunner.getStjp() != null) {\r\n        return nmRunner.getStjp();\r\n    } else {\r\n        try {\r\n            return new SynthTraceJobProducer(getConf(), new Path(inputTraces[0]));\r\n        } catch (IOException e) {\r\n            throw new YarnException(\"Failed to initialize SynthTraceJobProducer\", e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getSimulateInfoMap",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, Object> getSimulateInfoMap()\n{\r\n    return Collections.unmodifiableMap(simulateInfoMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setSimulationParams",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void setSimulationParams(TraceType inputType, String[] inTraces, String nodes, String metricsOutputDir, Set<String> trackApps, boolean printSimulation) throws YarnException\n{\r\n    this.inputType = inputType;\r\n    this.inputTraces = inTraces.clone();\r\n    this.amRunner.setInputType(inputType);\r\n    this.amRunner.setInputTraces(this.inputTraces);\r\n    this.amRunner.setTrackedApps(trackApps);\r\n    this.nmRunner.setNodeFile(nodes);\r\n    this.nmRunner.setInputType(inputType);\r\n    this.nmRunner.setInputTraces(this.inputTraces);\r\n    this.printSimulation = printSimulation;\r\n    this.rmRunner.setMetricsOutputDir(metricsOutputDir);\r\n    String tableMapping = metricsOutputDir + \"/tableMapping.csv\";\r\n    this.rmRunner.setTableMapping(tableMapping);\r\n    this.nmRunner.setTableMapping(tableMapping);\r\n    if (inputType == TraceType.SYNTH) {\r\n        this.stjp = getSynthJobTraceProducer();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "start",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void start() throws IOException, ClassNotFoundException, YarnException, InterruptedException\n{\r\n    enableDNSCaching(getConf());\r\n    rmRunner.startRM();\r\n    nmRunner.setRm(rmRunner.getRm());\r\n    amRunner.setResourceManager(rmRunner.getRm());\r\n    nmRunner.startNM();\r\n    amRunner.startAM();\r\n    SchedulerWrapper resourceScheduler = (SchedulerWrapper) rmRunner.getRm().getResourceScheduler();\r\n    resourceScheduler.setSLSRunner(this);\r\n    Tracker tracker = resourceScheduler.getTracker();\r\n    tracker.setQueueSet(rmRunner.getQueueAppNumMap().keySet());\r\n    tracker.setTrackedAppSet(amRunner.getTrackedApps());\r\n    printSimulationInfo();\r\n    nmRunner.waitForNodesRunning();\r\n    runner.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "enableDNSCaching",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void enableDNSCaching(Configuration conf)\n{\r\n    if (conf.getBoolean(SLSConfiguration.DNS_CACHING_ENABLED, SLSConfiguration.DNS_CACHING_ENABLED_DEFAULT)) {\r\n        Security.setProperty(NETWORK_CACHE_TTL, \"-1\");\r\n        Security.setProperty(NETWORK_NEGATIVE_CACHE_TTL, \"-1\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getDefaultContainerResource",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Resource getDefaultContainerResource()\n{\r\n    int containerMemory = getConf().getInt(SLSConfiguration.CONTAINER_MEMORY_MB, SLSConfiguration.CONTAINER_MEMORY_MB_DEFAULT);\r\n    int containerVCores = getConf().getInt(SLSConfiguration.CONTAINER_VCORES, SLSConfiguration.CONTAINER_VCORES_DEFAULT);\r\n    return Resources.createResource(containerMemory, containerVCores);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "increaseQueueAppNum",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void increaseQueueAppNum(String queue) throws YarnException\n{\r\n    rmRunner.increaseQueueAppNum(queue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "printSimulationInfo",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void printSimulationInfo()\n{\r\n    final int numAMs = amRunner.getNumAMs();\r\n    final int numTasks = amRunner.getNumTasks();\r\n    final long maxRuntime = amRunner.getMaxRuntime();\r\n    Map<String, AMSimulator> amMap = amRunner.getAmMap();\r\n    Map<String, Integer> queueAppNumMap = rmRunner.getQueueAppNumMap();\r\n    if (printSimulation) {\r\n        LOG.info(\"------------------------------------\");\r\n        LOG.info(\"# nodes = {}, # racks = {}, capacity \" + \"of each node {}.\", nmRunner.getNumNMs(), nmRunner.getNumRacks(), nmRunner.getNodeManagerResource());\r\n        LOG.info(\"------------------------------------\");\r\n        LOG.info(\"# applications = {}, # total \" + \"tasks = {}, average # tasks per application = {}\", numAMs, numTasks, (int) (Math.ceil((numTasks + 0.0) / numAMs)));\r\n        LOG.info(\"JobId\\tQueue\\tAMType\\tDuration\\t#Tasks\");\r\n        for (Map.Entry<String, AMSimulator> entry : amMap.entrySet()) {\r\n            AMSimulator am = entry.getValue();\r\n            LOG.info(entry.getKey() + \"\\t\" + am.getQueue() + \"\\t\" + am.getAMType() + \"\\t\" + am.getDuration() + \"\\t\" + am.getNumTasks());\r\n        }\r\n        LOG.info(\"------------------------------------\");\r\n        LOG.info(\"number of queues = {}  average number of apps = {}\", queueAppNumMap.size(), (int) (Math.ceil((numAMs + 0.0) / queueAppNumMap.size())));\r\n        LOG.info(\"------------------------------------\");\r\n        LOG.info(\"estimated simulation time is {} seconds\", (long) (Math.ceil(maxRuntime / 1000.0)));\r\n        LOG.info(\"------------------------------------\");\r\n    }\r\n    simulateInfoMap.put(\"Number of racks\", nmRunner.getNumRacks());\r\n    simulateInfoMap.put(\"Number of nodes\", nmRunner.getNumNMs());\r\n    simulateInfoMap.put(\"Node memory (MB)\", nmRunner.getNodeManagerResource().getResourceValue(ResourceInformation.MEMORY_URI));\r\n    simulateInfoMap.put(\"Node VCores\", nmRunner.getNodeManagerResource().getResourceValue(ResourceInformation.VCORES_URI));\r\n    simulateInfoMap.put(\"Number of applications\", numAMs);\r\n    simulateInfoMap.put(\"Number of tasks\", numTasks);\r\n    simulateInfoMap.put(\"Average tasks per applicaion\", (int) (Math.ceil((numTasks + 0.0) / numAMs)));\r\n    simulateInfoMap.put(\"Number of queues\", queueAppNumMap.size());\r\n    simulateInfoMap.put(\"Average applications per queue\", (int) (Math.ceil((numAMs + 0.0) / queueAppNumMap.size())));\r\n    simulateInfoMap.put(\"Estimated simulate time (s)\", (long) (Math.ceil(maxRuntime / 1000.0)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getNmMap",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<NodeId, NMSimulator> getNmMap()\n{\r\n    return nmRunner.getNmMap();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "decreaseRemainingApps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void decreaseRemainingApps()\n{\r\n    amRunner.remainingApps--;\r\n    if (amRunner.remainingApps == 0) {\r\n        exitSLSRunner();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "exitSLSRunner",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void exitSLSRunner()\n{\r\n    LOG.info(\"SLSRunner tears down.\");\r\n    if (exitAtTheFinish) {\r\n        System.exit(0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stop() throws InterruptedException\n{\r\n    rmRunner.stop();\r\n    runner.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "int run(final String[] argv) throws IOException, InterruptedException, ParseException, ClassNotFoundException, YarnException\n{\r\n    Options options = new Options();\r\n    options.addOption(\"inputrumen\", true, \"input rumen files\");\r\n    options.addOption(\"inputsls\", true, \"input sls files\");\r\n    options.addOption(\"tracetype\", true, \"the type of trace\");\r\n    options.addOption(\"tracelocation\", true, \"input trace files\");\r\n    options.addOption(\"nodes\", true, \"input topology\");\r\n    options.addOption(\"output\", true, \"output directory\");\r\n    options.addOption(\"trackjobs\", true, \"jobs to be tracked during simulating\");\r\n    options.addOption(\"printsimulation\", false, \"print out simulation information\");\r\n    CommandLineParser parser = new GnuParser();\r\n    CommandLine cmd = parser.parse(options, argv);\r\n    boolean hasInputRumenOption = cmd.hasOption(\"inputrumen\");\r\n    boolean hasInputSlsOption = cmd.hasOption(\"inputsls\");\r\n    boolean hasTraceTypeOption = cmd.hasOption(\"tracetype\");\r\n    TraceType traceType = determineTraceType(cmd, hasInputRumenOption, hasInputSlsOption, hasTraceTypeOption);\r\n    String traceLocation = determineTraceLocation(cmd, hasInputRumenOption, hasInputSlsOption, hasTraceTypeOption);\r\n    String output = cmd.getOptionValue(\"output\");\r\n    File outputFile = new File(output);\r\n    if (!outputFile.exists() && !outputFile.mkdirs()) {\r\n        System.err.println(\"ERROR: Cannot create output directory \" + outputFile.getAbsolutePath());\r\n        throw new YarnException(\"Cannot create output directory\");\r\n    }\r\n    Set<String> trackedJobSet = new HashSet<>();\r\n    if (cmd.hasOption(\"trackjobs\")) {\r\n        String trackjobs = cmd.getOptionValue(\"trackjobs\");\r\n        String[] jobIds = trackjobs.split(\",\");\r\n        trackedJobSet.addAll(Arrays.asList(jobIds));\r\n    }\r\n    String tempNodeFile = cmd.hasOption(\"nodes\") ? cmd.getOptionValue(\"nodes\") : \"\";\r\n    String[] inputFiles = traceLocation.split(\",\");\r\n    setSimulationParams(traceType, inputFiles, tempNodeFile, output, trackedJobSet, cmd.hasOption(\"printsimulation\"));\r\n    start();\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "determineTraceType",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TraceType determineTraceType(CommandLine cmd, boolean hasInputRumenOption, boolean hasInputSlsOption, boolean hasTraceTypeOption) throws YarnException\n{\r\n    String traceType = null;\r\n    if (hasInputRumenOption) {\r\n        traceType = \"RUMEN\";\r\n    }\r\n    if (hasInputSlsOption) {\r\n        traceType = \"SLS\";\r\n    }\r\n    if (hasTraceTypeOption) {\r\n        traceType = cmd.getOptionValue(\"tracetype\");\r\n    }\r\n    if (traceType == null) {\r\n        throw new YarnException(\"Misconfigured input\");\r\n    }\r\n    switch(traceType) {\r\n        case \"SLS\":\r\n            return TraceType.SLS;\r\n        case \"RUMEN\":\r\n            return TraceType.RUMEN;\r\n        case \"SYNTH\":\r\n            return TraceType.SYNTH;\r\n        default:\r\n            printUsage();\r\n            throw new YarnException(\"Misconfigured input\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "determineTraceLocation",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String determineTraceLocation(CommandLine cmd, boolean hasInputRumenOption, boolean hasInputSlsOption, boolean hasTraceTypeOption) throws YarnException\n{\r\n    if (hasInputRumenOption) {\r\n        return cmd.getOptionValue(\"inputrumen\");\r\n    }\r\n    if (hasInputSlsOption) {\r\n        return cmd.getOptionValue(\"inputsls\");\r\n    }\r\n    if (hasTraceTypeOption) {\r\n        return cmd.getOptionValue(\"tracelocation\");\r\n    }\r\n    throw new YarnException(\"Misconfigured input! \");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    exitAtTheFinish = true;\r\n    ToolRunner.run(new Configuration(), new SLSRunner(), argv);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void printUsage()\n{\r\n    System.err.println();\r\n    System.err.println(\"ERROR: Wrong tracetype\");\r\n    System.err.println();\r\n    System.err.println(\"Options: -tracetype \" + \"SLS|RUMEN|SYNTH -tracelocation FILE,FILE... \" + \"(deprecated alternative options --inputsls FILE, FILE,... \" + \" | --inputrumen FILE,FILE,...)\" + \"-output FILE [-nodes FILE] [-trackjobs JobId,JobId...] \" + \"[-printsimulation]\");\r\n    System.err.println();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getStjp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SynthTraceJobProducer getStjp()\n{\r\n    return stjp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setStjp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStjp(SynthTraceJobProducer stjp)\n{\r\n    this.stjp = stjp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "getAMSimulatorByAppId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AMSimulator getAMSimulatorByAppId(ApplicationId appId)\n{\r\n    return amRunner.getAMSimulator(appId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\main\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "createMRReservation",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "ReservationSubmissionRequest createMRReservation(ReservationId reservationId, String name, Resource maxMapRes, int numberMaps, long maxMapDur, Resource maxRedRes, int numberReduces, long maxRedDur, long arrival, long deadline, String queueName)\n{\r\n    ReservationRequest mapRR = ReservationRequest.newInstance(maxMapRes, numberMaps, numberMaps, maxMapDur);\r\n    ReservationRequest redRR = ReservationRequest.newInstance(maxRedRes, numberReduces, numberReduces, maxRedDur);\r\n    List<ReservationRequest> listResReq = new ArrayList<ReservationRequest>();\r\n    listResReq.add(mapRR);\r\n    listResReq.add(redRR);\r\n    ReservationRequests reservationRequests = ReservationRequests.newInstance(listResReq, ReservationRequestInterpreter.R_ORDER_NO_GAP);\r\n    ReservationDefinition resDef = ReservationDefinition.newInstance(arrival, deadline, reservationRequests, name);\r\n    return ReservationSubmissionRequest.newInstance(resDef, queueName, reservationId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
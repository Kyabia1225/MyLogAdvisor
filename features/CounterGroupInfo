[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getValue()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    commonPreHead(html);\r\n    String tid = $(TASK_ID);\r\n    String activeNav = \"3\";\r\n    if (tid == null || tid.isEmpty()) {\r\n        activeNav = \"2\";\r\n    }\r\n    set(initID(ACCORDION, \"nav\"), \"{autoHeight:false, active:\" + activeNav + \"}\");\r\n    set(DATATABLES_SELECTOR, \"#counters .dt-counters\");\r\n    set(initSelector(DATATABLES), \"{bJQueryUI:true, sDom:'t', iDisplayLength:-1}\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "postHead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void postHead(Page.HTML<__> html)\n{\r\n    html.style(\"#counters, .dt-counters { table-layout: fixed }\", \"#counters th { overflow: hidden; vertical-align: middle }\", \"#counters .dataTables_wrapper { min-height: 1em }\", \"#counters .group { width: 15em }\", \"#counters .name { width: 30em }\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return CountersBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getFinalState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobStatus.State getFinalState()\n{\r\n    return finalState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "createForecast",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SimpleExponentialSmoothing createForecast(final long timeConstant, final int skipCnt, final long stagnatedWindow, final long timeStamp)\n{\r\n    return new SimpleExponentialSmoothing(timeConstant, skipCnt, stagnatedWindow, timeStamp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "isDataStagnated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isDataStagnated(final long timeStamp)\n{\r\n    ForecastRecord rec = forecastRefEntry.get();\r\n    if (rec != null && rec.myIndex > kMinimumReads) {\r\n        return (rec.timeStamp + kStagnatedWindow) > timeStamp;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "processRawData",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double processRawData(final double oldRawData, final long oldTime, final double newRawData, final long newTime)\n{\r\n    double rate = (newRawData - oldRawData) / (newTime - oldTime);\r\n    return rate;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "incorporateReading",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void incorporateReading(final long timeStamp, final double currRawData)\n{\r\n    ForecastRecord oldRec = forecastRefEntry.get();\r\n    if (oldRec == null) {\r\n        double oldForecast = processRawData(0, startTime, currRawData, timeStamp);\r\n        forecastRefEntry.compareAndSet(null, new ForecastRecord(oldForecast, 0.0, startTime));\r\n        incorporateReading(timeStamp, currRawData);\r\n        return;\r\n    }\r\n    while (!forecastRefEntry.compareAndSet(oldRec, oldRec.append(timeStamp, currRawData))) {\r\n        oldRec = forecastRefEntry.get();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "getForecast",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double getForecast()\n{\r\n    ForecastRecord rec = forecastRefEntry.get();\r\n    if (rec != null && rec.myIndex > kMinimumReads) {\r\n        return rec.forecast;\r\n    }\r\n    return DEFAULT_FORECAST;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "isDefaultForecast",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isDefaultForecast(final double value)\n{\r\n    return value == DEFAULT_FORECAST;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "getSSE",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double getSSE()\n{\r\n    ForecastRecord rec = forecastRefEntry.get();\r\n    if (rec != null) {\r\n        return rec.sseError;\r\n    }\r\n    return DEFAULT_FORECAST;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "isErrorWithinBound",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isErrorWithinBound(final double bound)\n{\r\n    double squaredErr = getSSE();\r\n    if (squaredErr < 0) {\r\n        return false;\r\n    }\r\n    return bound > squaredErr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "getRawData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double getRawData()\n{\r\n    ForecastRecord rec = forecastRefEntry.get();\r\n    if (rec != null) {\r\n        return rec.rawData;\r\n    }\r\n    return DEFAULT_FORECAST;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "getTimeStamp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTimeStamp()\n{\r\n    ForecastRecord rec = forecastRefEntry.get();\r\n    if (rec != null) {\r\n        return rec.timeStamp;\r\n    }\r\n    return 0L;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "getForecastRefEntry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AtomicReference<ForecastRecord> getForecastRefEntry()\n{\r\n    return forecastRefEntry;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    String res = \"NULL\";\r\n    ForecastRecord rec = forecastRefEntry.get();\r\n    if (rec != null) {\r\n        res = \"rec.index = \" + rec.myIndex + \", forecast t: \" + rec.timeStamp + \", forecast: \" + rec.forecast + \", sample: \" + rec.sample + \", raw: \" + rec.rawData + \", error: \" + rec.sseError + \", alpha: \" + rec.alpha;\r\n    }\r\n    return res;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void add(JobInfo jobInfo)\n{\r\n    job.add(jobInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getJobs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ArrayList<JobInfo> getJobs()\n{\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getTaskAttemptID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptId getTaskAttemptID()\n{\r\n    return attemptID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : [ "FSError", "Exception", "Throwable", "Exception" ],
  "containingMethodsNum" : 55,
  "sourceCodeText" : "void main(String[] args) throws Throwable\n{\r\n    Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\r\n    LOG.debug(\"Child starting\");\r\n    final JobConf job = new JobConf(MRJobConfig.JOB_CONF_FILE);\r\n    Limits.init(job);\r\n    UserGroupInformation.setConfiguration(job);\r\n    SecurityUtil.setConfiguration(job);\r\n    String host = args[0];\r\n    int port = Integer.parseInt(args[1]);\r\n    final InetSocketAddress address = NetUtils.createSocketAddrForHost(host, port);\r\n    final TaskAttemptID firstTaskid = TaskAttemptID.forName(args[2]);\r\n    long jvmIdLong = Long.parseLong(args[3]);\r\n    JVMId jvmId = new JVMId(firstTaskid.getJobID(), firstTaskid.getTaskType() == TaskType.MAP, jvmIdLong);\r\n    CallerContext.setCurrent(new CallerContext.Builder(\"mr_\" + firstTaskid.toString()).build());\r\n    DefaultMetricsSystem.initialize(StringUtils.camelize(firstTaskid.getTaskType().name()) + \"Task\");\r\n    Credentials credentials = UserGroupInformation.getCurrentUser().getCredentials();\r\n    LOG.info(\"Executing with tokens: {}\", credentials.getAllTokens());\r\n    UserGroupInformation taskOwner = UserGroupInformation.createRemoteUser(firstTaskid.getJobID().toString());\r\n    Token<JobTokenIdentifier> jt = TokenCache.getJobToken(credentials);\r\n    SecurityUtil.setTokenService(jt, address);\r\n    taskOwner.addToken(jt);\r\n    final TaskUmbilicalProtocol umbilical = taskOwner.doAs(new PrivilegedExceptionAction<TaskUmbilicalProtocol>() {\r\n\r\n        @Override\r\n        public TaskUmbilicalProtocol run() throws Exception {\r\n            return (TaskUmbilicalProtocol) RPC.getProxy(TaskUmbilicalProtocol.class, TaskUmbilicalProtocol.versionID, address, job);\r\n        }\r\n    });\r\n    JvmContext context = new JvmContext(jvmId, \"-1000\");\r\n    LOG.debug(\"PID: \" + System.getenv().get(\"JVM_PID\"));\r\n    Task task = null;\r\n    UserGroupInformation childUGI = null;\r\n    ScheduledExecutorService logSyncer = null;\r\n    try {\r\n        int idleLoopCount = 0;\r\n        JvmTask myTask = null;\r\n        for (int idle = 0; null == myTask; ++idle) {\r\n            long sleepTimeMilliSecs = Math.min(idle * 500, 1500);\r\n            LOG.info(\"Sleeping for \" + sleepTimeMilliSecs + \"ms before retrying again. Got null now.\");\r\n            MILLISECONDS.sleep(sleepTimeMilliSecs);\r\n            myTask = umbilical.getTask(context);\r\n        }\r\n        if (myTask.shouldDie()) {\r\n            return;\r\n        }\r\n        task = myTask.getTask();\r\n        YarnChild.taskid = task.getTaskID();\r\n        configureTask(job, task, credentials, jt);\r\n        String systemPropsToLog = MRApps.getSystemPropertiesToLog(job);\r\n        if (systemPropsToLog != null) {\r\n            LOG.info(systemPropsToLog);\r\n        }\r\n        JvmMetrics.initSingleton(jvmId.toString(), job.getSessionId());\r\n        childUGI = UserGroupInformation.createRemoteUser(System.getenv(ApplicationConstants.Environment.USER.toString()));\r\n        childUGI.addCredentials(credentials);\r\n        MRApps.setJobClassLoader(job);\r\n        logSyncer = TaskLog.createLogSyncer();\r\n        final Task taskFinal = task;\r\n        childUGI.doAs(new PrivilegedExceptionAction<Object>() {\r\n\r\n            @Override\r\n            public Object run() throws Exception {\r\n                setEncryptedSpillKeyIfRequired(taskFinal);\r\n                FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());\r\n                taskFinal.run(job, umbilical);\r\n                return null;\r\n            }\r\n        });\r\n    } catch (FSError e) {\r\n        LOG.error(\"FSError from child\", e);\r\n        if (!ShutdownHookManager.get().isShutdownInProgress()) {\r\n            umbilical.fsError(taskid, e.getMessage());\r\n        }\r\n    } catch (Exception exception) {\r\n        LOG.warn(\"Exception running child : \" + StringUtils.stringifyException(exception));\r\n        try {\r\n            if (task != null) {\r\n                if (childUGI == null) {\r\n                    task.taskCleanup(umbilical);\r\n                } else {\r\n                    final Task taskFinal = task;\r\n                    childUGI.doAs(new PrivilegedExceptionAction<Object>() {\r\n\r\n                        @Override\r\n                        public Object run() throws Exception {\r\n                            taskFinal.taskCleanup(umbilical);\r\n                            return null;\r\n                        }\r\n                    });\r\n                }\r\n            }\r\n        } catch (Exception e) {\r\n            LOG.info(\"Exception cleaning up: \" + StringUtils.stringifyException(e));\r\n        }\r\n        if (taskid != null) {\r\n            if (!ShutdownHookManager.get().isShutdownInProgress()) {\r\n                reportError(exception, task, umbilical);\r\n            }\r\n        }\r\n    } catch (Throwable throwable) {\r\n        LOG.error(\"Error running child : \" + StringUtils.stringifyException(throwable));\r\n        if (taskid != null) {\r\n            if (!ShutdownHookManager.get().isShutdownInProgress()) {\r\n                Throwable tCause = throwable.getCause();\r\n                String cause = tCause == null ? throwable.getMessage() : StringUtils.stringifyException(tCause);\r\n                umbilical.fatalError(taskid, cause, false);\r\n            }\r\n        }\r\n    } finally {\r\n        RPC.stopProxy(umbilical);\r\n        DefaultMetricsSystem.shutdown();\r\n        TaskLog.syncLogsShutdown(logSyncer);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "reportError",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void reportError(Exception exception, Task task, TaskUmbilicalProtocol umbilical) throws IOException\n{\r\n    boolean fastFailJob = false;\r\n    boolean hasClusterStorageCapacityExceededException = ExceptionUtils.indexOfType(exception, ClusterStorageCapacityExceededException.class) != -1;\r\n    if (hasClusterStorageCapacityExceededException) {\r\n        boolean killJobWhenExceedClusterStorageCapacity = task.getConf().getBoolean(MRJobConfig.JOB_DFS_STORAGE_CAPACITY_KILL_LIMIT_EXCEED, MRJobConfig.DEFAULT_JOB_DFS_STORAGE_CAPACITY_KILL_LIMIT_EXCEED);\r\n        if (killJobWhenExceedClusterStorageCapacity) {\r\n            LOG.error(\"Fast fail the job because the cluster storage capacity was exceeded.\");\r\n            fastFailJob = true;\r\n        }\r\n    }\r\n    umbilical.fatalError(taskid, StringUtils.stringifyException(exception), fastFailJob);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setEncryptedSpillKeyIfRequired",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setEncryptedSpillKeyIfRequired(Task task) throws Exception\n{\r\n    if ((task != null) && (task.getEncryptedSpillKey() != null) && (task.getEncryptedSpillKey().length > 1)) {\r\n        Credentials creds = UserGroupInformation.getCurrentUser().getCredentials();\r\n        TokenCache.setEncryptedSpillKey(task.getEncryptedSpillKey(), creds);\r\n        UserGroupInformation.getCurrentUser().addCredentials(creds);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configureLocalDirs",
  "errType" : [ "DiskErrorException", "FileAlreadyExistsException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void configureLocalDirs(Task task, JobConf job) throws IOException\n{\r\n    String[] localSysDirs = StringUtils.getTrimmedStrings(System.getenv(Environment.LOCAL_DIRS.name()));\r\n    job.setStrings(MRConfig.LOCAL_DIR, localSysDirs);\r\n    LOG.info(MRConfig.LOCAL_DIR + \" for child: \" + job.get(MRConfig.LOCAL_DIR));\r\n    LocalDirAllocator lDirAlloc = new LocalDirAllocator(MRConfig.LOCAL_DIR);\r\n    Path workDir = null;\r\n    try {\r\n        workDir = lDirAlloc.getLocalPathToRead(\"work\", job);\r\n    } catch (DiskErrorException e) {\r\n    }\r\n    if (workDir == null) {\r\n        workDir = lDirAlloc.getLocalPathForWrite(\"work\", job);\r\n        FileSystem lfs = FileSystem.getLocal(job).getRaw();\r\n        boolean madeDir = false;\r\n        try {\r\n            madeDir = lfs.mkdirs(workDir);\r\n        } catch (FileAlreadyExistsException e) {\r\n            madeDir = true;\r\n            workDir = lDirAlloc.getLocalPathToRead(\"work\", job);\r\n        }\r\n        if (!madeDir) {\r\n            throw new IOException(\"Mkdirs failed to create \" + workDir.toString());\r\n        }\r\n    }\r\n    job.set(MRJobConfig.JOB_LOCAL_DIR, workDir.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configureTask",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void configureTask(JobConf job, Task task, Credentials credentials, Token<JobTokenIdentifier> jt) throws IOException\n{\r\n    job.setCredentials(credentials);\r\n    ApplicationAttemptId appAttemptId = ContainerId.fromString(System.getenv(Environment.CONTAINER_ID.name())).getApplicationAttemptId();\r\n    LOG.debug(\"APPLICATION_ATTEMPT_ID: \" + appAttemptId);\r\n    job.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptId.getAttemptId());\r\n    job.setBoolean(\"ipc.client.tcpnodelay\", true);\r\n    job.setClass(MRConfig.TASK_LOCAL_OUTPUT_CLASS, YarnOutputFiles.class, MapOutputFile.class);\r\n    task.setJobTokenSecret(JobTokenSecretManager.createSecretKey(jt.getPassword()));\r\n    byte[] shuffleSecret = TokenCache.getShuffleSecretKey(credentials);\r\n    if (shuffleSecret == null) {\r\n        LOG.warn(\"Shuffle secret missing from task credentials.\" + \" Using job token secret as shuffle secret.\");\r\n        shuffleSecret = jt.getPassword();\r\n    }\r\n    task.setShuffleSecret(JobTokenSecretManager.createSecretKey(shuffleSecret));\r\n    configureLocalDirs(task, job);\r\n    task.localizeConfiguration(job);\r\n    MRApps.setupDistributedCacheLocal(job);\r\n    Path localTaskFile = new Path(MRJobConfig.JOB_CONF_FILE);\r\n    writeLocalJobFile(localTaskFile, job);\r\n    task.setJobFile(localTaskFile.toString());\r\n    task.setConf(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeLocalJobFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void writeLocalJobFile(Path jobFile, JobConf conf) throws IOException\n{\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    localFs.delete(jobFile);\r\n    OutputStream out = null;\r\n    try {\r\n        out = FileSystem.create(localFs, jobFile, urw_gr);\r\n        conf.writeXml(out);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "setJob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJob(Job job)\n{\r\n    this.job = job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getJob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Job getJob()\n{\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "setTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTask(Task task)\n{\r\n    this.task = task;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Task getTask()\n{\r\n    return task;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "enrollAttempt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void enrollAttempt(TaskAttemptStatus status, long timestamp)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "attemptEnrolledTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long attemptEnrolledTime(TaskAttemptId attemptID)\n{\r\n    return Long.MAX_VALUE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "updateAttempt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void updateAttempt(TaskAttemptStatus status, long timestamp)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "contextualize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void contextualize(Configuration conf, AppContext context)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "thresholdRuntime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long thresholdRuntime(TaskId id)\n{\r\n    return Long.MAX_VALUE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "estimatedRuntime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long estimatedRuntime(TaskAttemptId id)\n{\r\n    return -1L;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "estimatedNewAttemptRuntime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long estimatedNewAttemptRuntime(TaskId id)\n{\r\n    return -1L;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "runtimeEstimateVariance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long runtimeEstimateVariance(TaskAttemptId id)\n{\r\n    return -1L;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void init(AppContext context)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "preempt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void preempt(Context ctxt, PreemptionMessage preemptionRequests)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "handleFailedContainer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void handleFailedContainer(TaskAttemptId attemptID)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "isPreempted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isPreempted(TaskAttemptId yarnAttemptID)\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "reportSuccessfulPreemption",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reportSuccessfulPreemption(TaskAttemptId taskAttemptID)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "getCheckpointID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskCheckpointID getCheckpointID(TaskId taskId)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "setCheckpointID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCheckpointID(TaskId taskId, TaskCheckpointID cid)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "handleCompletedContainer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void handleCompletedContainer(TaskAttemptId attemptID)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return this.name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getValue()\n{\r\n    return this.value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] getSource()\n{\r\n    return source;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "render",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void render(Block html)\n{\r\n    String jid = $(JOB_ID);\r\n    if (jid.isEmpty()) {\r\n        html.p().__(\"Sorry, can't do anything without a JobID.\").__();\r\n        return;\r\n    }\r\n    JobId jobID = MRApps.toJobID(jid);\r\n    Job job = appContext.getJob(jobID);\r\n    if (job == null) {\r\n        html.p().__(\"Sorry, \", jid, \" not found.\").__();\r\n        return;\r\n    }\r\n    List<AMInfo> amInfos = job.getAMInfos();\r\n    String amString = amInfos.size() == 1 ? \"ApplicationMaster\" : \"ApplicationMasters\";\r\n    JobInfo jinfo = new JobInfo(job, true);\r\n    info(\"Job Overview\").__(\"Job Name:\", jinfo.getName()).__(\"User Name:\", jinfo.getUserName()).__(\"Queue Name:\", jinfo.getQueueName()).__(\"State:\", jinfo.getState()).__(\"Uberized:\", jinfo.isUberized()).__(\"Started:\", new Date(jinfo.getStartTime())).__(\"Elapsed:\", StringUtils.formatTime(jinfo.getElapsedTime()));\r\n    DIV<Hamlet> div = html.__(InfoBlock.class).div(_INFO_WRAP);\r\n    TABLE<DIV<Hamlet>> table = div.table(\"#job\");\r\n    table.tr().th(amString).__().tr().th(_TH, \"Attempt Number\").th(_TH, \"Start Time\").th(_TH, \"Node\").th(_TH, \"Logs\").__();\r\n    for (AMInfo amInfo : amInfos) {\r\n        AMAttemptInfo attempt = new AMAttemptInfo(amInfo, jinfo.getId(), jinfo.getUserName());\r\n        table.tr().td(String.valueOf(attempt.getAttemptId())).td(new Date(attempt.getStartTime()).toString()).td().a(\".nodelink\", url(MRWebAppUtil.getYARNWebappScheme(), attempt.getNodeHttpAddress()), attempt.getNodeHttpAddress()).__().td().a(\".logslink\", url(attempt.getLogsLink()), \"logs\").__().__();\r\n    }\r\n    table.__();\r\n    div.__();\r\n    html.div(_INFO_WRAP).table(\"#job\").tr().th(_TH, \"Task Type\").th(_TH, \"Progress\").th(_TH, \"Total\").th(_TH, \"Pending\").th(_TH, \"Running\").th(_TH, \"Complete\").__().tr(_ODD).th(\"Map\").td().div(_PROGRESSBAR).$title(join(jinfo.getMapProgressPercent(), '%')).div(_PROGRESSBAR_VALUE).$style(join(\"width:\", jinfo.getMapProgressPercent(), '%')).__().__().__().td().a(url(\"tasks\", jid, \"m\", \"ALL\"), String.valueOf(jinfo.getMapsTotal())).__().td().a(url(\"tasks\", jid, \"m\", \"PENDING\"), String.valueOf(jinfo.getMapsPending())).__().td().a(url(\"tasks\", jid, \"m\", \"RUNNING\"), String.valueOf(jinfo.getMapsRunning())).__().td().a(url(\"tasks\", jid, \"m\", \"COMPLETED\"), String.valueOf(jinfo.getMapsCompleted())).__().__().tr(_EVEN).th(\"Reduce\").td().div(_PROGRESSBAR).$title(join(jinfo.getReduceProgressPercent(), '%')).div(_PROGRESSBAR_VALUE).$style(join(\"width:\", jinfo.getReduceProgressPercent(), '%')).__().__().__().td().a(url(\"tasks\", jid, \"r\", \"ALL\"), String.valueOf(jinfo.getReducesTotal())).__().td().a(url(\"tasks\", jid, \"r\", \"PENDING\"), String.valueOf(jinfo.getReducesPending())).__().td().a(url(\"tasks\", jid, \"r\", \"RUNNING\"), String.valueOf(jinfo.getReducesRunning())).__().td().a(url(\"tasks\", jid, \"r\", \"COMPLETED\"), String.valueOf(jinfo.getReducesCompleted())).__().__().__().table(\"#job\").tr().th(_TH, \"Attempt Type\").th(_TH, \"New\").th(_TH, \"Running\").th(_TH, \"Failed\").th(_TH, \"Killed\").th(_TH, \"Successful\").__().tr(_ODD).th(\"Maps\").td().a(url(\"attempts\", jid, \"m\", TaskAttemptStateUI.NEW.toString()), String.valueOf(jinfo.getNewMapAttempts())).__().td().a(url(\"attempts\", jid, \"m\", TaskAttemptStateUI.RUNNING.toString()), String.valueOf(jinfo.getRunningMapAttempts())).__().td().a(url(\"attempts\", jid, \"m\", TaskAttemptStateUI.FAILED.toString()), String.valueOf(jinfo.getFailedMapAttempts())).__().td().a(url(\"attempts\", jid, \"m\", TaskAttemptStateUI.KILLED.toString()), String.valueOf(jinfo.getKilledMapAttempts())).__().td().a(url(\"attempts\", jid, \"m\", TaskAttemptStateUI.SUCCESSFUL.toString()), String.valueOf(jinfo.getSuccessfulMapAttempts())).__().__().tr(_EVEN).th(\"Reduces\").td().a(url(\"attempts\", jid, \"r\", TaskAttemptStateUI.NEW.toString()), String.valueOf(jinfo.getNewReduceAttempts())).__().td().a(url(\"attempts\", jid, \"r\", TaskAttemptStateUI.RUNNING.toString()), String.valueOf(jinfo.getRunningReduceAttempts())).__().td().a(url(\"attempts\", jid, \"r\", TaskAttemptStateUI.FAILED.toString()), String.valueOf(jinfo.getFailedReduceAttempts())).__().td().a(url(\"attempts\", jid, \"r\", TaskAttemptStateUI.KILLED.toString()), String.valueOf(jinfo.getKilledReduceAttempts())).__().td().a(url(\"attempts\", jid, \"r\", TaskAttemptStateUI.SUCCESSFUL.toString()), String.valueOf(jinfo.getSuccessfulReduceAttempts())).__().__().__().__();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getMessage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getMessage()\n{\r\n    return message;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getTaskInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskInfo getTaskInfo()\n{\r\n    return taskInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OutputCommitter getOutputCommitter()\n{\r\n    return committer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getRecoverTaskOutput",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getRecoverTaskOutput()\n{\r\n    return recoverTaskOutput;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    commonPreHead(html);\r\n    set(DATATABLES_ID, \"jobs\");\r\n    set(initID(DATATABLES, \"jobs\"), jobsTableInit());\r\n    setTableStyles(html, \"jobs\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "commonPreHead",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void commonPreHead(Page.HTML<__> html)\n{\r\n    set(ACCORDION_ID, \"nav\");\r\n    set(initID(ACCORDION, \"nav\"), \"{autoHeight:false, active:1}\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "nav",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> nav()\n{\r\n    return NavBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return JobsBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "jobsTableInit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String jobsTableInit()\n{\r\n    return tableInit().append(\", aaSorting: [[0, 'asc']]\").append(\",aoColumns:[{sType:'title-numeric'},\").append(\"null,null,{sType:'title-numeric', bSearchable:false},null,\").append(\"null,{sType:'title-numeric',bSearchable:false}, null, null]}\").toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "getContainer",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Container getContainer(ContainerLauncherEvent event)\n{\r\n    ContainerId id = event.getContainerID();\r\n    Container c = containers.get(id);\r\n    if (c == null) {\r\n        c = new Container(event.getTaskAttemptID(), event.getContainerID(), event.getContainerMgrAddress());\r\n        Container old = containers.putIfAbsent(id, c);\r\n        if (old != null) {\r\n            c = old;\r\n        }\r\n    }\r\n    return c;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "removeContainerIfDone",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeContainerIfDone(ContainerId id)\n{\r\n    Container c = containers.get(id);\r\n    if (c != null && c.isCompletelyDone()) {\r\n        containers.remove(id);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    this.limitOnPoolSize = conf.getInt(MRJobConfig.MR_AM_CONTAINERLAUNCHER_THREAD_COUNT_LIMIT, MRJobConfig.DEFAULT_MR_AM_CONTAINERLAUNCHER_THREAD_COUNT_LIMIT);\r\n    LOG.info(\"Upper limit on the thread pool size is \" + this.limitOnPoolSize);\r\n    this.initialPoolSize = conf.getInt(MRJobConfig.MR_AM_CONTAINERLAUNCHER_THREADPOOL_INITIAL_SIZE, MRJobConfig.DEFAULT_MR_AM_CONTAINERLAUNCHER_THREADPOOL_INITIAL_SIZE);\r\n    LOG.info(\"The thread pool initial size is \" + this.initialPoolSize);\r\n    super.serviceInit(conf);\r\n    cmProxy = new ContainerManagementProtocolProxy(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "serviceStart",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    ThreadFactory tf = new ThreadFactoryBuilder().setNameFormat(\"ContainerLauncher #%d\").setDaemon(true).build();\r\n    launcherPool = new HadoopThreadPoolExecutor(initialPoolSize, Integer.MAX_VALUE, 1, TimeUnit.HOURS, new LinkedBlockingQueue<Runnable>(), tf);\r\n    eventHandlingThread = new Thread() {\r\n\r\n        @Override\r\n        public void run() {\r\n            ContainerLauncherEvent event = null;\r\n            Set<String> allNodes = new HashSet<String>();\r\n            while (!stopped.get() && !Thread.currentThread().isInterrupted()) {\r\n                try {\r\n                    event = eventQueue.take();\r\n                } catch (InterruptedException e) {\r\n                    if (!stopped.get()) {\r\n                        LOG.error(\"Returning, interrupted : \" + e);\r\n                    }\r\n                    return;\r\n                }\r\n                allNodes.add(event.getContainerMgrAddress());\r\n                int poolSize = launcherPool.getCorePoolSize();\r\n                if (poolSize != limitOnPoolSize) {\r\n                    int numNodes = allNodes.size();\r\n                    int idealPoolSize = Math.min(limitOnPoolSize, numNodes);\r\n                    if (poolSize < idealPoolSize) {\r\n                        int newPoolSize = Math.min(limitOnPoolSize, idealPoolSize + initialPoolSize);\r\n                        LOG.info(\"Setting ContainerLauncher pool size to \" + newPoolSize + \" as number-of-nodes to talk to is \" + numNodes);\r\n                        launcherPool.setCorePoolSize(newPoolSize);\r\n                    }\r\n                }\r\n                launcherPool.execute(createEventProcessor(event));\r\n            }\r\n        }\r\n    };\r\n    eventHandlingThread.setName(\"ContainerLauncher Event Handler\");\r\n    eventHandlingThread.start();\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "shutdownAllContainers",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void shutdownAllContainers()\n{\r\n    for (Container ct : this.containers.values()) {\r\n        if (ct != null) {\r\n            ct.kill();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    if (stopped.getAndSet(true)) {\r\n        return;\r\n    }\r\n    shutdownAllContainers();\r\n    if (eventHandlingThread != null) {\r\n        eventHandlingThread.interrupt();\r\n    }\r\n    if (launcherPool != null) {\r\n        launcherPool.shutdownNow();\r\n    }\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "createEventProcessor",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventProcessor createEventProcessor(ContainerLauncherEvent event)\n{\r\n    return new EventProcessor(event);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "sendContainerLaunchFailedMsg",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void sendContainerLaunchFailedMsg(TaskAttemptId taskAttemptID, String message)\n{\r\n    LOG.error(message);\r\n    context.getEventHandler().handle(new TaskAttemptDiagnosticsUpdateEvent(taskAttemptID, message));\r\n    context.getEventHandler().handle(new TaskAttemptEvent(taskAttemptID, TaskAttemptEventType.TA_CONTAINER_LAUNCH_FAILED));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "handle",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void handle(ContainerLauncherEvent event)\n{\r\n    try {\r\n        eventQueue.put(event);\r\n    } catch (InterruptedException e) {\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "getCMProxy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ContainerManagementProtocolProxy.ContainerManagementProtocolProxyData getCMProxy(String containerMgrBindAddr, ContainerId containerId) throws IOException\n{\r\n    return cmProxy.getProxy(containerMgrBindAddr, containerId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getMessage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getMessage()\n{\r\n    return this.message;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getAttemptOutputDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getAttemptOutputDir()\n{\r\n    return new Path(JOB_OUTPUT_DIR, conf.get(JobContext.TASK_ATTEMPT_ID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getOutputFile() throws IOException\n{\r\n    Path attemptOutput = new Path(getAttemptOutputDir(), MAP_OUTPUT_FILENAME_STRING);\r\n    return lDirAlloc.getLocalPathToRead(attemptOutput.toString(), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getOutputFileForWrite(long size) throws IOException\n{\r\n    Path attemptOutput = new Path(getAttemptOutputDir(), MAP_OUTPUT_FILENAME_STRING);\r\n    return lDirAlloc.getLocalPathForWrite(attemptOutput.toString(), size, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputFileForWriteInVolume",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getOutputFileForWriteInVolume(Path existing)\n{\r\n    Path outputDir = new Path(existing.getParent(), JOB_OUTPUT_DIR);\r\n    Path attemptOutputDir = new Path(outputDir, conf.get(JobContext.TASK_ATTEMPT_ID));\r\n    return new Path(attemptOutputDir, MAP_OUTPUT_FILENAME_STRING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputIndexFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getOutputIndexFile() throws IOException\n{\r\n    Path attemptIndexOutput = new Path(getAttemptOutputDir(), MAP_OUTPUT_FILENAME_STRING + MAP_OUTPUT_INDEX_SUFFIX_STRING);\r\n    return lDirAlloc.getLocalPathToRead(attemptIndexOutput.toString(), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputIndexFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getOutputIndexFileForWrite(long size) throws IOException\n{\r\n    Path attemptIndexOutput = new Path(getAttemptOutputDir(), MAP_OUTPUT_FILENAME_STRING + MAP_OUTPUT_INDEX_SUFFIX_STRING);\r\n    return lDirAlloc.getLocalPathForWrite(attemptIndexOutput.toString(), size, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputIndexFileForWriteInVolume",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getOutputIndexFileForWriteInVolume(Path existing)\n{\r\n    Path outputDir = new Path(existing.getParent(), JOB_OUTPUT_DIR);\r\n    Path attemptOutputDir = new Path(outputDir, conf.get(JobContext.TASK_ATTEMPT_ID));\r\n    return new Path(attemptOutputDir, MAP_OUTPUT_FILENAME_STRING + MAP_OUTPUT_INDEX_SUFFIX_STRING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSpillFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getSpillFile(int spillNumber) throws IOException\n{\r\n    return lDirAlloc.getLocalPathToRead(String.format(SPILL_FILE_PATTERN, conf.get(JobContext.TASK_ATTEMPT_ID), spillNumber), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSpillFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getSpillFileForWrite(int spillNumber, long size) throws IOException\n{\r\n    return lDirAlloc.getLocalPathForWrite(String.format(SPILL_FILE_PATTERN, conf.get(JobContext.TASK_ATTEMPT_ID), spillNumber), size, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSpillIndexFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getSpillIndexFile(int spillNumber) throws IOException\n{\r\n    return lDirAlloc.getLocalPathToRead(String.format(SPILL_INDEX_FILE_PATTERN, conf.get(JobContext.TASK_ATTEMPT_ID), spillNumber), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSpillIndexFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getSpillIndexFileForWrite(int spillNumber, long size) throws IOException\n{\r\n    return lDirAlloc.getLocalPathForWrite(String.format(SPILL_INDEX_FILE_PATTERN, conf.get(JobContext.TASK_ATTEMPT_ID), spillNumber), size, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getInputFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getInputFile(int mapId) throws IOException\n{\r\n    throw new UnsupportedOperationException(\"Incompatible with LocalRunner\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getInputFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getInputFileForWrite(org.apache.hadoop.mapreduce.TaskID mapId, long size) throws IOException\n{\r\n    return lDirAlloc.getLocalPathForWrite(String.format(REDUCE_INPUT_FILE_FORMAT_STRING, getAttemptOutputDir().toString(), mapId.getId()), size, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "removeAll",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void removeAll() throws IOException\n{\r\n    throw new UnsupportedOperationException(\"Incompatible with LocalRunner\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    if (conf instanceof JobConf) {\r\n        this.conf = (JobConf) conf;\r\n    } else {\r\n        this.conf = new JobConf(conf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\security\\authorize",
  "methodName" : "getServices",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Service[] getServices()\n{\r\n    return mapReduceApplicationMasterServices;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return FewAttemptsBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void add(final double newNum)\n{\r\n    this.count++;\r\n    this.sum += newNum;\r\n    this.sumSquares += newNum * newNum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "updateStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void updateStatistics(final double old, final double update)\n{\r\n    this.sum += update - old;\r\n    this.sumSquares += (update * update) - (old * old);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "mean",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double mean()\n{\r\n    return count == 0 ? 0.0 : sum / count;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "var",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "double var()\n{\r\n    if (count <= 1) {\r\n        return 0.0;\r\n    }\r\n    double mean = mean();\r\n    return Math.max((sumSquares / count) - mean * mean, 0.0d);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "std",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double std()\n{\r\n    return Math.sqrt(this.var());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "outlier",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "double outlier(final float sigma)\n{\r\n    if (count != 0.0) {\r\n        return mean() + std() * sigma;\r\n    }\r\n    return 0.0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "count",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double count()\n{\r\n    return count;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "meanCI",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "double meanCI()\n{\r\n    if (count <= 1) {\r\n        return 0.0;\r\n    }\r\n    double currMean = mean();\r\n    double currStd = std();\r\n    return currMean + (DEFAULT_CI_FACTOR * currStd / Math.sqrt(count));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    return \"DataStatistics: count is \" + count + \", sum is \" + sum + \", sumSquares is \" + sumSquares + \" mean is \" + mean() + \" std() is \" + std() + \", meanCI() is \" + meanCI();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "extend",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void extend(double newProgress, int newValue)\n{\r\n    real.extend(newProgress, newValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "render",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void render(Block html)\n{\r\n    if (job == null) {\r\n        html.p().__(\"Sorry, no counters for nonexistent\", $(JOB_ID, \"job\")).__();\r\n        return;\r\n    }\r\n    if (!$(TASK_ID).isEmpty() && task == null) {\r\n        html.p().__(\"Sorry, no counters for nonexistent\", $(TASK_ID, \"task\")).__();\r\n        return;\r\n    }\r\n    if (total == null || total.getGroupNames() == null || total.countCounters() == 0) {\r\n        String type = $(TASK_ID);\r\n        if (type == null || type.isEmpty()) {\r\n            type = $(JOB_ID, \"the job\");\r\n        }\r\n        html.p().__(\"Sorry it looks like \", type, \" has no counters.\").__();\r\n        return;\r\n    }\r\n    String urlBase;\r\n    String urlId;\r\n    if (task != null) {\r\n        urlBase = \"singletaskcounter\";\r\n        urlId = MRApps.toString(task.getID());\r\n    } else {\r\n        urlBase = \"singlejobcounter\";\r\n        urlId = MRApps.toString(job.getID());\r\n    }\r\n    int numGroups = 0;\r\n    TBODY<TABLE<DIV<Hamlet>>> tbody = html.div(_INFO_WRAP).table(\"#counters\").thead().tr().th(\".group.ui-state-default\", \"Counter Group\").th(\".ui-state-default\", \"Counters\").__().__().tbody();\r\n    for (CounterGroup g : total) {\r\n        CounterGroup mg = map == null ? null : map.getGroup(g.getName());\r\n        CounterGroup rg = reduce == null ? null : reduce.getGroup(g.getName());\r\n        ++numGroups;\r\n        TR<THEAD<TABLE<TD<TR<TBODY<TABLE<DIV<Hamlet>>>>>>>> groupHeadRow = tbody.tr().th().$title(g.getName()).$class(\"ui-state-default\").__(fixGroupDisplayName(g.getDisplayName())).__().td().$class(C_TABLE).table(\".dt-counters\").$id(job.getID() + \".\" + g.getName()).thead().tr().th(\".name\", \"Name\");\r\n        if (map != null) {\r\n            groupHeadRow.th(\"Map\").th(\"Reduce\");\r\n        }\r\n        TBODY<TABLE<TD<TR<TBODY<TABLE<DIV<Hamlet>>>>>>> group = groupHeadRow.th(map == null ? \"Value\" : \"Total\").__().__().tbody();\r\n        for (Counter counter : g) {\r\n            TR<TBODY<TABLE<TD<TR<TBODY<TABLE<DIV<Hamlet>>>>>>>> groupRow = group.tr();\r\n            if (task == null && mg == null && rg == null) {\r\n                groupRow.td().$title(counter.getName()).__(counter.getDisplayName()).__();\r\n            } else {\r\n                groupRow.td().$title(counter.getName()).a(url(urlBase, urlId, g.getName(), counter.getName()), counter.getDisplayName()).__();\r\n            }\r\n            if (map != null) {\r\n                Counter mc = mg == null ? null : mg.findCounter(counter.getName());\r\n                Counter rc = rg == null ? null : rg.findCounter(counter.getName());\r\n                groupRow.td(mc == null ? \"0\" : String.format(\"%,d\", mc.getValue())).td(rc == null ? \"0\" : String.format(\"%,d\", rc.getValue()));\r\n            }\r\n            groupRow.td(String.format(\"%,d\", counter.getValue())).__();\r\n        }\r\n        group.__().__().__().__();\r\n    }\r\n    tbody.__().__().__();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void getCounters(AppContext ctx)\n{\r\n    JobId jobID = null;\r\n    TaskId taskID = null;\r\n    String tid = $(TASK_ID);\r\n    if (!tid.isEmpty()) {\r\n        taskID = MRApps.toTaskID(tid);\r\n        jobID = taskID.getJobId();\r\n    } else {\r\n        String jid = $(JOB_ID);\r\n        if (jid != null && !jid.isEmpty()) {\r\n            jobID = MRApps.toJobID(jid);\r\n        }\r\n    }\r\n    if (jobID == null) {\r\n        return;\r\n    }\r\n    job = ctx.getJob(jobID);\r\n    if (job == null) {\r\n        return;\r\n    }\r\n    if (taskID != null) {\r\n        task = job.getTask(taskID);\r\n        if (task == null) {\r\n            return;\r\n        }\r\n        total = task.getCounters();\r\n        return;\r\n    }\r\n    Map<TaskId, Task> tasks = job.getTasks();\r\n    total = job.getAllCounters();\r\n    boolean needTotalCounters = false;\r\n    if (total == null) {\r\n        total = new Counters();\r\n        needTotalCounters = true;\r\n    }\r\n    map = new Counters();\r\n    reduce = new Counters();\r\n    for (Task t : tasks.values()) {\r\n        Counters counters = t.getCounters();\r\n        if (counters == null) {\r\n            continue;\r\n        }\r\n        switch(t.getType()) {\r\n            case MAP:\r\n                map.incrAllCounters(counters);\r\n                break;\r\n            case REDUCE:\r\n                reduce.incrAllCounters(counters);\r\n                break;\r\n        }\r\n        if (needTotalCounters) {\r\n            total.incrAllCounters(counters);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "fixGroupDisplayName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String fixGroupDisplayName(CharSequence name)\n{\r\n    return name.toString().replace(\".\", \".\\u200B\").replace(\"$\", \"\\u200B$\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getAssignedContainerIdStr",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAssignedContainerIdStr()\n{\r\n    return this.assignedContainerId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getAssignedContainerId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ContainerId getAssignedContainerId()\n{\r\n    return this.assignedContainer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getState()\n{\r\n    return this.state.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStatus()\n{\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getId()\n{\r\n    return this.id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return this.startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return this.finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getProgress()\n{\r\n    return this.progress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getElapsedTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getElapsedTime()\n{\r\n    return this.elapsedTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getNode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNode()\n{\r\n    return this.nodeHttpAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getRack",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRack()\n{\r\n    return this.rack;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getNote",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNote()\n{\r\n    return this.diagnostics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getType()\n{\r\n    return type;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getMessage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getMessage()\n{\r\n    return message;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getRescheduleAttempt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getRescheduleAttempt()\n{\r\n    return rescheduleAttempt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "render",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void render(Block html)\n{\r\n    if (job == null) {\r\n        html.p().__(\"Sorry, no counters for nonexistent\", $(JOB_ID, \"job\")).__();\r\n        return;\r\n    }\r\n    if (!$(TASK_ID).isEmpty() && task == null) {\r\n        html.p().__(\"Sorry, no counters for nonexistent\", $(TASK_ID, \"task\")).__();\r\n        return;\r\n    }\r\n    String columnType = task == null ? \"Task\" : \"Task Attempt\";\r\n    TBODY<TABLE<DIV<Hamlet>>> tbody = html.div(_INFO_WRAP).table(\"#singleCounter\").thead().tr().th(\".ui-state-default\", columnType).th(\".ui-state-default\", \"Value\").__().__().tbody();\r\n    for (Map.Entry<String, Long> entry : values.entrySet()) {\r\n        TR<TBODY<TABLE<DIV<Hamlet>>>> row = tbody.tr();\r\n        String id = entry.getKey();\r\n        String val = entry.getValue().toString();\r\n        if (task != null) {\r\n            row.td(id);\r\n            row.td().br().$title(val).__().__(val).__();\r\n        } else {\r\n            row.td().a(url(\"singletaskcounter\", entry.getKey(), $(COUNTER_GROUP), $(COUNTER_NAME)), id).__();\r\n            row.td().br().$title(val).__().a(url(\"singletaskcounter\", entry.getKey(), $(COUNTER_GROUP), $(COUNTER_NAME)), val).__();\r\n        }\r\n        row.__();\r\n    }\r\n    tbody.__().__().__();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "populateMembers",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void populateMembers(AppContext ctx)\n{\r\n    JobId jobID = null;\r\n    TaskId taskID = null;\r\n    String tid = $(TASK_ID);\r\n    if ($(TITLE).contains(\"MAPS\")) {\r\n        counterType = TaskType.MAP;\r\n    } else if ($(TITLE).contains(\"REDUCES\")) {\r\n        counterType = TaskType.REDUCE;\r\n    } else {\r\n        counterType = null;\r\n    }\r\n    if (!tid.isEmpty()) {\r\n        taskID = MRApps.toTaskID(tid);\r\n        jobID = taskID.getJobId();\r\n    } else {\r\n        String jid = $(JOB_ID);\r\n        if (!jid.isEmpty()) {\r\n            jobID = MRApps.toJobID(jid);\r\n        }\r\n    }\r\n    if (jobID == null) {\r\n        return;\r\n    }\r\n    job = ctx.getJob(jobID);\r\n    if (job == null) {\r\n        return;\r\n    }\r\n    if (taskID != null) {\r\n        task = job.getTask(taskID);\r\n        if (task == null) {\r\n            return;\r\n        }\r\n        for (Map.Entry<TaskAttemptId, TaskAttempt> entry : task.getAttempts().entrySet()) {\r\n            long value = 0;\r\n            Counters counters = entry.getValue().getCounters();\r\n            CounterGroup group = (counters != null) ? counters.getGroup($(COUNTER_GROUP)) : null;\r\n            if (group != null) {\r\n                Counter c = group.findCounter($(COUNTER_NAME));\r\n                if (c != null) {\r\n                    value = c.getValue();\r\n                }\r\n            }\r\n            values.put(MRApps.toString(entry.getKey()), value);\r\n        }\r\n        return;\r\n    }\r\n    Map<TaskId, Task> tasks = job.getTasks();\r\n    for (Map.Entry<TaskId, Task> entry : tasks.entrySet()) {\r\n        long value = 0;\r\n        Counters counters = entry.getValue().getCounters();\r\n        CounterGroup group = (counters != null) ? counters.getGroup($(COUNTER_GROUP)) : null;\r\n        if (group != null) {\r\n            Counter c = group.findCounter($(COUNTER_NAME));\r\n            if (c != null) {\r\n                value = c.getValue();\r\n            }\r\n        }\r\n        if (counterType == null || counterType == entry.getValue().getType()) {\r\n            values.put(MRApps.toString(entry.getKey()), value);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getTaskID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskId getTaskID()\n{\r\n    return taskID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getNewReduceAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNewReduceAttempts()\n{\r\n    return this.newReduceAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getKilledReduceAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getKilledReduceAttempts()\n{\r\n    return this.killedReduceAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getFailedReduceAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getFailedReduceAttempts()\n{\r\n    return this.failedReduceAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getRunningReduceAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRunningReduceAttempts()\n{\r\n    return this.runningReduceAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getSuccessfulReduceAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getSuccessfulReduceAttempts()\n{\r\n    return this.successfulReduceAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getNewMapAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNewMapAttempts()\n{\r\n    return this.newMapAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getKilledMapAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getKilledMapAttempts()\n{\r\n    return this.killedMapAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getAcls",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ArrayList<ConfEntryInfo> getAcls()\n{\r\n    return acls;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getFailedMapAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getFailedMapAttempts()\n{\r\n    return this.failedMapAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getRunningMapAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRunningMapAttempts()\n{\r\n    return this.runningMapAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getSuccessfulMapAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getSuccessfulMapAttempts()\n{\r\n    return this.successfulMapAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getReducesCompleted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReducesCompleted()\n{\r\n    return this.reducesCompleted;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getReducesTotal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReducesTotal()\n{\r\n    return this.reducesTotal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getReducesPending",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReducesPending()\n{\r\n    return this.reducesPending;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getReducesRunning",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReducesRunning()\n{\r\n    return this.reducesRunning;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getMapsCompleted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMapsCompleted()\n{\r\n    return this.mapsCompleted;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getMapsTotal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMapsTotal()\n{\r\n    return this.mapsTotal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getMapsPending",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMapsPending()\n{\r\n    return this.mapsPending;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getMapsRunning",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMapsRunning()\n{\r\n    return this.mapsRunning;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getState()\n{\r\n    return this.state.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getUserName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUserName()\n{\r\n    return this.user;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return this.name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getQueueName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getQueueName()\n{\r\n    return this.queue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getId()\n{\r\n    return this.id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return this.startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getElapsedTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getElapsedTime()\n{\r\n    return this.elapsedTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return this.finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "isUberized",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isUberized()\n{\r\n    return this.uberized;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getdiagnostics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getdiagnostics()\n{\r\n    return this.diagnostics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getMapProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getMapProgress()\n{\r\n    return this.mapProgress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getMapProgressPercent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getMapProgressPercent()\n{\r\n    return this.mapProgressPercent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getReduceProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getReduceProgress()\n{\r\n    return this.reduceProgress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getReduceProgressPercent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getReduceProgressPercent()\n{\r\n    return this.reduceProgressPercent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "countTasksAndAttempts",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void countTasksAndAttempts(Job job)\n{\r\n    final Map<TaskId, Task> tasks = job.getTasks();\r\n    if (tasks == null) {\r\n        return;\r\n    }\r\n    for (Task task : tasks.values()) {\r\n        switch(task.getType()) {\r\n            case MAP:\r\n                switch(task.getState()) {\r\n                    case RUNNING:\r\n                        ++this.mapsRunning;\r\n                        break;\r\n                    case SCHEDULED:\r\n                        ++this.mapsPending;\r\n                        break;\r\n                    default:\r\n                        break;\r\n                }\r\n                break;\r\n            case REDUCE:\r\n                switch(task.getState()) {\r\n                    case RUNNING:\r\n                        ++this.reducesRunning;\r\n                        break;\r\n                    case SCHEDULED:\r\n                        ++this.reducesPending;\r\n                        break;\r\n                    default:\r\n                        break;\r\n                }\r\n                break;\r\n            default:\r\n                throw new IllegalStateException(\"Task type is neither map nor reduce: \" + task.getType());\r\n        }\r\n        Map<TaskAttemptId, TaskAttempt> attempts = task.getAttempts();\r\n        int newAttempts, running, successful, failed, killed;\r\n        for (TaskAttempt attempt : attempts.values()) {\r\n            newAttempts = 0;\r\n            running = 0;\r\n            successful = 0;\r\n            failed = 0;\r\n            killed = 0;\r\n            if (TaskAttemptStateUI.NEW.correspondsTo(attempt.getState())) {\r\n                ++newAttempts;\r\n            } else if (TaskAttemptStateUI.RUNNING.correspondsTo(attempt.getState())) {\r\n                ++running;\r\n            } else if (TaskAttemptStateUI.SUCCESSFUL.correspondsTo(attempt.getState())) {\r\n                ++successful;\r\n            } else if (TaskAttemptStateUI.FAILED.correspondsTo(attempt.getState())) {\r\n                ++failed;\r\n            } else if (TaskAttemptStateUI.KILLED.correspondsTo(attempt.getState())) {\r\n                ++killed;\r\n            }\r\n            switch(task.getType()) {\r\n                case MAP:\r\n                    this.newMapAttempts += newAttempts;\r\n                    this.runningMapAttempts += running;\r\n                    this.successfulMapAttempts += successful;\r\n                    this.failedMapAttempts += failed;\r\n                    this.killedMapAttempts += killed;\r\n                    break;\r\n                case REDUCE:\r\n                    this.newReduceAttempts += newAttempts;\r\n                    this.runningReduceAttempts += running;\r\n                    this.successfulReduceAttempts += successful;\r\n                    this.failedReduceAttempts += failed;\r\n                    this.killedReduceAttempts += killed;\r\n                    break;\r\n                default:\r\n                    throw new IllegalStateException(\"Task type neither map nor reduce: \" + task.getType());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "addCounterUpdate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addCounterUpdate(Enum<?> key, long incrValue)\n{\r\n    counterUpdates.add(new CounterIncrementalUpdate(key, incrValue));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getCounterUpdates",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<CounterIncrementalUpdate> getCounterUpdates()\n{\r\n    return counterUpdates;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void setup()\n{\r\n    bind(JAXBContextResolver.class);\r\n    bind(GenericExceptionHandler.class);\r\n    bind(AMWebServices.class);\r\n    route(\"/\", AppController.class);\r\n    route(\"/app\", AppController.class);\r\n    route(pajoin(\"/job\", JOB_ID), AppController.class, \"job\");\r\n    route(pajoin(\"/conf\", JOB_ID), AppController.class, \"conf\");\r\n    route(pajoin(\"/jobcounters\", JOB_ID), AppController.class, \"jobCounters\");\r\n    route(pajoin(\"/singlejobcounter\", JOB_ID, COUNTER_GROUP, COUNTER_NAME), AppController.class, \"singleJobCounter\");\r\n    route(pajoin(\"/tasks\", JOB_ID, TASK_TYPE, TASK_STATE), AppController.class, \"tasks\");\r\n    route(pajoin(\"/attempts\", JOB_ID, TASK_TYPE, ATTEMPT_STATE), AppController.class, \"attempts\");\r\n    route(pajoin(\"/task\", TASK_ID), AppController.class, \"task\");\r\n    route(pajoin(\"/taskcounters\", TASK_ID), AppController.class, \"taskCounters\");\r\n    route(pajoin(\"/singletaskcounter\", TASK_ID, COUNTER_GROUP, COUNTER_NAME), AppController.class, \"singleTaskCounter\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void init(Configuration conf)\n{\r\n    super.init(conf);\r\n    int expireIntvl = conf.getInt(MRJobConfig.TASK_EXIT_TIMEOUT, MRJobConfig.TASK_EXIT_TIMEOUT_DEFAULT);\r\n    int checkIntvl = conf.getInt(MRJobConfig.TASK_EXIT_TIMEOUT_CHECK_INTERVAL_MS, MRJobConfig.TASK_EXIT_TIMEOUT_CHECK_INTERVAL_MS_DEFAULT);\r\n    setExpireInterval(expireIntvl);\r\n    setMonitorInterval(checkIntvl);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "expire",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expire(TaskAttemptId id)\n{\r\n    eventHandler.handle(new TaskAttemptEvent(id, TaskAttemptEventType.TA_TIMED_OUT));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "incorporateReading",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void incorporateReading(TaskAttemptId attemptID, float newProgress, long newTime)\n{\r\n    AtomicReference<EstimateVector> vectorRef = estimates.get(attemptID);\r\n    if (vectorRef == null) {\r\n        estimates.putIfAbsent(attemptID, new AtomicReference<EstimateVector>(null));\r\n        incorporateReading(attemptID, newProgress, newTime);\r\n        return;\r\n    }\r\n    EstimateVector oldVector = vectorRef.get();\r\n    if (oldVector == null) {\r\n        if (vectorRef.compareAndSet(null, new EstimateVector(-1.0, 0.0F, Long.MIN_VALUE))) {\r\n            return;\r\n        }\r\n        incorporateReading(attemptID, newProgress, newTime);\r\n        return;\r\n    }\r\n    while (!vectorRef.compareAndSet(oldVector, oldVector.incorporate(newProgress, newTime))) {\r\n        oldVector = vectorRef.get();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "getEstimateVector",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "EstimateVector getEstimateVector(TaskAttemptId attemptID)\n{\r\n    AtomicReference<EstimateVector> vectorRef = estimates.get(attemptID);\r\n    if (vectorRef == null) {\r\n        return null;\r\n    }\r\n    return vectorRef.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "contextualize",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void contextualize(Configuration conf, AppContext context)\n{\r\n    super.contextualize(conf, context);\r\n    lambda = conf.getLong(MRJobConfig.MR_AM_TASK_ESTIMATOR_SMOOTH_LAMBDA_MS, MRJobConfig.DEFAULT_MR_AM_TASK_ESTIMATOR_SMOOTH_LAMBDA_MS);\r\n    smoothedValue = conf.getBoolean(MRJobConfig.MR_AM_TASK_ESTIMATOR_EXPONENTIAL_RATE_ENABLE, true) ? SmoothedValue.RATE : SmoothedValue.TIME_PER_UNIT_PROGRESS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "estimatedRuntime",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long estimatedRuntime(TaskAttemptId id)\n{\r\n    Long startTime = startTimes.get(id);\r\n    if (startTime == null) {\r\n        return -1L;\r\n    }\r\n    EstimateVector vector = getEstimateVector(id);\r\n    if (vector == null) {\r\n        return -1L;\r\n    }\r\n    long sunkTime = vector.atTime - startTime;\r\n    double value = vector.value;\r\n    float progress = vector.basedOnProgress;\r\n    if (value == 0) {\r\n        return -1L;\r\n    }\r\n    double rate = smoothedValue == SmoothedValue.RATE ? value : 1.0 / value;\r\n    if (rate == 0.0) {\r\n        return -1L;\r\n    }\r\n    double remainingTime = (1.0 - progress) / rate;\r\n    return sunkTime + (long) remainingTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "runtimeEstimateVariance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long runtimeEstimateVariance(TaskAttemptId id)\n{\r\n    return -1L;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "updateAttempt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateAttempt(TaskAttemptStatus status, long timestamp)\n{\r\n    super.updateAttempt(status, timestamp);\r\n    TaskAttemptId attemptID = status.id;\r\n    float progress = status.progress;\r\n    incorporateReading(attemptID, progress, timestamp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getShuffleFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getShuffleFinishTime()\n{\r\n    return this.shuffleFinishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getMergeFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMergeFinishTime()\n{\r\n    return this.mergeFinishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getElapsedShuffleTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getElapsedShuffleTime()\n{\r\n    return this.elapsedShuffleTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getElapsedMergeTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getElapsedMergeTime()\n{\r\n    return this.elapsedMergeTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getElapsedReduceTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getElapsedReduceTime()\n{\r\n    return this.elapsedReduceTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobId getJobID()\n{\r\n    return jobID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "getJobContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobContext getJobContext()\n{\r\n    return jobContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "getEstimator",
  "errType" : [ "InstantiationException", "IllegalAccessException", "InvocationTargetException", "NoSuchMethodException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "TaskRuntimeEstimator getEstimator(Configuration conf, AppContext context)\n{\r\n    TaskRuntimeEstimator estimator;\r\n    try {\r\n        Class<? extends TaskRuntimeEstimator> estimatorClass = conf.getClass(MRJobConfig.MR_AM_TASK_ESTIMATOR, LegacyTaskRuntimeEstimator.class, TaskRuntimeEstimator.class);\r\n        Constructor<? extends TaskRuntimeEstimator> estimatorConstructor = estimatorClass.getConstructor();\r\n        estimator = estimatorConstructor.newInstance();\r\n        estimator.contextualize(conf, context);\r\n    } catch (InstantiationException ex) {\r\n        LOG.error(\"Can't make a speculation runtime estimator\", ex);\r\n        throw new YarnRuntimeException(ex);\r\n    } catch (IllegalAccessException ex) {\r\n        LOG.error(\"Can't make a speculation runtime estimator\", ex);\r\n        throw new YarnRuntimeException(ex);\r\n    } catch (InvocationTargetException ex) {\r\n        LOG.error(\"Can't make a speculation runtime estimator\", ex);\r\n        throw new YarnRuntimeException(ex);\r\n    } catch (NoSuchMethodException ex) {\r\n        LOG.error(\"Can't make a speculation runtime estimator\", ex);\r\n        throw new YarnRuntimeException(ex);\r\n    }\r\n    return estimator;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "serviceStart",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    Runnable speculationBackgroundCore = new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            while (!stopped && !Thread.currentThread().isInterrupted()) {\r\n                long backgroundRunStartTime = clock.getTime();\r\n                try {\r\n                    int speculations = computeSpeculations();\r\n                    long mininumRecomp = speculations > 0 ? soonestRetryAfterSpeculate : soonestRetryAfterNoSpeculate;\r\n                    long wait = Math.max(mininumRecomp, clock.getTime() - backgroundRunStartTime);\r\n                    if (speculations > 0) {\r\n                        LOG.info(\"We launched \" + speculations + \" speculations.  Sleeping \" + wait + \" milliseconds.\");\r\n                    }\r\n                    Object pollResult = scanControl.poll(wait, TimeUnit.MILLISECONDS);\r\n                } catch (InterruptedException e) {\r\n                    if (!stopped) {\r\n                        LOG.error(\"Background thread returning, interrupted\", e);\r\n                    }\r\n                    return;\r\n                }\r\n            }\r\n        }\r\n    };\r\n    speculationBackgroundThread = new Thread(speculationBackgroundCore, \"DefaultSpeculator background processing\");\r\n    speculationBackgroundThread.start();\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    stopped = true;\r\n    if (speculationBackgroundThread != null) {\r\n        speculationBackgroundThread.interrupt();\r\n    }\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "handleAttempt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void handleAttempt(TaskAttemptStatus status)\n{\r\n    long timestamp = clock.getTime();\r\n    statusUpdate(status, timestamp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "eventQueueEmpty",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean eventQueueEmpty()\n{\r\n    return scanControl.isEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "scanForSpeculations",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void scanForSpeculations()\n{\r\n    LOG.info(\"We got asked to run a debug speculation scan.\");\r\n    System.out.println(\"We got asked to run a debug speculation scan.\");\r\n    System.out.println(\"There are \" + scanControl.size() + \" events stacked already.\");\r\n    scanControl.add(new Object());\r\n    Thread.yield();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "containerNeed",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AtomicInteger containerNeed(TaskId taskID)\n{\r\n    JobId jobID = taskID.getJobId();\r\n    TaskType taskType = taskID.getTaskType();\r\n    ConcurrentMap<JobId, AtomicInteger> relevantMap = taskType == TaskType.MAP ? mapContainerNeeds : reduceContainerNeeds;\r\n    AtomicInteger result = relevantMap.get(jobID);\r\n    if (result == null) {\r\n        relevantMap.putIfAbsent(jobID, new AtomicInteger(0));\r\n        result = relevantMap.get(jobID);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "processSpeculatorEvent",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void processSpeculatorEvent(SpeculatorEvent event)\n{\r\n    switch(event.getType()) {\r\n        case ATTEMPT_STATUS_UPDATE:\r\n            statusUpdate(event.getReportedStatus(), event.getTimestamp());\r\n            break;\r\n        case TASK_CONTAINER_NEED_UPDATE:\r\n            {\r\n                AtomicInteger need = containerNeed(event.getTaskID());\r\n                need.addAndGet(event.containersNeededChange());\r\n                break;\r\n            }\r\n        case ATTEMPT_START:\r\n            {\r\n                LOG.info(\"ATTEMPT_START \" + event.getTaskID());\r\n                estimator.enrollAttempt(event.getReportedStatus(), event.getTimestamp());\r\n                break;\r\n            }\r\n        case JOB_CREATE:\r\n            {\r\n                LOG.info(\"JOB_CREATE \" + event.getJobID());\r\n                estimator.contextualize(getConfig(), context);\r\n                break;\r\n            }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "statusUpdate",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void statusUpdate(TaskAttemptStatus reportedStatus, long timestamp)\n{\r\n    String stateString = reportedStatus.taskState.toString();\r\n    TaskAttemptId attemptID = reportedStatus.id;\r\n    TaskId taskID = attemptID.getTaskId();\r\n    Job job = context.getJob(taskID.getJobId());\r\n    if (job == null) {\r\n        return;\r\n    }\r\n    Task task = job.getTask(taskID);\r\n    if (task == null) {\r\n        return;\r\n    }\r\n    estimator.updateAttempt(reportedStatus, timestamp);\r\n    if (stateString.equals(TaskAttemptState.RUNNING.name())) {\r\n        runningTasks.putIfAbsent(taskID, Boolean.TRUE);\r\n    } else {\r\n        runningTasks.remove(taskID, Boolean.TRUE);\r\n        if (!stateString.equals(TaskAttemptState.STARTING.name())) {\r\n            runningTaskAttemptStatistics.remove(attemptID);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "speculationValue",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "long speculationValue(TaskId taskID, long now)\n{\r\n    Job job = context.getJob(taskID.getJobId());\r\n    Task task = job.getTask(taskID);\r\n    Map<TaskAttemptId, TaskAttempt> attempts = task.getAttempts();\r\n    long acceptableRuntime = Long.MIN_VALUE;\r\n    long result = Long.MIN_VALUE;\r\n    if (!mayHaveSpeculated.contains(taskID)) {\r\n        acceptableRuntime = estimator.thresholdRuntime(taskID);\r\n        if (acceptableRuntime == Long.MAX_VALUE) {\r\n            return ON_SCHEDULE;\r\n        }\r\n    }\r\n    TaskAttemptId runningTaskAttemptID = null;\r\n    int numberRunningAttempts = 0;\r\n    for (TaskAttempt taskAttempt : attempts.values()) {\r\n        if (taskAttempt.getState() == TaskAttemptState.RUNNING || taskAttempt.getState() == TaskAttemptState.STARTING) {\r\n            if (++numberRunningAttempts > 1) {\r\n                return ALREADY_SPECULATING;\r\n            }\r\n            runningTaskAttemptID = taskAttempt.getID();\r\n            long estimatedRunTime = estimator.estimatedRuntime(runningTaskAttemptID);\r\n            long taskAttemptStartTime = estimator.attemptEnrolledTime(runningTaskAttemptID);\r\n            if (taskAttemptStartTime > now) {\r\n                return TOO_NEW;\r\n            }\r\n            long estimatedEndTime = estimatedRunTime + taskAttemptStartTime;\r\n            long estimatedReplacementEndTime = now + estimator.estimatedNewAttemptRuntime(taskID);\r\n            float progress = taskAttempt.getProgress();\r\n            TaskAttemptHistoryStatistics data = runningTaskAttemptStatistics.get(runningTaskAttemptID);\r\n            if (data == null) {\r\n                runningTaskAttemptStatistics.put(runningTaskAttemptID, new TaskAttemptHistoryStatistics(estimatedRunTime, progress, now));\r\n            } else {\r\n                if (estimatedRunTime == data.getEstimatedRunTime() && progress == data.getProgress()) {\r\n                    if (data.notHeartbeatedInAWhile(now) || estimator.hasStagnatedProgress(runningTaskAttemptID, now)) {\r\n                        TaskAttemptStatus taskAttemptStatus = new TaskAttemptStatus();\r\n                        taskAttemptStatus.id = runningTaskAttemptID;\r\n                        taskAttemptStatus.progress = progress;\r\n                        taskAttemptStatus.taskState = taskAttempt.getState();\r\n                        handleAttempt(taskAttemptStatus);\r\n                    }\r\n                } else {\r\n                    data.setEstimatedRunTime(estimatedRunTime);\r\n                    data.setProgress(progress);\r\n                    data.resetHeartBeatTime(now);\r\n                }\r\n            }\r\n            if (estimatedEndTime < now) {\r\n                return PROGRESS_IS_GOOD;\r\n            }\r\n            if (estimatedReplacementEndTime >= estimatedEndTime) {\r\n                return TOO_LATE_TO_SPECULATE;\r\n            }\r\n            result = estimatedEndTime - estimatedReplacementEndTime;\r\n        }\r\n    }\r\n    if (numberRunningAttempts == 0) {\r\n        return NOT_RUNNING;\r\n    }\r\n    if (acceptableRuntime == Long.MIN_VALUE) {\r\n        acceptableRuntime = estimator.thresholdRuntime(taskID);\r\n        if (acceptableRuntime == Long.MAX_VALUE) {\r\n            return ON_SCHEDULE;\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "addSpeculativeAttempt",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addSpeculativeAttempt(TaskId taskID)\n{\r\n    LOG.info(\"DefaultSpeculator.addSpeculativeAttempt -- we are speculating \" + taskID);\r\n    eventHandler.handle(new TaskEvent(taskID, TaskEventType.T_ADD_SPEC_ATTEMPT));\r\n    mayHaveSpeculated.add(taskID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "handle",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void handle(SpeculatorEvent event)\n{\r\n    processSpeculatorEvent(event);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "maybeScheduleAMapSpeculation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int maybeScheduleAMapSpeculation()\n{\r\n    return maybeScheduleASpeculation(TaskType.MAP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "maybeScheduleAReduceSpeculation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int maybeScheduleAReduceSpeculation()\n{\r\n    return maybeScheduleASpeculation(TaskType.REDUCE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "maybeScheduleASpeculation",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "int maybeScheduleASpeculation(TaskType type)\n{\r\n    int successes = 0;\r\n    long now = clock.getTime();\r\n    ConcurrentMap<JobId, AtomicInteger> containerNeeds = type == TaskType.MAP ? mapContainerNeeds : reduceContainerNeeds;\r\n    for (ConcurrentMap.Entry<JobId, AtomicInteger> jobEntry : containerNeeds.entrySet()) {\r\n        if (jobEntry.getValue().get() > 0) {\r\n            continue;\r\n        }\r\n        int numberSpeculationsAlready = 0;\r\n        int numberRunningTasks = 0;\r\n        Job job = context.getJob(jobEntry.getKey());\r\n        Map<TaskId, Task> tasks = job.getTasks(type);\r\n        int numberAllowedSpeculativeTasks = (int) Math.max(minimumAllowedSpeculativeTasks, proportionTotalTasksSpeculatable * tasks.size());\r\n        TaskId bestTaskID = null;\r\n        long bestSpeculationValue = -1L;\r\n        for (Map.Entry<TaskId, Task> taskEntry : tasks.entrySet()) {\r\n            long mySpeculationValue = speculationValue(taskEntry.getKey(), now);\r\n            if (mySpeculationValue == ALREADY_SPECULATING) {\r\n                ++numberSpeculationsAlready;\r\n            }\r\n            if (mySpeculationValue != NOT_RUNNING) {\r\n                ++numberRunningTasks;\r\n            }\r\n            if (mySpeculationValue > bestSpeculationValue) {\r\n                bestTaskID = taskEntry.getKey();\r\n                bestSpeculationValue = mySpeculationValue;\r\n            }\r\n        }\r\n        numberAllowedSpeculativeTasks = (int) Math.max(numberAllowedSpeculativeTasks, proportionRunningTasksSpeculatable * numberRunningTasks);\r\n        if (bestTaskID != null && numberAllowedSpeculativeTasks > numberSpeculationsAlready) {\r\n            addSpeculativeAttempt(bestTaskID);\r\n            ++successes;\r\n        }\r\n    }\r\n    return successes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "computeSpeculations",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int computeSpeculations()\n{\r\n    return maybeScheduleAMapSpeculation() + maybeScheduleAReduceSpeculation();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "getSoonestRetryAfterNoSpeculate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSoonestRetryAfterNoSpeculate()\n{\r\n    return soonestRetryAfterNoSpeculate;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "getSoonestRetryAfterSpeculate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSoonestRetryAfterSpeculate()\n{\r\n    return soonestRetryAfterSpeculate;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "getProportionRunningTasksSpeculatable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getProportionRunningTasksSpeculatable()\n{\r\n    return proportionRunningTasksSpeculatable;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "getProportionTotalTasksSpeculatable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getProportionTotalTasksSpeculatable()\n{\r\n    return proportionTotalTasksSpeculatable;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "getMinimumAllowedSpeculativeTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMinimumAllowedSpeculativeTasks()\n{\r\n    return minimumAllowedSpeculativeTasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "render",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void render(Block html)\n{\r\n    TBODY<TABLE<Hamlet>> tbody = html.h2(\"Active Jobs\").table(\"#jobs\").thead().tr().th(\".id\", \"Job ID\").th(\".name\", \"Name\").th(\".state\", \"State\").th(\"Map Progress\").th(\"Maps Total\").th(\"Maps Completed\").th(\"Reduce Progress\").th(\"Reduces Total\").th(\"Reduces Completed\").__().__().tbody();\r\n    for (Job j : appContext.getAllJobs().values()) {\r\n        JobInfo job = new JobInfo(j, false);\r\n        tbody.tr().td().span().$title(String.valueOf(job.getId())).__().a(url(\"job\", job.getId()), job.getId()).__().td(job.getName()).td(job.getState()).td().span().$title(job.getMapProgressPercent()).__().div(_PROGRESSBAR).$title(join(job.getMapProgressPercent(), '%')).div(_PROGRESSBAR_VALUE).$style(join(\"width:\", job.getMapProgressPercent(), '%')).__().__().__().td(String.valueOf(job.getMapsTotal())).td(String.valueOf(job.getMapsCompleted())).td().span().$title(job.getReduceProgressPercent()).__().div(_PROGRESSBAR).$title(join(job.getReduceProgressPercent(), '%')).div(_PROGRESSBAR_VALUE).$style(join(\"width:\", job.getReduceProgressPercent(), '%')).__().__().__().td(String.valueOf(job.getReducesTotal())).td(String.valueOf(job.getReducesCompleted())).__();\r\n    }\r\n    tbody.__().__();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "getContainerLaunchContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ContainerLaunchContext getContainerLaunchContext()\n{\r\n    return this.containerLaunchContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "getAllocatedContainer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Container getAllocatedContainer()\n{\r\n    return this.allocatedContainer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "getRemoteTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Task getRemoteTask()\n{\r\n    return this.task;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return super.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    return super.equals(obj);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JAXBContext getContext(Class<?> objectType)\n{\r\n    return typesContextMap.get(objectType);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "getReportedStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptStatus getReportedStatus()\n{\r\n    return reportedStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "containersNeededChange",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int containersNeededChange()\n{\r\n    return containersNeededChange;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "getTaskID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskId getTaskID()\n{\r\n    return taskID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobId getJobID()\n{\r\n    return jobID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void handleEvent(HistoryEvent event) throws IOException\n{\r\n    if (!(event instanceof AMStartedEvent)) {\r\n        handler.handle(new JobHistoryEvent(jobId, event));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "serviceStart",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    try {\r\n        parse();\r\n    } catch (IOException e) {\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "parse",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void parse() throws IOException\n{\r\n    FSDataInputStream in = null;\r\n    try {\r\n        in = getPreviousJobHistoryFileStream(getConfig(), applicationAttemptId);\r\n    } catch (IOException e) {\r\n        LOG.warn(\"error trying to open previous history file. No history data \" + \"will be copied over.\", e);\r\n        return;\r\n    }\r\n    JobHistoryParser parser = new JobHistoryParser(in);\r\n    parser.parse(this);\r\n    Exception parseException = parser.getParseException();\r\n    if (parseException != null) {\r\n        LOG.info(\"Got an error parsing job-history file\" + \", ignoring incomplete events.\", parseException);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getPreviousJobHistoryFileStream",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "FSDataInputStream getPreviousJobHistoryFileStream(Configuration conf, ApplicationAttemptId applicationAttemptId) throws IOException\n{\r\n    FSDataInputStream in = null;\r\n    Path historyFile = null;\r\n    String jobId = TypeConverter.fromYarn(applicationAttemptId.getApplicationId()).toString();\r\n    String jobhistoryDir = JobHistoryUtils.getConfiguredHistoryStagingDirPrefix(conf, jobId);\r\n    Path histDirPath = FileContext.getFileContext(conf).makeQualified(new Path(jobhistoryDir));\r\n    FileContext fc = FileContext.getFileContext(histDirPath.toUri(), conf);\r\n    historyFile = fc.makeQualified(JobHistoryUtils.getStagingJobHistoryFile(histDirPath, jobId, (applicationAttemptId.getAttemptId() - 1)));\r\n    LOG.info(\"History file is at \" + historyFile);\r\n    in = fc.open(historyFile);\r\n    return in;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getDiagnosticInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDiagnosticInfo()\n{\r\n    return diagnosticInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "getTaskAttemptID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptId getTaskAttemptID()\n{\r\n    return this.taskAttemptID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "getContainerID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ContainerId getContainerID()\n{\r\n    return containerID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "getContainerMgrAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getContainerMgrAddress()\n{\r\n    return containerMgrAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "getContainerToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Token getContainerToken()\n{\r\n    return containerToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "getDumpContainerThreads",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getDumpContainerThreads()\n{\r\n    return dumpContainerThreads;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return super.toString() + \" for container \" + containerID + \" taskAttempt \" + taskAttemptID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int hashCode()\n{\r\n    final int prime = 31;\r\n    int result = 1;\r\n    result = prime * result + ((containerID == null) ? 0 : containerID.hashCode());\r\n    result = prime * result + ((containerMgrAddress == null) ? 0 : containerMgrAddress.hashCode());\r\n    result = prime * result + ((containerToken == null) ? 0 : containerToken.hashCode());\r\n    result = prime * result + ((taskAttemptID == null) ? 0 : taskAttemptID.hashCode());\r\n    result = prime * result + (dumpContainerThreads ? 1 : 0);\r\n    return result;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (this == obj)\r\n        return true;\r\n    if (obj == null)\r\n        return false;\r\n    if (getClass() != obj.getClass())\r\n        return false;\r\n    ContainerLauncherEvent other = (ContainerLauncherEvent) obj;\r\n    if (containerID == null) {\r\n        if (other.containerID != null)\r\n            return false;\r\n    } else if (!containerID.equals(other.containerID))\r\n        return false;\r\n    if (containerMgrAddress == null) {\r\n        if (other.containerMgrAddress != null)\r\n            return false;\r\n    } else if (!containerMgrAddress.equals(other.containerMgrAddress))\r\n        return false;\r\n    if (containerToken == null) {\r\n        if (other.containerToken != null)\r\n            return false;\r\n    } else if (!containerToken.equals(other.containerToken))\r\n        return false;\r\n    if (taskAttemptID == null) {\r\n        if (other.taskAttemptID != null)\r\n            return false;\r\n    } else if (!taskAttemptID.equals(other.taskAttemptID))\r\n        return false;\r\n    return dumpContainerThreads == other.dumpContainerThreads;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "getAttemptID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptId getAttemptID()\n{\r\n    return attemptID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "getAttemptContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptContext getAttemptContext()\n{\r\n    return attemptContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "render",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void render(Block html)\n{\r\n    if (app.getJob() == null) {\r\n        html.h2($(TITLE));\r\n        return;\r\n    }\r\n    TaskType type = null;\r\n    String symbol = $(TASK_TYPE);\r\n    if (!symbol.isEmpty()) {\r\n        type = MRApps.taskType(symbol);\r\n    }\r\n    TBODY<TABLE<Hamlet>> tbody = html.table(\"#tasks\").thead().tr().th(\"Task\").th(\"Progress\").th(\"Status\").th(\"State\").th(\"Start Time\").th(\"Finish Time\").th(\"Elapsed Time\").__().__().tbody();\r\n    StringBuilder tasksTableData = new StringBuilder(\"[\\n\");\r\n    for (Task task : app.getJob().getTasks().values()) {\r\n        if (type != null && task.getType() != type) {\r\n            continue;\r\n        }\r\n        String taskStateStr = $(TASK_STATE);\r\n        if (taskStateStr == null || taskStateStr.trim().equals(\"\")) {\r\n            taskStateStr = \"ALL\";\r\n        }\r\n        if (!taskStateStr.equalsIgnoreCase(\"ALL\")) {\r\n            try {\r\n                MRApps.TaskStateUI stateUI = MRApps.taskState(taskStateStr);\r\n                if (!stateUI.correspondsTo(task.getState())) {\r\n                    continue;\r\n                }\r\n            } catch (IllegalArgumentException e) {\r\n                continue;\r\n            }\r\n        }\r\n        TaskInfo info = new TaskInfo(task);\r\n        String tid = info.getId();\r\n        String pct = StringUtils.format(\"%.2f\", info.getProgress());\r\n        tasksTableData.append(\"[\\\"<a href='\").append(url(\"task\", tid)).append(\"'>\").append(tid).append(\"</a>\\\",\\\"\").append(\"<br title='\").append(pct).append(\"'> <div class='\").append(C_PROGRESSBAR).append(\"' title='\").append(join(pct, '%')).append(\"'> \").append(\"<div class='\").append(C_PROGRESSBAR_VALUE).append(\"' style='\").append(join(\"width:\", pct, '%')).append(\"'> </div> </div>\\\",\\\"\").append(StringEscapeUtils.escapeEcmaScript(StringEscapeUtils.escapeHtml4(info.getStatus()))).append(\"\\\",\\\"\").append(info.getState()).append(\"\\\",\\\"\").append(info.getStartTime()).append(\"\\\",\\\"\").append(info.getFinishTime()).append(\"\\\",\\\"\").append(info.getElapsedTime()).append(\"\\\"],\\n\");\r\n    }\r\n    if (tasksTableData.charAt(tasksTableData.length() - 2) == ',') {\r\n        tasksTableData.delete(tasksTableData.length() - 2, tasksTableData.length() - 1);\r\n    }\r\n    tasksTableData.append(\"]\");\r\n    html.script().$type(\"text/javascript\").__(\"var tasksTableData=\" + tasksTableData).__();\r\n    tbody.__().__();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "index",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void index()\n{\r\n    setTitle(join(\"MapReduce Application \", $(APP_ID)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "info",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void info()\n{\r\n    AppInfo info = new AppInfo(app, app.context);\r\n    info(\"Application Master Overview\").__(\"Application ID:\", info.getId()).__(\"Application Name:\", info.getName()).__(\"User:\", info.getUser()).__(\"Started on:\", Times.format(info.getStartTime())).__(\"Elasped: \", org.apache.hadoop.util.StringUtils.formatTime(info.getElapsedTime()));\r\n    render(InfoPage.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "jobPage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends View> jobPage()\n{\r\n    return JobPage.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "job",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void job()\n{\r\n    try {\r\n        requireJob();\r\n    } catch (Exception e) {\r\n        renderText(e.getMessage());\r\n        return;\r\n    }\r\n    render(jobPage());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "countersPage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends View> countersPage()\n{\r\n    return CountersPage.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "jobCounters",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void jobCounters()\n{\r\n    try {\r\n        requireJob();\r\n    } catch (Exception e) {\r\n        renderText(e.getMessage());\r\n        return;\r\n    }\r\n    if (app.getJob() != null) {\r\n        setTitle(join(\"Counters for \", $(JOB_ID)));\r\n    }\r\n    render(countersPage());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "taskCounters",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void taskCounters()\n{\r\n    try {\r\n        requireTask();\r\n    } catch (Exception e) {\r\n        renderText(e.getMessage());\r\n        return;\r\n    }\r\n    if (app.getTask() != null) {\r\n        setTitle(StringHelper.join(\"Counters for \", $(TASK_ID)));\r\n    }\r\n    render(countersPage());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "singleCounterPage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends View> singleCounterPage()\n{\r\n    return SingleCounterPage.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "singleJobCounter",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void singleJobCounter() throws IOException\n{\r\n    try {\r\n        requireJob();\r\n    } catch (Exception e) {\r\n        renderText(e.getMessage());\r\n        return;\r\n    }\r\n    set(COUNTER_GROUP, URLDecoder.decode($(COUNTER_GROUP), \"UTF-8\"));\r\n    set(COUNTER_NAME, URLDecoder.decode($(COUNTER_NAME), \"UTF-8\"));\r\n    if (app.getJob() != null) {\r\n        setTitle(StringHelper.join($(COUNTER_GROUP), \" \", $(COUNTER_NAME), \" for \", $(JOB_ID)));\r\n    }\r\n    render(singleCounterPage());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "singleTaskCounter",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void singleTaskCounter() throws IOException\n{\r\n    try {\r\n        requireTask();\r\n    } catch (Exception e) {\r\n        renderText(e.getMessage());\r\n        return;\r\n    }\r\n    set(COUNTER_GROUP, URLDecoder.decode($(COUNTER_GROUP), \"UTF-8\"));\r\n    set(COUNTER_NAME, URLDecoder.decode($(COUNTER_NAME), \"UTF-8\"));\r\n    if (app.getTask() != null) {\r\n        setTitle(StringHelper.join($(COUNTER_GROUP), \" \", $(COUNTER_NAME), \" for \", $(TASK_ID)));\r\n    }\r\n    render(singleCounterPage());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "tasksPage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends View> tasksPage()\n{\r\n    return TasksPage.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "tasks",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void tasks()\n{\r\n    try {\r\n        requireJob();\r\n    } catch (Exception e) {\r\n        renderText(e.getMessage());\r\n        return;\r\n    }\r\n    if (app.getJob() != null) {\r\n        try {\r\n            String tt = $(TASK_TYPE);\r\n            tt = tt.isEmpty() ? \"All\" : StringUtils.capitalize(org.apache.hadoop.util.StringUtils.toLowerCase(MRApps.taskType(tt).toString()));\r\n            setTitle(join(tt, \" Tasks for \", $(JOB_ID)));\r\n        } catch (Exception e) {\r\n            LOG.error(\"Failed to render tasks page with task type : \" + $(TASK_TYPE) + \" for job id : \" + $(JOB_ID), e);\r\n            badRequest(e.getMessage());\r\n        }\r\n    }\r\n    render(tasksPage());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "taskPage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends View> taskPage()\n{\r\n    return TaskPage.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "task",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void task()\n{\r\n    try {\r\n        requireTask();\r\n    } catch (Exception e) {\r\n        renderText(e.getMessage());\r\n        return;\r\n    }\r\n    if (app.getTask() != null) {\r\n        setTitle(join(\"Attempts for \", $(TASK_ID)));\r\n    }\r\n    render(taskPage());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "attemptsPage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends View> attemptsPage()\n{\r\n    return AttemptsPage.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "attempts",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void attempts()\n{\r\n    try {\r\n        requireJob();\r\n    } catch (Exception e) {\r\n        renderText(e.getMessage());\r\n        return;\r\n    }\r\n    if (app.getJob() != null) {\r\n        try {\r\n            String taskType = $(TASK_TYPE);\r\n            if (taskType.isEmpty()) {\r\n                throw new RuntimeException(\"missing task-type.\");\r\n            }\r\n            String attemptState = $(ATTEMPT_STATE);\r\n            if (attemptState.isEmpty()) {\r\n                throw new RuntimeException(\"missing attempt-state.\");\r\n            }\r\n            setTitle(join(attemptState, \" \", MRApps.taskType(taskType).toString(), \" attempts in \", $(JOB_ID)));\r\n            render(attemptsPage());\r\n        } catch (Exception e) {\r\n            LOG.error(\"Failed to render attempts page with task type : \" + $(TASK_TYPE) + \" for job id : \" + $(JOB_ID), e);\r\n            badRequest(e.getMessage());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "confPage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends View> confPage()\n{\r\n    return JobConfPage.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "conf",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void conf()\n{\r\n    try {\r\n        requireJob();\r\n    } catch (Exception e) {\r\n        renderText(e.getMessage());\r\n        return;\r\n    }\r\n    render(confPage());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "downloadConf",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void downloadConf()\n{\r\n    try {\r\n        requireJob();\r\n    } catch (Exception e) {\r\n        renderText(e.getMessage());\r\n        return;\r\n    }\r\n    writeJobConf();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "writeJobConf",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void writeJobConf()\n{\r\n    String jobId = $(JOB_ID);\r\n    assert (!jobId.isEmpty());\r\n    JobId jobID = MRApps.toJobID($(JOB_ID));\r\n    Job job = app.context.getJob(jobID);\r\n    assert (job != null);\r\n    try {\r\n        Configuration jobConf = job.loadConfFile();\r\n        response().setContentType(\"text/xml\");\r\n        response().setHeader(\"Content-Disposition\", \"attachment; filename=\" + jobId + \".xml\");\r\n        jobConf.writeXml(writer());\r\n    } catch (IOException e) {\r\n        LOG.error(\"Error reading/writing job\" + \" conf file for job: \" + jobId, e);\r\n        renderText(e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "badRequest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void badRequest(String s)\n{\r\n    setStatus(HttpServletResponse.SC_BAD_REQUEST);\r\n    String title = \"Bad request: \";\r\n    setTitle((s != null) ? join(title, s) : title);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "notFound",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void notFound(String s)\n{\r\n    setStatus(HttpServletResponse.SC_NOT_FOUND);\r\n    setTitle(join(\"Not found: \", s));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "accessDenied",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void accessDenied(String s)\n{\r\n    setStatus(HttpServletResponse.SC_FORBIDDEN);\r\n    setTitle(join(\"Access denied: \", s));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "checkAccess",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean checkAccess(Job job)\n{\r\n    String remoteUser = request().getRemoteUser();\r\n    UserGroupInformation callerUGI = null;\r\n    if (remoteUser != null) {\r\n        callerUGI = UserGroupInformation.createRemoteUser(remoteUser);\r\n    }\r\n    if (callerUGI != null && !job.checkAccess(callerUGI, JobACL.VIEW_JOB)) {\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "requireJob",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void requireJob()\n{\r\n    if ($(JOB_ID).isEmpty()) {\r\n        badRequest(\"missing job ID\");\r\n        throw new RuntimeException(\"Bad Request: Missing job ID\");\r\n    }\r\n    JobId jobID = MRApps.toJobID($(JOB_ID));\r\n    app.setJob(app.context.getJob(jobID));\r\n    if (app.getJob() == null) {\r\n        notFound($(JOB_ID));\r\n        throw new RuntimeException(\"Not Found: \" + $(JOB_ID));\r\n    }\r\n    Job job = app.context.getJob(jobID);\r\n    if (!checkAccess(job)) {\r\n        accessDenied(\"User \" + request().getRemoteUser() + \" does not have \" + \" permission to view job \" + $(JOB_ID));\r\n        throw new RuntimeException(\"Access denied: User \" + request().getRemoteUser() + \" does not have permission to view job \" + $(JOB_ID));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "requireTask",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void requireTask()\n{\r\n    if ($(TASK_ID).isEmpty()) {\r\n        badRequest(\"missing task ID\");\r\n        throw new RuntimeException(\"missing task ID\");\r\n    }\r\n    TaskId taskID = MRApps.toTaskID($(TASK_ID));\r\n    Job job = app.context.getJob(taskID.getJobId());\r\n    app.setJob(job);\r\n    if (app.getJob() == null) {\r\n        notFound(MRApps.toString(taskID.getJobId()));\r\n        throw new RuntimeException(\"Not Found: \" + $(JOB_ID));\r\n    } else {\r\n        app.setTask(app.getJob().getTask(taskID));\r\n        if (app.getTask() == null) {\r\n            notFound($(TASK_ID));\r\n            throw new RuntimeException(\"Not Found: \" + $(TASK_ID));\r\n        }\r\n    }\r\n    if (!checkAccess(job)) {\r\n        accessDenied(\"User \" + request().getRemoteUser() + \" does not have \" + \" permission to view job \" + $(JOB_ID));\r\n        throw new RuntimeException(\"Access denied: User \" + request().getRemoteUser() + \" does not have permission to view job \" + $(JOB_ID));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getMaxContainerCapability",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Resource getMaxContainerCapability()\n{\r\n    return maxContainerCapability;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "setMaxContainerCapability",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMaxContainerCapability(Resource maxContainerCapability)\n{\r\n    this.maxContainerCapability = maxContainerCapability;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskLogFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTaskLogFile(LogName filter)\n{\r\n    return ApplicationConstants.LOG_DIR_EXPANSION_VAR + Path.SEPARATOR + filter.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getChildEnvProp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getChildEnvProp(JobConf jobConf, boolean isMap)\n{\r\n    if (isMap) {\r\n        return JobConf.MAPRED_MAP_TASK_ENV;\r\n    }\r\n    return JobConf.MAPRED_REDUCE_TASK_ENV;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getChildEnvDefaultValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getChildEnvDefaultValue(JobConf jobConf)\n{\r\n    return jobConf.get(JobConf.MAPRED_TASK_ENV);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setVMEnv",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void setVMEnv(Map<String, String> environment, Task task)\n{\r\n    JobConf conf = task.conf;\r\n    boolean isMap = task.isMapTask();\r\n    String hadoopRootLoggerKey = \"HADOOP_ROOT_LOGGER\";\r\n    String hadoopClientOptsKey = \"HADOOP_CLIENT_OPTS\";\r\n    environment.remove(hadoopRootLoggerKey);\r\n    environment.remove(hadoopClientOptsKey);\r\n    MRApps.setEnvFromInputProperty(environment, getChildEnvProp(conf, isMap), getChildEnvDefaultValue(conf), conf);\r\n    if (!environment.containsKey(hadoopRootLoggerKey)) {\r\n        environment.put(hadoopRootLoggerKey, MRApps.getChildLogLevel(conf, task.isMapTask()) + \",console\");\r\n    }\r\n    if (!environment.containsKey(hadoopClientOptsKey)) {\r\n        String hadoopClientOptsValue = System.getenv(hadoopClientOptsKey);\r\n        if (hadoopClientOptsValue == null) {\r\n            hadoopClientOptsValue = \"\";\r\n        } else {\r\n            hadoopClientOptsValue = hadoopClientOptsValue + \" \";\r\n        }\r\n        environment.put(hadoopClientOptsKey, hadoopClientOptsValue);\r\n    }\r\n    environment.put(MRJobConfig.STDOUT_LOGFILE_ENV, getTaskLogFile(TaskLog.LogName.STDOUT));\r\n    environment.put(MRJobConfig.STDERR_LOGFILE_ENV, getTaskLogFile(TaskLog.LogName.STDERR));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getChildJavaOpts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getChildJavaOpts(JobConf jobConf, boolean isMapTask)\n{\r\n    return jobConf.getTaskJavaOpts(isMapTask ? TaskType.MAP : TaskType.REDUCE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getVMCommand",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "List<String> getVMCommand(InetSocketAddress taskAttemptListenerAddr, Task task, JVMId jvmID)\n{\r\n    TaskAttemptID attemptID = task.getTaskID();\r\n    JobConf conf = task.conf;\r\n    Vector<String> vargs = new Vector<String>(8);\r\n    vargs.add(MRApps.crossPlatformifyMREnv(task.conf, Environment.JAVA_HOME) + \"/bin/java\");\r\n    String javaOpts = getChildJavaOpts(conf, task.isMapTask());\r\n    javaOpts = javaOpts.replace(\"@taskid@\", attemptID.toString());\r\n    String[] javaOptsSplit = javaOpts.split(\" \");\r\n    for (int i = 0; i < javaOptsSplit.length; i++) {\r\n        vargs.add(javaOptsSplit[i]);\r\n    }\r\n    Path childTmpDir = new Path(MRApps.crossPlatformifyMREnv(conf, Environment.PWD), YarnConfiguration.DEFAULT_CONTAINER_TEMP_DIR);\r\n    vargs.add(\"-Djava.io.tmpdir=\" + childTmpDir);\r\n    MRApps.addLog4jSystemProperties(task, vargs, conf);\r\n    if (conf.getProfileEnabled()) {\r\n        if (conf.getProfileTaskRange(task.isMapTask()).isIncluded(task.getPartition())) {\r\n            final String profileParams = conf.get(task.isMapTask() ? MRJobConfig.TASK_MAP_PROFILE_PARAMS : MRJobConfig.TASK_REDUCE_PROFILE_PARAMS, conf.getProfileParams());\r\n            vargs.add(String.format(profileParams, getTaskLogFile(TaskLog.LogName.PROFILE)));\r\n        }\r\n    }\r\n    vargs.add(YarnChild.class.getName());\r\n    vargs.add(taskAttemptListenerAddr.getAddress().getHostAddress());\r\n    vargs.add(Integer.toString(taskAttemptListenerAddr.getPort()));\r\n    vargs.add(attemptID.toString());\r\n    vargs.add(String.valueOf(jvmID.getId()));\r\n    vargs.add(\"1>\" + getTaskLogFile(TaskLog.LogName.STDOUT));\r\n    vargs.add(\"2>\" + getTaskLogFile(TaskLog.LogName.STDERR));\r\n    StringBuilder mergedCommand = new StringBuilder();\r\n    for (CharSequence str : vargs) {\r\n        mergedCommand.append(str).append(\" \");\r\n    }\r\n    Vector<String> vargsFinal = new Vector<String>(1);\r\n    vargsFinal.add(mergedCommand.toString());\r\n    return vargsFinal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createRemoteTask",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Task createRemoteTask()\n{\r\n    MapTask mapTask = new MapTask(\"\", TypeConverter.fromYarn(getID()), partition, splitInfo.getSplitIndex(), 1);\r\n    mapTask.setUser(conf.get(MRJobConfig.USER_NAME));\r\n    mapTask.setConf(conf);\r\n    return mapTask;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getDiagnosticUpdate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDiagnosticUpdate()\n{\r\n    return this.diagnosticUpdate;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobId getJobID()\n{\r\n    return jobID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getHistoryEvent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HistoryEvent getHistoryEvent()\n{\r\n    return historyEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n    rmPollInterval = conf.getInt(MRJobConfig.MR_AM_TO_RM_HEARTBEAT_INTERVAL_MS, MRJobConfig.DEFAULT_MR_AM_TO_RM_HEARTBEAT_INTERVAL_MS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    scheduler = createSchedulerProxy();\r\n    JobID id = TypeConverter.fromYarn(this.applicationId);\r\n    JobId jobId = TypeConverter.toYarn(id);\r\n    job = context.getJob(jobId);\r\n    register();\r\n    startAllocatorThread();\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AppContext getContext()\n{\r\n    return context;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getJob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Job getJob()\n{\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getApplicationProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getApplicationProgress()\n{\r\n    return this.job.getProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "register",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void register()\n{\r\n    InetSocketAddress serviceAddr = null;\r\n    if (clientService != null) {\r\n        serviceAddr = clientService.getBindAddress();\r\n    }\r\n    try {\r\n        RegisterApplicationMasterRequest request = recordFactory.newRecordInstance(RegisterApplicationMasterRequest.class);\r\n        if (serviceAddr != null) {\r\n            request.setHost(serviceAddr.getHostName());\r\n            request.setRpcPort(serviceAddr.getPort());\r\n            request.setTrackingUrl(MRWebAppUtil.getAMWebappScheme(getConfig()) + serviceAddr.getHostName() + \":\" + clientService.getHttpPort());\r\n        }\r\n        RegisterApplicationMasterResponse response = scheduler.registerApplicationMaster(request);\r\n        isApplicationMasterRegistered = true;\r\n        maxContainerCapability = response.getMaximumResourceCapability();\r\n        this.context.getClusterInfo().setMaxContainerCapability(maxContainerCapability);\r\n        if (UserGroupInformation.isSecurityEnabled()) {\r\n            setClientToAMToken(response.getClientToAMTokenMasterKey());\r\n        }\r\n        this.applicationACLs = response.getApplicationACLs();\r\n        LOG.info(\"maxContainerCapability: \" + maxContainerCapability);\r\n        String queue = response.getQueue();\r\n        LOG.info(\"queue: \" + queue);\r\n        job.setQueueName(queue);\r\n        this.schedulerResourceTypes.addAll(response.getSchedulerResourceTypes());\r\n    } catch (Exception are) {\r\n        LOG.error(\"Exception while registering\", are);\r\n        throw new YarnRuntimeException(are);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "setClientToAMToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setClientToAMToken(ByteBuffer clientToAMTokenMasterKey)\n{\r\n    byte[] key = clientToAMTokenMasterKey.array();\r\n    context.getClientToAMTokenSecretManager().setMasterKey(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "unregister",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void unregister()\n{\r\n    try {\r\n        doUnregistration();\r\n    } catch (Exception are) {\r\n        LOG.error(\"Exception while unregistering \", are);\r\n        RunningAppContext raContext = (RunningAppContext) context;\r\n        raContext.resetIsLastAMRetry();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "doUnregistration",
  "errType" : [ "ApplicationMasterNotRegisteredException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void doUnregistration() throws YarnException, IOException, InterruptedException\n{\r\n    FinalApplicationStatus finishState = FinalApplicationStatus.UNDEFINED;\r\n    JobImpl jobImpl = (JobImpl) job;\r\n    if (jobImpl.getInternalState() == JobStateInternal.SUCCEEDED) {\r\n        finishState = FinalApplicationStatus.SUCCEEDED;\r\n    } else if (jobImpl.getInternalState() == JobStateInternal.KILLED || (jobImpl.getInternalState() == JobStateInternal.RUNNING && isSignalled)) {\r\n        finishState = FinalApplicationStatus.KILLED;\r\n    } else if (jobImpl.getInternalState() == JobStateInternal.FAILED || jobImpl.getInternalState() == JobStateInternal.ERROR) {\r\n        finishState = FinalApplicationStatus.FAILED;\r\n    }\r\n    StringBuffer sb = new StringBuffer();\r\n    for (String s : job.getDiagnostics()) {\r\n        sb.append(s).append(\"\\n\");\r\n    }\r\n    LOG.info(\"Setting job diagnostics to \" + sb.toString());\r\n    String historyUrl = context.getHistoryUrl();\r\n    LOG.info(\"History url is \" + historyUrl);\r\n    FinishApplicationMasterRequest request = FinishApplicationMasterRequest.newInstance(finishState, sb.toString(), historyUrl);\r\n    try {\r\n        while (true) {\r\n            FinishApplicationMasterResponse response = scheduler.finishApplicationMaster(request);\r\n            if (response.getIsUnregistered()) {\r\n                RunningAppContext raContext = (RunningAppContext) context;\r\n                raContext.markSuccessfulUnregistration();\r\n                break;\r\n            }\r\n            LOG.info(\"Waiting for application to be successfully unregistered.\");\r\n            Thread.sleep(rmPollInterval);\r\n        }\r\n    } catch (ApplicationMasterNotRegisteredException e) {\r\n        register();\r\n        doUnregistration();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getMaxContainerCapability",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Resource getMaxContainerCapability()\n{\r\n    return maxContainerCapability;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "serviceStop",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    if (stopped.getAndSet(true)) {\r\n        return;\r\n    }\r\n    if (allocatorThread != null) {\r\n        allocatorThread.interrupt();\r\n        try {\r\n            allocatorThread.join();\r\n        } catch (InterruptedException ie) {\r\n            LOG.warn(\"InterruptedException while stopping\", ie);\r\n        }\r\n    }\r\n    if (isApplicationMasterRegistered && shouldUnregister) {\r\n        unregister();\r\n    }\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "startAllocatorThread",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void startAllocatorThread()\n{\r\n    allocatorThread = new Thread(new AllocatorRunnable());\r\n    allocatorThread.setName(\"RMCommunicator Allocator\");\r\n    allocatorThread.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "createSchedulerProxy",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ApplicationMasterProtocol createSchedulerProxy()\n{\r\n    final Configuration conf = getConfig();\r\n    try {\r\n        return ClientRMProxy.createRMProxy(conf, ApplicationMasterProtocol.class);\r\n    } catch (IOException e) {\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "heartbeat",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void heartbeat() throws Exception",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "executeHeartbeatCallbacks",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void executeHeartbeatCallbacks()\n{\r\n    Runnable callback = null;\r\n    while ((callback = heartbeatCallbacks.poll()) != null) {\r\n        callback.run();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getLastHeartbeatTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLastHeartbeatTime()\n{\r\n    return lastHeartbeatTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "runOnNextHeartbeat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void runOnNextHeartbeat(Runnable callback)\n{\r\n    heartbeatCallbacks.add(callback);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "setShouldUnregister",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setShouldUnregister(boolean shouldUnregister)\n{\r\n    this.shouldUnregister = shouldUnregister;\r\n    LOG.info(\"RMCommunicator notified that shouldUnregistered is: \" + shouldUnregister);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "setSignalled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSignalled(boolean isSignalled)\n{\r\n    this.isSignalled = isSignalled;\r\n    LOG.info(\"RMCommunicator notified that isSignalled is: \" + isSignalled);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "isApplicationMasterRegistered",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isApplicationMasterRegistered()\n{\r\n    return isApplicationMasterRegistered;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getSchedulerResourceTypes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EnumSet<SchedulerResourceTypes> getSchedulerResourceTypes()\n{\r\n    return schedulerResourceTypes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobId getJobId()\n{\r\n    return jobID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "getForecastEntry",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SimpleExponentialSmoothing getForecastEntry(final TaskAttemptId attemptID)\n{\r\n    AtomicReference<SimpleExponentialSmoothing> entryRef = estimates.get(attemptID);\r\n    if (entryRef == null) {\r\n        return null;\r\n    }\r\n    return entryRef.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "incorporateReading",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void incorporateReading(final TaskAttemptId attemptID, final float newRawData, final long newTimeStamp)\n{\r\n    SimpleExponentialSmoothing foreCastEntry = getForecastEntry(attemptID);\r\n    if (foreCastEntry == null) {\r\n        Long tStartTime = startTimes.get(attemptID);\r\n        if (tStartTime == null) {\r\n            return;\r\n        }\r\n        estimates.putIfAbsent(attemptID, new AtomicReference<>(SimpleExponentialSmoothing.createForecast(constTime, skipCount, stagnatedWindow, tStartTime)));\r\n        incorporateReading(attemptID, newRawData, newTimeStamp);\r\n        return;\r\n    }\r\n    foreCastEntry.incorporateReading(newTimeStamp, newRawData);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "contextualize",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void contextualize(final Configuration conf, final AppContext context)\n{\r\n    super.contextualize(conf, context);\r\n    constTime = conf.getLong(MRJobConfig.MR_AM_TASK_ESTIMATOR_SIMPLE_SMOOTH_LAMBDA_MS, MRJobConfig.DEFAULT_MR_AM_TASK_ESTIMATOR_SIMPLE_SMOOTH_LAMBDA_MS);\r\n    stagnatedWindow = Math.max(2 * constTime, conf.getLong(MRJobConfig.MR_AM_TASK_ESTIMATOR_SIMPLE_SMOOTH_STAGNATED_MS, MRJobConfig.DEFAULT_MR_AM_TASK_ESTIMATOR_SIMPLE_SMOOTH_STAGNATED_MS));\r\n    skipCount = conf.getInt(MRJobConfig.MR_AM_TASK_ESTIMATOR_SIMPLE_SMOOTH_SKIP_INITIALS, MRJobConfig.DEFAULT_MR_AM_TASK_ESTIMATOR_SIMPLE_SMOOTH_INITIALS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "estimatedRuntime",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long estimatedRuntime(final TaskAttemptId id)\n{\r\n    SimpleExponentialSmoothing foreCastEntry = getForecastEntry(id);\r\n    if (foreCastEntry == null) {\r\n        return DEFAULT_ESTIMATE_RUNTIME;\r\n    }\r\n    double remainingWork = Math.max(0.0, Math.min(1.0, 1.0 - foreCastEntry.getRawData()));\r\n    double forecast = Math.max(DEFAULT_PROGRESS_VALUE, foreCastEntry.getForecast());\r\n    long remainingTime = (long) (remainingWork / forecast);\r\n    long estimatedRuntime = remainingTime + foreCastEntry.getTimeStamp() - foreCastEntry.getStartTime();\r\n    return estimatedRuntime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "estimatedNewAttemptRuntime",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long estimatedNewAttemptRuntime(final TaskId id)\n{\r\n    DataStatistics statistics = dataStatisticsForTask(id);\r\n    if (statistics == null) {\r\n        return DEFAULT_ESTIMATE_RUNTIME;\r\n    }\r\n    double statsMeanCI = statistics.meanCI();\r\n    double expectedVal = statsMeanCI + Math.min(statsMeanCI * CONFIDENCE_INTERVAL_FACTOR, statistics.std() / 2);\r\n    return (long) (expectedVal);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "hasStagnatedProgress",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean hasStagnatedProgress(final TaskAttemptId id, final long timeStamp)\n{\r\n    SimpleExponentialSmoothing foreCastEntry = getForecastEntry(id);\r\n    if (foreCastEntry == null) {\r\n        return false;\r\n    }\r\n    return foreCastEntry.isDataStagnated(timeStamp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "runtimeEstimateVariance",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long runtimeEstimateVariance(final TaskAttemptId id)\n{\r\n    SimpleExponentialSmoothing forecastEntry = getForecastEntry(id);\r\n    if (forecastEntry == null) {\r\n        return DEFAULT_ESTIMATE_RUNTIME;\r\n    }\r\n    double forecast = forecastEntry.getForecast();\r\n    if (forecastEntry.isDefaultForecast(forecast)) {\r\n        return DEFAULT_ESTIMATE_RUNTIME;\r\n    }\r\n    return 0L;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "updateAttempt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateAttempt(final TaskAttemptStatus status, final long timestamp)\n{\r\n    super.updateAttempt(status, timestamp);\r\n    TaskAttemptId attemptID = status.id;\r\n    float progress = status.progress;\r\n    incorporateReading(attemptID, progress, timestamp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "setState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setState(String state)\n{\r\n    this.state = state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getState()\n{\r\n    return this.state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getTaskID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskId getTaskID()\n{\r\n    return taskID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "serviceInit",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n    commitThreadCancelTimeoutMs = conf.getInt(MRJobConfig.MR_AM_COMMITTER_CANCEL_TIMEOUT_MS, MRJobConfig.DEFAULT_MR_AM_COMMITTER_CANCEL_TIMEOUT_MS);\r\n    commitWindowMs = conf.getLong(MRJobConfig.MR_AM_COMMIT_WINDOW_MS, MRJobConfig.DEFAULT_MR_AM_COMMIT_WINDOW_MS);\r\n    try {\r\n        fs = FileSystem.get(conf);\r\n        JobID id = TypeConverter.fromYarn(context.getApplicationID());\r\n        JobId jobId = TypeConverter.toYarn(id);\r\n        String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n        startCommitFile = MRApps.getStartJobCommitFile(conf, user, jobId);\r\n        endCommitSuccessFile = MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\r\n        endCommitFailureFile = MRApps.getEndJobCommitFailureFile(conf, user, jobId);\r\n    } catch (IOException e) {\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "serviceStart",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    ThreadFactoryBuilder tfBuilder = new ThreadFactoryBuilder().setNameFormat(\"CommitterEvent Processor #%d\");\r\n    if (jobClassLoader != null) {\r\n        ThreadFactory backingTf = new ThreadFactory() {\r\n\r\n            @Override\r\n            public Thread newThread(Runnable r) {\r\n                Thread thread = new Thread(r);\r\n                thread.setContextClassLoader(jobClassLoader);\r\n                return thread;\r\n            }\r\n        };\r\n        tfBuilder.setThreadFactory(backingTf);\r\n    }\r\n    ThreadFactory tf = tfBuilder.build();\r\n    launcherPool = new HadoopThreadPoolExecutor(5, 5, 1, TimeUnit.HOURS, new LinkedBlockingQueue<Runnable>(), tf);\r\n    eventHandlingThread = new Thread(new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            CommitterEvent event = null;\r\n            while (!stopped.get() && !Thread.currentThread().isInterrupted()) {\r\n                try {\r\n                    event = eventQueue.take();\r\n                } catch (InterruptedException e) {\r\n                    if (!stopped.get()) {\r\n                        LOG.error(\"Returning, interrupted : \" + e);\r\n                    }\r\n                    return;\r\n                }\r\n                launcherPool.execute(new EventProcessor(event));\r\n            }\r\n        }\r\n    });\r\n    eventHandlingThread.setName(\"CommitterEvent Handler\");\r\n    eventHandlingThread.start();\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "handle",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void handle(CommitterEvent event)\n{\r\n    try {\r\n        eventQueue.put(event);\r\n    } catch (InterruptedException e) {\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    if (stopped.getAndSet(true)) {\r\n        return;\r\n    }\r\n    if (eventHandlingThread != null) {\r\n        eventHandlingThread.interrupt();\r\n    }\r\n    if (launcherPool != null) {\r\n        launcherPool.shutdown();\r\n    }\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "jobCommitStarted",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void jobCommitStarted() throws IOException\n{\r\n    if (jobCommitThread != null) {\r\n        throw new IOException(\"Commit while another commit thread active: \" + jobCommitThread.toString());\r\n    }\r\n    jobCommitThread = Thread.currentThread();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "jobCommitEnded",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void jobCommitEnded()\n{\r\n    if (jobCommitThread == Thread.currentThread()) {\r\n        jobCommitThread = null;\r\n        notifyAll();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "cancelJobCommit",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void cancelJobCommit()\n{\r\n    Thread threadCommitting = jobCommitThread;\r\n    if (threadCommitting != null && threadCommitting.isAlive()) {\r\n        LOG.info(\"Cancelling commit\");\r\n        threadCommitting.interrupt();\r\n        long now = context.getClock().getTime();\r\n        long timeoutTimestamp = now + commitThreadCancelTimeoutMs;\r\n        try {\r\n            while (jobCommitThread == threadCommitting && now > timeoutTimestamp) {\r\n                wait(now - timeoutTimestamp);\r\n                now = context.getClock().getTime();\r\n            }\r\n        } catch (InterruptedException e) {\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n    taskTimeOut = conf.getLong(MRJobConfig.TASK_TIMEOUT, MRJobConfig.DEFAULT_TASK_TIMEOUT_MILLIS);\r\n    unregisterTimeOut = conf.getLong(MRJobConfig.TASK_EXIT_TIMEOUT, MRJobConfig.TASK_EXIT_TIMEOUT_DEFAULT);\r\n    taskStuckTimeOut = conf.getLong(MRJobConfig.TASK_STUCK_TIMEOUT_MS, MRJobConfig.DEFAULT_TASK_STUCK_TIMEOUT_MS);\r\n    long taskProgressReportIntervalMillis = MRJobConfUtil.getTaskProgressReportInterval(conf);\r\n    long minimumTaskTimeoutAllowed = taskProgressReportIntervalMillis * 2;\r\n    if (taskTimeOut < minimumTaskTimeoutAllowed) {\r\n        taskTimeOut = minimumTaskTimeoutAllowed;\r\n        LOG.info(\"Task timeout must be as least twice as long as the task \" + \"status report interval. Setting task timeout to \" + taskTimeOut);\r\n    }\r\n    taskTimeOutCheckInterval = conf.getInt(MRJobConfig.TASK_TIMEOUT_CHECK_INTERVAL_MS, 30 * 1000);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    lostTaskCheckerThread = new Thread(new PingChecker());\r\n    lostTaskCheckerThread.setName(\"TaskHeartbeatHandler PingChecker\");\r\n    lostTaskCheckerThread.start();\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    stopped = true;\r\n    if (lostTaskCheckerThread != null) {\r\n        lostTaskCheckerThread.interrupt();\r\n    }\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "progressing",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void progressing(TaskAttemptId attemptID)\n{\r\n    ReportTime time = runningAttempts.get(attemptID);\r\n    if (time != null) {\r\n        time.reported.compareAndSet(false, true);\r\n        time.setLastProgress(clock.getTime());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "register",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void register(TaskAttemptId attemptID)\n{\r\n    runningAttempts.put(attemptID, new ReportTime(clock.getTime()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "unregister",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void unregister(TaskAttemptId attemptID)\n{\r\n    runningAttempts.remove(attemptID);\r\n    recentlyUnregisteredAttempts.put(attemptID, new ReportTime(clock.getTime()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "hasRecentlyUnregistered",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasRecentlyUnregistered(TaskAttemptId attemptID)\n{\r\n    return recentlyUnregisteredAttempts.containsKey(attemptID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getRunningAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ConcurrentMap<TaskAttemptId, ReportTime> getRunningAttempts()\n{\r\n    return runningAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getTaskTimeOut",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTaskTimeOut()\n{\r\n    return taskTimeOut;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getMaxAttempts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMaxAttempts()\n{\r\n    return conf.getInt(MRJobConfig.REDUCE_MAX_ATTEMPTS, 4);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createAttempt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptImpl createAttempt()\n{\r\n    return new ReduceTaskAttemptImpl(getID(), nextAttemptNumber, eventHandler, jobFile, partition, numMapTasks, conf, taskAttemptListener, jobToken, credentials, clock, appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskType getType()\n{\r\n    return TaskType.REDUCE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void init(AppContext context)\n{\r\n    this.eventHandler = context.getEventHandler();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "preempt",
  "errType" : null,
  "containingMethodsNum" : 36,
  "sourceCodeText" : "void preempt(Context ctxt, PreemptionMessage preemptionRequests)\n{\r\n    if (preemptionRequests != null) {\r\n        StrictPreemptionContract cStrict = preemptionRequests.getStrictContract();\r\n        if (cStrict != null && cStrict.getContainers() != null && cStrict.getContainers().size() > 0) {\r\n            LOG.info(\"strict preemption :\" + preemptionRequests.getStrictContract().getContainers().size() + \" containers to kill\");\r\n            for (PreemptionContainer c : preemptionRequests.getStrictContract().getContainers()) {\r\n                ContainerId reqCont = c.getId();\r\n                TaskAttemptId reqTask = ctxt.getTaskAttempt(reqCont);\r\n                if (reqTask != null) {\r\n                    if (org.apache.hadoop.mapreduce.v2.api.records.TaskType.REDUCE.equals(reqTask.getTaskId().getTaskType())) {\r\n                        toBePreempted.add(reqTask);\r\n                        LOG.info(\"preempting \" + reqCont + \" running task:\" + reqTask);\r\n                    } else {\r\n                        LOG.info(\"NOT preempting \" + reqCont + \" running task:\" + reqTask);\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        PreemptionContract cNegot = preemptionRequests.getContract();\r\n        if (cNegot != null && cNegot.getResourceRequest() != null && cNegot.getResourceRequest().size() > 0 && cNegot.getContainers() != null && cNegot.getContainers().size() > 0) {\r\n            LOG.info(\"negotiable preemption :\" + preemptionRequests.getContract().getResourceRequest().size() + \" resourceReq, \" + preemptionRequests.getContract().getContainers().size() + \" containers\");\r\n            List<PreemptionResourceRequest> reqResources = preemptionRequests.getContract().getResourceRequest();\r\n            int pendingPreemptionRam = 0;\r\n            int pendingPreemptionCores = 0;\r\n            for (Resource r : pendingFlexiblePreemptions.values()) {\r\n                pendingPreemptionRam += r.getMemorySize();\r\n                pendingPreemptionCores += r.getVirtualCores();\r\n            }\r\n            for (PreemptionResourceRequest rr : reqResources) {\r\n                ResourceRequest reqRsrc = rr.getResourceRequest();\r\n                if (!ResourceRequest.ANY.equals(reqRsrc.getResourceName())) {\r\n                    continue;\r\n                }\r\n                LOG.info(\"ResourceRequest:\" + reqRsrc);\r\n                int reqCont = reqRsrc.getNumContainers();\r\n                long reqMem = reqRsrc.getCapability().getMemorySize();\r\n                long totalMemoryToRelease = reqCont * reqMem;\r\n                int reqCores = reqRsrc.getCapability().getVirtualCores();\r\n                int totalCoresToRelease = reqCont * reqCores;\r\n                if (pendingPreemptionRam > 0) {\r\n                    totalMemoryToRelease -= pendingPreemptionRam;\r\n                    pendingPreemptionRam -= totalMemoryToRelease;\r\n                }\r\n                if (pendingPreemptionCores > 0) {\r\n                    totalCoresToRelease -= pendingPreemptionCores;\r\n                    pendingPreemptionCores -= totalCoresToRelease;\r\n                }\r\n                List<Container> listOfCont = ctxt.getContainers(TaskType.REDUCE);\r\n                Collections.sort(listOfCont, new Comparator<Container>() {\r\n\r\n                    @Override\r\n                    public int compare(final Container o1, final Container o2) {\r\n                        return o2.getId().compareTo(o1.getId());\r\n                    }\r\n                });\r\n                for (Container cont : listOfCont) {\r\n                    if (totalMemoryToRelease <= 0 && totalCoresToRelease <= 0) {\r\n                        break;\r\n                    }\r\n                    TaskAttemptId reduceId = ctxt.getTaskAttempt(cont.getId());\r\n                    int cMem = (int) cont.getResource().getMemorySize();\r\n                    int cCores = cont.getResource().getVirtualCores();\r\n                    if (!toBePreempted.contains(reduceId)) {\r\n                        totalMemoryToRelease -= cMem;\r\n                        totalCoresToRelease -= cCores;\r\n                        toBePreempted.add(reduceId);\r\n                        pendingFlexiblePreemptions.put(reduceId, cont.getResource());\r\n                    }\r\n                    LOG.info(\"ResourceRequest:\" + reqRsrc + \" satisfied preempting \" + reduceId);\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "handleFailedContainer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void handleFailedContainer(TaskAttemptId attemptID)\n{\r\n    toBePreempted.remove(attemptID);\r\n    checkpoints.remove(attemptID.getTaskId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "handleCompletedContainer",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void handleCompletedContainer(TaskAttemptId attemptID)\n{\r\n    LOG.info(\" task completed:\" + attemptID);\r\n    toBePreempted.remove(attemptID);\r\n    pendingFlexiblePreemptions.remove(attemptID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "isPreempted",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isPreempted(TaskAttemptId yarnAttemptID)\n{\r\n    if (toBePreempted.contains(yarnAttemptID)) {\r\n        updatePreemptionCounters(yarnAttemptID);\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "reportSuccessfulPreemption",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reportSuccessfulPreemption(TaskAttemptId taskAttemptID)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "getCheckpointID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskCheckpointID getCheckpointID(TaskId taskId)\n{\r\n    return checkpoints.get(taskId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "setCheckpointID",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setCheckpointID(TaskId taskId, TaskCheckpointID cid)\n{\r\n    checkpoints.put(taskId, cid);\r\n    if (cid != null) {\r\n        updateCheckpointCounters(taskId, cid);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "updateCheckpointCounters",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void updateCheckpointCounters(TaskId taskId, TaskCheckpointID cid)\n{\r\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(taskId.getJobId());\r\n    jce.addCounterUpdate(JobCounter.CHECKPOINTS, 1);\r\n    eventHandler.handle(jce);\r\n    jce = new JobCounterUpdateEvent(taskId.getJobId());\r\n    jce.addCounterUpdate(JobCounter.CHECKPOINT_BYTES, cid.getCheckpointBytes());\r\n    eventHandler.handle(jce);\r\n    jce = new JobCounterUpdateEvent(taskId.getJobId());\r\n    jce.addCounterUpdate(JobCounter.CHECKPOINT_TIME, cid.getCheckpointTime());\r\n    eventHandler.handle(jce);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "updatePreemptionCounters",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void updatePreemptionCounters(TaskAttemptId yarnAttemptID)\n{\r\n    if (!countedPreemptions.contains(yarnAttemptID)) {\r\n        countedPreemptions.add(yarnAttemptID);\r\n        JobCounterUpdateEvent jce = new JobCounterUpdateEvent(yarnAttemptID.getTaskId().getJobId());\r\n        jce.addCounterUpdate(JobCounter.TASKS_REQ_PREEMPT, 1);\r\n        eventHandler.handle(jce);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getId()\n{\r\n    return this.appId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return this.name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUser()\n{\r\n    return this.user;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return this.startedOn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getElapsedTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getElapsedTime()\n{\r\n    return this.elapsedTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "isFastFail",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isFastFail()\n{\r\n    return fastFail;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    String jobID = $(JOB_ID);\r\n    set(TITLE, jobID.isEmpty() ? \"Bad request: missing job ID\" : join(\"Configuration for MapReduce Job \", $(JOB_ID)));\r\n    commonPreHead(html);\r\n    set(initID(ACCORDION, \"nav\"), \"{autoHeight:false, active:2}\");\r\n    set(DATATABLES_ID, \"conf\");\r\n    set(initID(DATATABLES, \"conf\"), confTableInit());\r\n    set(postInitID(DATATABLES, \"conf\"), confPostTableInit());\r\n    setTableStyles(html, \"conf\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return ConfBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "confTableInit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String confTableInit()\n{\r\n    return tableInit().append(\"}\").toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "confPostTableInit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String confPostTableInit()\n{\r\n    return \"var confInitVals = new Array();\\n\" + \"$('tfoot input').keyup( function () \\n{\" + \"  confDataTable.fnFilter( this.value, $('tfoot input').index(this) );\\n\" + \"} );\\n\" + \"$('tfoot input').each( function (i) {\\n\" + \"  confInitVals[i] = this.value;\\n\" + \"} );\\n\" + \"$('tfoot input').focus( function () {\\n\" + \"  if ( this.className == 'search_init' )\\n\" + \"  {\\n\" + \"    this.className = '';\\n\" + \"    this.value = '';\\n\" + \"  }\\n\" + \"} );\\n\" + \"$('tfoot input').blur( function (i) {\\n\" + \"  if ( this.value == '' )\\n\" + \"  {\\n\" + \"    this.className = 'search_init';\\n\" + \"    this.value = confInitVals[$('tfoot input').index(this)];\\n\" + \"  }\\n\" + \"} );\\n\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getTaskAttemptStatusRef",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AtomicReference<TaskAttemptStatus> getTaskAttemptStatusRef()\n{\r\n    return taskAttemptStatusRef;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "canSpeculate",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean canSpeculate(AppContext context, TaskId taskID)\n{\r\n    JobId jobID = taskID.getJobId();\r\n    Job job = context.getJob(jobID);\r\n    Task task = job.getTask(taskID);\r\n    return task.getAttempts().size() == 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getStateMachine",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StateMachine<JobStateInternal, JobEventType, JobEvent> getStateMachine()\n{\r\n    return stateMachine;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobId getID()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getEventHandler",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventHandler getEventHandler()\n{\r\n    return this.eventHandler;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getJobContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobContext getJobContext()\n{\r\n    return this.jobContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "checkAccess",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean checkAccess(UserGroupInformation callerUGI, JobACL jobOperation)\n{\r\n    AccessControlList jobACL = jobACLs.get(jobOperation);\r\n    if (jobACL == null) {\r\n        return true;\r\n    }\r\n    return aclsManager.checkAccess(callerUGI, jobOperation, userName, jobACL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getTask",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Task getTask(TaskId taskID)\n{\r\n    readLock.lock();\r\n    try {\r\n        return tasks.get(taskID);\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getCompletedMaps",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getCompletedMaps()\n{\r\n    readLock.lock();\r\n    try {\r\n        return succeededMapTaskCount + failedMapTaskCount + killedMapTaskCount;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getCompletedReduces",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getCompletedReduces()\n{\r\n    readLock.lock();\r\n    try {\r\n        return succeededReduceTaskCount + failedReduceTaskCount + killedReduceTaskCount;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "isUber",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isUber()\n{\r\n    return isUber;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getAllCounters",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Counters getAllCounters()\n{\r\n    readLock.lock();\r\n    try {\r\n        JobStateInternal state = getInternalState();\r\n        if (state == JobStateInternal.ERROR || state == JobStateInternal.FAILED || state == JobStateInternal.KILLED || state == JobStateInternal.SUCCEEDED) {\r\n            this.mayBeConstructFinalFullCounters();\r\n            return fullCounters;\r\n        }\r\n        Counters counters = new Counters();\r\n        counters.incrAllCounters(jobCounters);\r\n        return incrTaskCounters(counters, tasks.values());\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "incrTaskCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counters incrTaskCounters(Counters counters, Collection<Task> tasks)\n{\r\n    for (Task task : tasks) {\r\n        counters.incrAllCounters(task.getCounters());\r\n    }\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getTaskAttemptCompletionEvents",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "TaskAttemptCompletionEvent[] getTaskAttemptCompletionEvents(int fromEventId, int maxEvents)\n{\r\n    TaskAttemptCompletionEvent[] events = EMPTY_TASK_ATTEMPT_COMPLETION_EVENTS;\r\n    readLock.lock();\r\n    try {\r\n        if (taskAttemptCompletionEvents.size() > fromEventId) {\r\n            int actualMax = Math.min(maxEvents, (taskAttemptCompletionEvents.size() - fromEventId));\r\n            events = taskAttemptCompletionEvents.subList(fromEventId, actualMax + fromEventId).toArray(events);\r\n        }\r\n        return events;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getMapAttemptCompletionEvents",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "TaskCompletionEvent[] getMapAttemptCompletionEvents(int startIndex, int maxEvents)\n{\r\n    TaskCompletionEvent[] events = EMPTY_TASK_COMPLETION_EVENTS;\r\n    readLock.lock();\r\n    try {\r\n        if (mapAttemptCompletionEvents.size() > startIndex) {\r\n            int actualMax = Math.min(maxEvents, (mapAttemptCompletionEvents.size() - startIndex));\r\n            events = mapAttemptCompletionEvents.subList(startIndex, actualMax + startIndex).toArray(events);\r\n        }\r\n        return events;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<String> getDiagnostics()\n{\r\n    readLock.lock();\r\n    try {\r\n        return diagnostics;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getReport",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "JobReport getReport()\n{\r\n    readLock.lock();\r\n    try {\r\n        JobState state = getState();\r\n        String jobFile = remoteJobConfFile == null ? \"\" : remoteJobConfFile.toString();\r\n        StringBuilder diagsb = new StringBuilder();\r\n        for (String s : getDiagnostics()) {\r\n            diagsb.append(s).append(\"\\n\");\r\n        }\r\n        if (getInternalState() == JobStateInternal.NEW) {\r\n            return MRBuilderUtils.newJobReport(jobId, jobName, reporterUserName, state, appSubmitTime, startTime, finishTime, setupProgress, 0.0f, 0.0f, cleanupProgress, jobFile, amInfos, isUber, diagsb.toString());\r\n        }\r\n        computeProgress();\r\n        JobReport report = MRBuilderUtils.newJobReport(jobId, jobName, reporterUserName, state, appSubmitTime, startTime, finishTime, setupProgress, this.mapProgress, this.reduceProgress, cleanupProgress, jobFile, amInfos, isUber, diagsb.toString(), jobPriority);\r\n        return report;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "float getProgress()\n{\r\n    this.readLock.lock();\r\n    try {\r\n        computeProgress();\r\n        return (this.setupProgress * this.setupWeight + this.cleanupProgress * this.cleanupWeight + this.mapProgress * this.mapWeight + this.reduceProgress * this.reduceWeight);\r\n    } finally {\r\n        this.readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "computeProgress",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void computeProgress()\n{\r\n    this.readLock.lock();\r\n    try {\r\n        float mapProgress = 0f;\r\n        float reduceProgress = 0f;\r\n        for (Task task : this.tasks.values()) {\r\n            if (task.getType() == TaskType.MAP) {\r\n                mapProgress += (task.isFinished() ? 1f : task.getProgress());\r\n            } else {\r\n                reduceProgress += (task.isFinished() ? 1f : task.getProgress());\r\n            }\r\n        }\r\n        if (this.numMapTasks != 0) {\r\n            mapProgress = mapProgress / this.numMapTasks;\r\n        }\r\n        if (this.numReduceTasks != 0) {\r\n            reduceProgress = reduceProgress / this.numReduceTasks;\r\n        }\r\n        this.mapProgress = mapProgress;\r\n        this.reduceProgress = reduceProgress;\r\n    } finally {\r\n        this.readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getTasks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<TaskId, Task> getTasks()\n{\r\n    synchronized (tasksSyncHandle) {\r\n        lazyTasksCopyNeeded = true;\r\n        return Collections.unmodifiableMap(tasks);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getTasks",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<TaskId, Task> getTasks(TaskType taskType)\n{\r\n    Map<TaskId, Task> localTasksCopy = tasks;\r\n    Map<TaskId, Task> result = new HashMap<TaskId, Task>();\r\n    Set<TaskId> tasksOfGivenType = null;\r\n    readLock.lock();\r\n    try {\r\n        if (TaskType.MAP == taskType) {\r\n            tasksOfGivenType = mapTasks;\r\n        } else {\r\n            tasksOfGivenType = reduceTasks;\r\n        }\r\n        for (TaskId taskID : tasksOfGivenType) result.put(taskID, localTasksCopy.get(taskID));\r\n        return result;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "JobState getState()\n{\r\n    readLock.lock();\r\n    try {\r\n        JobState state = getExternalState(getInternalState());\r\n        if (!appContext.hasSuccessfullyUnregistered() && (state == JobState.SUCCEEDED || state == JobState.FAILED || state == JobState.KILLED || state == JobState.ERROR)) {\r\n            return lastNonFinalState;\r\n        } else {\r\n            return state;\r\n        }\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "scheduleTasks",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void scheduleTasks(Set<TaskId> taskIDs, boolean recoverTaskOutput)\n{\r\n    for (TaskId taskID : taskIDs) {\r\n        TaskInfo taskInfo = completedTasksFromPreviousRun.remove(taskID);\r\n        if (taskInfo != null) {\r\n            eventHandler.handle(new TaskRecoverEvent(taskID, taskInfo, committer, recoverTaskOutput));\r\n        } else {\r\n            eventHandler.handle(new TaskEvent(taskID, TaskEventType.T_SCHEDULE));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "handle",
  "errType" : [ "InvalidStateTransitionException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void handle(JobEvent event)\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Processing \" + event.getJobId() + \" of type \" + event.getType());\r\n    }\r\n    try {\r\n        writeLock.lock();\r\n        JobStateInternal oldState = getInternalState();\r\n        try {\r\n            getStateMachine().doTransition(event.getType(), event);\r\n        } catch (InvalidStateTransitionException e) {\r\n            LOG.error(\"Can't handle this event at current state\", e);\r\n            addDiagnostic(\"Invalid event \" + event.getType() + \" on Job \" + this.jobId);\r\n            eventHandler.handle(new JobEvent(this.jobId, JobEventType.INTERNAL_ERROR));\r\n        }\r\n        if (oldState != getInternalState()) {\r\n            LOG.info(jobId + \"Job Transitioned from \" + oldState + \" to \" + getInternalState());\r\n            rememberLastNonFinalState(oldState);\r\n        }\r\n    } finally {\r\n        writeLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "rememberLastNonFinalState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void rememberLastNonFinalState(JobStateInternal stateInternal)\n{\r\n    JobState state = getExternalState(stateInternal);\r\n    if (state != JobState.SUCCEEDED && state != JobState.FAILED && state != JobState.KILLED && state != JobState.ERROR) {\r\n        lastNonFinalState = state;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getInternalState",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobStateInternal getInternalState()\n{\r\n    readLock.lock();\r\n    try {\r\n        if (forcedState != null) {\r\n            return forcedState;\r\n        }\r\n        return getStateMachine().getCurrentState();\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getExternalState",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobState getExternalState(JobStateInternal smState)\n{\r\n    switch(smState) {\r\n        case KILL_WAIT:\r\n        case KILL_ABORT:\r\n            return JobState.KILLED;\r\n        case SETUP:\r\n        case COMMITTING:\r\n            return JobState.RUNNING;\r\n        case FAIL_WAIT:\r\n        case FAIL_ABORT:\r\n            return JobState.FAILED;\r\n        case REBOOT:\r\n            if (appContext.isLastAMRetry()) {\r\n                return JobState.ERROR;\r\n            } else {\r\n                return JobState.RUNNING;\r\n            }\r\n        default:\r\n            return JobState.valueOf(smState.name());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "addTask",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void addTask(Task task)\n{\r\n    synchronized (tasksSyncHandle) {\r\n        if (lazyTasksCopyNeeded) {\r\n            Map<TaskId, Task> newTasks = new LinkedHashMap<TaskId, Task>();\r\n            newTasks.putAll(tasks);\r\n            tasks = newTasks;\r\n            lazyTasksCopyNeeded = false;\r\n        }\r\n    }\r\n    tasks.put(task.getID(), task);\r\n    if (task.getType() == TaskType.MAP) {\r\n        mapTasks.add(task.getID());\r\n    } else if (task.getType() == TaskType.REDUCE) {\r\n        reduceTasks.add(task.getID());\r\n    }\r\n    metrics.waitingTask(task);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "setFinishTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFinishTime()\n{\r\n    finishTime = clock.getTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "logJobHistoryFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void logJobHistoryFinishedEvent()\n{\r\n    this.setFinishTime();\r\n    JobFinishedEvent jfe = createJobFinishedEvent(this);\r\n    LOG.info(\"Calling handler for JobFinishedEvent \");\r\n    this.getEventHandler().handle(new JobHistoryEvent(this.jobId, jfe));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getFileSystem(Configuration conf) throws IOException\n{\r\n    return FileSystem.get(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "checkReadyForCommit",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "JobStateInternal checkReadyForCommit()\n{\r\n    JobStateInternal currentState = getInternalState();\r\n    if (completedTaskCount == tasks.size() && currentState == JobStateInternal.RUNNING) {\r\n        eventHandler.handle(new CommitterJobCommitEvent(jobId, getJobContext()));\r\n        return JobStateInternal.COMMITTING;\r\n    }\r\n    return getInternalState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "finished",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "JobStateInternal finished(JobStateInternal finalState)\n{\r\n    if (getInternalState() == JobStateInternal.RUNNING) {\r\n        metrics.endRunningJob(this);\r\n    }\r\n    if (finishTime == 0)\r\n        setFinishTime();\r\n    eventHandler.handle(new JobFinishEvent(jobId));\r\n    switch(finalState) {\r\n        case KILLED:\r\n            metrics.killedJob(this);\r\n            break;\r\n        case REBOOT:\r\n        case ERROR:\r\n        case FAILED:\r\n            metrics.failedJob(this);\r\n            break;\r\n        case SUCCEEDED:\r\n            metrics.completedJob(this);\r\n            break;\r\n        default:\r\n            throw new IllegalArgumentException(\"Illegal job state: \" + finalState);\r\n    }\r\n    return finalState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getUserName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUserName()\n{\r\n    return userName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getQueueName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getQueueName()\n{\r\n    return queueName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "setQueueName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setQueueName(String queueName)\n{\r\n    this.queueName = queueName;\r\n    JobQueueChangeEvent jqce = new JobQueueChangeEvent(oldJobId, queueName);\r\n    eventHandler.handle(new JobHistoryEvent(jobId, jqce));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getConfFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getConfFile()\n{\r\n    return remoteJobConfFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return jobName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getTotalMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTotalMaps()\n{\r\n    return mapTasks.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getTotalReduces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTotalReduces()\n{\r\n    return reduceTasks.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getJobACLs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<JobACL, AccessControlList> getJobACLs()\n{\r\n    return Collections.unmodifiableMap(jobACLs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getAMInfos",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<AMInfo> getAMInfos()\n{\r\n    return amInfos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "makeUberDecision",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void makeUberDecision(long dataInputLength)\n{\r\n    int sysMaxMaps = conf.getInt(MRJobConfig.JOB_UBERTASK_MAXMAPS, 9);\r\n    int sysMaxReduces = conf.getInt(MRJobConfig.JOB_UBERTASK_MAXREDUCES, 1);\r\n    long sysMaxBytes = conf.getLong(MRJobConfig.JOB_UBERTASK_MAXBYTES, fs.getDefaultBlockSize(this.remoteJobSubmitDir));\r\n    long sysMemSizeForUberSlot = conf.getInt(MRJobConfig.MR_AM_VMEM_MB, MRJobConfig.DEFAULT_MR_AM_VMEM_MB);\r\n    long sysCPUSizeForUberSlot = conf.getInt(MRJobConfig.MR_AM_CPU_VCORES, MRJobConfig.DEFAULT_MR_AM_CPU_VCORES);\r\n    boolean uberEnabled = conf.getBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    boolean smallNumMapTasks = (numMapTasks <= sysMaxMaps);\r\n    boolean smallNumReduceTasks = (numReduceTasks <= sysMaxReduces);\r\n    boolean smallInput = (dataInputLength <= sysMaxBytes);\r\n    long requiredMapMB = conf.getLong(MRJobConfig.MAP_MEMORY_MB, 0);\r\n    long requiredReduceMB = conf.getLong(MRJobConfig.REDUCE_MEMORY_MB, 0);\r\n    long requiredMB = Math.max(requiredMapMB, requiredReduceMB);\r\n    int requiredMapCores = conf.getInt(MRJobConfig.MAP_CPU_VCORES, MRJobConfig.DEFAULT_MAP_CPU_VCORES);\r\n    int requiredReduceCores = conf.getInt(MRJobConfig.REDUCE_CPU_VCORES, MRJobConfig.DEFAULT_REDUCE_CPU_VCORES);\r\n    int requiredCores = Math.max(requiredMapCores, requiredReduceCores);\r\n    if (numReduceTasks == 0) {\r\n        requiredMB = requiredMapMB;\r\n        requiredCores = requiredMapCores;\r\n    }\r\n    boolean smallMemory = (requiredMB <= sysMemSizeForUberSlot) || (sysMemSizeForUberSlot == JobConf.DISABLED_MEMORY_LIMIT);\r\n    boolean smallCpu = requiredCores <= sysCPUSizeForUberSlot;\r\n    boolean notChainJob = !isChainJob(conf);\r\n    isUber = uberEnabled && smallNumMapTasks && smallNumReduceTasks && smallInput && smallMemory && smallCpu && notChainJob;\r\n    if (isUber) {\r\n        LOG.info(\"Uberizing job \" + jobId + \": \" + numMapTasks + \"m+\" + numReduceTasks + \"r tasks (\" + dataInputLength + \" input bytes) will run sequentially on single node.\");\r\n        conf.setFloat(MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART, 1.0f);\r\n        conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, 1);\r\n        conf.setInt(MRJobConfig.REDUCE_MAX_ATTEMPTS, 1);\r\n        conf.setBoolean(MRJobConfig.MAP_SPECULATIVE, false);\r\n        conf.setBoolean(MRJobConfig.REDUCE_SPECULATIVE, false);\r\n    } else {\r\n        StringBuilder msg = new StringBuilder();\r\n        msg.append(\"Not uberizing \").append(jobId).append(\" because:\");\r\n        if (!uberEnabled)\r\n            msg.append(\" not enabled;\");\r\n        if (!smallNumMapTasks)\r\n            msg.append(\" too many maps;\");\r\n        if (!smallNumReduceTasks)\r\n            msg.append(\" too many reduces;\");\r\n        if (!smallInput)\r\n            msg.append(\" too much input;\");\r\n        if (!smallCpu)\r\n            msg.append(\" too much CPU;\");\r\n        if (!smallMemory)\r\n            msg.append(\" too much RAM;\");\r\n        if (!smallCpu)\r\n            msg.append(\" too much CPU;\");\r\n        if (!notChainJob)\r\n            msg.append(\" chainjob;\");\r\n        LOG.info(msg.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "isChainJob",
  "errType" : [ "ClassNotFoundException", "NoClassDefFoundError", "ClassNotFoundException", "NoClassDefFoundError" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean isChainJob(Configuration conf)\n{\r\n    boolean isChainJob = false;\r\n    try {\r\n        String mapClassName = conf.get(MRJobConfig.MAP_CLASS_ATTR);\r\n        if (mapClassName != null) {\r\n            Class<?> mapClass = Class.forName(mapClassName);\r\n            if (ChainMapper.class.isAssignableFrom(mapClass))\r\n                isChainJob = true;\r\n        }\r\n    } catch (ClassNotFoundException cnfe) {\r\n    } catch (NoClassDefFoundError ignored) {\r\n    }\r\n    try {\r\n        String reduceClassName = conf.get(MRJobConfig.REDUCE_CLASS_ATTR);\r\n        if (reduceClassName != null) {\r\n            Class<?> reduceClass = Class.forName(reduceClassName);\r\n            if (ChainReducer.class.isAssignableFrom(reduceClass))\r\n                isChainJob = true;\r\n        }\r\n    } catch (ClassNotFoundException cnfe) {\r\n    } catch (NoClassDefFoundError ignored) {\r\n    }\r\n    return isChainJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "actOnUnusableNode",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void actOnUnusableNode(NodeId nodeId, NodeState nodeState)\n{\r\n    if (getInternalState() == JobStateInternal.RUNNING && !allReducersComplete()) {\r\n        List<TaskAttemptId> taskAttemptIdList = nodesToSucceededTaskAttempts.get(nodeId);\r\n        if (taskAttemptIdList != null) {\r\n            String mesg = \"TaskAttempt killed because it ran on unusable node \" + nodeId;\r\n            for (TaskAttemptId id : taskAttemptIdList) {\r\n                if (TaskType.MAP == id.getTaskId().getTaskType()) {\r\n                    LOG.info(mesg + \". AttemptId:\" + id);\r\n                    eventHandler.handle(new TaskAttemptKillEvent(id, mesg, true));\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "allReducersComplete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean allReducersComplete()\n{\r\n    return numReduceTasks == 0 || numReduceTasks == getCompletedReduces();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getWorkflowAdjacencies",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "String getWorkflowAdjacencies(Configuration conf)\n{\r\n    int prefixLen = MRJobConfig.WORKFLOW_ADJACENCY_PREFIX_STRING.length();\r\n    Map<String, String> adjacencies = conf.getValByRegex(MRJobConfig.WORKFLOW_ADJACENCY_PREFIX_PATTERN);\r\n    if (adjacencies.isEmpty()) {\r\n        return \"\";\r\n    }\r\n    int size = 0;\r\n    for (Entry<String, String> entry : adjacencies.entrySet()) {\r\n        int keyLen = entry.getKey().length();\r\n        size += keyLen - prefixLen;\r\n        size += entry.getValue().length() + 6;\r\n    }\r\n    StringBuilder sb = new StringBuilder(size);\r\n    for (Entry<String, String> entry : adjacencies.entrySet()) {\r\n        int keyLen = entry.getKey().length();\r\n        sb.append(\"\\\"\");\r\n        sb.append(escapeString(entry.getKey().substring(prefixLen, keyLen)));\r\n        sb.append(\"\\\"=\\\"\");\r\n        sb.append(escapeString(entry.getValue()));\r\n        sb.append(\"\\\" \");\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "escapeString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String escapeString(String data)\n{\r\n    return StringUtils.escapeString(data, StringUtils.ESCAPE_CHAR, new char[] { '\"', '=', '.' });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "cleanupSharedCacheUploadPolicies",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cleanupSharedCacheUploadPolicies(Configuration conf)\n{\r\n    Job.setArchiveSharedCacheUploadPolicies(conf, Collections.emptyMap());\r\n    Job.setFileSharedCacheUploadPolicies(conf, Collections.emptyMap());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "unsuccessfulFinish",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void unsuccessfulFinish(JobStateInternal finalState)\n{\r\n    if (finishTime == 0)\r\n        setFinishTime();\r\n    cleanupProgress = 1.0f;\r\n    JobUnsuccessfulCompletionEvent unsuccessfulJobEvent = new JobUnsuccessfulCompletionEvent(oldJobId, finishTime, succeededMapTaskCount, succeededReduceTaskCount, failedMapTaskCount, failedReduceTaskCount, killedMapTaskCount, killedReduceTaskCount, finalState.toString(), diagnostics);\r\n    eventHandler.handle(new JobHistoryEvent(jobId, unsuccessfulJobEvent));\r\n    finished(finalState);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createJobFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobFinishedEvent createJobFinishedEvent(JobImpl job)\n{\r\n    job.mayBeConstructFinalFullCounters();\r\n    JobFinishedEvent jfe = new JobFinishedEvent(job.oldJobId, job.finishTime, job.succeededMapTaskCount, job.succeededReduceTaskCount, job.failedMapTaskCount, job.failedReduceTaskCount, job.killedMapTaskCount, job.killedReduceTaskCount, job.finalMapCounters, job.finalReduceCounters, job.fullCounters);\r\n    return jfe;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "mayBeConstructFinalFullCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void mayBeConstructFinalFullCounters()\n{\r\n    synchronized (this.fullCountersLock) {\r\n        if (this.fullCounters != null) {\r\n            return;\r\n        }\r\n        this.constructFinalFullcounters();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "constructFinalFullcounters",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void constructFinalFullcounters()\n{\r\n    this.fullCounters = new Counters();\r\n    this.finalMapCounters = new Counters();\r\n    this.finalReduceCounters = new Counters();\r\n    this.fullCounters.incrAllCounters(jobCounters);\r\n    for (Task t : this.tasks.values()) {\r\n        Counters counters = t.getCounters();\r\n        switch(t.getType()) {\r\n            case MAP:\r\n                this.finalMapCounters.incrAllCounters(counters);\r\n                break;\r\n            case REDUCE:\r\n                this.finalReduceCounters.incrAllCounters(counters);\r\n                break;\r\n            default:\r\n                throw new IllegalStateException(\"Task type neither map nor reduce: \" + t.getType());\r\n        }\r\n        this.fullCounters.incrAllCounters(counters);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "decrementSucceededMapperCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void decrementSucceededMapperCount()\n{\r\n    completedTaskCount--;\r\n    succeededMapTaskCount--;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "addDiagnostic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addDiagnostic(String diag)\n{\r\n    diagnostics.add(diag);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "loadConfFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration loadConfFile() throws IOException\n{\r\n    Path confPath = getConfFile();\r\n    FileContext fc = FileContext.getFileContext(confPath.toUri(), conf);\r\n    Configuration jobConf = new Configuration(false);\r\n    jobConf.addResource(fc.open(confPath), confPath.toString());\r\n    return jobConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getMaxAllowedFetchFailuresFraction",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getMaxAllowedFetchFailuresFraction()\n{\r\n    return maxAllowedFetchFailuresFraction;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getMaxFetchFailuresNotifications",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxFetchFailuresNotifications()\n{\r\n    return maxFetchFailuresNotifications;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "setJobPriority",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobPriority(Priority priority)\n{\r\n    this.jobPriority = priority;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getFailedMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getFailedMaps()\n{\r\n    return failedMapTaskCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getFailedReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getFailedReduces()\n{\r\n    return failedReduceTaskCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getKilledMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getKilledMaps()\n{\r\n    return killedMapTaskCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getKilledReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getKilledReduces()\n{\r\n    return killedReduceTaskCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "createAssignedRequests",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AssignedRequests createAssignedRequests()\n{\r\n    return new AssignedRequests();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n    reduceSlowStart = conf.getFloat(MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART, DEFAULT_COMPLETED_MAPS_PERCENT_FOR_REDUCE_SLOWSTART);\r\n    maxReduceRampupLimit = conf.getFloat(MRJobConfig.MR_AM_JOB_REDUCE_RAMPUP_UP_LIMIT, MRJobConfig.DEFAULT_MR_AM_JOB_REDUCE_RAMP_UP_LIMIT);\r\n    maxReducePreemptionLimit = conf.getFloat(MRJobConfig.MR_AM_JOB_REDUCE_PREEMPTION_LIMIT, MRJobConfig.DEFAULT_MR_AM_JOB_REDUCE_PREEMPTION_LIMIT);\r\n    reducerUnconditionalPreemptionDelayMs = 1000 * conf.getInt(MRJobConfig.MR_JOB_REDUCER_UNCONDITIONAL_PREEMPT_DELAY_SEC, MRJobConfig.DEFAULT_MR_JOB_REDUCER_UNCONDITIONAL_PREEMPT_DELAY_SEC);\r\n    reducerNoHeadroomPreemptionDelayMs = conf.getInt(MRJobConfig.MR_JOB_REDUCER_PREEMPT_DELAY_SEC, MRJobConfig.DEFAULT_MR_JOB_REDUCER_PREEMPT_DELAY_SEC) * 1000;\r\n    maxRunningMaps = conf.getInt(MRJobConfig.JOB_RUNNING_MAP_LIMIT, MRJobConfig.DEFAULT_JOB_RUNNING_MAP_LIMIT);\r\n    maxRunningReduces = conf.getInt(MRJobConfig.JOB_RUNNING_REDUCE_LIMIT, MRJobConfig.DEFAULT_JOB_RUNNING_REDUCE_LIMIT);\r\n    RackResolver.init(conf);\r\n    retryInterval = getConfig().getLong(MRJobConfig.MR_AM_TO_RM_WAIT_INTERVAL_MS, MRJobConfig.DEFAULT_MR_AM_TO_RM_WAIT_INTERVAL_MS);\r\n    mapNodeLabelExpression = conf.get(MRJobConfig.MAP_NODE_LABEL_EXP);\r\n    reduceNodeLabelExpression = conf.get(MRJobConfig.REDUCE_NODE_LABEL_EXP);\r\n    retrystartTime = System.currentTimeMillis();\r\n    this.scheduledRequests.setNumOpportunisticMapsPercent(conf.getInt(MRJobConfig.MR_NUM_OPPORTUNISTIC_MAPS_PERCENT, MRJobConfig.DEFAULT_MR_NUM_OPPORTUNISTIC_MAPS_PERCENT));\r\n    LOG.info(this.scheduledRequests.getNumOpportunisticMapsPercent() + \"% of the mappers will be scheduled using OPPORTUNISTIC containers\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "serviceStart",
  "errType" : [ "InterruptedException", "Throwable" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    this.eventHandlingThread = new Thread() {\r\n\r\n        @SuppressWarnings(\"unchecked\")\r\n        @Override\r\n        public void run() {\r\n            ContainerAllocatorEvent event;\r\n            while (!stopped.get() && !Thread.currentThread().isInterrupted()) {\r\n                try {\r\n                    event = RMContainerAllocator.this.eventQueue.take();\r\n                } catch (InterruptedException e) {\r\n                    if (!stopped.get()) {\r\n                        LOG.error(\"Returning, interrupted : \" + e);\r\n                    }\r\n                    return;\r\n                }\r\n                try {\r\n                    handleEvent(event);\r\n                } catch (Throwable t) {\r\n                    LOG.error(\"Error in handling event type \" + event.getType() + \" to the ContainreAllocator\", t);\r\n                    eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.INTERNAL_ERROR));\r\n                    return;\r\n                }\r\n            }\r\n        }\r\n    };\r\n    this.eventHandlingThread.start();\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "heartbeat",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void heartbeat() throws Exception\n{\r\n    scheduleStats.updateAndLogIfChanged(\"Before Scheduling: \");\r\n    List<Container> allocatedContainers = getResources();\r\n    if (allocatedContainers != null && allocatedContainers.size() > 0) {\r\n        scheduledRequests.assign(allocatedContainers);\r\n    }\r\n    int completedMaps = getJob().getCompletedMaps();\r\n    int completedTasks = completedMaps + getJob().getCompletedReduces();\r\n    if ((lastCompletedTasks != completedTasks) || (scheduledRequests.maps.size() > 0)) {\r\n        lastCompletedTasks = completedTasks;\r\n        recalculateReduceSchedule = true;\r\n    }\r\n    if (recalculateReduceSchedule) {\r\n        boolean reducerPreempted = preemptReducesIfNeeded();\r\n        if (!reducerPreempted) {\r\n            scheduleReduces(getJob().getTotalMaps(), completedMaps, scheduledRequests.maps.size(), scheduledRequests.reduces.size(), assignedRequests.maps.size(), assignedRequests.reduces.size(), mapResourceRequest, reduceResourceRequest, pendingReduces.size(), maxReduceRampupLimit, reduceSlowStart);\r\n        }\r\n        recalculateReduceSchedule = false;\r\n    }\r\n    scheduleStats.updateAndLogIfChanged(\"After Scheduling: \");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    if (stopped.getAndSet(true)) {\r\n        return;\r\n    }\r\n    if (eventHandlingThread != null) {\r\n        eventHandlingThread.interrupt();\r\n    }\r\n    super.serviceStop();\r\n    scheduleStats.log(\"Final Stats: \");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getAssignedRequests",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AssignedRequests getAssignedRequests()\n{\r\n    return assignedRequests;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getScheduledRequests",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ScheduledRequests getScheduledRequests()\n{\r\n    return scheduledRequests;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getNumOfPendingReduces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumOfPendingReduces()\n{\r\n    return pendingReduces.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getIsReduceStarted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getIsReduceStarted()\n{\r\n    return reduceStarted;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "setIsReduceStarted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setIsReduceStarted(boolean reduceStarted)\n{\r\n    this.reduceStarted = reduceStarted;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "handle",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void handle(ContainerAllocatorEvent event)\n{\r\n    int qSize = eventQueue.size();\r\n    if (qSize != 0 && qSize % 1000 == 0) {\r\n        LOG.info(\"Size of event-queue in RMContainerAllocator is \" + qSize);\r\n    }\r\n    int remCapacity = eventQueue.remainingCapacity();\r\n    if (remCapacity < 1000) {\r\n        LOG.warn(\"Very low remaining capacity in the event-queue \" + \"of RMContainerAllocator: \" + remCapacity);\r\n    }\r\n    try {\r\n        eventQueue.put(event);\r\n    } catch (InterruptedException e) {\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "handleEvent",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void handleEvent(ContainerAllocatorEvent event)\n{\r\n    recalculateReduceSchedule = true;\r\n    if (event.getType() == ContainerAllocator.EventType.CONTAINER_REQ) {\r\n        ContainerRequestEvent reqEvent = (ContainerRequestEvent) event;\r\n        boolean isMap = reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP);\r\n        if (isMap) {\r\n            handleMapContainerRequest(reqEvent);\r\n        } else {\r\n            handleReduceContainerRequest(reqEvent);\r\n        }\r\n    } else if (event.getType() == ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\r\n        LOG.info(\"Processing the event \" + event.toString());\r\n        TaskAttemptId aId = event.getAttemptID();\r\n        boolean removed = scheduledRequests.remove(aId);\r\n        if (!removed) {\r\n            ContainerId containerId = assignedRequests.get(aId);\r\n            if (containerId != null) {\r\n                removed = true;\r\n                assignedRequests.remove(aId);\r\n                containersReleased++;\r\n                pendingRelease.add(containerId);\r\n                release(containerId);\r\n            }\r\n        }\r\n        if (!removed) {\r\n            LOG.error(\"Could not deallocate container for task attemptId \" + aId);\r\n        }\r\n        preemptionPolicy.handleCompletedContainer(event.getAttemptID());\r\n    } else if (event.getType() == ContainerAllocator.EventType.CONTAINER_FAILED) {\r\n        ContainerFailedEvent fEv = (ContainerFailedEvent) event;\r\n        String host = getHost(fEv.getContMgrAddress());\r\n        containerFailedOnHost(host);\r\n        preemptionPolicy.handleFailedContainer(event.getAttemptID());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "handleReduceContainerRequest",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void handleReduceContainerRequest(ContainerRequestEvent reqEvent)\n{\r\n    assert (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.REDUCE));\r\n    Resource supportedMaxContainerCapability = getMaxContainerCapability();\r\n    JobId jobId = getJob().getID();\r\n    if (reduceResourceRequest.equals(Resources.none())) {\r\n        reduceResourceRequest = reqEvent.getCapability();\r\n        eventHandler.handle(new JobHistoryEvent(jobId, new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.REDUCE, reduceResourceRequest.getMemorySize())));\r\n        LOG.info(\"reduceResourceRequest:\" + reduceResourceRequest);\r\n    }\r\n    boolean reduceContainerRequestAccepted = true;\r\n    if (reduceResourceRequest.getMemorySize() > supportedMaxContainerCapability.getMemorySize() || reduceResourceRequest.getVirtualCores() > supportedMaxContainerCapability.getVirtualCores()) {\r\n        reduceContainerRequestAccepted = false;\r\n    }\r\n    if (reduceContainerRequestAccepted) {\r\n        reqEvent.getCapability().setVirtualCores(reduceResourceRequest.getVirtualCores());\r\n        reqEvent.getCapability().setMemorySize(reduceResourceRequest.getMemorySize());\r\n        if (reqEvent.getEarlierAttemptFailed()) {\r\n            pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE, reduceNodeLabelExpression));\r\n        } else {\r\n            pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE, reduceNodeLabelExpression));\r\n        }\r\n    } else {\r\n        String diagMsg = \"REDUCE capability required is more than the \" + \"supported max container capability in the cluster. Killing\" + \" the Job. reduceResourceRequest: \" + reduceResourceRequest + \" maxContainerCapability:\" + supportedMaxContainerCapability;\r\n        LOG.info(diagMsg);\r\n        eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\r\n        eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "handleMapContainerRequest",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void handleMapContainerRequest(ContainerRequestEvent reqEvent)\n{\r\n    assert (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP));\r\n    Resource supportedMaxContainerCapability = getMaxContainerCapability();\r\n    JobId jobId = getJob().getID();\r\n    if (mapResourceRequest.equals(Resources.none())) {\r\n        mapResourceRequest = reqEvent.getCapability();\r\n        eventHandler.handle(new JobHistoryEvent(jobId, new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP, mapResourceRequest.getMemorySize())));\r\n        LOG.info(\"mapResourceRequest:\" + mapResourceRequest);\r\n    }\r\n    boolean mapContainerRequestAccepted = true;\r\n    if (mapResourceRequest.getMemorySize() > supportedMaxContainerCapability.getMemorySize() || mapResourceRequest.getVirtualCores() > supportedMaxContainerCapability.getVirtualCores()) {\r\n        mapContainerRequestAccepted = false;\r\n    }\r\n    if (mapContainerRequestAccepted) {\r\n        reqEvent.getCapability().setMemorySize(mapResourceRequest.getMemorySize());\r\n        reqEvent.getCapability().setVirtualCores(mapResourceRequest.getVirtualCores());\r\n        scheduledRequests.addMap(reqEvent);\r\n    } else {\r\n        String diagMsg = \"The required MAP capability is more than the \" + \"supported max container capability in the cluster. Killing\" + \" the Job. mapResourceRequest: \" + mapResourceRequest + \" maxContainerCapability:\" + supportedMaxContainerCapability;\r\n        LOG.info(diagMsg);\r\n        eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\r\n        eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getHost",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getHost(String contMgrAddress)\n{\r\n    String host = contMgrAddress;\r\n    String[] hostport = host.split(\":\");\r\n    if (hostport.length == 2) {\r\n        host = hostport[0];\r\n    }\r\n    return host;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "setReduceResourceRequest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setReduceResourceRequest(Resource res)\n{\r\n    this.reduceResourceRequest = res;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "setMapResourceRequest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMapResourceRequest(Resource res)\n{\r\n    this.mapResourceRequest = res;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "preemptReducesIfNeeded",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "boolean preemptReducesIfNeeded()\n{\r\n    if (reduceResourceRequest.equals(Resources.none())) {\r\n        return false;\r\n    }\r\n    if (assignedRequests.maps.size() > 0) {\r\n        return false;\r\n    }\r\n    if (scheduledRequests.maps.size() <= 0) {\r\n        return false;\r\n    }\r\n    if (reducerUnconditionalPreemptionDelayMs >= 0) {\r\n        if (preemptReducersForHangingMapRequests(reducerUnconditionalPreemptionDelayMs)) {\r\n            return true;\r\n        }\r\n    }\r\n    Resource scheduledReducesResource = Resources.multiply(reduceResourceRequest, scheduledRequests.reduces.size());\r\n    Resource availableResourceForMap = Resources.subtract(getAvailableResources(), scheduledReducesResource);\r\n    if (ResourceCalculatorUtils.computeAvailableContainers(availableResourceForMap, mapResourceRequest, getSchedulerResourceTypes()) > 0) {\r\n        return false;\r\n    }\r\n    return preemptReducersForHangingMapRequests(reducerNoHeadroomPreemptionDelayMs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "preemptReducersForHangingMapRequests",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean preemptReducersForHangingMapRequests(long pendingThreshold)\n{\r\n    int hangingMapRequests = getNumHangingRequests(pendingThreshold, scheduledRequests.maps);\r\n    if (hangingMapRequests > 0) {\r\n        preemptReducer(hangingMapRequests);\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "clearAllPendingReduceRequests",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearAllPendingReduceRequests()\n{\r\n    rampDownReduces(Integer.MAX_VALUE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "preemptReducer",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void preemptReducer(int hangingMapRequests)\n{\r\n    clearAllPendingReduceRequests();\r\n    int preemptionReduceNumForOneMap = ResourceCalculatorUtils.divideAndCeilContainers(mapResourceRequest, reduceResourceRequest, getSchedulerResourceTypes());\r\n    int preemptionReduceNumForPreemptionLimit = ResourceCalculatorUtils.divideAndCeilContainers(Resources.multiply(getResourceLimit(), maxReducePreemptionLimit), reduceResourceRequest, getSchedulerResourceTypes());\r\n    int preemptionReduceNumForAllMaps = ResourceCalculatorUtils.divideAndCeilContainers(Resources.multiply(mapResourceRequest, hangingMapRequests), reduceResourceRequest, getSchedulerResourceTypes());\r\n    int toPreempt = Math.min(Math.max(preemptionReduceNumForOneMap, preemptionReduceNumForPreemptionLimit), preemptionReduceNumForAllMaps);\r\n    LOG.info(\"Going to preempt \" + toPreempt + \" due to lack of space for maps\");\r\n    assignedRequests.preemptReduce(toPreempt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getNumHangingRequests",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getNumHangingRequests(long allocationDelayThresholdMs, Map<TaskAttemptId, ContainerRequest> requestMap)\n{\r\n    if (allocationDelayThresholdMs <= 0)\r\n        return requestMap.size();\r\n    int hangingRequests = 0;\r\n    long currTime = clock.getTime();\r\n    for (ContainerRequest request : requestMap.values()) {\r\n        long delay = currTime - request.requestTimeMs;\r\n        if (delay > allocationDelayThresholdMs)\r\n            hangingRequests++;\r\n    }\r\n    return hangingRequests;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "scheduleReduces",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void scheduleReduces(int totalMaps, int completedMaps, int scheduledMaps, int scheduledReduces, int assignedMaps, int assignedReduces, Resource mapResourceReqt, Resource reduceResourceReqt, int numPendingReduces, float maxReduceRampupLimit, float reduceSlowStart)\n{\r\n    if (numPendingReduces == 0) {\r\n        return;\r\n    }\r\n    Resource headRoom = getAvailableResources();\r\n    LOG.info(\"Recalculating schedule, headroom=\" + headRoom);\r\n    if (!getIsReduceStarted()) {\r\n        int completedMapsForReduceSlowstart = (int) Math.ceil(reduceSlowStart * totalMaps);\r\n        if (completedMaps < completedMapsForReduceSlowstart) {\r\n            LOG.info(\"Reduce slow start threshold not met. \" + \"completedMapsForReduceSlowstart \" + completedMapsForReduceSlowstart);\r\n            return;\r\n        } else {\r\n            LOG.info(\"Reduce slow start threshold reached. Scheduling reduces.\");\r\n            setIsReduceStarted(true);\r\n        }\r\n    }\r\n    if (scheduledMaps == 0 && numPendingReduces > 0) {\r\n        LOG.info(\"All maps assigned. \" + \"Ramping up all remaining reduces:\" + numPendingReduces);\r\n        scheduleAllReduces();\r\n        return;\r\n    }\r\n    float completedMapPercent = 0f;\r\n    if (totalMaps != 0) {\r\n        completedMapPercent = (float) completedMaps / totalMaps;\r\n    } else {\r\n        completedMapPercent = 1;\r\n    }\r\n    Resource netScheduledMapResource = Resources.multiply(mapResourceReqt, (scheduledMaps + assignedMaps));\r\n    Resource netScheduledReduceResource = Resources.multiply(reduceResourceReqt, (scheduledReduces + assignedReduces));\r\n    Resource finalMapResourceLimit;\r\n    Resource finalReduceResourceLimit;\r\n    Resource totalResourceLimit = getResourceLimit();\r\n    Resource idealReduceResourceLimit = Resources.multiply(totalResourceLimit, Math.min(completedMapPercent, maxReduceRampupLimit));\r\n    Resource ideaMapResourceLimit = Resources.subtract(totalResourceLimit, idealReduceResourceLimit);\r\n    if (ResourceCalculatorUtils.computeAvailableContainers(ideaMapResourceLimit, mapResourceReqt, getSchedulerResourceTypes()) >= (scheduledMaps + assignedMaps)) {\r\n        Resource unusedMapResourceLimit = Resources.subtract(ideaMapResourceLimit, netScheduledMapResource);\r\n        finalReduceResourceLimit = Resources.add(idealReduceResourceLimit, unusedMapResourceLimit);\r\n        finalMapResourceLimit = Resources.subtract(totalResourceLimit, finalReduceResourceLimit);\r\n    } else {\r\n        finalMapResourceLimit = ideaMapResourceLimit;\r\n        finalReduceResourceLimit = idealReduceResourceLimit;\r\n    }\r\n    LOG.info(\"completedMapPercent \" + completedMapPercent + \" totalResourceLimit:\" + totalResourceLimit + \" finalMapResourceLimit:\" + finalMapResourceLimit + \" finalReduceResourceLimit:\" + finalReduceResourceLimit + \" netScheduledMapResource:\" + netScheduledMapResource + \" netScheduledReduceResource:\" + netScheduledReduceResource);\r\n    int rampUp = ResourceCalculatorUtils.computeAvailableContainers(Resources.subtract(finalReduceResourceLimit, netScheduledReduceResource), reduceResourceReqt, getSchedulerResourceTypes());\r\n    if (rampUp > 0) {\r\n        rampUp = Math.min(rampUp, numPendingReduces);\r\n        LOG.info(\"Ramping up \" + rampUp);\r\n        rampUpReduces(rampUp);\r\n    } else if (rampUp < 0) {\r\n        int rampDown = -1 * rampUp;\r\n        rampDown = Math.min(rampDown, scheduledReduces);\r\n        LOG.info(\"Ramping down \" + rampDown);\r\n        rampDownReduces(rampDown);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "scheduleAllReduces",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void scheduleAllReduces()\n{\r\n    for (ContainerRequest req : pendingReduces) {\r\n        scheduledRequests.addReduce(req);\r\n    }\r\n    pendingReduces.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "rampUpReduces",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void rampUpReduces(int rampUp)\n{\r\n    for (int i = 0; i < rampUp; i++) {\r\n        ContainerRequest request = pendingReduces.removeFirst();\r\n        scheduledRequests.addReduce(request);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "rampDownReduces",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void rampDownReduces(int rampDown)\n{\r\n    while (rampDown > 0) {\r\n        ContainerRequest request = scheduledRequests.removeReduce();\r\n        if (request == null) {\r\n            return;\r\n        }\r\n        pendingReduces.add(request);\r\n        rampDown--;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getResources",
  "errType" : [ "ApplicationAttemptNotFoundException", "ApplicationMasterNotRegisteredException", "InvalidLabelResourceRequestException", "Exception" ],
  "containingMethodsNum" : 42,
  "sourceCodeText" : "List<Container> getResources() throws Exception\n{\r\n    applyConcurrentTaskLimits();\r\n    Resource headRoom = Resources.clone(getAvailableResources());\r\n    AllocateResponse response;\r\n    try {\r\n        response = makeRemoteRequest();\r\n        retrystartTime = System.currentTimeMillis();\r\n    } catch (ApplicationAttemptNotFoundException e) {\r\n        eventHandler.handle(new JobEvent(this.getJob().getID(), JobEventType.JOB_AM_REBOOT));\r\n        throw new RMContainerAllocationException(\"Resource Manager doesn't recognize AttemptId: \" + this.getContext().getApplicationAttemptId(), e);\r\n    } catch (ApplicationMasterNotRegisteredException e) {\r\n        LOG.info(\"ApplicationMaster is out of sync with ResourceManager,\" + \" hence resync and send outstanding requests.\");\r\n        lastResponseID = 0;\r\n        register();\r\n        addOutstandingRequestOnResync();\r\n        return null;\r\n    } catch (InvalidLabelResourceRequestException e) {\r\n        String diagMsg = \"Requested node-label-expression is invalid: \" + StringUtils.stringifyException(e);\r\n        LOG.info(diagMsg);\r\n        JobId jobId = this.getJob().getID();\r\n        eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\r\n        eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\r\n        throw e;\r\n    } catch (Exception e) {\r\n        if (System.currentTimeMillis() - retrystartTime >= retryInterval) {\r\n            LOG.error(\"Could not contact RM after \" + retryInterval + \" milliseconds.\");\r\n            eventHandler.handle(new JobEvent(this.getJob().getID(), JobEventType.JOB_AM_REBOOT));\r\n            throw new RMContainerAllocationException(\"Could not contact RM after \" + retryInterval + \" milliseconds.\");\r\n        }\r\n        throw e;\r\n    }\r\n    Resource newHeadRoom = getAvailableResources();\r\n    List<Container> newContainers = response.getAllocatedContainers();\r\n    if (response.getNMTokens() != null) {\r\n        for (NMToken nmToken : response.getNMTokens()) {\r\n            NMTokenCache.setNMToken(nmToken.getNodeId().toString(), nmToken.getToken());\r\n        }\r\n    }\r\n    if (response.getAMRMToken() != null) {\r\n        updateAMRMToken(response.getAMRMToken());\r\n    }\r\n    List<ContainerStatus> finishedContainers = response.getCompletedContainersStatuses();\r\n    final PreemptionMessage preemptReq = response.getPreemptionMessage();\r\n    if (preemptReq != null) {\r\n        preemptionPolicy.preempt(new PreemptionContext(assignedRequests), preemptReq);\r\n    }\r\n    if (newContainers.size() + finishedContainers.size() > 0 || !headRoom.equals(newHeadRoom)) {\r\n        recalculateReduceSchedule = true;\r\n        if (LOG.isDebugEnabled() && !headRoom.equals(newHeadRoom)) {\r\n            LOG.debug(\"headroom=\" + newHeadRoom);\r\n        }\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        for (Container cont : newContainers) {\r\n            LOG.debug(\"Received new Container :\" + cont);\r\n        }\r\n    }\r\n    computeIgnoreBlacklisting();\r\n    handleUpdatedNodes(response);\r\n    handleJobPriorityChange(response);\r\n    MRAppMaster.RunningAppContext appContext = (MRAppMaster.RunningAppContext) this.getContext();\r\n    if (appContext.getTimelineV2Client() != null) {\r\n        appContext.getTimelineV2Client().setTimelineCollectorInfo(response.getCollectorInfo());\r\n    }\r\n    for (ContainerStatus cont : finishedContainers) {\r\n        processFinishedContainer(cont);\r\n    }\r\n    return newContainers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "processFinishedContainer",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void processFinishedContainer(ContainerStatus container)\n{\r\n    LOG.info(\"Received completed container \" + container.getContainerId());\r\n    TaskAttemptId attemptID = assignedRequests.get(container.getContainerId());\r\n    if (attemptID == null) {\r\n        LOG.error(\"Container complete event for unknown container \" + container.getContainerId());\r\n    } else {\r\n        pendingRelease.remove(container.getContainerId());\r\n        assignedRequests.remove(attemptID);\r\n        String diagnostic = StringInterner.weakIntern(container.getDiagnostics());\r\n        eventHandler.handle(new TaskAttemptDiagnosticsUpdateEvent(attemptID, diagnostic));\r\n        eventHandler.handle(createContainerFinishedEvent(container, attemptID));\r\n        preemptionPolicy.handleCompletedContainer(attemptID);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "applyConcurrentTaskLimits",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void applyConcurrentTaskLimits()\n{\r\n    int numScheduledMaps = scheduledRequests.maps.size();\r\n    if (maxRunningMaps > 0 && numScheduledMaps > 0 && getJob().getTotalMaps() > maxRunningMaps) {\r\n        int maxRequestedMaps = Math.max(0, maxRunningMaps - assignedRequests.maps.size());\r\n        int numScheduledFailMaps = scheduledRequests.earlierFailedMaps.size();\r\n        int failedMapRequestLimit = Math.min(maxRequestedMaps, numScheduledFailMaps);\r\n        int normalMapRequestLimit = Math.min(maxRequestedMaps - failedMapRequestLimit, numScheduledMaps - numScheduledFailMaps);\r\n        setRequestLimit(PRIORITY_FAST_FAIL_MAP, mapResourceRequest, failedMapRequestLimit);\r\n        setRequestLimit(PRIORITY_MAP, mapResourceRequest, normalMapRequestLimit);\r\n        setRequestLimit(PRIORITY_OPPORTUNISTIC_MAP, mapResourceRequest, normalMapRequestLimit);\r\n    }\r\n    int numScheduledReduces = scheduledRequests.reduces.size();\r\n    if (maxRunningReduces > 0 && numScheduledReduces > 0 && getJob().getTotalReduces() > maxRunningReduces) {\r\n        int maxRequestedReduces = Math.max(0, maxRunningReduces - assignedRequests.reduces.size());\r\n        int reduceRequestLimit = Math.min(maxRequestedReduces, numScheduledReduces);\r\n        setRequestLimit(PRIORITY_REDUCE, reduceResourceRequest, reduceRequestLimit);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "canAssignMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean canAssignMaps()\n{\r\n    return (maxRunningMaps <= 0 || assignedRequests.maps.size() < maxRunningMaps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "canAssignReduces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean canAssignReduces()\n{\r\n    return (maxRunningReduces <= 0 || assignedRequests.reduces.size() < maxRunningReduces);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "updateAMRMToken",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void updateAMRMToken(Token token) throws IOException\n{\r\n    org.apache.hadoop.security.token.Token<AMRMTokenIdentifier> amrmToken = new org.apache.hadoop.security.token.Token<AMRMTokenIdentifier>(token.getIdentifier().array(), token.getPassword().array(), new Text(token.getKind()), new Text(token.getService()));\r\n    UserGroupInformation currentUGI = UserGroupInformation.getCurrentUser();\r\n    currentUGI.addToken(amrmToken);\r\n    amrmToken.setService(ClientRMProxy.getAMRMTokenService(getConfig()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "createContainerFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptEvent createContainerFinishedEvent(ContainerStatus cont, TaskAttemptId attemptId)\n{\r\n    TaskAttemptEvent event;\r\n    switch(cont.getExitStatus()) {\r\n        case ContainerExitStatus.ABORTED:\r\n        case ContainerExitStatus.PREEMPTED:\r\n        case ContainerExitStatus.KILLED_BY_CONTAINER_SCHEDULER:\r\n            event = new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_KILL);\r\n            break;\r\n        default:\r\n            event = new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_CONTAINER_COMPLETED);\r\n    }\r\n    return event;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "handleUpdatedNodes",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void handleUpdatedNodes(AllocateResponse response)\n{\r\n    List<NodeReport> updatedNodes = response.getUpdatedNodes();\r\n    if (!updatedNodes.isEmpty()) {\r\n        eventHandler.handle(new JobUpdatedNodesEvent(getJob().getID(), updatedNodes));\r\n        HashSet<NodeId> unusableNodes = new HashSet<NodeId>();\r\n        for (NodeReport nr : updatedNodes) {\r\n            NodeState nodeState = nr.getNodeState();\r\n            if (nodeState.isUnusable()) {\r\n                unusableNodes.add(nr.getNodeId());\r\n            }\r\n        }\r\n        for (int i = 0; i < 2; ++i) {\r\n            HashMap<TaskAttemptId, Container> taskSet = i == 0 ? assignedRequests.maps : assignedRequests.reduces;\r\n            for (Map.Entry<TaskAttemptId, Container> entry : taskSet.entrySet()) {\r\n                TaskAttemptId tid = entry.getKey();\r\n                NodeId taskAttemptNodeId = entry.getValue().getNodeId();\r\n                if (unusableNodes.contains(taskAttemptNodeId)) {\r\n                    LOG.info(\"Killing taskAttempt:\" + tid + \" because it is running on unusable node:\" + taskAttemptNodeId);\r\n                    boolean rescheduleNextAttempt = (i == 0) ? true : false;\r\n                    eventHandler.handle(new TaskAttemptKillEvent(tid, \"TaskAttempt killed because it ran on unusable node\" + taskAttemptNodeId, rescheduleNextAttempt));\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "handleJobPriorityChange",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void handleJobPriorityChange(AllocateResponse response)\n{\r\n    Priority applicationPriority = response.getApplicationPriority();\r\n    if (null != applicationPriority) {\r\n        Priority priorityFromResponse = Priority.newInstance(applicationPriority.getPriority());\r\n        getJob().setJobPriority(priorityFromResponse);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getResourceLimit",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Resource getResourceLimit()\n{\r\n    Resource headRoom = getAvailableResources();\r\n    Resource assignedMapResource = Resources.multiply(mapResourceRequest, assignedRequests.maps.size());\r\n    Resource assignedReduceResource = Resources.multiply(reduceResourceRequest, assignedRequests.reduces.size());\r\n    return Resources.add(headRoom, Resources.add(assignedMapResource, assignedReduceResource));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createRemoteTask",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Task createRemoteTask()\n{\r\n    ReduceTask reduceTask = new ReduceTask(\"\", TypeConverter.fromYarn(getID()), partition, numMapTasks, 1);\r\n    reduceTask.setUser(conf.get(MRJobConfig.USER_NAME));\r\n    reduceTask.setConf(conf);\r\n    return reduceTask;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getProperties",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ArrayList<ConfEntryInfo> getProperties()\n{\r\n    return this.property;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPath()\n{\r\n    return this.path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getTaskAttemptInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptInfo getTaskAttemptInfo()\n{\r\n    return taInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OutputCommitter getCommitter()\n{\r\n    return committer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getRecoverOutput",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getRecoverOutput()\n{\r\n    return recoverAttemptOutput;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getTaskID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskId getTaskID()\n{\r\n    return taskID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskState getState()\n{\r\n    return taskState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "setConf",
  "errType" : [ "NumberFormatException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.conf = conf;\r\n    numTries = Math.min(conf.getInt(MRJobConfig.MR_JOB_END_RETRY_ATTEMPTS, 0) + 1, conf.getInt(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_ATTEMPTS, 1));\r\n    waitInterval = Math.min(conf.getInt(MRJobConfig.MR_JOB_END_RETRY_INTERVAL, 5000), conf.getInt(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_RETRY_INTERVAL, 5000));\r\n    waitInterval = (waitInterval < 0) ? 5000 : waitInterval;\r\n    timeout = conf.getInt(JobContext.MR_JOB_END_NOTIFICATION_TIMEOUT, JobContext.DEFAULT_MR_JOB_END_NOTIFICATION_TIMEOUT);\r\n    userUrl = conf.get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL);\r\n    proxyConf = conf.get(MRJobConfig.MR_JOB_END_NOTIFICATION_PROXY);\r\n    customJobEndNotifierClassName = StringUtils.stripToNull(conf.get(MRJobConfig.MR_JOB_END_NOTIFICATION_CUSTOM_NOTIFIER_CLASS));\r\n    if (proxyConf != null && !proxyConf.equals(\"\") && proxyConf.lastIndexOf(\":\") != -1) {\r\n        int typeIndex = proxyConf.indexOf(\"@\");\r\n        Proxy.Type proxyType = Proxy.Type.HTTP;\r\n        if (typeIndex != -1 && proxyConf.substring(0, typeIndex).compareToIgnoreCase(\"socks\") == 0) {\r\n            proxyType = Proxy.Type.SOCKS;\r\n        }\r\n        String hostname = proxyConf.substring(typeIndex + 1, proxyConf.lastIndexOf(\":\"));\r\n        String portConf = proxyConf.substring(proxyConf.lastIndexOf(\":\") + 1);\r\n        try {\r\n            int port = Integer.parseInt(portConf);\r\n            proxyToUse = new Proxy(proxyType, new InetSocketAddress(hostname, port));\r\n            Log.getLog().info(\"Job end notification using proxy type \\\"\" + proxyType + \"\\\" hostname \\\"\" + hostname + \"\\\" and port \\\"\" + port + \"\\\"\");\r\n        } catch (NumberFormatException nfe) {\r\n            Log.getLog().warn(\"Job end notification couldn't parse configured\" + \"proxy's port \" + portConf + \". Not going to use a proxy\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "notifyURLOnce",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean notifyURLOnce()\n{\r\n    if (customJobEndNotifierClassName == null) {\r\n        return notifyViaBuiltInNotifier();\r\n    } else {\r\n        return notifyViaCustomNotifier();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "notifyViaBuiltInNotifier",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "boolean notifyViaBuiltInNotifier()\n{\r\n    boolean success = false;\r\n    try {\r\n        Log.getLog().info(\"Job end notification trying \" + urlToNotify);\r\n        HttpURLConnection conn = (HttpURLConnection) urlToNotify.openConnection(proxyToUse);\r\n        conn.setConnectTimeout(timeout);\r\n        conn.setReadTimeout(timeout);\r\n        conn.setAllowUserInteraction(false);\r\n        if (conn.getResponseCode() != HttpURLConnection.HTTP_OK) {\r\n            Log.getLog().warn(\"Job end notification to \" + urlToNotify + \" failed with code: \" + conn.getResponseCode() + \" and message \\\"\" + conn.getResponseMessage() + \"\\\"\");\r\n        } else {\r\n            success = true;\r\n            Log.getLog().info(\"Job end notification to \" + urlToNotify + \" succeeded\");\r\n        }\r\n    } catch (IOException ioe) {\r\n        Log.getLog().warn(\"Job end notification to \" + urlToNotify + \" failed\", ioe);\r\n    }\r\n    return success;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "notifyViaCustomNotifier",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean notifyViaCustomNotifier()\n{\r\n    try {\r\n        Log.getLog().info(\"Will be using \" + customJobEndNotifierClassName + \" for Job end notification\");\r\n        final Class<? extends CustomJobEndNotifier> customJobEndNotifierClass = Class.forName(customJobEndNotifierClassName).asSubclass(CustomJobEndNotifier.class);\r\n        final CustomJobEndNotifier customJobEndNotifier = customJobEndNotifierClass.getDeclaredConstructor().newInstance();\r\n        boolean success = customJobEndNotifier.notifyOnce(urlToNotify, conf);\r\n        if (success) {\r\n            Log.getLog().info(\"Job end notification to \" + urlToNotify + \" succeeded\");\r\n        } else {\r\n            Log.getLog().warn(\"Job end notification to \" + urlToNotify + \" failed\");\r\n        }\r\n        return success;\r\n    } catch (Exception e) {\r\n        Log.getLog().warn(\"Job end notification to \" + urlToNotify + \" failed\", e);\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "notify",
  "errType" : [ "MalformedURLException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void notify(JobReport jobReport) throws InterruptedException\n{\r\n    if (userUrl.contains(JOB_ID)) {\r\n        userUrl = userUrl.replace(JOB_ID, jobReport.getJobId().toString());\r\n    }\r\n    if (userUrl.contains(JOB_STATUS)) {\r\n        userUrl = userUrl.replace(JOB_STATUS, jobReport.getJobState().toString());\r\n    }\r\n    try {\r\n        urlToNotify = new URL(userUrl);\r\n    } catch (MalformedURLException mue) {\r\n        Log.getLog().warn(\"Job end notification couldn't parse \" + userUrl, mue);\r\n        return;\r\n    }\r\n    boolean success = false;\r\n    while (numTries-- > 0 && !success) {\r\n        Log.getLog().info(\"Job end notification attempts left \" + numTries);\r\n        success = notifyURLOnce();\r\n        if (!success) {\r\n            Thread.sleep(waitInterval);\r\n        }\r\n    }\r\n    if (!success) {\r\n        Log.getLog().warn(\"Job end notification failed to notify : \" + urlToNotify);\r\n    } else {\r\n        Log.getLog().info(\"Job end notification succeeded for \" + jobReport.getJobId());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getRecoveredJobStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getRecoveredJobStartTime()\n{\r\n    return recoveredJobStartTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "divideAndCeil",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int divideAndCeil(long a, long b)\n{\r\n    if (b == 0) {\r\n        return 0;\r\n    }\r\n    return (int) ((a + (b - 1)) / b);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "computeAvailableContainers",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int computeAvailableContainers(Resource available, Resource required, EnumSet<SchedulerResourceTypes> resourceTypes)\n{\r\n    if (resourceTypes.contains(SchedulerResourceTypes.CPU)) {\r\n        return Math.min(calculateRatioOrMaxValue(available.getMemorySize(), required.getMemorySize()), calculateRatioOrMaxValue(available.getVirtualCores(), required.getVirtualCores()));\r\n    }\r\n    return calculateRatioOrMaxValue(available.getMemorySize(), required.getMemorySize());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "divideAndCeilContainers",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int divideAndCeilContainers(Resource required, Resource factor, EnumSet<SchedulerResourceTypes> resourceTypes)\n{\r\n    if (resourceTypes.contains(SchedulerResourceTypes.CPU)) {\r\n        return Math.max(divideAndCeil(required.getMemorySize(), factor.getMemorySize()), divideAndCeil(required.getVirtualCores(), factor.getVirtualCores()));\r\n    }\r\n    return divideAndCeil(required.getMemorySize(), factor.getMemorySize());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "calculateRatioOrMaxValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int calculateRatioOrMaxValue(long numerator, long denominator)\n{\r\n    if (denominator == 0) {\r\n        return Integer.MAX_VALUE;\r\n    }\r\n    return (int) (numerator / denominator);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void add(AMAttemptInfo info)\n{\r\n    this.attempt.add(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ArrayList<AMAttemptInfo> getAttempts()\n{\r\n    return this.attempt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobId getJobID()\n{\r\n    return jobID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "getJobContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobContext getJobContext()\n{\r\n    return jobContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getNodeHttpAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNodeHttpAddress()\n{\r\n    return this.nodeHttpAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getNodeId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNodeId()\n{\r\n    return this.nodeId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getAttemptId()\n{\r\n    return this.id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return this.startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getContainerId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getContainerId()\n{\r\n    return this.containerId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getLogsLink",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLogsLink()\n{\r\n    return this.logsLink;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    commonPreHead(html);\r\n    setTitle(\"About the Application Master\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return InfoBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "render",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void render(Block html)\n{\r\n    String jid = $(JOB_ID);\r\n    if (jid.isEmpty()) {\r\n        html.p().__(\"Sorry, can't do anything without a JobID.\").__();\r\n        return;\r\n    }\r\n    JobId jobID = MRApps.toJobID(jid);\r\n    Job job = appContext.getJob(jobID);\r\n    if (job == null) {\r\n        html.p().__(\"Sorry, \", jid, \" not found.\").__();\r\n        return;\r\n    }\r\n    Path confPath = job.getConfFile();\r\n    try {\r\n        ConfInfo info = new ConfInfo(job);\r\n        html.div().a(\"/jobhistory/downloadconf/\" + jid, confPath.toString()).__();\r\n        TBODY<TABLE<Hamlet>> tbody = html.table(\"#conf\").thead().tr().th(_TH, \"key\").th(_TH, \"value\").th(_TH, \"source chain\").__().__().tbody();\r\n        for (ConfEntryInfo entry : info.getProperties()) {\r\n            StringBuffer buffer = new StringBuffer();\r\n            String[] sources = entry.getSource();\r\n            boolean first = true;\r\n            for (int i = (sources.length - 2); i >= 0; i--) {\r\n                if (!first) {\r\n                    buffer.append(\" <- \");\r\n                }\r\n                first = false;\r\n                buffer.append(sources[i]);\r\n            }\r\n            tbody.tr().td(entry.getName()).td(entry.getValue()).td(buffer.toString()).__();\r\n        }\r\n        tbody.__().tfoot().tr().th().input(\"search_init\").$type(InputType.text).$name(\"key\").$value(\"key\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"value\").$value(\"value\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"source chain\").$value(\"source chain\").__().__().__().__().__();\r\n    } catch (IOException e) {\r\n        LOG.error(\"Error while reading \" + confPath, e);\r\n        html.p().__(\"Sorry got an error while reading conf file. \", confPath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobId getJobID()\n{\r\n    return jobID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "getJobContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobContext getJobContext()\n{\r\n    return jobContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "getFinalState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobStatus.State getFinalState()\n{\r\n    return finalState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "populateResourceCapability",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void populateResourceCapability(TaskType taskType)\n{\r\n    String resourceTypePrefix = getResourceTypePrefix(taskType);\r\n    boolean memorySet = false;\r\n    boolean cpuVcoresSet = false;\r\n    if (RESOURCE_REQUEST_CACHE.get(taskType) != null) {\r\n        resourceCapability = RESOURCE_REQUEST_CACHE.get(taskType);\r\n        return;\r\n    }\r\n    if (resourceTypePrefix != null) {\r\n        List<ResourceInformation> resourceRequests = ResourceUtils.getRequestedResourcesFromConfig(conf, resourceTypePrefix);\r\n        for (ResourceInformation resourceRequest : resourceRequests) {\r\n            String resourceName = resourceRequest.getName();\r\n            if (MRJobConfig.RESOURCE_TYPE_NAME_MEMORY.equals(resourceName) || MRJobConfig.RESOURCE_TYPE_ALTERNATIVE_NAME_MEMORY.equals(resourceName)) {\r\n                if (memorySet) {\r\n                    throw new IllegalArgumentException(\"Only one of the following keys \" + \"can be specified for a single job: \" + MRJobConfig.RESOURCE_TYPE_ALTERNATIVE_NAME_MEMORY + \", \" + MRJobConfig.RESOURCE_TYPE_NAME_MEMORY);\r\n                }\r\n                String units = isEmpty(resourceRequest.getUnits()) ? ResourceUtils.getDefaultUnit(ResourceInformation.MEMORY_URI) : resourceRequest.getUnits();\r\n                this.resourceCapability.setMemorySize(UnitsConversionUtil.convert(units, \"Mi\", resourceRequest.getValue()));\r\n                memorySet = true;\r\n                String memoryKey = getMemoryKey(taskType);\r\n                if (memoryKey != null && conf.get(memoryKey) != null) {\r\n                    LOG.warn(\"Configuration \" + resourceTypePrefix + resourceName + \"=\" + resourceRequest.getValue() + resourceRequest.getUnits() + \" is overriding the \" + memoryKey + \"=\" + conf.get(memoryKey) + \" configuration\");\r\n                }\r\n            } else if (MRJobConfig.RESOURCE_TYPE_NAME_VCORE.equals(resourceName)) {\r\n                this.resourceCapability.setVirtualCores((int) UnitsConversionUtil.convert(resourceRequest.getUnits(), \"\", resourceRequest.getValue()));\r\n                cpuVcoresSet = true;\r\n                String cpuKey = getCpuVcoresKey(taskType);\r\n                if (cpuKey != null && conf.get(cpuKey) != null) {\r\n                    LOG.warn(\"Configuration \" + resourceTypePrefix + MRJobConfig.RESOURCE_TYPE_NAME_VCORE + \"=\" + resourceRequest.getValue() + resourceRequest.getUnits() + \" is overriding the \" + cpuKey + \"=\" + conf.get(cpuKey) + \" configuration\");\r\n                }\r\n            } else {\r\n                ResourceInformation resourceInformation = this.resourceCapability.getResourceInformation(resourceName);\r\n                resourceInformation.setUnits(resourceRequest.getUnits());\r\n                resourceInformation.setValue(resourceRequest.getValue());\r\n                this.resourceCapability.setResourceInformation(resourceName, resourceInformation);\r\n            }\r\n        }\r\n    }\r\n    if (!memorySet) {\r\n        this.resourceCapability.setMemorySize(getMemoryRequired(conf, taskType));\r\n    }\r\n    if (!cpuVcoresSet) {\r\n        this.resourceCapability.setVirtualCores(getCpuRequired(conf, taskType));\r\n    }\r\n    RESOURCE_REQUEST_CACHE.put(taskType, resourceCapability);\r\n    LOG.info(\"Resource capability of task type {} is set to {}\", taskType, resourceCapability);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getCpuVcoresKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCpuVcoresKey(TaskType taskType)\n{\r\n    switch(taskType) {\r\n        case MAP:\r\n            return MRJobConfig.MAP_CPU_VCORES;\r\n        case REDUCE:\r\n            return MRJobConfig.REDUCE_CPU_VCORES;\r\n        default:\r\n            return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getMemoryKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getMemoryKey(TaskType taskType)\n{\r\n    switch(taskType) {\r\n        case MAP:\r\n            return MRJobConfig.MAP_MEMORY_MB;\r\n        case REDUCE:\r\n            return MRJobConfig.REDUCE_MEMORY_MB;\r\n        default:\r\n            return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getCpuVcoreDefault",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Integer getCpuVcoreDefault(TaskType taskType)\n{\r\n    switch(taskType) {\r\n        case MAP:\r\n            return MRJobConfig.DEFAULT_MAP_CPU_VCORES;\r\n        case REDUCE:\r\n            return MRJobConfig.DEFAULT_REDUCE_CPU_VCORES;\r\n        default:\r\n            return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getMemoryRequired",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMemoryRequired(JobConf conf, TaskType taskType)\n{\r\n    return conf.getMemoryRequired(TypeConverter.fromYarn(taskType));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getCpuRequired",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getCpuRequired(Configuration conf, TaskType taskType)\n{\r\n    int vcores = 1;\r\n    String cpuVcoreKey = getCpuVcoresKey(taskType);\r\n    if (cpuVcoreKey != null) {\r\n        Integer defaultCpuVcores = getCpuVcoreDefault(taskType);\r\n        if (null == defaultCpuVcores) {\r\n            defaultCpuVcores = vcores;\r\n        }\r\n        vcores = conf.getInt(cpuVcoreKey, defaultCpuVcores);\r\n    }\r\n    return vcores;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getResourceTypePrefix",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getResourceTypePrefix(TaskType taskType)\n{\r\n    switch(taskType) {\r\n        case MAP:\r\n            return MRJobConfig.MAP_RESOURCE_TYPE_PREFIX;\r\n        case REDUCE:\r\n            return MRJobConfig.REDUCE_RESOURCE_TYPE_PREFIX;\r\n        default:\r\n            LOG.info(\"TaskType \" + taskType + \" does not support custom resource types - this support can be \" + \"added in \" + getClass().getSimpleName());\r\n            return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createLocalResource",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "LocalResource createLocalResource(FileSystem fc, Path file, String fileSymlink, LocalResourceType type, LocalResourceVisibility visibility) throws IOException\n{\r\n    FileStatus fstat = fc.getFileStatus(file);\r\n    Path qualifiedPath = fc.resolvePath(fstat.getPath());\r\n    URI uriWithFragment = null;\r\n    boolean useFragment = fileSymlink != null && !fileSymlink.equals(\"\");\r\n    try {\r\n        if (useFragment) {\r\n            uriWithFragment = new URI(qualifiedPath.toUri() + \"#\" + fileSymlink);\r\n        } else {\r\n            uriWithFragment = qualifiedPath.toUri();\r\n        }\r\n    } catch (URISyntaxException e) {\r\n        throw new IOException(\"Error parsing local resource path.\" + \" Path was not able to be converted to a URI: \" + qualifiedPath, e);\r\n    }\r\n    URL resourceURL = URL.fromURI(uriWithFragment);\r\n    long resourceSize = fstat.getLen();\r\n    long resourceModificationTime = fstat.getModificationTime();\r\n    return LocalResource.newInstance(resourceURL, type, visibility, resourceSize, resourceModificationTime, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getInitialClasspath",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getInitialClasspath(Configuration conf) throws IOException\n{\r\n    synchronized (classpathLock) {\r\n        if (initialClasspathFlag.get()) {\r\n            return initialClasspath;\r\n        }\r\n        Map<String, String> env = new HashMap<String, String>();\r\n        MRApps.setClasspath(env, conf);\r\n        initialClasspath = env.get(Environment.CLASSPATH.name());\r\n        initialAppClasspath = env.get(Environment.APP_CLASSPATH.name());\r\n        initialClasspathFlag.set(true);\r\n        return initialClasspath;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createCommonContainerLaunchContext",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ContainerLaunchContext createCommonContainerLaunchContext(Map<ApplicationAccessType, String> applicationACLs, Configuration conf, Token<JobTokenIdentifier> jobToken, final org.apache.hadoop.mapred.JobID oldJobId, Credentials credentials)\n{\r\n    Map<String, LocalResource> localResources = new HashMap<String, LocalResource>();\r\n    Map<String, String> environment;\r\n    Map<String, ByteBuffer> serviceData = new HashMap<String, ByteBuffer>();\r\n    ByteBuffer taskCredentialsBuffer = ByteBuffer.wrap(new byte[] {});\r\n    try {\r\n        configureJobJar(conf, localResources);\r\n        configureJobConf(conf, localResources, oldJobId);\r\n        MRApps.setupDistributedCache(conf, localResources);\r\n        taskCredentialsBuffer = configureTokens(jobToken, credentials, serviceData);\r\n        addExternalShuffleProviders(conf, serviceData);\r\n        environment = configureEnv(conf);\r\n    } catch (IOException e) {\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n    ContainerLaunchContext container = ContainerLaunchContext.newInstance(localResources, environment, null, serviceData, taskCredentialsBuffer, applicationACLs);\r\n    return container;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "configureEnv",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Map<String, String> configureEnv(Configuration conf) throws IOException\n{\r\n    Map<String, String> environment = new HashMap<String, String>();\r\n    MRApps.addToEnvironment(environment, Environment.CLASSPATH.name(), getInitialClasspath(conf), conf);\r\n    if (initialAppClasspath != null) {\r\n        MRApps.addToEnvironment(environment, Environment.APP_CLASSPATH.name(), initialAppClasspath, conf);\r\n    }\r\n    environment.put(Environment.SHELL.name(), conf.get(MRJobConfig.MAPRED_ADMIN_USER_SHELL, MRJobConfig.DEFAULT_SHELL));\r\n    MRApps.addToEnvironment(environment, Environment.LD_LIBRARY_PATH.name(), MRApps.crossPlatformifyMREnv(conf, Environment.PWD), conf);\r\n    MRApps.setEnvFromInputProperty(environment, MRJobConfig.MAPRED_ADMIN_USER_ENV, MRJobConfig.DEFAULT_MAPRED_ADMIN_USER_ENV, conf);\r\n    return environment;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "configureJobJar",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void configureJobJar(Configuration conf, Map<String, LocalResource> localResources) throws IOException\n{\r\n    String jobJar = conf.get(MRJobConfig.JAR);\r\n    if (jobJar != null) {\r\n        final Path jobJarPath = new Path(jobJar);\r\n        final FileSystem jobJarFs = FileSystem.get(jobJarPath.toUri(), conf);\r\n        Path remoteJobJar = jobJarPath.makeQualified(jobJarFs.getUri(), jobJarFs.getWorkingDirectory());\r\n        LocalResourceVisibility jobJarViz = conf.getBoolean(MRJobConfig.JOBJAR_VISIBILITY, MRJobConfig.JOBJAR_VISIBILITY_DEFAULT) ? LocalResourceVisibility.PUBLIC : LocalResourceVisibility.APPLICATION;\r\n        LocalResource rc = createLocalResource(jobJarFs, remoteJobJar, MRJobConfig.JOB_JAR, LocalResourceType.PATTERN, jobJarViz);\r\n        String pattern = conf.getPattern(JobContext.JAR_UNPACK_PATTERN, JobConf.UNPACK_JAR_PATTERN_DEFAULT).pattern();\r\n        rc.setPattern(pattern);\r\n        localResources.put(MRJobConfig.JOB_JAR, rc);\r\n        LOG.info(\"The job-jar file on the remote FS is \" + remoteJobJar.toUri().toASCIIString());\r\n    } else {\r\n        LOG.info(\"Job jar is not present. \" + \"Not adding any jar to the list of resources.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "configureJobConf",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void configureJobConf(Configuration conf, Map<String, LocalResource> localResources, final org.apache.hadoop.mapred.JobID oldJobId) throws IOException\n{\r\n    Path path = MRApps.getStagingAreaDir(conf, UserGroupInformation.getCurrentUser().getShortUserName());\r\n    Path remoteJobSubmitDir = new Path(path, oldJobId.toString());\r\n    Path remoteJobConfPath = new Path(remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\r\n    FileSystem remoteFS = FileSystem.get(conf);\r\n    localResources.put(MRJobConfig.JOB_CONF_FILE, createLocalResource(remoteFS, remoteJobConfPath, null, LocalResourceType.FILE, LocalResourceVisibility.APPLICATION));\r\n    LOG.info(\"The job-conf file on the remote FS is \" + remoteJobConfPath.toUri().toASCIIString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "configureTokens",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "ByteBuffer configureTokens(Token<JobTokenIdentifier> jobToken, Credentials credentials, Map<String, ByteBuffer> serviceData) throws IOException\n{\r\n    LOG.info(\"Adding #\" + credentials.numberOfTokens() + \" tokens and #\" + credentials.numberOfSecretKeys() + \" secret keys for NM use for launching container\");\r\n    Credentials taskCredentials = new Credentials(credentials);\r\n    TokenCache.setJobToken(jobToken, taskCredentials);\r\n    DataOutputBuffer containerTokens_dob = new DataOutputBuffer();\r\n    LOG.info(\"Size of containertokens_dob is \" + taskCredentials.numberOfTokens());\r\n    taskCredentials.writeTokenStorageToStream(containerTokens_dob);\r\n    ByteBuffer taskCredentialsBuffer = ByteBuffer.wrap(containerTokens_dob.getData(), 0, containerTokens_dob.getLength());\r\n    LOG.info(\"Putting shuffle token in serviceData\");\r\n    byte[] shuffleSecret = TokenCache.getShuffleSecretKey(credentials);\r\n    if (shuffleSecret == null) {\r\n        LOG.warn(\"Cannot locate shuffle secret in credentials.\" + \" Using job token as shuffle secret.\");\r\n        shuffleSecret = jobToken.getPassword();\r\n    }\r\n    Token<JobTokenIdentifier> shuffleToken = new Token<JobTokenIdentifier>(jobToken.getIdentifier(), shuffleSecret, jobToken.getKind(), jobToken.getService());\r\n    serviceData.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID, ShuffleHandler.serializeServiceData(shuffleToken));\r\n    return taskCredentialsBuffer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "addExternalShuffleProviders",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void addExternalShuffleProviders(Configuration conf, Map<String, ByteBuffer> serviceData)\n{\r\n    Collection<String> shuffleProviders = conf.getStringCollection(MRJobConfig.MAPREDUCE_JOB_SHUFFLE_PROVIDER_SERVICES);\r\n    if (!shuffleProviders.isEmpty()) {\r\n        Collection<String> auxNames = conf.getStringCollection(YarnConfiguration.NM_AUX_SERVICES);\r\n        for (final String shuffleProvider : shuffleProviders) {\r\n            if (shuffleProvider.equals(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID)) {\r\n                continue;\r\n            }\r\n            if (auxNames.contains(shuffleProvider)) {\r\n                LOG.info(\"Adding ShuffleProvider Service: \" + shuffleProvider + \" to serviceData\");\r\n                serviceData.put(shuffleProvider, ByteBuffer.allocate(0));\r\n            } else {\r\n                throw new YarnRuntimeException(\"ShuffleProvider Service: \" + shuffleProvider + \" was NOT found in the list of aux-services that are \" + \"available in this NM. You may need to specify this \" + \"ShuffleProvider as an aux-service in your yarn-site.xml\");\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createContainerLaunchContext",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "ContainerLaunchContext createContainerLaunchContext(Map<ApplicationAccessType, String> applicationACLs, Configuration conf, Token<JobTokenIdentifier> jobToken, Task remoteTask, final org.apache.hadoop.mapred.JobID oldJobId, WrappedJvmID jvmID, TaskAttemptListener taskAttemptListener, Credentials credentials)\n{\r\n    synchronized (commonContainerSpecLock) {\r\n        if (commonContainerSpec == null) {\r\n            commonContainerSpec = createCommonContainerLaunchContext(applicationACLs, conf, jobToken, oldJobId, credentials);\r\n        }\r\n    }\r\n    boolean userClassesTakesPrecedence = conf.getBoolean(MRJobConfig.MAPREDUCE_JOB_USER_CLASSPATH_FIRST, false);\r\n    Map<String, String> env = commonContainerSpec.getEnvironment();\r\n    Map<String, String> myEnv = new HashMap<String, String>(env.size());\r\n    myEnv.putAll(env);\r\n    if (userClassesTakesPrecedence) {\r\n        myEnv.put(Environment.CLASSPATH_PREPEND_DISTCACHE.name(), \"true\");\r\n    }\r\n    MapReduceChildJVM.setVMEnv(myEnv, remoteTask);\r\n    List<String> commands = MapReduceChildJVM.getVMCommand(taskAttemptListener.getAddress(), remoteTask, jvmID);\r\n    Map<String, ByteBuffer> myServiceData = new HashMap<String, ByteBuffer>();\r\n    for (Entry<String, ByteBuffer> entry : commonContainerSpec.getServiceData().entrySet()) {\r\n        myServiceData.put(entry.getKey(), entry.getValue().duplicate());\r\n    }\r\n    ContainerLaunchContext container = ContainerLaunchContext.newInstance(commonContainerSpec.getLocalResources(), myEnv, commands, myServiceData, commonContainerSpec.getTokens().duplicate(), applicationACLs);\r\n    return container;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getAssignedContainerID",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ContainerId getAssignedContainerID()\n{\r\n    readLock.lock();\r\n    try {\r\n        return container == null ? null : container.getId();\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getAssignedContainerMgrAddress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getAssignedContainerMgrAddress()\n{\r\n    readLock.lock();\r\n    try {\r\n        return container == null ? null : StringInterner.weakIntern(container.getNodeId().toString());\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getLaunchTime",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getLaunchTime()\n{\r\n    readLock.lock();\r\n    try {\r\n        return launchTime;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    readLock.lock();\r\n    try {\r\n        return finishTime;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getShuffleFinishTime",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getShuffleFinishTime()\n{\r\n    readLock.lock();\r\n    try {\r\n        return this.reportedStatus.shuffleFinishTime;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getSortFinishTime",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getSortFinishTime()\n{\r\n    readLock.lock();\r\n    try {\r\n        return this.reportedStatus.sortFinishTime;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getShufflePort",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getShufflePort()\n{\r\n    readLock.lock();\r\n    try {\r\n        return shufflePort;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getNodeId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "NodeId getNodeId()\n{\r\n    readLock.lock();\r\n    try {\r\n        return container == null ? null : container.getNodeId();\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getNodeHttpAddress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getNodeHttpAddress()\n{\r\n    readLock.lock();\r\n    try {\r\n        return container == null ? null : container.getNodeHttpAddress();\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getNodeRackName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getNodeRackName()\n{\r\n    this.readLock.lock();\r\n    try {\r\n        return this.nodeRackName;\r\n    } finally {\r\n        this.readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createRemoteTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "org.apache.hadoop.mapred.Task createRemoteTask()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptId getID()\n{\r\n    return attemptId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "isFinished",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean isFinished()\n{\r\n    readLock.lock();\r\n    try {\r\n        return (getInternalState() == TaskAttemptStateInternal.SUCCEEDED || getInternalState() == TaskAttemptStateInternal.FAILED || getInternalState() == TaskAttemptStateInternal.KILLED);\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getReport",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "TaskAttemptReport getReport()\n{\r\n    TaskAttemptReport result = recordFactory.newRecordInstance(TaskAttemptReport.class);\r\n    readLock.lock();\r\n    try {\r\n        result.setTaskAttemptId(attemptId);\r\n        result.setTaskAttemptState(getState());\r\n        result.setProgress(reportedStatus.progress);\r\n        result.setStartTime(launchTime);\r\n        result.setFinishTime(finishTime);\r\n        result.setShuffleFinishTime(this.reportedStatus.shuffleFinishTime);\r\n        result.setDiagnosticInfo(StringUtils.join(LINE_SEPARATOR, getDiagnostics()));\r\n        result.setPhase(reportedStatus.phase);\r\n        result.setStateString(reportedStatus.stateString);\r\n        result.setCounters(TypeConverter.toYarn(getCounters()));\r\n        result.setContainerId(this.getAssignedContainerID());\r\n        result.setNodeManagerHost(trackerName);\r\n        result.setNodeManagerHttpPort(httpPort);\r\n        if (this.container != null) {\r\n            result.setNodeManagerPort(this.container.getNodeId().getPort());\r\n        }\r\n        return result;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<String> getDiagnostics()\n{\r\n    List<String> result = new ArrayList<String>();\r\n    readLock.lock();\r\n    try {\r\n        result.addAll(diagnostics);\r\n        return result;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Counters getCounters()\n{\r\n    readLock.lock();\r\n    try {\r\n        Counters counters = reportedStatus.counters;\r\n        if (counters == null) {\r\n            counters = EMPTY_COUNTERS;\r\n        }\r\n        return counters;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "float getProgress()\n{\r\n    readLock.lock();\r\n    try {\r\n        return reportedStatus.progress;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getPhase",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Phase getPhase()\n{\r\n    readLock.lock();\r\n    try {\r\n        return reportedStatus.phase;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "TaskAttemptState getState()\n{\r\n    readLock.lock();\r\n    try {\r\n        return getExternalState(stateMachine.getCurrentState());\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "handle",
  "errType" : [ "InvalidStateTransitionException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void handle(TaskAttemptEvent event)\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Processing \" + event.getTaskAttemptID() + \" of type \" + event.getType());\r\n    }\r\n    writeLock.lock();\r\n    try {\r\n        final TaskAttemptStateInternal oldState = getInternalState();\r\n        try {\r\n            stateMachine.doTransition(event.getType(), event);\r\n        } catch (InvalidStateTransitionException e) {\r\n            LOG.error(\"Can't handle this event at current state for \" + this.attemptId, e);\r\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(this.attemptId.getTaskId().getJobId(), \"Invalid event \" + event.getType() + \" on TaskAttempt \" + this.attemptId));\r\n            eventHandler.handle(new JobEvent(this.attemptId.getTaskId().getJobId(), JobEventType.INTERNAL_ERROR));\r\n        }\r\n        if (oldState != getInternalState()) {\r\n            if (getInternalState() == TaskAttemptStateInternal.FAILED) {\r\n                String nodeId = null == this.container ? \"Not-assigned\" : this.container.getNodeId().toString();\r\n                LOG.info(attemptId + \" transitioned from state \" + oldState + \" to \" + getInternalState() + \", event type is \" + event.getType() + \" and nodeId=\" + nodeId);\r\n            } else {\r\n                LOG.info(attemptId + \" TaskAttempt Transitioned from \" + oldState + \" to \" + getInternalState());\r\n            }\r\n        }\r\n    } finally {\r\n        writeLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getInternalState",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "TaskAttemptStateInternal getInternalState()\n{\r\n    readLock.lock();\r\n    try {\r\n        return stateMachine.getCurrentState();\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getLocality",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Locality getLocality()\n{\r\n    return locality;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "setLocality",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLocality(Locality locality)\n{\r\n    this.locality = locality;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getAvataar",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Avataar getAvataar()\n{\r\n    return avataar;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "setAvataar",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAvataar(Avataar avataar)\n{\r\n    this.avataar = avataar;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "setTaskFailFast",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTaskFailFast(boolean failFast)\n{\r\n    this.failFast = failFast;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "isTaskFailFast",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isTaskFailFast()\n{\r\n    return failFast;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "recover",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 45,
  "sourceCodeText" : "TaskAttemptStateInternal recover(TaskAttemptInfo taInfo, OutputCommitter committer, boolean recoverOutput)\n{\r\n    ContainerId containerId = taInfo.getContainerId();\r\n    NodeId containerNodeId = NodeId.fromString(taInfo.getHostname() + \":\" + taInfo.getPort());\r\n    String nodeHttpAddress = StringInterner.weakIntern(taInfo.getHostname() + \":\" + taInfo.getHttpPort());\r\n    container = Container.newInstance(containerId, containerNodeId, nodeHttpAddress, null, null, null);\r\n    computeRackAndLocality();\r\n    launchTime = taInfo.getStartTime();\r\n    finishTime = (taInfo.getFinishTime() != -1) ? taInfo.getFinishTime() : clock.getTime();\r\n    shufflePort = taInfo.getShufflePort();\r\n    trackerName = taInfo.getHostname();\r\n    httpPort = taInfo.getHttpPort();\r\n    sendLaunchedEvents();\r\n    reportedStatus.id = attemptId;\r\n    reportedStatus.progress = 1.0f;\r\n    reportedStatus.counters = taInfo.getCounters();\r\n    reportedStatus.stateString = taInfo.getState();\r\n    reportedStatus.phase = Phase.CLEANUP;\r\n    reportedStatus.mapFinishTime = taInfo.getMapFinishTime();\r\n    reportedStatus.shuffleFinishTime = taInfo.getShuffleFinishTime();\r\n    reportedStatus.sortFinishTime = taInfo.getSortFinishTime();\r\n    addDiagnosticInfo(taInfo.getError());\r\n    boolean needToClean = false;\r\n    String recoveredState = taInfo.getTaskStatus();\r\n    if (recoverOutput && TaskAttemptState.SUCCEEDED.toString().equals(recoveredState)) {\r\n        TaskAttemptContext tac = new TaskAttemptContextImpl(conf, TypeConverter.fromYarn(attemptId));\r\n        try {\r\n            committer.recoverTask(tac);\r\n            LOG.info(\"Recovered output from task attempt \" + attemptId);\r\n        } catch (Exception e) {\r\n            LOG.error(\"Unable to recover task attempt \" + attemptId, e);\r\n            LOG.info(\"Task attempt \" + attemptId + \" will be recovered as KILLED\");\r\n            recoveredState = TaskAttemptState.KILLED.toString();\r\n            needToClean = true;\r\n        }\r\n    }\r\n    TaskAttemptStateInternal attemptState;\r\n    if (TaskAttemptState.SUCCEEDED.toString().equals(recoveredState)) {\r\n        attemptState = TaskAttemptStateInternal.SUCCEEDED;\r\n        reportedStatus.taskState = TaskAttemptState.SUCCEEDED;\r\n        eventHandler.handle(createJobCounterUpdateEventTASucceeded(this));\r\n        logAttemptFinishedEvent(attemptState);\r\n    } else if (TaskAttemptState.FAILED.toString().equals(recoveredState)) {\r\n        attemptState = TaskAttemptStateInternal.FAILED;\r\n        reportedStatus.taskState = TaskAttemptState.FAILED;\r\n        eventHandler.handle(createJobCounterUpdateEventTAFailed(this, false));\r\n        TaskAttemptUnsuccessfulCompletionEvent tauce = createTaskAttemptUnsuccessfulCompletionEvent(this, TaskAttemptStateInternal.FAILED);\r\n        eventHandler.handle(new JobHistoryEvent(attemptId.getTaskId().getJobId(), tauce));\r\n    } else {\r\n        if (!TaskAttemptState.KILLED.toString().equals(recoveredState)) {\r\n            if (String.valueOf(recoveredState).isEmpty()) {\r\n                LOG.info(\"TaskAttempt\" + attemptId + \" had not completed, recovering as KILLED\");\r\n            } else {\r\n                LOG.warn(\"TaskAttempt \" + attemptId + \" found in unexpected state \" + recoveredState + \", recovering as KILLED\");\r\n            }\r\n            addDiagnosticInfo(\"Killed during application recovery\");\r\n            needToClean = true;\r\n        }\r\n        attemptState = TaskAttemptStateInternal.KILLED;\r\n        reportedStatus.taskState = TaskAttemptState.KILLED;\r\n        eventHandler.handle(createJobCounterUpdateEventTAKilled(this, false));\r\n        TaskAttemptUnsuccessfulCompletionEvent tauce = createTaskAttemptUnsuccessfulCompletionEvent(this, TaskAttemptStateInternal.KILLED);\r\n        eventHandler.handle(new JobHistoryEvent(attemptId.getTaskId().getJobId(), tauce));\r\n    }\r\n    if (needToClean) {\r\n        TaskAttemptContext tac = new TaskAttemptContextImpl(conf, TypeConverter.fromYarn(attemptId));\r\n        try {\r\n            committer.abortTask(tac);\r\n        } catch (Exception e) {\r\n            LOG.warn(\"Task cleanup failed for attempt \" + attemptId, e);\r\n        }\r\n    }\r\n    return attemptState;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getExternalState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptState getExternalState(TaskAttemptStateInternal smState)\n{\r\n    switch(smState) {\r\n        case ASSIGNED:\r\n        case UNASSIGNED:\r\n            return TaskAttemptState.STARTING;\r\n        case COMMIT_PENDING:\r\n            return TaskAttemptState.COMMIT_PENDING;\r\n        case FAIL_CONTAINER_CLEANUP:\r\n        case FAIL_TASK_CLEANUP:\r\n        case FAIL_FINISHING_CONTAINER:\r\n        case FAILED:\r\n            return TaskAttemptState.FAILED;\r\n        case KILL_CONTAINER_CLEANUP:\r\n        case KILL_TASK_CLEANUP:\r\n        case KILLED:\r\n            return TaskAttemptState.KILLED;\r\n        case RUNNING:\r\n            return TaskAttemptState.RUNNING;\r\n        case NEW:\r\n            return TaskAttemptState.NEW;\r\n        case SUCCESS_CONTAINER_CLEANUP:\r\n        case SUCCESS_FINISHING_CONTAINER:\r\n        case SUCCEEDED:\r\n            return TaskAttemptState.SUCCEEDED;\r\n        default:\r\n            throw new YarnRuntimeException(\"Attempt to convert invalid \" + \"stateMachineTaskAttemptState to externalTaskAttemptState: \" + smState);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "isContainerAssigned",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isContainerAssigned()\n{\r\n    return container != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getRescheduleNextAttempt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getRescheduleNextAttempt()\n{\r\n    return rescheduleNextAttempt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "setRescheduleNextAttempt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRescheduleNextAttempt(boolean reschedule)\n{\r\n    rescheduleNextAttempt = reschedule;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "setFinishTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFinishTime()\n{\r\n    if (launchTime != 0) {\r\n        finishTime = clock.getTime();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "computeRackAndLocality",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void computeRackAndLocality()\n{\r\n    NodeId containerNodeId = container.getNodeId();\r\n    nodeRackName = RackResolver.resolve(containerNodeId.getHost()).getNetworkLocation();\r\n    locality = Locality.OFF_SWITCH;\r\n    if (dataLocalHosts.size() > 0) {\r\n        String cHost = resolveHost(containerNodeId.getHost());\r\n        if (dataLocalHosts.contains(cHost)) {\r\n            locality = Locality.NODE_LOCAL;\r\n        }\r\n    }\r\n    if (locality == Locality.OFF_SWITCH) {\r\n        if (dataLocalRacks.contains(nodeRackName)) {\r\n            locality = Locality.RACK_LOCAL;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "updateMillisCounters",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void updateMillisCounters(JobCounterUpdateEvent jce, TaskAttemptImpl taskAttempt)\n{\r\n    if (null == taskAttempt.container || null == taskAttempt.container.getResource()) {\r\n        return;\r\n    }\r\n    long duration = (taskAttempt.getFinishTime() - taskAttempt.getLaunchTime());\r\n    Resource allocatedResource = taskAttempt.container.getResource();\r\n    int mbAllocated = (int) allocatedResource.getMemorySize();\r\n    int vcoresAllocated = allocatedResource.getVirtualCores();\r\n    int minSlotMemSize = taskAttempt.conf.getInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, YarnConfiguration.DEFAULT_RM_SCHEDULER_MINIMUM_ALLOCATION_MB);\r\n    int simSlotsAllocated = minSlotMemSize == 0 ? 0 : (int) Math.ceil((float) mbAllocated / minSlotMemSize);\r\n    TaskType taskType = taskAttempt.getID().getTaskId().getTaskType();\r\n    if (taskType == TaskType.MAP) {\r\n        jce.addCounterUpdate(JobCounter.SLOTS_MILLIS_MAPS, simSlotsAllocated * duration);\r\n        jce.addCounterUpdate(JobCounter.MB_MILLIS_MAPS, duration * mbAllocated);\r\n        jce.addCounterUpdate(JobCounter.VCORES_MILLIS_MAPS, duration * vcoresAllocated);\r\n        jce.addCounterUpdate(JobCounter.MILLIS_MAPS, duration);\r\n    } else {\r\n        jce.addCounterUpdate(JobCounter.SLOTS_MILLIS_REDUCES, simSlotsAllocated * duration);\r\n        jce.addCounterUpdate(JobCounter.MB_MILLIS_REDUCES, duration * mbAllocated);\r\n        jce.addCounterUpdate(JobCounter.VCORES_MILLIS_REDUCES, duration * vcoresAllocated);\r\n        jce.addCounterUpdate(JobCounter.MILLIS_REDUCES, duration);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createJobCounterUpdateEventTASucceeded",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobCounterUpdateEvent createJobCounterUpdateEventTASucceeded(TaskAttemptImpl taskAttempt)\n{\r\n    TaskId taskId = taskAttempt.attemptId.getTaskId();\r\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(taskId.getJobId());\r\n    updateMillisCounters(jce, taskAttempt);\r\n    return jce;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createJobCounterUpdateEventTAFailed",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "JobCounterUpdateEvent createJobCounterUpdateEventTAFailed(TaskAttemptImpl taskAttempt, boolean taskAlreadyCompleted)\n{\r\n    TaskType taskType = taskAttempt.getID().getTaskId().getTaskType();\r\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(taskAttempt.getID().getTaskId().getJobId());\r\n    if (taskType == TaskType.MAP) {\r\n        jce.addCounterUpdate(JobCounter.NUM_FAILED_MAPS, 1);\r\n    } else {\r\n        jce.addCounterUpdate(JobCounter.NUM_FAILED_REDUCES, 1);\r\n    }\r\n    if (!taskAlreadyCompleted) {\r\n        updateMillisCounters(jce, taskAttempt);\r\n    }\r\n    return jce;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createJobCounterUpdateEventTAKilled",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "JobCounterUpdateEvent createJobCounterUpdateEventTAKilled(TaskAttemptImpl taskAttempt, boolean taskAlreadyCompleted)\n{\r\n    TaskType taskType = taskAttempt.getID().getTaskId().getTaskType();\r\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(taskAttempt.getID().getTaskId().getJobId());\r\n    if (taskType == TaskType.MAP) {\r\n        jce.addCounterUpdate(JobCounter.NUM_KILLED_MAPS, 1);\r\n    } else {\r\n        jce.addCounterUpdate(JobCounter.NUM_KILLED_REDUCES, 1);\r\n    }\r\n    if (!taskAlreadyCompleted) {\r\n        updateMillisCounters(jce, taskAttempt);\r\n    }\r\n    return jce;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createTaskAttemptUnsuccessfulCompletionEvent",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "TaskAttemptUnsuccessfulCompletionEvent createTaskAttemptUnsuccessfulCompletionEvent(TaskAttemptImpl taskAttempt, TaskAttemptStateInternal attemptState)\n{\r\n    TaskAttemptUnsuccessfulCompletionEvent tauce = new TaskAttemptUnsuccessfulCompletionEvent(TypeConverter.fromYarn(taskAttempt.attemptId), TypeConverter.fromYarn(taskAttempt.attemptId.getTaskId().getTaskType()), attemptState.toString(), taskAttempt.finishTime, taskAttempt.container == null ? \"UNKNOWN\" : taskAttempt.container.getNodeId().getHost(), taskAttempt.container == null ? -1 : taskAttempt.container.getNodeId().getPort(), taskAttempt.nodeRackName == null ? \"UNKNOWN\" : taskAttempt.nodeRackName, StringUtils.join(LINE_SEPARATOR, taskAttempt.getDiagnostics()), taskAttempt.getCounters(), taskAttempt.getProgressSplitBlock().burst(), taskAttempt.launchTime);\r\n    return tauce;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "sendJHStartEventForAssignedFailTask",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void sendJHStartEventForAssignedFailTask(TaskAttemptImpl taskAttempt)\n{\r\n    if (null == taskAttempt.container) {\r\n        return;\r\n    }\r\n    taskAttempt.launchTime = taskAttempt.clock.getTime();\r\n    InetSocketAddress nodeHttpInetAddr = NetUtils.createSocketAddr(taskAttempt.container.getNodeHttpAddress());\r\n    taskAttempt.trackerName = nodeHttpInetAddr.getHostName();\r\n    taskAttempt.httpPort = nodeHttpInetAddr.getPort();\r\n    taskAttempt.sendLaunchedEvents();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "sendLaunchedEvents",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void sendLaunchedEvents()\n{\r\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(attemptId.getTaskId().getJobId());\r\n    jce.addCounterUpdate(attemptId.getTaskId().getTaskType() == TaskType.MAP ? JobCounter.TOTAL_LAUNCHED_MAPS : JobCounter.TOTAL_LAUNCHED_REDUCES, 1);\r\n    eventHandler.handle(jce);\r\n    LOG.info(\"TaskAttempt: [\" + attemptId + \"] using containerId: [\" + container.getId() + \" on NM: [\" + StringInterner.weakIntern(container.getNodeId().toString()) + \"]\");\r\n    TaskAttemptStartedEvent tase = new TaskAttemptStartedEvent(TypeConverter.fromYarn(attemptId), TypeConverter.fromYarn(attemptId.getTaskId().getTaskType()), launchTime, trackerName, httpPort, shufflePort, container.getId(), locality.toString(), avataar.toString());\r\n    eventHandler.handle(new JobHistoryEvent(attemptId.getTaskId().getJobId(), tase));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getProgressSplitBlock",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "WrappedProgressSplitsBlock getProgressSplitBlock()\n{\r\n    readLock.lock();\r\n    try {\r\n        if (progressSplitBlock == null) {\r\n            progressSplitBlock = new WrappedProgressSplitsBlock(conf.getInt(MRJobConfig.MR_AM_NUM_PROGRESS_SPLITS, MRJobConfig.DEFAULT_MR_AM_NUM_PROGRESS_SPLITS));\r\n        }\r\n        return progressSplitBlock;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "updateProgressSplits",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void updateProgressSplits()\n{\r\n    double newProgress = reportedStatus.progress;\r\n    newProgress = Math.max(Math.min(newProgress, 1.0D), 0.0D);\r\n    Counters counters = reportedStatus.counters;\r\n    if (counters == null)\r\n        return;\r\n    WrappedProgressSplitsBlock splitsBlock = getProgressSplitBlock();\r\n    if (splitsBlock != null) {\r\n        long now = clock.getTime();\r\n        long start = getLaunchTime();\r\n        if (start != 0 && now - start <= Integer.MAX_VALUE) {\r\n            splitsBlock.getProgressWallclockTime().extend(newProgress, (int) (now - start));\r\n        }\r\n        Counter cpuCounter = counters.findCounter(TaskCounter.CPU_MILLISECONDS);\r\n        if (cpuCounter != null && cpuCounter.getValue() <= Integer.MAX_VALUE) {\r\n            splitsBlock.getProgressCPUTime().extend(newProgress, (int) cpuCounter.getValue());\r\n        }\r\n        Counter virtualBytes = counters.findCounter(TaskCounter.VIRTUAL_MEMORY_BYTES);\r\n        if (virtualBytes != null) {\r\n            splitsBlock.getProgressVirtualMemoryKbytes().extend(newProgress, (int) (virtualBytes.getValue() / (MEMORY_SPLITS_RESOLUTION)));\r\n        }\r\n        Counter physicalBytes = counters.findCounter(TaskCounter.PHYSICAL_MEMORY_BYTES);\r\n        if (physicalBytes != null) {\r\n            splitsBlock.getProgressPhysicalMemoryKbytes().extend(newProgress, (int) (physicalBytes.getValue() / (MEMORY_SPLITS_RESOLUTION)));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "finalizeProgress",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void finalizeProgress(TaskAttemptImpl taskAttempt)\n{\r\n    taskAttempt.taskAttemptListener.unregister(taskAttempt.attemptId, taskAttempt.jvmID);\r\n    taskAttempt.reportedStatus.progress = 1.0f;\r\n    taskAttempt.updateProgressSplits();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "resolveHosts",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Set<String> resolveHosts(String[] src)\n{\r\n    Set<String> result = new HashSet<String>();\r\n    if (src != null) {\r\n        for (int i = 0; i < src.length; i++) {\r\n            if (src[i] == null) {\r\n                continue;\r\n            } else if (isIP(src[i])) {\r\n                result.add(resolveHost(src[i]));\r\n            } else {\r\n                result.add(src[i]);\r\n            }\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "resolveHost",
  "errType" : [ "UnknownHostException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String resolveHost(String src)\n{\r\n    String result = src;\r\n    try {\r\n        InetAddress addr = InetAddress.getByName(src);\r\n        result = addr.getHostName();\r\n    } catch (UnknownHostException e) {\r\n        LOG.warn(\"Failed to resolve address: \" + src + \". Continuing to use the same.\");\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "isIP",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isIP(String src)\n{\r\n    return ipPattern.matcher(src).matches();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "sendContainerCompleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sendContainerCompleted(TaskAttemptImpl taskAttempt)\n{\r\n    taskAttempt.eventHandler.handle(new ContainerLauncherEvent(taskAttempt.attemptId, taskAttempt.container.getId(), StringInterner.weakIntern(taskAttempt.container.getNodeId().toString()), taskAttempt.container.getContainerToken(), ContainerLauncher.EventType.CONTAINER_COMPLETED));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "logAttemptFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void logAttemptFinishedEvent(TaskAttemptStateInternal state)\n{\r\n    if (getLaunchTime() == 0)\r\n        return;\r\n    String containerHostName = this.container == null ? \"UNKNOWN\" : this.container.getNodeId().getHost();\r\n    int containerNodePort = this.container == null ? -1 : this.container.getNodeId().getPort();\r\n    if (attemptId.getTaskId().getTaskType() == TaskType.MAP) {\r\n        MapAttemptFinishedEvent mfe = new MapAttemptFinishedEvent(TypeConverter.fromYarn(attemptId), TypeConverter.fromYarn(attemptId.getTaskId().getTaskType()), state.toString(), this.reportedStatus.mapFinishTime, finishTime, containerHostName, containerNodePort, this.nodeRackName == null ? \"UNKNOWN\" : this.nodeRackName, this.reportedStatus.stateString, getCounters(), getProgressSplitBlock().burst(), launchTime);\r\n        eventHandler.handle(new JobHistoryEvent(attemptId.getTaskId().getJobId(), mfe));\r\n    } else {\r\n        ReduceAttemptFinishedEvent rfe = new ReduceAttemptFinishedEvent(TypeConverter.fromYarn(attemptId), TypeConverter.fromYarn(attemptId.getTaskId().getTaskType()), state.toString(), this.reportedStatus.shuffleFinishTime, this.reportedStatus.sortFinishTime, finishTime, containerHostName, containerNodePort, this.nodeRackName == null ? \"UNKNOWN\" : this.nodeRackName, this.reportedStatus.stateString, getCounters(), getProgressSplitBlock().burst(), launchTime);\r\n        eventHandler.handle(new JobHistoryEvent(attemptId.getTaskId().getJobId(), rfe));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "sendContainerCleanup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void sendContainerCleanup(TaskAttemptImpl taskAttempt, TaskAttemptEvent event)\n{\r\n    if (event instanceof TaskAttemptKillEvent) {\r\n        taskAttempt.addDiagnosticInfo(((TaskAttemptKillEvent) event).getMessage());\r\n    }\r\n    taskAttempt.eventHandler.handle(new ContainerLauncherEvent(taskAttempt.attemptId, taskAttempt.container.getId(), StringInterner.weakIntern(taskAttempt.container.getNodeId().toString()), taskAttempt.container.getContainerToken(), ContainerLauncher.EventType.CONTAINER_REMOTE_CLEANUP, event.getType() == TaskAttemptEventType.TA_TIMED_OUT));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "notifyTaskAttemptFailed",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void notifyTaskAttemptFailed(TaskAttemptImpl taskAttempt, boolean fastFail)\n{\r\n    if (taskAttempt.getLaunchTime() == 0) {\r\n        sendJHStartEventForAssignedFailTask(taskAttempt);\r\n    }\r\n    taskAttempt.setFinishTime();\r\n    taskAttempt.eventHandler.handle(createJobCounterUpdateEventTAFailed(taskAttempt, false));\r\n    TaskAttemptUnsuccessfulCompletionEvent tauce = createTaskAttemptUnsuccessfulCompletionEvent(taskAttempt, TaskAttemptStateInternal.FAILED);\r\n    taskAttempt.eventHandler.handle(new JobHistoryEvent(taskAttempt.attemptId.getTaskId().getJobId(), tauce));\r\n    taskAttempt.eventHandler.handle(new TaskTAttemptFailedEvent(taskAttempt.attemptId, fastFail));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "addDiagnosticInfo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addDiagnosticInfo(String diag)\n{\r\n    if (diag != null && !diag.equals(\"\")) {\r\n        diagnostics.add(diag);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "initTaskAttemptStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initTaskAttemptStatus(TaskAttemptStatus result)\n{\r\n    result.progress = 0.0f;\r\n    result.phase = Phase.STARTING;\r\n    result.stateString = \"NEW\";\r\n    result.taskState = TaskAttemptState.NEW;\r\n    Counters counters = EMPTY_COUNTERS;\r\n    result.counters = counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getRescheduleAttempt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getRescheduleAttempt()\n{\r\n    return rescheduleAttempt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    String jobID = $(JOB_ID);\r\n    set(TITLE, jobID.isEmpty() ? \"Bad request: missing job ID\" : join(\"MapReduce Job \", $(JOB_ID)));\r\n    commonPreHead(html);\r\n    set(initID(ACCORDION, \"nav\"), \"{autoHeight:false, active:2}\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return JobBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<TaskAttemptId> getMaps()\n{\r\n    return maps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getReduce",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptId getReduce()\n{\r\n    return reduce;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getHost",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHost()\n{\r\n    return hostname;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getProgress()\n{\r\n    return this.progress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getState()\n{\r\n    return this.state.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getId()\n{\r\n    return this.id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getTaskNum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTaskNum()\n{\r\n    return this.taskNum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return this.startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return this.finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getElapsedTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getElapsedTime()\n{\r\n    return this.elapsedTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getSuccessfulAttempt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSuccessfulAttempt()\n{\r\n    return this.successfulAttempt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getSuccessful",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttempt getSuccessful()\n{\r\n    return this.successful;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getSuccessfulAttempt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskAttempt getSuccessfulAttempt(Task task)\n{\r\n    for (TaskAttempt attempt : task.getAttempts().values()) {\r\n        if (attempt.getState() == TaskAttemptState.SUCCEEDED) {\r\n            return attempt;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStatus()\n{\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    taskRunner = HadoopExecutors.newSingleThreadExecutor(new ThreadFactoryBuilder().setDaemon(true).setNameFormat(\"uber-SubtaskRunner\").build());\r\n    eventHandler = new Thread(new EventHandler(), \"uber-EventHandler\");\r\n    if (jobClassLoader != null) {\r\n        LOG.info(\"Setting \" + jobClassLoader + \" as the context classloader of thread \" + eventHandler.getName());\r\n        eventHandler.setContextClassLoader(jobClassLoader);\r\n    } else {\r\n        LOG.info(\"Context classloader of thread \" + eventHandler.getName() + \": \" + eventHandler.getContextClassLoader());\r\n    }\r\n    eventHandler.start();\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    if (eventHandler != null) {\r\n        eventHandler.interrupt();\r\n    }\r\n    if (taskRunner != null) {\r\n        taskRunner.shutdownNow();\r\n    }\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "handle",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void handle(ContainerLauncherEvent event)\n{\r\n    try {\r\n        eventQueue.put(event);\r\n    } catch (InterruptedException e) {\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setEncryptedSpillKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setEncryptedSpillKey(byte[] encryptedSpillKey)\n{\r\n    if (encryptedSpillKey != null) {\r\n        this.encryptedSpillKey = encryptedSpillKey;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "renameMapOutputForReduce",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "MapOutputFile renameMapOutputForReduce(JobConf conf, TaskAttemptId mapId, MapOutputFile subMapOutputFile) throws IOException\n{\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    Path mapOut = subMapOutputFile.getOutputFile();\r\n    FileStatus mStatus = localFs.getFileStatus(mapOut);\r\n    Path reduceIn = subMapOutputFile.getInputFileForWrite(TypeConverter.fromYarn(mapId).getTaskID(), mStatus.getLen());\r\n    Path mapOutIndex = subMapOutputFile.getOutputIndexFile();\r\n    Path reduceInIndex = new Path(reduceIn.toString() + \".index\");\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Renaming map output file for task attempt \" + mapId.toString() + \" from original location \" + mapOut.toString() + \" to destination \" + reduceIn.toString());\r\n    }\r\n    if (!localFs.mkdirs(reduceIn.getParent())) {\r\n        throw new IOException(\"Mkdirs failed to create \" + reduceIn.getParent().toString());\r\n    }\r\n    if (!localFs.rename(mapOut, reduceIn))\r\n        throw new IOException(\"Couldn't rename \" + mapOut);\r\n    if (!localFs.rename(mapOutIndex, reduceInIndex))\r\n        throw new IOException(\"Couldn't rename \" + mapOutIndex);\r\n    return new RenamedMapOutputFile(reduceIn);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getReduceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptId getReduceId()\n{\r\n    return reduceID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getReduceHost",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getReduceHost()\n{\r\n    return reduceHostname;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getContainer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Container getContainer()\n{\r\n    return this.container;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getApplicationACLs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<ApplicationAccessType, String> getApplicationACLs()\n{\r\n    return this.applicationACLs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "TaskState getState()\n{\r\n    readLock.lock();\r\n    try {\r\n        return getExternalState(getInternalState());\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getAttempts",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<TaskAttemptId, TaskAttempt> getAttempts()\n{\r\n    readLock.lock();\r\n    try {\r\n        if (attempts.size() <= 1) {\r\n            return attempts;\r\n        }\r\n        Map<TaskAttemptId, TaskAttempt> result = new LinkedHashMap<TaskAttemptId, TaskAttempt>();\r\n        result.putAll(attempts);\r\n        return result;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getAttempt",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "TaskAttempt getAttempt(TaskAttemptId attemptID)\n{\r\n    readLock.lock();\r\n    try {\r\n        return attempts.get(attemptID);\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskId getID()\n{\r\n    return taskId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "isFinished",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean isFinished()\n{\r\n    readLock.lock();\r\n    try {\r\n        return (getInternalState() == TaskStateInternal.SUCCEEDED || getInternalState() == TaskStateInternal.FAILED || getInternalState() == TaskStateInternal.KILLED);\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getReport",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "TaskReport getReport()\n{\r\n    TaskReport report = recordFactory.newRecordInstance(TaskReport.class);\r\n    readLock.lock();\r\n    try {\r\n        TaskAttempt bestAttempt = selectBestAttempt();\r\n        report.setTaskId(taskId);\r\n        report.setStartTime(getLaunchTime());\r\n        report.setFinishTime(getFinishTime());\r\n        report.setTaskState(getState());\r\n        report.setProgress(bestAttempt == null ? 0f : bestAttempt.getProgress());\r\n        report.setStatus(bestAttempt == null ? \"\" : bestAttempt.getReport().getStateString());\r\n        for (TaskAttempt attempt : attempts.values()) {\r\n            if (TaskAttemptState.RUNNING.equals(attempt.getState())) {\r\n                report.addRunningAttempt(attempt.getID());\r\n            }\r\n        }\r\n        report.setSuccessfulAttempt(successfulAttempt);\r\n        for (TaskAttempt att : attempts.values()) {\r\n            String prefix = \"AttemptID:\" + att.getID() + \" Info:\";\r\n            for (CharSequence cs : att.getDiagnostics()) {\r\n                report.addDiagnostics(prefix + cs);\r\n            }\r\n        }\r\n        report.setCounters(TypeConverter.toYarn(bestAttempt == null ? TaskAttemptImpl.EMPTY_COUNTERS : bestAttempt.getCounters()));\r\n        return report;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Counters getCounters()\n{\r\n    Counters counters = null;\r\n    readLock.lock();\r\n    try {\r\n        TaskAttempt bestAttempt = selectBestAttempt();\r\n        if (bestAttempt != null) {\r\n            counters = bestAttempt.getCounters();\r\n        } else {\r\n            counters = TaskAttemptImpl.EMPTY_COUNTERS;\r\n        }\r\n        return counters;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "float getProgress()\n{\r\n    readLock.lock();\r\n    try {\r\n        TaskAttempt bestAttempt = selectBestAttempt();\r\n        if (bestAttempt == null) {\r\n            return 0f;\r\n        }\r\n        return bestAttempt.getProgress();\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getInternalState",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "TaskStateInternal getInternalState()\n{\r\n    readLock.lock();\r\n    try {\r\n        return stateMachine.getCurrentState();\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getExternalState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskState getExternalState(TaskStateInternal smState)\n{\r\n    if (smState == TaskStateInternal.KILL_WAIT) {\r\n        return TaskState.KILLED;\r\n    } else {\r\n        return TaskState.valueOf(smState.name());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getLaunchTime",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getLaunchTime()\n{\r\n    long taskLaunchTime = 0;\r\n    boolean launchTimeSet = false;\r\n    for (TaskAttempt at : attempts.values()) {\r\n        long attemptLaunchTime = at.getLaunchTime();\r\n        if (attemptLaunchTime != 0 && !launchTimeSet) {\r\n            launchTimeSet = true;\r\n            taskLaunchTime = attemptLaunchTime;\r\n        } else if (attemptLaunchTime != 0 && taskLaunchTime > attemptLaunchTime) {\r\n            taskLaunchTime = attemptLaunchTime;\r\n        }\r\n    }\r\n    if (!launchTimeSet) {\r\n        return this.scheduledTime;\r\n    }\r\n    return taskLaunchTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    if (!isFinished()) {\r\n        return 0;\r\n    }\r\n    long finishTime = 0;\r\n    for (TaskAttempt at : attempts.values()) {\r\n        if (finishTime < at.getFinishTime()) {\r\n            finishTime = at.getFinishTime();\r\n        }\r\n    }\r\n    return finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long getFinishTime(TaskAttemptId taId)\n{\r\n    if (taId == null) {\r\n        return clock.getTime();\r\n    }\r\n    long finishTime = 0;\r\n    for (TaskAttempt at : attempts.values()) {\r\n        if (at.getID().equals(taId)) {\r\n            return at.getFinishTime();\r\n        }\r\n    }\r\n    return finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "finished",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskStateInternal finished(TaskStateInternal finalState)\n{\r\n    if (getInternalState() == TaskStateInternal.RUNNING) {\r\n        metrics.endRunningTask(this);\r\n    }\r\n    return finalState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "selectBestAttempt",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "TaskAttempt selectBestAttempt()\n{\r\n    if (successfulAttempt != null) {\r\n        return attempts.get(successfulAttempt);\r\n    }\r\n    float progress = 0f;\r\n    TaskAttempt result = null;\r\n    for (TaskAttempt at : attempts.values()) {\r\n        switch(at.getState()) {\r\n            case FAILED:\r\n            case KILLED:\r\n                continue;\r\n        }\r\n        if (result == null) {\r\n            result = at;\r\n        }\r\n        float attemptProgress = at.getProgress();\r\n        if (attemptProgress > progress) {\r\n            result = at;\r\n            progress = attemptProgress;\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "canCommit",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean canCommit(TaskAttemptId taskAttemptID)\n{\r\n    readLock.lock();\r\n    boolean canCommit = false;\r\n    try {\r\n        if (commitAttempt != null) {\r\n            canCommit = taskAttemptID.equals(commitAttempt);\r\n            LOG.info(\"Result of canCommit for \" + taskAttemptID + \":\" + canCommit);\r\n        }\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n    return canCommit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createAttempt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptImpl createAttempt()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getMaxAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxAttempts()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getSuccessfulAttempt",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "TaskAttempt getSuccessfulAttempt()\n{\r\n    readLock.lock();\r\n    try {\r\n        if (null == successfulAttempt) {\r\n            return null;\r\n        }\r\n        return attempts.get(successfulAttempt);\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "addAndScheduleAttempt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addAndScheduleAttempt(Avataar avataar)\n{\r\n    addAndScheduleAttempt(avataar, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "addAndScheduleAttempt",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void addAndScheduleAttempt(Avataar avataar, boolean reschedule)\n{\r\n    TaskAttempt attempt = addAttempt(avataar);\r\n    inProgressAttempts.add(attempt.getID());\r\n    if (failedAttempts.size() > 0 || reschedule) {\r\n        eventHandler.handle(new TaskAttemptEvent(attempt.getID(), TaskAttemptEventType.TA_RESCHEDULE));\r\n    } else {\r\n        eventHandler.handle(new TaskAttemptEvent(attempt.getID(), TaskAttemptEventType.TA_SCHEDULE));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "addAttempt",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "TaskAttemptImpl addAttempt(Avataar avataar)\n{\r\n    TaskAttemptImpl attempt = createAttempt();\r\n    attempt.setAvataar(avataar);\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Created attempt \" + attempt.getID());\r\n    }\r\n    switch(attempts.size()) {\r\n        case 0:\r\n            attempts = Collections.singletonMap(attempt.getID(), (TaskAttempt) attempt);\r\n            break;\r\n        case 1:\r\n            Map<TaskAttemptId, TaskAttempt> newAttempts = new LinkedHashMap<TaskAttemptId, TaskAttempt>(maxAttempts);\r\n            newAttempts.putAll(attempts);\r\n            attempts = newAttempts;\r\n            attempts.put(attempt.getID(), attempt);\r\n            break;\r\n        default:\r\n            attempts.put(attempt.getID(), attempt);\r\n            break;\r\n    }\r\n    ++nextAttemptNumber;\r\n    return attempt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "handle",
  "errType" : [ "InvalidStateTransitionException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void handle(TaskEvent event)\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Processing \" + event.getTaskID() + \" of type \" + event.getType());\r\n    }\r\n    try {\r\n        writeLock.lock();\r\n        TaskStateInternal oldState = getInternalState();\r\n        try {\r\n            stateMachine.doTransition(event.getType(), event);\r\n        } catch (InvalidStateTransitionException e) {\r\n            LOG.error(\"Can't handle this event at current state for \" + this.taskId, e);\r\n            internalError(event.getType());\r\n        }\r\n        if (oldState != getInternalState()) {\r\n            LOG.info(taskId + \" Task Transitioned from \" + oldState + \" to \" + getInternalState());\r\n        }\r\n    } finally {\r\n        writeLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "internalError",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void internalError(TaskEventType type)\n{\r\n    LOG.error(\"Invalid event \" + type + \" on Task \" + this.taskId);\r\n    eventHandler.handle(new JobDiagnosticsUpdateEvent(this.taskId.getJobId(), \"Invalid event \" + type + \" on Task \" + this.taskId));\r\n    eventHandler.handle(new JobEvent(this.taskId.getJobId(), JobEventType.INTERNAL_ERROR));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "handleTaskAttemptCompletion",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void handleTaskAttemptCompletion(TaskAttemptId attemptId, TaskAttemptCompletionEventStatus status)\n{\r\n    TaskAttempt attempt = attempts.get(attemptId);\r\n    if (attempt.getNodeHttpAddress() != null) {\r\n        TaskAttemptCompletionEvent tce = recordFactory.newRecordInstance(TaskAttemptCompletionEvent.class);\r\n        tce.setEventId(-1);\r\n        String scheme = (encryptedShuffle) ? \"https://\" : \"http://\";\r\n        tce.setMapOutputServerAddress(StringInterner.weakIntern(scheme + attempt.getNodeHttpAddress().split(\":\")[0] + \":\" + attempt.getShufflePort()));\r\n        tce.setStatus(status);\r\n        tce.setAttemptId(attempt.getID());\r\n        int runTime = 0;\r\n        if (attempt.getFinishTime() != 0 && attempt.getLaunchTime() != 0)\r\n            runTime = (int) (attempt.getFinishTime() - attempt.getLaunchTime());\r\n        tce.setAttemptRunTime(runTime);\r\n        eventHandler.handle(new JobTaskAttemptCompletedEvent(tce));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "sendTaskStartedEvent",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void sendTaskStartedEvent()\n{\r\n    launchTime = getLaunchTime();\r\n    TaskStartedEvent tse = new TaskStartedEvent(TypeConverter.fromYarn(taskId), launchTime, TypeConverter.fromYarn(taskId.getTaskType()), getSplitsAsString());\r\n    eventHandler.handle(new JobHistoryEvent(taskId.getJobId(), tse));\r\n    historyTaskStartGenerated = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createTaskFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "TaskFinishedEvent createTaskFinishedEvent(TaskImpl task, TaskStateInternal taskState)\n{\r\n    TaskFinishedEvent tfe = new TaskFinishedEvent(TypeConverter.fromYarn(task.taskId), TypeConverter.fromYarn(task.successfulAttempt), task.getFinishTime(task.successfulAttempt), TypeConverter.fromYarn(task.taskId.getTaskType()), taskState.toString(), task.getCounters(), task.launchTime);\r\n    return tfe;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createTaskFailedEvent",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "TaskFailedEvent createTaskFailedEvent(TaskImpl task, List<String> diag, TaskStateInternal taskState, TaskAttemptId taId)\n{\r\n    StringBuilder errorSb = new StringBuilder();\r\n    if (diag != null) {\r\n        for (String d : diag) {\r\n            errorSb.append(\", \").append(d);\r\n        }\r\n    }\r\n    TaskFailedEvent taskFailedEvent = new TaskFailedEvent(TypeConverter.fromYarn(task.taskId), task.getFinishTime(taId), TypeConverter.fromYarn(task.getType()), errorSb.toString(), taskState.toString(), taId == null ? null : TypeConverter.fromYarn(taId), task.getCounters(), task.launchTime);\r\n    return taskFailedEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "unSucceed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void unSucceed(TaskImpl task)\n{\r\n    task.commitAttempt = null;\r\n    task.successfulAttempt = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "sendTaskSucceededEvents",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void sendTaskSucceededEvents()\n{\r\n    eventHandler.handle(new JobTaskEvent(taskId, TaskState.SUCCEEDED));\r\n    LOG.info(\"Task succeeded with attempt \" + successfulAttempt);\r\n    if (historyTaskStartGenerated) {\r\n        TaskFinishedEvent tfe = createTaskFinishedEvent(this, TaskStateInternal.SUCCEEDED);\r\n        eventHandler.handle(new JobHistoryEvent(taskId.getJobId(), tfe));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getSplitsAsString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSplitsAsString()\n{\r\n    return \"\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "recover",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "TaskStateInternal recover(TaskInfo taskInfo, OutputCommitter committer, boolean recoverTaskOutput)\n{\r\n    LOG.info(\"Recovering task \" + taskId + \" from prior app attempt, status was \" + taskInfo.getTaskStatus());\r\n    scheduledTime = taskInfo.getStartTime();\r\n    sendTaskStartedEvent();\r\n    Collection<TaskAttemptInfo> attemptInfos = taskInfo.getAllTaskAttempts().values();\r\n    if (attemptInfos.size() > 0) {\r\n        metrics.launchedTask(this);\r\n    }\r\n    int savedNextAttemptNumber = nextAttemptNumber;\r\n    ArrayList<TaskAttemptInfo> taInfos = new ArrayList<TaskAttemptInfo>(taskInfo.getAllTaskAttempts().values());\r\n    Collections.sort(taInfos, TA_INFO_COMPARATOR);\r\n    for (TaskAttemptInfo taInfo : taInfos) {\r\n        nextAttemptNumber = taInfo.getAttemptId().getId();\r\n        TaskAttemptImpl attempt = addAttempt(Avataar.VIRGIN);\r\n        attempt.handle(new TaskAttemptRecoverEvent(attempt.getID(), taInfo, committer, recoverTaskOutput));\r\n        finishedAttempts.add(attempt.getID());\r\n        TaskAttemptCompletionEventStatus taces = null;\r\n        TaskAttemptState attemptState = attempt.getState();\r\n        switch(attemptState) {\r\n            case FAILED:\r\n                taces = TaskAttemptCompletionEventStatus.FAILED;\r\n                break;\r\n            case KILLED:\r\n                taces = TaskAttemptCompletionEventStatus.KILLED;\r\n                break;\r\n            case SUCCEEDED:\r\n                taces = TaskAttemptCompletionEventStatus.SUCCEEDED;\r\n                break;\r\n            default:\r\n                throw new IllegalStateException(\"Unexpected attempt state during recovery: \" + attemptState);\r\n        }\r\n        if (attemptState == TaskAttemptState.FAILED) {\r\n            failedAttempts.add(attempt.getID());\r\n            if (failedAttempts.size() >= maxAttempts) {\r\n                taces = TaskAttemptCompletionEventStatus.TIPFAILED;\r\n            }\r\n        }\r\n        if (successfulAttempt == null) {\r\n            handleTaskAttemptCompletion(attempt.getID(), taces);\r\n            if (attemptState == TaskAttemptState.SUCCEEDED) {\r\n                successfulAttempt = attempt.getID();\r\n            }\r\n        }\r\n    }\r\n    nextAttemptNumber = savedNextAttemptNumber;\r\n    TaskStateInternal taskState = TaskStateInternal.valueOf(taskInfo.getTaskStatus());\r\n    switch(taskState) {\r\n        case SUCCEEDED:\r\n            if (successfulAttempt != null) {\r\n                sendTaskSucceededEvents();\r\n            } else {\r\n                LOG.info(\"Missing successful attempt for task \" + taskId + \", recovering as RUNNING\");\r\n                taskState = TaskStateInternal.RUNNING;\r\n                metrics.runningTask(this);\r\n                addAndScheduleAttempt(Avataar.VIRGIN);\r\n            }\r\n            break;\r\n        case FAILED:\r\n        case KILLED:\r\n            {\r\n                if (taskState == TaskStateInternal.KILLED && attemptInfos.size() == 0) {\r\n                    metrics.endWaitingTask(this);\r\n                }\r\n                TaskFailedEvent tfe = new TaskFailedEvent(taskInfo.getTaskId(), taskInfo.getFinishTime(), taskInfo.getTaskType(), taskInfo.getError(), taskInfo.getTaskStatus(), taskInfo.getFailedDueToAttemptId(), taskInfo.getCounters(), launchTime);\r\n                eventHandler.handle(new JobHistoryEvent(taskId.getJobId(), tfe));\r\n                eventHandler.handle(new JobTaskEvent(taskId, getExternalState(taskState)));\r\n                break;\r\n            }\r\n        default:\r\n            throw new java.lang.AssertionError(\"Unexpected recovered task state: \" + taskState);\r\n    }\r\n    return taskState;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "killUnfinishedAttempt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void killUnfinishedAttempt(TaskAttempt attempt, String logMsg)\n{\r\n    if (attempt != null && !attempt.isFinished()) {\r\n        eventHandler.handle(new TaskAttemptKillEvent(attempt.getID(), logMsg));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "render",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void render(Block html)\n{\r\n    String rmweb = $(RM_WEB);\r\n    DIV<Hamlet> nav = html.div(\"#nav\").h3(\"Cluster\").ul().li().a(url(rmweb, \"cluster\", \"cluster\"), \"About\").__().li().a(url(rmweb, \"cluster\", \"apps\"), \"Applications\").__().li().a(url(rmweb, \"cluster\", \"scheduler\"), \"Scheduler\").__().__().h3(\"Application\").ul().li().a(url(\"app/info\"), \"About\").__().li().a(url(\"app\"), \"Jobs\").__().__();\r\n    if (app.getJob() != null) {\r\n        String jobid = MRApps.toString(app.getJob().getID());\r\n        List<AMInfo> amInfos = app.getJob().getAMInfos();\r\n        AMInfo thisAmInfo = amInfos.get(amInfos.size() - 1);\r\n        String nodeHttpAddress = thisAmInfo.getNodeManagerHost() + \":\" + thisAmInfo.getNodeManagerHttpPort();\r\n        nav.h3(\"Job\").ul().li().a(url(\"job\", jobid), \"Overview\").__().li().a(url(\"jobcounters\", jobid), \"Counters\").__().li().a(url(\"conf\", jobid), \"Configuration\").__().li().a(url(\"tasks\", jobid, \"m\"), \"Map tasks\").__().li().a(url(\"tasks\", jobid, \"r\"), \"Reduce tasks\").__().li().a(\".logslink\", url(MRWebAppUtil.getYARNWebappScheme(), nodeHttpAddress, \"node\", \"containerlogs\", thisAmInfo.getContainerId().toString(), app.getJob().getUserName()), \"AM Logs\").__().__();\r\n        if (app.getTask() != null) {\r\n            String taskid = MRApps.toString(app.getTask().getID());\r\n            nav.h3(\"Task\").ul().li().a(url(\"task\", taskid), \"Task Overview\").__().li().a(url(\"taskcounters\", taskid), \"Counters\").__().__();\r\n        }\r\n    }\r\n    nav.h3(\"Tools\").ul().li().a(\"/conf\", \"Configuration\").__().li().a(\"/logs\", \"Local logs\").__().li().a(\"/stacks\", \"Server stacks\").__().li().a(\"/jmx?qry=Hadoop:*\", \"Server metrics\").__().__().__();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "burst",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int[][] burst()\n{\r\n    return super.burst();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProgressWallclockTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "WrappedPeriodicStatsAccumulator getProgressWallclockTime()\n{\r\n    if (wrappedProgressWallclockTime == null) {\r\n        wrappedProgressWallclockTime = new WrappedPeriodicStatsAccumulator(progressWallclockTime);\r\n    }\r\n    return wrappedProgressWallclockTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProgressCPUTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "WrappedPeriodicStatsAccumulator getProgressCPUTime()\n{\r\n    if (wrappedProgressCPUTime == null) {\r\n        wrappedProgressCPUTime = new WrappedPeriodicStatsAccumulator(progressCPUTime);\r\n    }\r\n    return wrappedProgressCPUTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProgressVirtualMemoryKbytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "WrappedPeriodicStatsAccumulator getProgressVirtualMemoryKbytes()\n{\r\n    if (wrappedProgressVirtualMemoryKbytes == null) {\r\n        wrappedProgressVirtualMemoryKbytes = new WrappedPeriodicStatsAccumulator(progressVirtualMemoryKbytes);\r\n    }\r\n    return wrappedProgressVirtualMemoryKbytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProgressPhysicalMemoryKbytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "WrappedPeriodicStatsAccumulator getProgressPhysicalMemoryKbytes()\n{\r\n    if (wrappedProgressPhysicalMemoryKbytes == null) {\r\n        wrappedProgressPhysicalMemoryKbytes = new WrappedPeriodicStatsAccumulator(progressPhysicalMemoryKbytes);\r\n    }\r\n    return wrappedProgressPhysicalMemoryKbytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "isFastFail",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isFastFail()\n{\r\n    return fastFail;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    commonPreHead(html);\r\n    set(initID(ACCORDION, \"nav\"), \"{autoHeight:false, active:3}\");\r\n    set(DATATABLES_ID, \"attempts\");\r\n    set(initID(DATATABLES, \"attempts\"), attemptsTableInit());\r\n    setTableStyles(html, \"attempts\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return AttemptsBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "attemptsTableInit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String attemptsTableInit()\n{\r\n    return tableInit().append(\", 'aaData': attemptsTableData\").append(\", bDeferRender: true\").append(\", bProcessing: true\").append(\"\\n,aoColumnDefs:[\\n\").append(\"\\n{'aTargets': [ 5 ]\").append(\", 'bSearchable': false }\").append(\"\\n, {'sType':'natural', 'aTargets': [ 0 ]\").append(\", 'mRender': parseHadoopID }\").append(\"\\n, {'sType':'numeric', 'aTargets': [ 6, 7\").append(\" ], 'mRender': renderHadoopDate }\").append(\"\\n, {'sType':'numeric', 'aTargets': [ 8\").append(\" ], 'mRender': renderHadoopElapsedTime }]\").append(\"\\n, aaSorting: [[0, 'asc']]\").append(\"}\").toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getCompletionEvent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptCompletionEvent getCompletionEvent()\n{\r\n    return completionEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "updateAttempt",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void updateAttempt(TaskAttemptStatus status, long timestamp)\n{\r\n    super.updateAttempt(status, timestamp);\r\n    TaskAttemptId attemptID = status.id;\r\n    TaskId taskID = attemptID.getTaskId();\r\n    JobId jobID = taskID.getJobId();\r\n    Job job = context.getJob(jobID);\r\n    if (job == null) {\r\n        return;\r\n    }\r\n    Task task = job.getTask(taskID);\r\n    if (task == null) {\r\n        return;\r\n    }\r\n    TaskAttempt taskAttempt = task.getAttempt(attemptID);\r\n    if (taskAttempt == null) {\r\n        return;\r\n    }\r\n    Long boxedStart = startTimes.get(attemptID);\r\n    long start = boxedStart == null ? Long.MIN_VALUE : boxedStart;\r\n    if (taskAttempt.getState() == TaskAttemptState.RUNNING) {\r\n        AtomicLong estimateContainer = attemptRuntimeEstimates.get(taskAttempt);\r\n        AtomicLong estimateVarianceContainer = attemptRuntimeEstimateVariances.get(taskAttempt);\r\n        if (estimateContainer == null) {\r\n            if (attemptRuntimeEstimates.get(taskAttempt) == null) {\r\n                attemptRuntimeEstimates.put(taskAttempt, new AtomicLong());\r\n                estimateContainer = attemptRuntimeEstimates.get(taskAttempt);\r\n            }\r\n        }\r\n        if (estimateVarianceContainer == null) {\r\n            attemptRuntimeEstimateVariances.putIfAbsent(taskAttempt, new AtomicLong());\r\n            estimateVarianceContainer = attemptRuntimeEstimateVariances.get(taskAttempt);\r\n        }\r\n        long estimate = -1;\r\n        long varianceEstimate = -1;\r\n        if (start > 0 && timestamp > start) {\r\n            estimate = (long) ((timestamp - start) / Math.max(0.0001, status.progress));\r\n            varianceEstimate = (long) (estimate * status.progress / 10);\r\n        }\r\n        if (estimateContainer != null) {\r\n            estimateContainer.set(estimate);\r\n        }\r\n        if (estimateVarianceContainer != null) {\r\n            estimateVarianceContainer.set(varianceEstimate);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "storedPerAttemptValue",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "long storedPerAttemptValue(Map<TaskAttempt, AtomicLong> data, TaskAttemptId attemptID)\n{\r\n    TaskId taskID = attemptID.getTaskId();\r\n    JobId jobID = taskID.getJobId();\r\n    Job job = context.getJob(jobID);\r\n    Task task = job.getTask(taskID);\r\n    if (task == null) {\r\n        return -1L;\r\n    }\r\n    TaskAttempt taskAttempt = task.getAttempt(attemptID);\r\n    if (taskAttempt == null) {\r\n        return -1L;\r\n    }\r\n    AtomicLong estimate = data.get(taskAttempt);\r\n    return estimate == null ? -1L : estimate.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "estimatedRuntime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long estimatedRuntime(TaskAttemptId attemptID)\n{\r\n    return storedPerAttemptValue(attemptRuntimeEstimates, attemptID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "runtimeEstimateVariance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long runtimeEstimateVariance(TaskAttemptId attemptID)\n{\r\n    return storedPerAttemptValue(attemptRuntimeEstimateVariances, attemptID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createTaskAttemptFinishingMonitor",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptFinishingMonitor createTaskAttemptFinishingMonitor(EventHandler eventHandler)\n{\r\n    TaskAttemptFinishingMonitor monitor = new TaskAttemptFinishingMonitor(eventHandler);\r\n    return monitor;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "serviceInit",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 67,
  "sourceCodeText" : "void serviceInit(final Configuration conf) throws Exception\n{\r\n    createJobClassLoader(conf);\r\n    initJobCredentialsAndUGI(conf);\r\n    dispatcher = createDispatcher();\r\n    addIfService(dispatcher);\r\n    taskAttemptFinishingMonitor = createTaskAttemptFinishingMonitor(dispatcher.getEventHandler());\r\n    addIfService(taskAttemptFinishingMonitor);\r\n    context = new RunningAppContext(conf, taskAttemptFinishingMonitor);\r\n    appName = conf.get(MRJobConfig.JOB_NAME, \"<missing app name>\");\r\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());\r\n    newApiCommitter = false;\r\n    jobId = MRBuilderUtils.newJobId(appAttemptID.getApplicationId(), appAttemptID.getApplicationId().getId());\r\n    int numReduceTasks = conf.getInt(MRJobConfig.NUM_REDUCES, 0);\r\n    if ((numReduceTasks > 0 && conf.getBoolean(\"mapred.reducer.new-api\", false)) || (numReduceTasks == 0 && conf.getBoolean(\"mapred.mapper.new-api\", false))) {\r\n        newApiCommitter = true;\r\n        LOG.info(\"Using mapred newApiCommitter.\");\r\n    }\r\n    boolean copyHistory = false;\r\n    committer = createOutputCommitter(conf);\r\n    try {\r\n        String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n        Path stagingDir = MRApps.getStagingAreaDir(conf, user);\r\n        FileSystem fs = getFileSystem(conf);\r\n        boolean stagingExists = fs.exists(stagingDir);\r\n        Path startCommitFile = MRApps.getStartJobCommitFile(conf, user, jobId);\r\n        boolean commitStarted = fs.exists(startCommitFile);\r\n        Path endCommitSuccessFile = MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\r\n        boolean commitSuccess = fs.exists(endCommitSuccessFile);\r\n        Path endCommitFailureFile = MRApps.getEndJobCommitFailureFile(conf, user, jobId);\r\n        boolean commitFailure = fs.exists(endCommitFailureFile);\r\n        if (!stagingExists) {\r\n            isLastAMRetry = true;\r\n            LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() + \" is last retry: \" + isLastAMRetry + \" because the staging dir doesn't exist.\");\r\n            errorHappenedShutDown = true;\r\n            forcedState = JobStateInternal.ERROR;\r\n            shutDownMessage = \"Staging dir does not exist \" + stagingDir;\r\n            LOG.error(shutDownMessage);\r\n        } else if (commitStarted) {\r\n            errorHappenedShutDown = true;\r\n            isLastAMRetry = true;\r\n            LOG.info(\"Attempt num: \" + appAttemptID.getAttemptId() + \" is last retry: \" + isLastAMRetry + \" because a commit was started.\");\r\n            copyHistory = true;\r\n            if (commitSuccess) {\r\n                shutDownMessage = \"Job commit succeeded in a prior MRAppMaster attempt \" + \"before it crashed. Recovering.\";\r\n                forcedState = JobStateInternal.SUCCEEDED;\r\n            } else if (commitFailure) {\r\n                shutDownMessage = \"Job commit failed in a prior MRAppMaster attempt \" + \"before it crashed. Not retrying.\";\r\n                forcedState = JobStateInternal.FAILED;\r\n            } else {\r\n                if (isCommitJobRepeatable()) {\r\n                    errorHappenedShutDown = false;\r\n                    cleanupInterruptedCommit(conf, fs, startCommitFile);\r\n                } else {\r\n                    shutDownMessage = \"Job commit from a prior MRAppMaster attempt is \" + \"potentially in progress. Preventing multiple commit executions\";\r\n                    forcedState = JobStateInternal.ERROR;\r\n                }\r\n            }\r\n        }\r\n    } catch (IOException e) {\r\n        throw new YarnRuntimeException(\"Error while initializing\", e);\r\n    }\r\n    if (errorHappenedShutDown) {\r\n        NoopEventHandler eater = new NoopEventHandler();\r\n        dispatcher.register(JobEventType.class, eater);\r\n        EventHandler<JobHistoryEvent> historyService = null;\r\n        if (copyHistory) {\r\n            historyService = createJobHistoryHandler(context);\r\n            dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class, historyService);\r\n        } else {\r\n            dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class, eater);\r\n        }\r\n        if (copyHistory) {\r\n            addService(createStagingDirCleaningService());\r\n        }\r\n        containerAllocator = createContainerAllocator(null, context);\r\n        addIfService(containerAllocator);\r\n        dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\r\n        if (copyHistory) {\r\n            addIfService(historyService);\r\n            JobHistoryCopyService cpHist = new JobHistoryCopyService(appAttemptID, dispatcher.getEventHandler());\r\n            addIfService(cpHist);\r\n        }\r\n    } else {\r\n        clientService = createClientService(context);\r\n        clientService.init(conf);\r\n        containerAllocator = createContainerAllocator(clientService, context);\r\n        committerEventHandler = createCommitterEventHandler(context, committer);\r\n        addIfService(committerEventHandler);\r\n        callWithJobClassLoader(conf, new Action<Void>() {\r\n\r\n            public Void call(Configuration conf) {\r\n                preemptionPolicy = createPreemptionPolicy(conf);\r\n                preemptionPolicy.init(context);\r\n                return null;\r\n            }\r\n        });\r\n        taskAttemptListener = createTaskAttemptListener(context, preemptionPolicy);\r\n        addIfService(taskAttemptListener);\r\n        EventHandler<JobHistoryEvent> historyService = createJobHistoryHandler(context);\r\n        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class, historyService);\r\n        this.jobEventDispatcher = new JobEventDispatcher();\r\n        dispatcher.register(JobEventType.class, jobEventDispatcher);\r\n        dispatcher.register(TaskEventType.class, new TaskEventDispatcher());\r\n        dispatcher.register(TaskAttemptEventType.class, new TaskAttemptEventDispatcher());\r\n        dispatcher.register(CommitterEventType.class, committerEventHandler);\r\n        if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false) || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {\r\n            speculator = createSpeculator(conf, context);\r\n            addIfService(speculator);\r\n        }\r\n        speculatorEventDispatcher = new SpeculatorEventDispatcher(conf);\r\n        dispatcher.register(Speculator.EventType.class, speculatorEventDispatcher);\r\n        addService(createStagingDirCleaningService());\r\n        addIfService(containerAllocator);\r\n        dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);\r\n        containerLauncher = createContainerLauncher(context);\r\n        addIfService(containerLauncher);\r\n        dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);\r\n        addIfService(historyService);\r\n    }\r\n    super.serviceInit(conf);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createDispatcher",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Dispatcher createDispatcher()\n{\r\n    return new AsyncDispatcher();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "isCommitJobRepeatable",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isCommitJobRepeatable() throws IOException\n{\r\n    boolean isRepeatable = false;\r\n    Configuration conf = getConfig();\r\n    if (committer != null) {\r\n        final JobContext jobContext = getJobContextFromConf(conf);\r\n        isRepeatable = callWithJobClassLoader(conf, new ExceptionAction<Boolean>() {\r\n\r\n            public Boolean call(Configuration conf) throws IOException {\r\n                return committer.isCommitJobRepeatable(jobContext);\r\n            }\r\n        });\r\n    }\r\n    return isRepeatable;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getJobContextFromConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobContext getJobContextFromConf(Configuration conf)\n{\r\n    if (newApiCommitter) {\r\n        return new JobContextImpl(conf, TypeConverter.fromYarn(getJobId()));\r\n    } else {\r\n        return new org.apache.hadoop.mapred.JobContextImpl(new JobConf(conf), TypeConverter.fromYarn(getJobId()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "cleanupInterruptedCommit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cleanupInterruptedCommit(Configuration conf, FileSystem fs, Path startCommitFile) throws IOException\n{\r\n    LOG.info(\"Delete startJobCommitFile in case commit is not finished as \" + \"successful or failed.\");\r\n    fs.delete(startCommitFile, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createOutputCommitter",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OutputCommitter createOutputCommitter(Configuration conf)\n{\r\n    return callWithJobClassLoader(conf, new Action<OutputCommitter>() {\r\n\r\n        public OutputCommitter call(Configuration conf) {\r\n            OutputCommitter committer = null;\r\n            LOG.info(\"OutputCommitter set in config \" + conf.get(\"mapred.output.committer.class\"));\r\n            if (newApiCommitter) {\r\n                org.apache.hadoop.mapreduce.v2.api.records.TaskId taskID = MRBuilderUtils.newTaskId(jobId, 0, TaskType.MAP);\r\n                org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = MRBuilderUtils.newTaskAttemptId(taskID, 0);\r\n                TaskAttemptContext taskContext = new TaskAttemptContextImpl(conf, TypeConverter.fromYarn(attemptID));\r\n                OutputFormat outputFormat;\r\n                try {\r\n                    outputFormat = ReflectionUtils.newInstance(taskContext.getOutputFormatClass(), conf);\r\n                    committer = outputFormat.getOutputCommitter(taskContext);\r\n                } catch (Exception e) {\r\n                    throw new YarnRuntimeException(e);\r\n                }\r\n            } else {\r\n                committer = ReflectionUtils.newInstance(conf.getClass(\"mapred.output.committer.class\", FileOutputCommitter.class, org.apache.hadoop.mapred.OutputCommitter.class), conf);\r\n            }\r\n            LOG.info(\"OutputCommitter is \" + committer.getClass().getName());\r\n            return committer;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createPreemptionPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AMPreemptionPolicy createPreemptionPolicy(Configuration conf)\n{\r\n    return ReflectionUtils.newInstance(conf.getClass(MRJobConfig.MR_AM_PREEMPTION_POLICY, NoopAMPreemptionPolicy.class, AMPreemptionPolicy.class), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "isJobNamePatternMatch",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean isJobNamePatternMatch(JobConf conf, String jobTempDir)\n{\r\n    if (conf.getKeepTaskFilesPattern() != null && jobTempDir != null) {\r\n        java.nio.file.Path pathName = Paths.get(jobTempDir).getFileName();\r\n        if (pathName != null) {\r\n            String jobFileName = pathName.toString();\r\n            Pattern pattern = Pattern.compile(conf.getKeepTaskFilesPattern());\r\n            Matcher matcher = pattern.matcher(jobFileName);\r\n            return matcher.find();\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "isKeepFailedTaskFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isKeepFailedTaskFiles(JobConf conf)\n{\r\n    return conf.getKeepFailedTaskFiles();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "keepJobFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean keepJobFiles(JobConf conf, String jobTempDir)\n{\r\n    return isJobNamePatternMatch(conf, jobTempDir) || isKeepFailedTaskFiles(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getFileSystem(Configuration conf) throws IOException\n{\r\n    return FileSystem.get(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getCredentials",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Credentials getCredentials()\n{\r\n    return jobCredentials;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "cleanupStagingDir",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void cleanupStagingDir() throws IOException\n{\r\n    String jobTempDir = getConfig().get(MRJobConfig.MAPREDUCE_JOB_DIR);\r\n    FileSystem fs = getFileSystem(getConfig());\r\n    try {\r\n        if (!keepJobFiles(new JobConf(getConfig()), jobTempDir)) {\r\n            if (jobTempDir == null) {\r\n                LOG.warn(\"Job Staging directory is null\");\r\n                return;\r\n            }\r\n            Path jobTempDirPath = new Path(jobTempDir);\r\n            LOG.info(\"Deleting staging directory \" + FileSystem.getDefaultUri(getConfig()) + \" \" + jobTempDir);\r\n            fs.delete(jobTempDirPath, true);\r\n        }\r\n    } catch (IOException io) {\r\n        LOG.error(\"Failed to cleanup staging dir \" + jobTempDir, io);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "sysexit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sysexit()\n{\r\n    System.exit(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "shutDownJob",
  "errType" : [ "Throwable", "InterruptedException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void shutDownJob()\n{\r\n    JobEndNotifier notifier = null;\r\n    if (getConfig().get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL) != null) {\r\n        notifier = new JobEndNotifier();\r\n        notifier.setConf(getConfig());\r\n    }\r\n    try {\r\n        if (!isLastAMRetry) {\r\n            if (((JobImpl) job).getInternalState() != JobStateInternal.REBOOT) {\r\n                LOG.info(\"Job finished cleanly, recording last MRAppMaster retry\");\r\n                isLastAMRetry = true;\r\n            }\r\n        }\r\n        notifyIsLastAMRetry(isLastAMRetry);\r\n        LOG.info(\"Calling stop for all the services\");\r\n        MRAppMaster.this.stop();\r\n        if (isLastAMRetry && notifier != null) {\r\n            sendJobEndNotify(notifier);\r\n            notifier = null;\r\n        }\r\n        try {\r\n            Thread.sleep(5000);\r\n        } catch (InterruptedException e) {\r\n            e.printStackTrace();\r\n        }\r\n        clientService.stop();\r\n    } catch (Throwable t) {\r\n        LOG.warn(\"Graceful stop failed. Exiting.. \", t);\r\n        exitMRAppMaster(1, t);\r\n    } finally {\r\n        if (isLastAMRetry && notifier != null) {\r\n            sendJobEndNotify(notifier);\r\n        }\r\n    }\r\n    exitMRAppMaster(0, null);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "sendJobEndNotify",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void sendJobEndNotify(JobEndNotifier notifier)\n{\r\n    try {\r\n        LOG.info(\"Job end notification started for jobID : \" + job.getReport().getJobId());\r\n        JobReport report = job.getReport();\r\n        if (!context.hasSuccessfullyUnregistered()) {\r\n            report.setJobState(JobState.FAILED);\r\n        }\r\n        notifier.notify(report);\r\n    } catch (InterruptedException ie) {\r\n        LOG.warn(\"Job end notification interrupted for jobID : \" + job.getReport().getJobId(), ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "exitMRAppMaster",
  "errType" : [ "ExitUtil.ExitException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void exitMRAppMaster(int status, Throwable t)\n{\r\n    if (!mainStarted) {\r\n        ExitUtil.disableSystemExit();\r\n    }\r\n    try {\r\n        if (t != null) {\r\n            ExitUtil.terminate(status, t);\r\n        } else {\r\n            ExitUtil.terminate(status);\r\n        }\r\n    } catch (ExitUtil.ExitException ee) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createJobFinishEventHandler",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventHandler<JobFinishEvent> createJobFinishEventHandler()\n{\r\n    return new JobFinishEventHandler();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createJob",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Job createJob(Configuration conf, JobStateInternal forcedState, String diagnostic)\n{\r\n    Job newJob = new JobImpl(jobId, appAttemptID, conf, dispatcher.getEventHandler(), taskAttemptListener, jobTokenSecretManager, jobCredentials, clock, completedTasksFromPreviousRun, metrics, committer, newApiCommitter, currentUser.getUserName(), appSubmitTime, amInfos, context, forcedState, diagnostic);\r\n    ((RunningAppContext) context).jobs.put(newJob.getID(), newJob);\r\n    dispatcher.register(JobFinishEvent.Type.class, createJobFinishEventHandler());\r\n    return newJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "initJobCredentialsAndUGI",
  "errType" : [ "IOException", "NoSuchAlgorithmException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void initJobCredentialsAndUGI(Configuration conf)\n{\r\n    try {\r\n        this.currentUser = UserGroupInformation.getCurrentUser();\r\n        this.jobCredentials = ((JobConf) conf).getCredentials();\r\n        if (CryptoUtils.isEncryptedSpillEnabled(conf)) {\r\n            int keyLen = conf.getInt(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS, MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS);\r\n            KeyGenerator keyGen = KeyGenerator.getInstance(INTERMEDIATE_DATA_ENCRYPTION_ALGO);\r\n            keyGen.init(keyLen);\r\n            encryptedSpillKey = keyGen.generateKey().getEncoded();\r\n        } else {\r\n            encryptedSpillKey = new byte[] { 0 };\r\n        }\r\n    } catch (IOException e) {\r\n        throw new YarnRuntimeException(e);\r\n    } catch (NoSuchAlgorithmException e) {\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createJobHistoryHandler",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EventHandler<JobHistoryEvent> createJobHistoryHandler(AppContext context)\n{\r\n    this.jobHistoryEventHandler = new JobHistoryEventHandler(context, getStartCount());\r\n    return this.jobHistoryEventHandler;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createStagingDirCleaningService",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractService createStagingDirCleaningService()\n{\r\n    return new StagingDirCleaningService();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createSpeculator",
  "errType" : [ "InstantiationException", "IllegalAccessException", "InvocationTargetException", "NoSuchMethodException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Speculator createSpeculator(Configuration conf, final AppContext context)\n{\r\n    return callWithJobClassLoader(conf, new Action<Speculator>() {\r\n\r\n        public Speculator call(Configuration conf) {\r\n            Class<? extends Speculator> speculatorClass;\r\n            try {\r\n                speculatorClass = conf.getClass(MRJobConfig.MR_AM_JOB_SPECULATOR, DefaultSpeculator.class, Speculator.class);\r\n                Constructor<? extends Speculator> speculatorConstructor = speculatorClass.getConstructor(Configuration.class, AppContext.class);\r\n                Speculator result = speculatorConstructor.newInstance(conf, context);\r\n                return result;\r\n            } catch (InstantiationException ex) {\r\n                LOG.error(\"Can't make a speculator -- check \" + MRJobConfig.MR_AM_JOB_SPECULATOR, ex);\r\n                throw new YarnRuntimeException(ex);\r\n            } catch (IllegalAccessException ex) {\r\n                LOG.error(\"Can't make a speculator -- check \" + MRJobConfig.MR_AM_JOB_SPECULATOR, ex);\r\n                throw new YarnRuntimeException(ex);\r\n            } catch (InvocationTargetException ex) {\r\n                LOG.error(\"Can't make a speculator -- check \" + MRJobConfig.MR_AM_JOB_SPECULATOR, ex);\r\n                throw new YarnRuntimeException(ex);\r\n            } catch (NoSuchMethodException ex) {\r\n                LOG.error(\"Can't make a speculator -- check \" + MRJobConfig.MR_AM_JOB_SPECULATOR, ex);\r\n                throw new YarnRuntimeException(ex);\r\n            }\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createTaskAttemptListener",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptListener createTaskAttemptListener(AppContext context, AMPreemptionPolicy preemptionPolicy)\n{\r\n    TaskAttemptListener lis = new TaskAttemptListenerImpl(context, jobTokenSecretManager, getRMHeartbeatHandler(), preemptionPolicy, encryptedSpillKey);\r\n    return lis;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createCommitterEventHandler",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EventHandler<CommitterEvent> createCommitterEventHandler(AppContext context, OutputCommitter committer)\n{\r\n    return new CommitterEventHandler(context, committer, getRMHeartbeatHandler(), jobClassLoader);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createContainerAllocator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ContainerAllocator createContainerAllocator(final ClientService clientService, final AppContext context)\n{\r\n    return new ContainerAllocatorRouter(clientService, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getRMHeartbeatHandler",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RMHeartbeatHandler getRMHeartbeatHandler()\n{\r\n    return (RMHeartbeatHandler) containerAllocator;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createContainerLauncher",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ContainerLauncher createContainerLauncher(final AppContext context)\n{\r\n    return new ContainerLauncherRouter(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createClientService",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientService createClientService(AppContext context)\n{\r\n    return new MRClientService(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getAppID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ApplicationId getAppID()\n{\r\n    return appAttemptID.getApplicationId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getAttemptID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ApplicationAttemptId getAttemptID()\n{\r\n    return appAttemptID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobId getJobId()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OutputCommitter getCommitter()\n{\r\n    return committer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "isNewApiCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isNewApiCommitter()\n{\r\n    return newApiCommitter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getStartCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getStartCount()\n{\r\n    return appAttemptID.getAttemptId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AppContext getContext()\n{\r\n    return context;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getDispatcher",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Dispatcher getDispatcher()\n{\r\n    return dispatcher;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getCompletedTaskFromPreviousRun",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<TaskId, TaskInfo> getCompletedTaskFromPreviousRun()\n{\r\n    return completedTasksFromPreviousRun;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getAllAMInfos",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<AMInfo> getAllAMInfos()\n{\r\n    return amInfos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getContainerAllocator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ContainerAllocator getContainerAllocator()\n{\r\n    return containerAllocator;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getContainerLauncher",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ContainerLauncher getContainerLauncher()\n{\r\n    return containerLauncher;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getTaskAttemptListener",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptListener getTaskAttemptListener()\n{\r\n    return taskAttemptListener;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "isLastAMRetry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Boolean isLastAMRetry()\n{\r\n    return isLastAMRetry;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    amInfos = new LinkedList<AMInfo>();\r\n    completedTasksFromPreviousRun = new HashMap<TaskId, TaskInfo>();\r\n    processRecovery();\r\n    cleanUpPreviousJobOutput();\r\n    AMInfo amInfo = MRBuilderUtils.newAMInfo(appAttemptID, startTime, containerID, nmHost, nmPort, nmHttpPort);\r\n    job = createJob(getConfig(), forcedState, shutDownMessage);\r\n    for (AMInfo info : amInfos) {\r\n        dispatcher.getEventHandler().handle(new JobHistoryEvent(job.getID(), new AMStartedEvent(info.getAppAttemptId(), info.getStartTime(), info.getContainerId(), info.getNodeManagerHost(), info.getNodeManagerPort(), info.getNodeManagerHttpPort(), appSubmitTime)));\r\n    }\r\n    dispatcher.getEventHandler().handle(new JobHistoryEvent(job.getID(), new AMStartedEvent(amInfo.getAppAttemptId(), amInfo.getStartTime(), amInfo.getContainerId(), amInfo.getNodeManagerHost(), amInfo.getNodeManagerPort(), amInfo.getNodeManagerHttpPort(), this.forcedState == null ? null : this.forcedState.toString(), appSubmitTime)));\r\n    amInfos.add(amInfo);\r\n    DefaultMetricsSystem.initialize(\"MRAppMaster\");\r\n    boolean initFailed = false;\r\n    if (!errorHappenedShutDown) {\r\n        JobEvent initJobEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT);\r\n        jobEventDispatcher.handle(initJobEvent);\r\n        initFailed = (((JobImpl) job).getInternalState() != JobStateInternal.INITED);\r\n        if (job.isUber()) {\r\n            speculatorEventDispatcher.disableSpeculation();\r\n            LOG.info(\"MRAppMaster uberizing job \" + job.getID() + \" in local container (\\\"uber-AM\\\") on node \" + nmHost + \":\" + nmPort + \".\");\r\n        } else {\r\n            dispatcher.getEventHandler().handle(new SpeculatorEvent(job.getID(), clock.getTime()));\r\n            LOG.info(\"MRAppMaster launching normal, non-uberized, multi-container \" + \"job \" + job.getID() + \".\");\r\n        }\r\n        clientService.start();\r\n    }\r\n    super.serviceStart();\r\n    MRApps.setClassLoader(jobClassLoader, getConfig());\r\n    if (initFailed) {\r\n        JobEvent initFailedEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT_FAILED);\r\n        jobEventDispatcher.handle(initFailedEvent);\r\n    } else {\r\n        startJobs();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void stop()\n{\r\n    super.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "isRecoverySupported",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isRecoverySupported() throws IOException\n{\r\n    boolean isSupported = false;\r\n    Configuration conf = getConfig();\r\n    if (committer != null) {\r\n        final JobContext _jobContext = getJobContextFromConf(conf);\r\n        isSupported = callWithJobClassLoader(conf, new ExceptionAction<Boolean>() {\r\n\r\n            public Boolean call(Configuration conf) throws IOException {\r\n                return committer.isRecoverySupported(_jobContext);\r\n            }\r\n        });\r\n    }\r\n    return isSupported;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "processRecovery",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void processRecovery() throws IOException\n{\r\n    boolean attemptRecovery = shouldAttemptRecovery();\r\n    boolean recoverySucceeded = true;\r\n    if (attemptRecovery) {\r\n        LOG.info(\"Attempting to recover.\");\r\n        try {\r\n            parsePreviousJobHistory();\r\n        } catch (IOException e) {\r\n            LOG.warn(\"Unable to parse prior job history, aborting recovery\", e);\r\n            recoverySucceeded = false;\r\n        }\r\n    }\r\n    if (!isFirstAttempt() && (!attemptRecovery || !recoverySucceeded)) {\r\n        amInfos.addAll(readJustAMInfos());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "isFirstAttempt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isFirstAttempt()\n{\r\n    return appAttemptID.getAttemptId() == 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "shouldAttemptRecovery",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "boolean shouldAttemptRecovery() throws IOException\n{\r\n    if (isFirstAttempt()) {\r\n        return false;\r\n    }\r\n    boolean recoveryEnabled = getConfig().getBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE_DEFAULT);\r\n    if (!recoveryEnabled) {\r\n        LOG.info(\"Not attempting to recover. Recovery disabled. To enable \" + \"recovery, set \" + MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE);\r\n        return false;\r\n    }\r\n    boolean recoverySupportedByCommitter = isRecoverySupported();\r\n    if (!recoverySupportedByCommitter) {\r\n        LOG.info(\"Not attempting to recover. Recovery is not supported by \" + committer.getClass() + \". Use an OutputCommitter that supports\" + \" recovery.\");\r\n        return false;\r\n    }\r\n    int reducerCount = getConfig().getInt(MRJobConfig.NUM_REDUCES, 0);\r\n    boolean shuffleKeyValidForRecovery = TokenCache.getShuffleSecretKey(jobCredentials) != null;\r\n    if (reducerCount > 0 && !shuffleKeyValidForRecovery) {\r\n        LOG.info(\"Not attempting to recover. The shuffle key is invalid for \" + \"recovery.\");\r\n        return false;\r\n    }\r\n    boolean spillEncrypted = CryptoUtils.isEncryptedSpillEnabled(getConfig());\r\n    if (reducerCount > 0 && spillEncrypted) {\r\n        LOG.info(\"Not attempting to recover. Intermediate spill encryption\" + \" is enabled.\");\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "cleanUpPreviousJobOutput",
  "errType" : [ "FileNotFoundException", "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void cleanUpPreviousJobOutput()\n{\r\n    if (!isFirstAttempt() && !recovered()) {\r\n        JobContext jobContext = getJobContextFromConf(getConfig());\r\n        try {\r\n            LOG.info(\"Starting to clean up previous job's temporary files\");\r\n            this.committer.abortJob(jobContext, State.FAILED);\r\n            LOG.info(\"Finished cleaning up previous job temporary files\");\r\n        } catch (FileNotFoundException e) {\r\n            LOG.info(\"Previous job temporary files do not exist, \" + \"no clean up was necessary.\");\r\n        } catch (Exception e) {\r\n            LOG.error(\"Error while trying to clean up previous job's temporary \" + \"files\", e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getPreviousJobHistoryStream",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FSDataInputStream getPreviousJobHistoryStream(Configuration conf, ApplicationAttemptId appAttemptId) throws IOException\n{\r\n    Path historyFile = JobHistoryUtils.getPreviousJobHistoryPath(conf, appAttemptId);\r\n    LOG.info(\"Previous history file is at \" + historyFile);\r\n    return historyFile.getFileSystem(conf).open(historyFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "parsePreviousJobHistory",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void parsePreviousJobHistory() throws IOException\n{\r\n    FSDataInputStream in = getPreviousJobHistoryStream(getConfig(), appAttemptID);\r\n    JobHistoryParser parser = new JobHistoryParser(in);\r\n    JobInfo jobInfo = parser.parse();\r\n    Exception parseException = parser.getParseException();\r\n    if (parseException != null) {\r\n        LOG.info(\"Got an error parsing job-history file\" + \", ignoring incomplete events.\", parseException);\r\n    }\r\n    Map<org.apache.hadoop.mapreduce.TaskID, TaskInfo> taskInfos = jobInfo.getAllTasks();\r\n    for (TaskInfo taskInfo : taskInfos.values()) {\r\n        if (TaskState.SUCCEEDED.toString().equals(taskInfo.getTaskStatus())) {\r\n            Iterator<Entry<TaskAttemptID, TaskAttemptInfo>> taskAttemptIterator = taskInfo.getAllTaskAttempts().entrySet().iterator();\r\n            while (taskAttemptIterator.hasNext()) {\r\n                Map.Entry<TaskAttemptID, TaskAttemptInfo> currentEntry = taskAttemptIterator.next();\r\n                if (!jobInfo.getAllCompletedTaskAttempts().containsKey(currentEntry.getKey())) {\r\n                    taskAttemptIterator.remove();\r\n                }\r\n            }\r\n            completedTasksFromPreviousRun.put(TypeConverter.toYarn(taskInfo.getTaskId()), taskInfo);\r\n            LOG.info(\"Read from history task \" + TypeConverter.toYarn(taskInfo.getTaskId()));\r\n        }\r\n    }\r\n    LOG.info(\"Read completed tasks from history \" + completedTasksFromPreviousRun.size());\r\n    recoveredJobStartTime = jobInfo.getLaunchTime();\r\n    List<JobHistoryParser.AMInfo> jhAmInfoList = jobInfo.getAMInfos();\r\n    if (jhAmInfoList != null) {\r\n        for (JobHistoryParser.AMInfo jhAmInfo : jhAmInfoList) {\r\n            AMInfo amInfo = MRBuilderUtils.newAMInfo(jhAmInfo.getAppAttemptId(), jhAmInfo.getStartTime(), jhAmInfo.getContainerId(), jhAmInfo.getNodeManagerHost(), jhAmInfo.getNodeManagerPort(), jhAmInfo.getNodeManagerHttpPort());\r\n            amInfos.add(amInfo);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "readJustAMInfos",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<AMInfo> readJustAMInfos()\n{\r\n    List<AMInfo> amInfos = new ArrayList<AMInfo>();\r\n    try (FSDataInputStream inputStream = getPreviousJobHistoryStream(getConfig(), appAttemptID)) {\r\n        EventReader jobHistoryEventReader = new EventReader(inputStream);\r\n        boolean amStartedEventsBegan = false;\r\n        HistoryEvent event;\r\n        while ((event = jobHistoryEventReader.getNextEvent()) != null) {\r\n            if (event.getEventType() == EventType.AM_STARTED) {\r\n                if (!amStartedEventsBegan) {\r\n                    amStartedEventsBegan = true;\r\n                }\r\n                AMStartedEvent amStartedEvent = (AMStartedEvent) event;\r\n                amInfos.add(MRBuilderUtils.newAMInfo(amStartedEvent.getAppAttemptId(), amStartedEvent.getStartTime(), amStartedEvent.getContainerId(), StringInterner.weakIntern(amStartedEvent.getNodeManagerHost()), amStartedEvent.getNodeManagerPort(), amStartedEvent.getNodeManagerHttpPort()));\r\n            } else if (amStartedEventsBegan) {\r\n                break;\r\n            }\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Could not parse the old history file. \" + \"Will not have old AMinfos \", e);\r\n    }\r\n    return amInfos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "recovered",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean recovered()\n{\r\n    return recoveredJobStartTime > 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "startJobs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void startJobs()\n{\r\n    JobEvent startJobEvent = new JobStartEvent(job.getID(), recoveredJobStartTime);\r\n    dispatcher.getEventHandler().handle(startJobEvent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "validateInputParam",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validateInputParam(String value, String param) throws IOException\n{\r\n    if (value == null) {\r\n        String msg = param + \" is null\";\r\n        LOG.error(msg);\r\n        throw new IOException(msg);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "main",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    try {\r\n        mainStarted = true;\r\n        Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\r\n        String containerIdStr = System.getenv(Environment.CONTAINER_ID.name());\r\n        String nodeHostString = System.getenv(Environment.NM_HOST.name());\r\n        String nodePortString = System.getenv(Environment.NM_PORT.name());\r\n        String nodeHttpPortString = System.getenv(Environment.NM_HTTP_PORT.name());\r\n        String appSubmitTimeStr = System.getenv(ApplicationConstants.APP_SUBMIT_TIME_ENV);\r\n        validateInputParam(containerIdStr, Environment.CONTAINER_ID.name());\r\n        validateInputParam(nodeHostString, Environment.NM_HOST.name());\r\n        validateInputParam(nodePortString, Environment.NM_PORT.name());\r\n        validateInputParam(nodeHttpPortString, Environment.NM_HTTP_PORT.name());\r\n        validateInputParam(appSubmitTimeStr, ApplicationConstants.APP_SUBMIT_TIME_ENV);\r\n        ContainerId containerId = ContainerId.fromString(containerIdStr);\r\n        ApplicationAttemptId applicationAttemptId = containerId.getApplicationAttemptId();\r\n        if (applicationAttemptId != null) {\r\n            CallerContext.setCurrent(new CallerContext.Builder(\"mr_appmaster_\" + applicationAttemptId.toString()).build());\r\n        }\r\n        long appSubmitTime = Long.parseLong(appSubmitTimeStr);\r\n        MRAppMaster appMaster = new MRAppMaster(applicationAttemptId, containerId, nodeHostString, Integer.parseInt(nodePortString), Integer.parseInt(nodeHttpPortString), appSubmitTime);\r\n        ShutdownHookManager.get().addShutdownHook(new MRAppMasterShutdownHook(appMaster), SHUTDOWN_HOOK_PRIORITY);\r\n        JobConf conf = new JobConf(new YarnConfiguration());\r\n        conf.addResource(new Path(MRJobConfig.JOB_CONF_FILE));\r\n        MRWebAppUtil.initialize(conf);\r\n        String systemPropsToLog = MRApps.getSystemPropertiesToLog(conf);\r\n        if (systemPropsToLog != null) {\r\n            LOG.info(systemPropsToLog);\r\n        }\r\n        String jobUserName = System.getenv(ApplicationConstants.Environment.USER.name());\r\n        conf.set(MRJobConfig.USER_NAME, jobUserName);\r\n        initAndStartAppMaster(appMaster, conf, jobUserName);\r\n    } catch (Throwable t) {\r\n        LOG.error(\"Error starting MRAppMaster\", t);\r\n        ExitUtil.terminate(1, t);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "notifyIsLastAMRetry",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void notifyIsLastAMRetry(boolean isLastAMRetry)\n{\r\n    if (containerAllocator instanceof ContainerAllocatorRouter) {\r\n        LOG.info(\"Notify RMCommunicator isAMLastRetry: \" + isLastAMRetry);\r\n        ((ContainerAllocatorRouter) containerAllocator).setShouldUnregister(isLastAMRetry);\r\n    }\r\n    if (jobHistoryEventHandler != null) {\r\n        LOG.info(\"Notify JHEH isAMLastRetry: \" + isLastAMRetry);\r\n        jobHistoryEventHandler.setForcejobCompletion(isLastAMRetry);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "initAndStartAppMaster",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void initAndStartAppMaster(final MRAppMaster appMaster, final JobConf conf, String jobUserName) throws IOException, InterruptedException\n{\r\n    UserGroupInformation.setConfiguration(conf);\r\n    SecurityUtil.setConfiguration(conf);\r\n    Credentials credentials = UserGroupInformation.getCurrentUser().getCredentials();\r\n    LOG.info(\"Executing with tokens: {}\", credentials.getAllTokens());\r\n    UserGroupInformation appMasterUgi = UserGroupInformation.createRemoteUser(jobUserName);\r\n    appMasterUgi.addCredentials(credentials);\r\n    Iterator<Token<?>> iter = credentials.getAllTokens().iterator();\r\n    while (iter.hasNext()) {\r\n        Token<?> token = iter.next();\r\n        if (token.getKind().equals(AMRMTokenIdentifier.KIND_NAME)) {\r\n            iter.remove();\r\n        }\r\n    }\r\n    conf.getCredentials().addAll(credentials);\r\n    appMasterUgi.doAs(new PrivilegedExceptionAction<Object>() {\r\n\r\n        @Override\r\n        public Object run() throws Exception {\r\n            appMaster.init(conf);\r\n            appMaster.start();\r\n            if (appMaster.errorHappenedShutDown) {\r\n                throw new IOException(\"Was asked to shut down.\");\r\n            }\r\n            return null;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createJobClassLoader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createJobClassLoader(Configuration conf) throws IOException\n{\r\n    jobClassLoader = MRApps.createJobClassLoader(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "callWithJobClassLoader",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "T callWithJobClassLoader(Configuration conf, Action<T> action)\n{\r\n    ClassLoader currentClassLoader = conf.getClassLoader();\r\n    boolean setJobClassLoader = jobClassLoader != null && currentClassLoader != jobClassLoader;\r\n    if (setJobClassLoader) {\r\n        MRApps.setClassLoader(jobClassLoader, conf);\r\n    }\r\n    try {\r\n        return action.call(conf);\r\n    } finally {\r\n        if (setJobClassLoader) {\r\n            MRApps.setClassLoader(currentClassLoader, conf);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "callWithJobClassLoader",
  "errType" : [ "IOException", "YarnRuntimeException", "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "T callWithJobClassLoader(Configuration conf, ExceptionAction<T> action) throws IOException\n{\r\n    ClassLoader currentClassLoader = conf.getClassLoader();\r\n    boolean setJobClassLoader = jobClassLoader != null && currentClassLoader != jobClassLoader;\r\n    if (setJobClassLoader) {\r\n        MRApps.setClassLoader(jobClassLoader, conf);\r\n    }\r\n    try {\r\n        return action.call(conf);\r\n    } catch (IOException e) {\r\n        throw e;\r\n    } catch (YarnRuntimeException e) {\r\n        throw e;\r\n    } catch (Exception e) {\r\n        throw new YarnRuntimeException(e);\r\n    } finally {\r\n        if (setJobClassLoader) {\r\n            MRApps.setClassLoader(currentClassLoader, conf);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getClientService",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientService getClientService()\n{\r\n    return clientService;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getShufflePort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getShufflePort()\n{\r\n    return shufflePort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getMaxAttempts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMaxAttempts()\n{\r\n    return conf.getInt(MRJobConfig.MAP_MAX_ATTEMPTS, 4);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createAttempt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptImpl createAttempt()\n{\r\n    return new MapTaskAttemptImpl(getID(), nextAttemptNumber, eventHandler, jobFile, partition, taskSplitMetaInfo, conf, taskAttemptListener, jobToken, credentials, clock, appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskType getType()\n{\r\n    return TaskType.MAP;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getTaskSplitMetaInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskSplitMetaInfo getTaskSplitMetaInfo()\n{\r\n    return this.taskSplitMetaInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getSplitsAsString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getSplitsAsString()\n{\r\n    String[] splits = getTaskSplitMetaInfo().getLocations();\r\n    if (splits == null || splits.length == 0)\r\n        return \"\";\r\n    StringBuilder sb = new StringBuilder();\r\n    for (int i = 0; i < splits.length; i++) {\r\n        if (i != 0)\r\n            sb.append(\",\");\r\n        sb.append(splits[i]);\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void getCounters(AppContext ctx, Job job)\n{\r\n    if (job == null) {\r\n        return;\r\n    }\r\n    total = job.getAllCounters();\r\n    boolean needTotalCounters = false;\r\n    if (total == null) {\r\n        total = new Counters();\r\n        needTotalCounters = true;\r\n    }\r\n    map = new Counters();\r\n    reduce = new Counters();\r\n    Map<TaskId, Task> tasks = job.getTasks();\r\n    for (Task t : tasks.values()) {\r\n        Counters counters = t.getCounters();\r\n        if (counters == null) {\r\n            continue;\r\n        }\r\n        switch(t.getType()) {\r\n            case MAP:\r\n                map.incrAllCounters(counters);\r\n                break;\r\n            case REDUCE:\r\n                reduce.incrAllCounters(counters);\r\n                break;\r\n        }\r\n        if (needTotalCounters) {\r\n            total.incrAllCounters(counters);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    registerHeartbeatHandler(conf);\r\n    commitWindowMs = conf.getLong(MRJobConfig.MR_AM_COMMIT_WINDOW_MS, MRJobConfig.DEFAULT_MR_AM_COMMIT_WINDOW_MS);\r\n    MRJobConfUtil.setTaskLogProgressDeltaThresholds(conf);\r\n    super.serviceInit(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    startRpcServer();\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "registerHeartbeatHandler",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void registerHeartbeatHandler(Configuration conf)\n{\r\n    taskHeartbeatHandler = new TaskHeartbeatHandler(context.getEventHandler(), context.getClock(), conf.getInt(MRJobConfig.MR_AM_TASK_LISTENER_THREAD_COUNT, MRJobConfig.DEFAULT_MR_AM_TASK_LISTENER_THREAD_COUNT));\r\n    addService(taskHeartbeatHandler);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "startRpcServer",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void startRpcServer()\n{\r\n    Configuration conf = getConfig();\r\n    try {\r\n        server = new RPC.Builder(conf).setProtocol(TaskUmbilicalProtocol.class).setInstance(this).setBindAddress(\"0.0.0.0\").setPortRangeConfig(MRJobConfig.MR_AM_JOB_CLIENT_PORT_RANGE).setNumHandlers(conf.getInt(MRJobConfig.MR_AM_TASK_LISTENER_THREAD_COUNT, MRJobConfig.DEFAULT_MR_AM_TASK_LISTENER_THREAD_COUNT)).setVerbose(false).setSecretManager(jobTokenSecretManager).build();\r\n        if (conf.getBoolean(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, false)) {\r\n            refreshServiceAcls(conf, new MRAMPolicyProvider());\r\n        }\r\n        server.start();\r\n        this.address = NetUtils.createSocketAddrForHost(context.getNMHostname(), server.getListenerAddress().getPort());\r\n    } catch (IOException e) {\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "refreshServiceAcls",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void refreshServiceAcls(Configuration configuration, PolicyProvider policyProvider)\n{\r\n    this.server.refreshServiceAcl(configuration, policyProvider);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    stopRpcServer();\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "stopRpcServer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void stopRpcServer()\n{\r\n    if (server != null) {\r\n        server.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InetSocketAddress getAddress()\n{\r\n    return address;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "canCommit",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "boolean canCommit(TaskAttemptID taskAttemptID) throws IOException\n{\r\n    LOG.info(\"Commit go/no-go request from \" + taskAttemptID.toString());\r\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = TypeConverter.toYarn(taskAttemptID);\r\n    taskHeartbeatHandler.progressing(attemptID);\r\n    long now = context.getClock().getTime();\r\n    if (now - rmHeartbeatHandler.getLastHeartbeatTime() > commitWindowMs) {\r\n        return false;\r\n    }\r\n    Job job = context.getJob(attemptID.getTaskId().getJobId());\r\n    Task task = job.getTask(attemptID.getTaskId());\r\n    return task.canCommit(attemptID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "commitPending",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void commitPending(TaskAttemptID taskAttemptID, TaskStatus taskStatsu) throws IOException, InterruptedException\n{\r\n    LOG.info(\"Commit-pending state update from \" + taskAttemptID.toString());\r\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = TypeConverter.toYarn(taskAttemptID);\r\n    taskHeartbeatHandler.progressing(attemptID);\r\n    context.getEventHandler().handle(new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_COMMIT_PENDING));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "preempted",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void preempted(TaskAttemptID taskAttemptID, TaskStatus taskStatus) throws IOException, InterruptedException\n{\r\n    LOG.info(\"Preempted state update from \" + taskAttemptID.toString());\r\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = TypeConverter.toYarn(taskAttemptID);\r\n    preemptionPolicy.reportSuccessfulPreemption(attemptID);\r\n    taskHeartbeatHandler.progressing(attemptID);\r\n    context.getEventHandler().handle(new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_PREEMPTED));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "done",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void done(TaskAttemptID taskAttemptID) throws IOException\n{\r\n    LOG.info(\"Done acknowledgment from \" + taskAttemptID.toString());\r\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = TypeConverter.toYarn(taskAttemptID);\r\n    taskHeartbeatHandler.progressing(attemptID);\r\n    context.getEventHandler().handle(new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_DONE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "fatalError",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void fatalError(TaskAttemptID taskAttemptID, String msg, boolean fastFail) throws IOException\n{\r\n    LOG.error(\"Task: \" + taskAttemptID + \" - exited : \" + msg);\r\n    reportDiagnosticInfo(taskAttemptID, \"Error: \" + msg);\r\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = TypeConverter.toYarn(taskAttemptID);\r\n    preemptionPolicy.handleFailedContainer(attemptID);\r\n    context.getEventHandler().handle(new TaskAttemptFailEvent(attemptID, fastFail));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "fsError",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void fsError(TaskAttemptID taskAttemptID, String message) throws IOException\n{\r\n    LOG.error(\"Task: \" + taskAttemptID + \" - failed due to FSError: \" + message);\r\n    reportDiagnosticInfo(taskAttemptID, \"FSError: \" + message);\r\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = TypeConverter.toYarn(taskAttemptID);\r\n    preemptionPolicy.handleFailedContainer(attemptID);\r\n    context.getEventHandler().handle(new TaskAttemptFailEvent(attemptID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "shuffleError",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void shuffleError(TaskAttemptID taskAttemptID, String message) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapCompletionEvents",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "MapTaskCompletionEventsUpdate getMapCompletionEvents(JobID jobIdentifier, int startIndex, int maxEvents, TaskAttemptID taskAttemptID) throws IOException\n{\r\n    LOG.info(\"MapCompletionEvents request from \" + taskAttemptID.toString() + \". startIndex \" + startIndex + \" maxEvents \" + maxEvents);\r\n    boolean shouldReset = false;\r\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = TypeConverter.toYarn(taskAttemptID);\r\n    TaskCompletionEvent[] events = context.getJob(attemptID.getTaskId().getJobId()).getMapAttemptCompletionEvents(startIndex, maxEvents);\r\n    taskHeartbeatHandler.progressing(attemptID);\r\n    return new MapTaskCompletionEventsUpdate(events, shouldReset);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "reportDiagnosticInfo",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void reportDiagnosticInfo(TaskAttemptID taskAttemptID, String diagnosticInfo) throws IOException\n{\r\n    diagnosticInfo = StringInterner.weakIntern(diagnosticInfo);\r\n    LOG.info(\"Diagnostics report from \" + taskAttemptID.toString() + \": \" + diagnosticInfo);\r\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = TypeConverter.toYarn(taskAttemptID);\r\n    taskHeartbeatHandler.progressing(attemptID);\r\n    context.getEventHandler().handle(new TaskAttemptDiagnosticsUpdateEvent(attemptID, diagnosticInfo));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "statusUpdate",
  "errType" : null,
  "containingMethodsNum" : 32,
  "sourceCodeText" : "AMFeedback statusUpdate(TaskAttemptID taskAttemptID, TaskStatus taskStatus) throws IOException, InterruptedException\n{\r\n    org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId yarnAttemptID = TypeConverter.toYarn(taskAttemptID);\r\n    AMFeedback feedback = new AMFeedback();\r\n    feedback.setTaskFound(true);\r\n    AtomicReference<TaskAttemptStatus> lastStatusRef = attemptIdToStatus.get(yarnAttemptID);\r\n    if (lastStatusRef == null) {\r\n        if (!taskHeartbeatHandler.hasRecentlyUnregistered(yarnAttemptID)) {\r\n            LOG.error(\"Status update was called with illegal TaskAttemptId: \" + yarnAttemptID);\r\n            feedback.setTaskFound(false);\r\n        }\r\n        return feedback;\r\n    }\r\n    if (getConfig().getBoolean(MRJobConfig.TASK_PREEMPTION, false) && preemptionPolicy.isPreempted(yarnAttemptID)) {\r\n        feedback.setPreemption(true);\r\n        LOG.info(\"Setting preemption bit for task: \" + yarnAttemptID + \" of type \" + yarnAttemptID.getTaskId().getTaskType());\r\n    }\r\n    if (taskStatus == null) {\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"Ping from \" + taskAttemptID.toString());\r\n        }\r\n        return feedback;\r\n    }\r\n    taskHeartbeatHandler.progressing(yarnAttemptID);\r\n    TaskAttemptStatus taskAttemptStatus = new TaskAttemptStatus();\r\n    taskAttemptStatus.id = yarnAttemptID;\r\n    taskAttemptStatus.progress = taskStatus.getProgress();\r\n    taskAttemptLogProgressStamps.computeIfAbsent(taskAttemptID, k -> new TaskProgressLogPair(taskAttemptID)).update(taskStatus.getProgress());\r\n    taskAttemptStatus.stateString = taskStatus.getStateString();\r\n    taskAttemptStatus.phase = TypeConverter.toYarn(taskStatus.getPhase());\r\n    taskAttemptStatus.counters = new org.apache.hadoop.mapreduce.Counters(taskStatus.getCounters());\r\n    if (taskStatus.getIsMap() && taskStatus.getMapFinishTime() != 0) {\r\n        taskAttemptStatus.mapFinishTime = taskStatus.getMapFinishTime();\r\n    }\r\n    if (!taskStatus.getIsMap() && taskStatus.getShuffleFinishTime() != 0) {\r\n        taskAttemptStatus.shuffleFinishTime = taskStatus.getShuffleFinishTime();\r\n    }\r\n    if (!taskStatus.getIsMap() && taskStatus.getSortFinishTime() != 0) {\r\n        taskAttemptStatus.sortFinishTime = taskStatus.getSortFinishTime();\r\n    }\r\n    if (taskStatus.getFetchFailedMaps() != null && taskStatus.getFetchFailedMaps().size() > 0) {\r\n        taskAttemptStatus.fetchFailedMaps = new ArrayList<org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId>();\r\n        for (TaskAttemptID failedMapId : taskStatus.getFetchFailedMaps()) {\r\n            taskAttemptStatus.fetchFailedMaps.add(TypeConverter.toYarn(failedMapId));\r\n        }\r\n    }\r\n    coalesceStatusUpdate(yarnAttemptID, taskAttemptStatus, lastStatusRef);\r\n    return feedback;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProtocolVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getProtocolVersion(String arg0, long arg1) throws IOException\n{\r\n    return TaskUmbilicalProtocol.versionID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "reportNextRecordRange",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reportNextRecordRange(TaskAttemptID taskAttemptID, Range range) throws IOException\n{\r\n    throw new IOException(\"Not yet implemented.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTask",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "JvmTask getTask(JvmContext context) throws IOException\n{\r\n    JVMId jvmId = context.jvmId;\r\n    LOG.info(\"JVM with ID : \" + jvmId + \" asked for a task\");\r\n    JvmTask jvmTask = null;\r\n    WrappedJvmID wJvmID = new WrappedJvmID(jvmId.getJobId(), jvmId.isMap, jvmId.getId());\r\n    if (!jvmIDToActiveAttemptMap.containsKey(wJvmID)) {\r\n        LOG.info(\"JVM with ID: \" + jvmId + \" is invalid and will be killed.\");\r\n        jvmTask = TASK_FOR_INVALID_JVM;\r\n    } else {\r\n        if (!launchedJVMs.contains(wJvmID)) {\r\n            jvmTask = null;\r\n            LOG.info(\"JVM with ID: \" + jvmId + \" asking for task before AM launch registered. Given null task\");\r\n        } else {\r\n            org.apache.hadoop.mapred.Task task = jvmIDToActiveAttemptMap.remove(wJvmID);\r\n            launchedJVMs.remove(wJvmID);\r\n            LOG.info(\"JVM with ID: \" + jvmId + \" given task: \" + task.getTaskID());\r\n            task.setEncryptedSpillKey(encryptedSpillKey);\r\n            jvmTask = new JvmTask(task, false);\r\n        }\r\n    }\r\n    return jvmTask;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "registerPendingTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void registerPendingTask(org.apache.hadoop.mapred.Task task, WrappedJvmID jvmID)\n{\r\n    jvmIDToActiveAttemptMap.put(jvmID, task);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "registerLaunchedTask",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void registerLaunchedTask(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID, WrappedJvmID jvmId)\n{\r\n    launchedJVMs.add(jvmId);\r\n    taskHeartbeatHandler.register(attemptID);\r\n    attemptIdToStatus.put(attemptID, new AtomicReference<>());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "unregister",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void unregister(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID, WrappedJvmID jvmID)\n{\r\n    launchedJVMs.remove(jvmID);\r\n    jvmIDToActiveAttemptMap.remove(jvmID);\r\n    taskHeartbeatHandler.unregister(attemptID);\r\n    attemptIdToStatus.remove(attemptID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProtocolSignature",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ProtocolSignature getProtocolSignature(String protocol, long clientVersion, int clientMethodsHash) throws IOException\n{\r\n    return ProtocolSignature.getProtocolSignature(this, protocol, clientVersion, clientMethodsHash);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCheckpointID",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskCheckpointID getCheckpointID(TaskID taskId)\n{\r\n    TaskId tid = TypeConverter.toYarn(taskId);\r\n    return preemptionPolicy.getCheckpointID(tid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setCheckpointID",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setCheckpointID(TaskID taskId, TaskCheckpointID cid)\n{\r\n    TaskId tid = TypeConverter.toYarn(taskId);\r\n    preemptionPolicy.setCheckpointID(tid, cid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "coalesceStatusUpdate",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void coalesceStatusUpdate(TaskAttemptId yarnAttemptID, TaskAttemptStatus taskAttemptStatus, AtomicReference<TaskAttemptStatus> lastStatusRef)\n{\r\n    List<TaskAttemptId> fetchFailedMaps = taskAttemptStatus.fetchFailedMaps;\r\n    TaskAttemptStatus lastStatus = null;\r\n    boolean done = false;\r\n    while (!done) {\r\n        lastStatus = lastStatusRef.get();\r\n        if (lastStatus != null && lastStatus.fetchFailedMaps != null) {\r\n            if (taskAttemptStatus.fetchFailedMaps == null) {\r\n                taskAttemptStatus.fetchFailedMaps = lastStatus.fetchFailedMaps;\r\n            } else {\r\n                taskAttemptStatus.fetchFailedMaps = new ArrayList<>(lastStatus.fetchFailedMaps.size() + fetchFailedMaps.size());\r\n                taskAttemptStatus.fetchFailedMaps.addAll(lastStatus.fetchFailedMaps);\r\n                taskAttemptStatus.fetchFailedMaps.addAll(fetchFailedMaps);\r\n            }\r\n        }\r\n        done = lastStatusRef.compareAndSet(lastStatus, taskAttemptStatus);\r\n        if (!done) {\r\n            LOG.info(\"TaskAttempt \" + yarnAttemptID + \": lastStatusRef changed by another thread, retrying...\");\r\n            taskAttemptStatus.fetchFailedMaps = fetchFailedMaps;\r\n        }\r\n    }\r\n    boolean asyncUpdatedNeeded = (lastStatus == null);\r\n    if (asyncUpdatedNeeded) {\r\n        context.getEventHandler().handle(new TaskAttemptStatusUpdateEvent(taskAttemptStatus.id, lastStatusRef));\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getAttemptIdToStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ConcurrentMap<TaskAttemptId, AtomicReference<TaskAttemptStatus>> getAttemptIdToStatus()\n{\r\n    return attemptIdToStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getTaskAttemptID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptId getTaskAttemptID()\n{\r\n    return attemptID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "enrollAttempt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void enrollAttempt(TaskAttemptStatus status, long timestamp)\n{\r\n    startTimes.put(status.id, timestamp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "attemptEnrolledTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long attemptEnrolledTime(TaskAttemptId attemptID)\n{\r\n    Long result = startTimes.get(attemptID);\r\n    return result == null ? Long.MAX_VALUE : result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "contextualize",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void contextualize(Configuration conf, AppContext context)\n{\r\n    this.context = context;\r\n    Map<JobId, Job> allJobs = context.getAllJobs();\r\n    for (Map.Entry<JobId, Job> entry : allJobs.entrySet()) {\r\n        final Job job = entry.getValue();\r\n        mapperStatistics.put(job, new DataStatistics());\r\n        reducerStatistics.put(job, new DataStatistics());\r\n        slowTaskRelativeTresholds.put(job, conf.getFloat(MRJobConfig.SPECULATIVE_SLOWTASK_THRESHOLD, 1.0f));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "dataStatisticsForTask",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "DataStatistics dataStatisticsForTask(TaskId taskID)\n{\r\n    JobId jobID = taskID.getJobId();\r\n    Job job = context.getJob(jobID);\r\n    if (job == null) {\r\n        return null;\r\n    }\r\n    Task task = job.getTask(taskID);\r\n    if (task == null) {\r\n        return null;\r\n    }\r\n    return task.getType() == TaskType.MAP ? mapperStatistics.get(job) : task.getType() == TaskType.REDUCE ? reducerStatistics.get(job) : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "thresholdRuntime",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "long thresholdRuntime(TaskId taskID)\n{\r\n    JobId jobID = taskID.getJobId();\r\n    Job job = context.getJob(jobID);\r\n    TaskType type = taskID.getTaskType();\r\n    DataStatistics statistics = dataStatisticsForTask(taskID);\r\n    int completedTasksOfType = type == TaskType.MAP ? job.getCompletedMaps() : job.getCompletedReduces();\r\n    int totalTasksOfType = type == TaskType.MAP ? job.getTotalMaps() : job.getTotalReduces();\r\n    if (completedTasksOfType < MINIMUM_COMPLETE_NUMBER_TO_SPECULATE || (((float) completedTasksOfType) / totalTasksOfType) < MINIMUM_COMPLETE_PROPORTION_TO_SPECULATE) {\r\n        return Long.MAX_VALUE;\r\n    }\r\n    long result = statistics == null ? Long.MAX_VALUE : (long) statistics.outlier(slowTaskRelativeTresholds.get(job));\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "estimatedNewAttemptRuntime",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long estimatedNewAttemptRuntime(TaskId id)\n{\r\n    DataStatistics statistics = dataStatisticsForTask(id);\r\n    if (statistics == null) {\r\n        return -1L;\r\n    }\r\n    return (long) statistics.mean();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "updateAttempt",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void updateAttempt(TaskAttemptStatus status, long timestamp)\n{\r\n    TaskAttemptId attemptID = status.id;\r\n    TaskId taskID = attemptID.getTaskId();\r\n    JobId jobID = taskID.getJobId();\r\n    Job job = context.getJob(jobID);\r\n    if (job == null) {\r\n        return;\r\n    }\r\n    Task task = job.getTask(taskID);\r\n    if (task == null) {\r\n        return;\r\n    }\r\n    Long boxedStart = startTimes.get(attemptID);\r\n    long start = boxedStart == null ? Long.MIN_VALUE : boxedStart;\r\n    TaskAttempt taskAttempt = task.getAttempt(attemptID);\r\n    if (taskAttempt.getState() == TaskAttemptState.SUCCEEDED) {\r\n        boolean isNew = false;\r\n        synchronized (doneTasks) {\r\n            if (!doneTasks.contains(task)) {\r\n                doneTasks.add(task);\r\n                isNew = true;\r\n            }\r\n        }\r\n        if (isNew) {\r\n            long finish = timestamp;\r\n            if (start > 1L && finish > 1L && start <= finish) {\r\n                long duration = finish - start;\r\n                DataStatistics statistics = dataStatisticsForTask(taskID);\r\n                if (statistics != null) {\r\n                    statistics.add(duration);\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    commonPreHead(html);\r\n    set(DATATABLES_ID, \"tasks\");\r\n    set(initID(ACCORDION, \"nav\"), \"{autoHeight:false, active:2}\");\r\n    set(initID(DATATABLES, \"tasks\"), tasksTableInit());\r\n    setTableStyles(html, \"tasks\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return TasksBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "tasksTableInit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String tasksTableInit()\n{\r\n    return tableInit().append(\", 'aaData': tasksTableData\").append(\", bDeferRender: true\").append(\", bProcessing: true\").append(\"\\n, aoColumnDefs: [\\n\").append(\"{'sType':'natural', 'aTargets': [0]\").append(\", 'mRender': parseHadoopID }\").append(\"\\n, {'sType':'numeric', bSearchable:false, 'aTargets': [1]\").append(\", 'mRender': parseHadoopProgress }\").append(\"\\n, {'sType':'numeric', 'aTargets': [4, 5]\").append(\", 'mRender': renderHadoopDate }\").append(\"\\n, {'sType':'numeric', 'aTargets': [6]\").append(\", 'mRender': renderHadoopElapsedTime }]\").append(\", aaSorting: [[0, 'asc']] }\").toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MRAppMetrics create()\n{\r\n    return create(DefaultMetricsSystem.instance());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MRAppMetrics create(MetricsSystem ms)\n{\r\n    JvmMetrics.initSingleton(\"MRAppMaster\", null);\r\n    return ms.register(new MRAppMetrics());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "submittedJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void submittedJob(Job job)\n{\r\n    jobsSubmitted.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "completedJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void completedJob(Job job)\n{\r\n    jobsCompleted.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "failedJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void failedJob(Job job)\n{\r\n    jobsFailed.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "killedJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void killedJob(Job job)\n{\r\n    jobsKilled.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "preparingJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void preparingJob(Job job)\n{\r\n    jobsPreparing.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "endPreparingJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void endPreparingJob(Job job)\n{\r\n    jobsPreparing.decr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "runningJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void runningJob(Job job)\n{\r\n    jobsRunning.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "endRunningJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void endRunningJob(Job job)\n{\r\n    jobsRunning.decr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "launchedTask",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void launchedTask(Task task)\n{\r\n    switch(task.getType()) {\r\n        case MAP:\r\n            mapsLaunched.incr();\r\n            break;\r\n        case REDUCE:\r\n            reducesLaunched.incr();\r\n            break;\r\n    }\r\n    endWaitingTask(task);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "completedTask",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void completedTask(Task task)\n{\r\n    switch(task.getType()) {\r\n        case MAP:\r\n            mapsCompleted.incr();\r\n            break;\r\n        case REDUCE:\r\n            reducesCompleted.incr();\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "failedTask",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void failedTask(Task task)\n{\r\n    switch(task.getType()) {\r\n        case MAP:\r\n            mapsFailed.incr();\r\n            break;\r\n        case REDUCE:\r\n            reducesFailed.incr();\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "killedTask",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void killedTask(Task task)\n{\r\n    switch(task.getType()) {\r\n        case MAP:\r\n            mapsKilled.incr();\r\n            break;\r\n        case REDUCE:\r\n            reducesKilled.incr();\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "runningTask",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void runningTask(Task task)\n{\r\n    switch(task.getType()) {\r\n        case MAP:\r\n            mapsRunning.incr();\r\n            break;\r\n        case REDUCE:\r\n            reducesRunning.incr();\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "endRunningTask",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void endRunningTask(Task task)\n{\r\n    switch(task.getType()) {\r\n        case MAP:\r\n            mapsRunning.decr();\r\n            break;\r\n        case REDUCE:\r\n            reducesRunning.decr();\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "waitingTask",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void waitingTask(Task task)\n{\r\n    switch(task.getType()) {\r\n        case MAP:\r\n            mapsWaiting.incr();\r\n            break;\r\n        case REDUCE:\r\n            reducesWaiting.incr();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "endWaitingTask",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void endWaitingTask(Task task)\n{\r\n    switch(task.getType()) {\r\n        case MAP:\r\n            mapsWaiting.decr();\r\n            break;\r\n        case REDUCE:\r\n            reducesWaiting.decr();\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\client",
  "methodName" : "serviceStart",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    Configuration conf = getConfig();\r\n    YarnRPC rpc = YarnRPC.create(conf);\r\n    InetSocketAddress address = new InetSocketAddress(0);\r\n    server = rpc.getServer(MRClientProtocol.class, protocolHandler, address, conf, appContext.getClientToAMTokenSecretManager(), conf.getInt(MRJobConfig.MR_AM_JOB_CLIENT_THREAD_COUNT, MRJobConfig.DEFAULT_MR_AM_JOB_CLIENT_THREAD_COUNT), MRJobConfig.MR_AM_JOB_CLIENT_PORT_RANGE);\r\n    if (conf.getBoolean(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, false)) {\r\n        refreshServiceAcls(conf, new MRAMPolicyProvider());\r\n    }\r\n    server.start();\r\n    this.bindAddress = NetUtils.createSocketAddrForHost(appContext.getNMHostname(), server.getListenerAddress().getPort());\r\n    LOG.info(\"Instantiated MRClientService at \" + this.bindAddress);\r\n    try {\r\n        HttpConfig.Policy httpPolicy = conf.getBoolean(MRJobConfig.MR_AM_WEBAPP_HTTPS_ENABLED, MRJobConfig.DEFAULT_MR_AM_WEBAPP_HTTPS_ENABLED) ? Policy.HTTPS_ONLY : Policy.HTTP_ONLY;\r\n        boolean needsClientAuth = conf.getBoolean(MRJobConfig.MR_AM_WEBAPP_HTTPS_CLIENT_AUTH, MRJobConfig.DEFAULT_MR_AM_WEBAPP_HTTPS_CLIENT_AUTH);\r\n        webApp = WebApps.$for(\"mapreduce\", AppContext.class, appContext, \"ws\").withHttpPolicy(conf, httpPolicy).withPortRange(conf, MRJobConfig.MR_AM_WEBAPP_PORT_RANGE).needsClientAuth(needsClientAuth).start(new AMWebApp());\r\n    } catch (Exception e) {\r\n        LOG.error(\"Webapps failed to start. Ignoring for now:\", e);\r\n    }\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\client",
  "methodName" : "refreshServiceAcls",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void refreshServiceAcls(Configuration configuration, PolicyProvider policyProvider)\n{\r\n    this.server.refreshServiceAcl(configuration, policyProvider);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\client",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    if (server != null) {\r\n        server.stop();\r\n    }\r\n    if (webApp != null) {\r\n        webApp.stop();\r\n    }\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\client",
  "methodName" : "getBindAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InetSocketAddress getBindAddress()\n{\r\n    return bindAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\client",
  "methodName" : "getHttpPort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getHttpPort()\n{\r\n    return webApp.port();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\client",
  "methodName" : "forceKillTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "KillTaskAttemptResponse forceKillTaskAttempt(KillTaskAttemptRequest request) throws YarnException, IOException\n{\r\n    return protocolHandler.killTaskAttempt(request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\client",
  "methodName" : "getWebApp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "WebApp getWebApp()\n{\r\n    return webApp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\local",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n    retryInterval = getConfig().getLong(MRJobConfig.MR_AM_TO_RM_WAIT_INTERVAL_MS, MRJobConfig.DEFAULT_MR_AM_TO_RM_WAIT_INTERVAL_MS);\r\n    retrystartTime = System.currentTimeMillis();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\local",
  "methodName" : "heartbeat",
  "errType" : [ "ApplicationAttemptNotFoundException", "ApplicationMasterNotRegisteredException", "Exception" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void heartbeat() throws Exception\n{\r\n    AllocateRequest allocateRequest = AllocateRequest.newInstance(this.lastResponseID, super.getApplicationProgress(), new ArrayList<ResourceRequest>(), new ArrayList<ContainerId>(), null);\r\n    AllocateResponse allocateResponse = null;\r\n    try {\r\n        allocateResponse = scheduler.allocate(allocateRequest);\r\n        retrystartTime = System.currentTimeMillis();\r\n    } catch (ApplicationAttemptNotFoundException e) {\r\n        LOG.info(\"Event from RM: shutting down Application Master\");\r\n        eventHandler.handle(new JobEvent(this.getJob().getID(), JobEventType.JOB_AM_REBOOT));\r\n        throw new YarnRuntimeException(\"Resource Manager doesn't recognize AttemptId: \" + this.getContext().getApplicationID(), e);\r\n    } catch (ApplicationMasterNotRegisteredException e) {\r\n        LOG.info(\"ApplicationMaster is out of sync with ResourceManager,\" + \" hence resync and send outstanding requests.\");\r\n        this.lastResponseID = 0;\r\n        register();\r\n    } catch (Exception e) {\r\n        if (System.currentTimeMillis() - retrystartTime >= retryInterval) {\r\n            LOG.error(\"Could not contact RM after \" + retryInterval + \" milliseconds.\");\r\n            eventHandler.handle(new JobEvent(this.getJob().getID(), JobEventType.INTERNAL_ERROR));\r\n            throw new YarnRuntimeException(\"Could not contact RM after \" + retryInterval + \" milliseconds.\");\r\n        }\r\n        throw e;\r\n    }\r\n    if (allocateResponse != null) {\r\n        this.lastResponseID = allocateResponse.getResponseId();\r\n        Token token = allocateResponse.getAMRMToken();\r\n        if (token != null) {\r\n            updateAMRMToken(token);\r\n        }\r\n        Priority priorityFromResponse = Priority.newInstance(allocateResponse.getApplicationPriority().getPriority());\r\n        getJob().setJobPriority(priorityFromResponse);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\local",
  "methodName" : "updateAMRMToken",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void updateAMRMToken(Token token) throws IOException\n{\r\n    org.apache.hadoop.security.token.Token<AMRMTokenIdentifier> amrmToken = new org.apache.hadoop.security.token.Token<AMRMTokenIdentifier>(token.getIdentifier().array(), token.getPassword().array(), new Text(token.getKind()), new Text(token.getService()));\r\n    UserGroupInformation currentUGI = UserGroupInformation.getCurrentUser();\r\n    currentUGI.addToken(amrmToken);\r\n    amrmToken.setService(ClientRMProxy.getAMRMTokenService(getConfig()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\local",
  "methodName" : "handle",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void handle(ContainerAllocatorEvent event)\n{\r\n    if (event.getType() == ContainerAllocator.EventType.CONTAINER_REQ) {\r\n        LOG.info(\"Processing the event \" + event.toString());\r\n        ContainerId cID = ContainerId.newContainerId(getContext().getApplicationAttemptId(), this.containerId.getContainerId());\r\n        Container container = recordFactory.newRecordInstance(Container.class);\r\n        container.setId(cID);\r\n        NodeId nodeId = NodeId.newInstance(this.nmHost, this.nmPort);\r\n        container.setResource(Resource.newInstance(0, 0));\r\n        container.setNodeId(nodeId);\r\n        container.setContainerToken(null);\r\n        container.setNodeHttpAddress(this.nmHost + \":\" + this.nmHttpPort);\r\n        if (event.getAttemptID().getTaskId().getTaskType() == TaskType.MAP) {\r\n            JobCounterUpdateEvent jce = new JobCounterUpdateEvent(event.getAttemptID().getTaskId().getJobId());\r\n            jce.addCounterUpdate(JobCounter.OTHER_LOCAL_MAPS, 1);\r\n            eventHandler.handle(jce);\r\n        }\r\n        eventHandler.handle(new TaskAttemptContainerAssignedEvent(event.getAttemptID(), container, applicationACLs));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void add(TaskInfo taskInfo)\n{\r\n    task.add(taskInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ArrayList<TaskInfo> getTasks()\n{\r\n    return task;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\security\\authorize",
  "methodName" : "getServices",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Service[] getServices()\n{\r\n    return mrHSServices;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "hasAccess",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Boolean hasAccess(Job job, HttpServletRequest request)\n{\r\n    String remoteUser = request.getRemoteUser();\r\n    UserGroupInformation callerUGI = null;\r\n    if (remoteUser != null) {\r\n        callerUGI = UserGroupInformation.createRemoteUser(remoteUser);\r\n    }\r\n    if (callerUGI != null && !job.checkAccess(callerUGI, JobACL.VIEW_JOB)) {\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void init()\n{\r\n    response.setContentType(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getJobFromJobIdString",
  "errType" : [ "YarnRuntimeException", "IllegalArgumentException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Job getJobFromJobIdString(String jid, AppContext appCtx) throws NotFoundException\n{\r\n    JobId jobId;\r\n    Job job;\r\n    try {\r\n        jobId = MRApps.toJobID(jid);\r\n    } catch (YarnRuntimeException e) {\r\n        throw new NotFoundException(e.getMessage());\r\n    } catch (IllegalArgumentException e) {\r\n        throw new NotFoundException(e.getMessage());\r\n    }\r\n    if (jobId == null) {\r\n        throw new NotFoundException(\"job, \" + jid + \", is not found\");\r\n    }\r\n    job = appCtx.getJob(jobId);\r\n    if (job == null) {\r\n        throw new NotFoundException(\"job, \" + jid + \", is not found\");\r\n    }\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getTaskFromTaskIdString",
  "errType" : [ "YarnRuntimeException", "NumberFormatException", "IllegalArgumentException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Task getTaskFromTaskIdString(String tid, Job job) throws NotFoundException\n{\r\n    TaskId taskID;\r\n    Task task;\r\n    try {\r\n        taskID = MRApps.toTaskID(tid);\r\n    } catch (YarnRuntimeException e) {\r\n        throw new NotFoundException(e.getMessage());\r\n    } catch (NumberFormatException ne) {\r\n        throw new NotFoundException(ne.getMessage());\r\n    } catch (IllegalArgumentException e) {\r\n        throw new NotFoundException(e.getMessage());\r\n    }\r\n    if (taskID == null) {\r\n        throw new NotFoundException(\"taskid \" + tid + \" not found or invalid\");\r\n    }\r\n    task = job.getTask(taskID);\r\n    if (task == null) {\r\n        throw new NotFoundException(\"task not found with id \" + tid);\r\n    }\r\n    return task;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getTaskAttemptFromTaskAttemptString",
  "errType" : [ "YarnRuntimeException", "NumberFormatException", "IllegalArgumentException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "TaskAttempt getTaskAttemptFromTaskAttemptString(String attId, Task task) throws NotFoundException\n{\r\n    TaskAttemptId attemptId;\r\n    TaskAttempt ta;\r\n    try {\r\n        attemptId = MRApps.toTaskAttemptID(attId);\r\n    } catch (YarnRuntimeException e) {\r\n        throw new NotFoundException(e.getMessage());\r\n    } catch (NumberFormatException ne) {\r\n        throw new NotFoundException(ne.getMessage());\r\n    } catch (IllegalArgumentException e) {\r\n        throw new NotFoundException(e.getMessage());\r\n    }\r\n    if (attemptId == null) {\r\n        throw new NotFoundException(\"task attempt id \" + attId + \" not found or invalid\");\r\n    }\r\n    ta = task.getAttempt(attemptId);\r\n    if (ta == null) {\r\n        throw new NotFoundException(\"Error getting info on task attempt id \" + attId);\r\n    }\r\n    return ta;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "checkAccess",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkAccess(Job job, HttpServletRequest request)\n{\r\n    if (!hasAccess(job, request)) {\r\n        throw new WebApplicationException(Status.UNAUTHORIZED);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AppInfo get()\n{\r\n    return getAppInfo();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getAppInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AppInfo getAppInfo()\n{\r\n    init();\r\n    return new AppInfo(this.app, this.app.context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getBlacklistedNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlacklistedNodesInfo getBlacklistedNodes()\n{\r\n    init();\r\n    return new BlacklistedNodesInfo(this.app.context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getJobs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "JobsInfo getJobs(@Context HttpServletRequest hsr)\n{\r\n    init();\r\n    JobsInfo allJobs = new JobsInfo();\r\n    for (Job job : appCtx.getAllJobs().values()) {\r\n        Job fullJob = appCtx.getJob(job.getID());\r\n        if (fullJob == null) {\r\n            continue;\r\n        }\r\n        allJobs.add(new JobInfo(fullJob, hasAccess(fullJob, hsr)));\r\n    }\r\n    return allJobs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getJob",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobInfo getJob(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid)\n{\r\n    init();\r\n    Job job = getJobFromJobIdString(jid, appCtx);\r\n    return new JobInfo(job, hasAccess(job, hsr));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getJobAttempts",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "AMAttemptsInfo getJobAttempts(@PathParam(\"jobid\") String jid)\n{\r\n    init();\r\n    Job job = getJobFromJobIdString(jid, appCtx);\r\n    AMAttemptsInfo amAttempts = new AMAttemptsInfo();\r\n    for (AMInfo amInfo : job.getAMInfos()) {\r\n        AMAttemptInfo attempt = new AMAttemptInfo(amInfo, MRApps.toString(job.getID()), job.getUserName());\r\n        amAttempts.add(attempt);\r\n    }\r\n    return amAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getJobCounters",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobCounterInfo getJobCounters(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid)\n{\r\n    init();\r\n    Job job = getJobFromJobIdString(jid, appCtx);\r\n    checkAccess(job, hsr);\r\n    return new JobCounterInfo(this.appCtx, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getJobConf",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ConfInfo getJobConf(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid)\n{\r\n    init();\r\n    Job job = getJobFromJobIdString(jid, appCtx);\r\n    checkAccess(job, hsr);\r\n    ConfInfo info;\r\n    try {\r\n        info = new ConfInfo(job);\r\n    } catch (IOException e) {\r\n        throw new NotFoundException(\"unable to load configuration for job: \" + jid);\r\n    }\r\n    return info;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getJobTasks",
  "errType" : [ "YarnRuntimeException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "TasksInfo getJobTasks(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid, @QueryParam(\"type\") String type)\n{\r\n    init();\r\n    Job job = getJobFromJobIdString(jid, appCtx);\r\n    checkAccess(job, hsr);\r\n    TasksInfo allTasks = new TasksInfo();\r\n    for (Task task : job.getTasks().values()) {\r\n        TaskType ttype = null;\r\n        if (type != null && !type.isEmpty()) {\r\n            try {\r\n                ttype = MRApps.taskType(type);\r\n            } catch (YarnRuntimeException e) {\r\n                throw new BadRequestException(\"tasktype must be either m or r\");\r\n            }\r\n        }\r\n        if (ttype != null && task.getType() != ttype) {\r\n            continue;\r\n        }\r\n        allTasks.add(new TaskInfo(task));\r\n    }\r\n    return allTasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getJobTask",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "TaskInfo getJobTask(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid, @PathParam(\"taskid\") String tid)\n{\r\n    init();\r\n    Job job = getJobFromJobIdString(jid, appCtx);\r\n    checkAccess(job, hsr);\r\n    Task task = getTaskFromTaskIdString(tid, job);\r\n    return new TaskInfo(task);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getSingleTaskCounters",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "JobTaskCounterInfo getSingleTaskCounters(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid, @PathParam(\"taskid\") String tid)\n{\r\n    init();\r\n    Job job = getJobFromJobIdString(jid, appCtx);\r\n    checkAccess(job, hsr);\r\n    Task task = getTaskFromTaskIdString(tid, job);\r\n    return new JobTaskCounterInfo(task);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getJobTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "TaskAttemptsInfo getJobTaskAttempts(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid, @PathParam(\"taskid\") String tid)\n{\r\n    init();\r\n    TaskAttemptsInfo attempts = new TaskAttemptsInfo();\r\n    Job job = getJobFromJobIdString(jid, appCtx);\r\n    checkAccess(job, hsr);\r\n    Task task = getTaskFromTaskIdString(tid, job);\r\n    for (TaskAttempt ta : task.getAttempts().values()) {\r\n        if (ta != null) {\r\n            if (task.getType() == TaskType.REDUCE) {\r\n                attempts.add(new ReduceTaskAttemptInfo(ta));\r\n            } else {\r\n                attempts.add(new MapTaskAttemptInfo(ta, true));\r\n            }\r\n        }\r\n    }\r\n    return attempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getJobTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "TaskAttemptInfo getJobTaskAttemptId(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid, @PathParam(\"taskid\") String tid, @PathParam(\"attemptid\") String attId)\n{\r\n    init();\r\n    Job job = getJobFromJobIdString(jid, appCtx);\r\n    checkAccess(job, hsr);\r\n    Task task = getTaskFromTaskIdString(tid, job);\r\n    TaskAttempt ta = getTaskAttemptFromTaskAttemptString(attId, task);\r\n    if (task.getType() == TaskType.REDUCE) {\r\n        return new ReduceTaskAttemptInfo(ta);\r\n    } else {\r\n        return new MapTaskAttemptInfo(ta, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getJobTaskAttemptState",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "JobTaskAttemptState getJobTaskAttemptState(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid, @PathParam(\"taskid\") String tid, @PathParam(\"attemptid\") String attId) throws IOException, InterruptedException\n{\r\n    init();\r\n    Job job = getJobFromJobIdString(jid, appCtx);\r\n    checkAccess(job, hsr);\r\n    Task task = getTaskFromTaskIdString(tid, job);\r\n    TaskAttempt ta = getTaskAttemptFromTaskAttemptString(attId, task);\r\n    return new JobTaskAttemptState(ta.getState().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "updateJobTaskAttemptState",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "Response updateJobTaskAttemptState(JobTaskAttemptState targetState, @Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid, @PathParam(\"taskid\") String tid, @PathParam(\"attemptid\") String attId) throws IOException, InterruptedException\n{\r\n    init();\r\n    Job job = getJobFromJobIdString(jid, appCtx);\r\n    checkAccess(job, hsr);\r\n    String remoteUser = hsr.getRemoteUser();\r\n    UserGroupInformation callerUGI = null;\r\n    if (remoteUser != null) {\r\n        callerUGI = UserGroupInformation.createRemoteUser(remoteUser);\r\n    }\r\n    Task task = getTaskFromTaskIdString(tid, job);\r\n    TaskAttempt ta = getTaskAttemptFromTaskAttemptString(attId, task);\r\n    if (!ta.getState().toString().equals(targetState.getState())) {\r\n        if (targetState.getState().equals(TaskAttemptState.KILLED.toString())) {\r\n            return killJobTaskAttempt(ta, callerUGI, hsr);\r\n        }\r\n        throw new BadRequestException(\"Only '\" + TaskAttemptState.KILLED.toString() + \"' is allowed as a target state.\");\r\n    }\r\n    JobTaskAttemptState ret = new JobTaskAttemptState();\r\n    ret.setState(ta.getState().toString());\r\n    return Response.status(Status.OK).entity(ret).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getJobTaskAttemptIdCounters",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "JobTaskAttemptCounterInfo getJobTaskAttemptIdCounters(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid, @PathParam(\"taskid\") String tid, @PathParam(\"attemptid\") String attId)\n{\r\n    init();\r\n    Job job = getJobFromJobIdString(jid, appCtx);\r\n    checkAccess(job, hsr);\r\n    Task task = getTaskFromTaskIdString(tid, job);\r\n    TaskAttempt ta = getTaskAttemptFromTaskAttemptString(attId, task);\r\n    return new JobTaskAttemptCounterInfo(ta);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "killJobTaskAttempt",
  "errType" : [ "UndeclaredThrowableException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "Response killJobTaskAttempt(TaskAttempt ta, UserGroupInformation callerUGI, HttpServletRequest hsr) throws IOException, InterruptedException\n{\r\n    Preconditions.checkNotNull(ta, \"ta cannot be null\");\r\n    String userName = callerUGI.getUserName();\r\n    final TaskAttemptId attemptId = ta.getID();\r\n    try {\r\n        callerUGI.doAs(new PrivilegedExceptionAction<KillTaskAttemptResponse>() {\r\n\r\n            @Override\r\n            public KillTaskAttemptResponse run() throws IOException, YarnException {\r\n                KillTaskAttemptRequest req = new KillTaskAttemptRequestPBImpl();\r\n                req.setTaskAttemptId(attemptId);\r\n                return service.forceKillTaskAttempt(req);\r\n            }\r\n        });\r\n    } catch (UndeclaredThrowableException ue) {\r\n        if (ue.getCause() instanceof YarnException) {\r\n            YarnException ye = (YarnException) ue.getCause();\r\n            if (ye.getCause() instanceof AccessControlException) {\r\n                String taId = attemptId.toString();\r\n                String msg = \"Unauthorized attempt to kill task attempt \" + taId + \" by remote user \" + userName;\r\n                return Response.status(Status.FORBIDDEN).entity(msg).build();\r\n            } else {\r\n                throw ue;\r\n            }\r\n        } else {\r\n            throw ue;\r\n        }\r\n    }\r\n    JobTaskAttemptState ret = new JobTaskAttemptState();\r\n    ret.setState(TaskAttemptState.KILLED.toString());\r\n    return Response.status(Status.OK).entity(ret).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getAttemptID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptId getAttemptID()\n{\r\n    return attemptID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop",
  "methodName" : "setupJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupJob(JobContext jobContext) throws IOException\n{\r\n    writeFile(jobContext.getJobConf(), JOB_SETUP_FILE_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop",
  "methodName" : "commitJob",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void commitJob(JobContext jobContext) throws IOException\n{\r\n    super.commitJob(jobContext);\r\n    writeFile(jobContext.getJobConf(), JOB_COMMIT_FILE_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop",
  "methodName" : "abortJob",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void abortJob(JobContext jobContext, int status) throws IOException\n{\r\n    super.abortJob(jobContext, status);\r\n    writeFile(jobContext.getJobConf(), JOB_ABORT_FILE_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop",
  "methodName" : "setupTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupTask(TaskAttemptContext taskContext) throws IOException\n{\r\n    writeFile(taskContext.getJobConf(), TASK_SETUP_FILE_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop",
  "methodName" : "needsTaskCommit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean needsTaskCommit(TaskAttemptContext taskContext) throws IOException\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop",
  "methodName" : "commitTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commitTask(TaskAttemptContext taskContext) throws IOException\n{\r\n    writeFile(taskContext.getJobConf(), TASK_COMMIT_FILE_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop",
  "methodName" : "abortTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortTask(TaskAttemptContext taskContext) throws IOException\n{\r\n    writeFile(taskContext.getJobConf(), TASK_ABORT_FILE_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeFile(JobConf conf, String filename) throws IOException\n{\r\n    System.out.println(\"writing file ----\" + filename);\r\n    Path outputPath = FileOutputFormat.getOutputPath(conf);\r\n    FileSystem fs = outputPath.getFileSystem(conf);\r\n    fs.create(new Path(outputPath, filename)).close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n    nodeBlacklistingEnabled = conf.getBoolean(MRJobConfig.MR_AM_JOB_NODE_BLACKLISTING_ENABLE, true);\r\n    LOG.info(\"nodeBlacklistingEnabled:\" + nodeBlacklistingEnabled);\r\n    maxTaskFailuresPerNode = conf.getInt(MRJobConfig.MAX_TASK_FAILURES_PER_TRACKER, 3);\r\n    blacklistDisablePercent = conf.getInt(MRJobConfig.MR_AM_IGNORE_BLACKLISTING_BLACKLISTED_NODE_PERECENT, MRJobConfig.DEFAULT_MR_AM_IGNORE_BLACKLISTING_BLACKLISTED_NODE_PERCENT);\r\n    LOG.info(\"maxTaskFailuresPerNode is \" + maxTaskFailuresPerNode);\r\n    if (blacklistDisablePercent < -1 || blacklistDisablePercent > 100) {\r\n        throw new YarnRuntimeException(\"Invalid blacklistDisablePercent: \" + blacklistDisablePercent + \". Should be an integer between 0 and 100 or -1 to disabled\");\r\n    }\r\n    LOG.info(\"blacklistDisablePercent is \" + blacklistDisablePercent);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "makeRemoteRequest",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "AllocateResponse makeRemoteRequest() throws YarnException, IOException\n{\r\n    applyRequestLimits();\r\n    ResourceBlacklistRequest blacklistRequest = ResourceBlacklistRequest.newInstance(new ArrayList<String>(blacklistAdditions), new ArrayList<String>(blacklistRemovals));\r\n    AllocateRequest allocateRequest = AllocateRequest.newInstance(lastResponseID, super.getApplicationProgress(), new ArrayList<ResourceRequest>(ask), new ArrayList<ContainerId>(release), blacklistRequest);\r\n    AllocateResponse allocateResponse = scheduler.allocate(allocateRequest);\r\n    lastResponseID = allocateResponse.getResponseId();\r\n    availableResources = allocateResponse.getAvailableResources();\r\n    lastClusterNmCount = clusterNmCount;\r\n    clusterNmCount = allocateResponse.getNumClusterNodes();\r\n    int numCompletedContainers = allocateResponse.getCompletedContainersStatuses().size();\r\n    if (ask.size() > 0 || release.size() > 0) {\r\n        LOG.info(\"getResources() for \" + applicationId + \":\" + \" ask=\" + ask.size() + \" release= \" + release.size() + \" newContainers=\" + allocateResponse.getAllocatedContainers().size() + \" finishedContainers=\" + numCompletedContainers + \" resourcelimit=\" + availableResources + \" knownNMs=\" + clusterNmCount);\r\n    }\r\n    ask.clear();\r\n    release.clear();\r\n    if (numCompletedContainers > 0) {\r\n        requestLimitsToUpdate.addAll(requestLimits.keySet());\r\n    }\r\n    if (blacklistAdditions.size() > 0 || blacklistRemovals.size() > 0) {\r\n        LOG.info(\"Update the blacklist for \" + applicationId + \": blacklistAdditions=\" + blacklistAdditions.size() + \" blacklistRemovals=\" + blacklistRemovals.size());\r\n    }\r\n    blacklistAdditions.clear();\r\n    blacklistRemovals.clear();\r\n    return allocateResponse;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "applyRequestLimits",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void applyRequestLimits()\n{\r\n    Iterator<ResourceRequest> iter = requestLimits.values().iterator();\r\n    while (iter.hasNext()) {\r\n        ResourceRequest reqLimit = iter.next();\r\n        int limit = reqLimit.getNumContainers();\r\n        Map<String, Map<Resource, ResourceRequest>> remoteRequests = remoteRequestsTable.get(reqLimit.getPriority());\r\n        Map<Resource, ResourceRequest> reqMap = (remoteRequests != null) ? remoteRequests.get(ResourceRequest.ANY) : null;\r\n        ResourceRequest req = (reqMap != null) ? reqMap.get(reqLimit.getCapability()) : null;\r\n        if (req == null) {\r\n            continue;\r\n        }\r\n        if (ask.remove(req) || requestLimitsToUpdate.contains(req)) {\r\n            ResourceRequest newReq = req.getNumContainers() > limit ? reqLimit : req;\r\n            ask.add(newReq);\r\n            LOG.info(\"Applying ask limit of \" + newReq.getNumContainers() + \" for priority:\" + reqLimit.getPriority() + \" and capability:\" + reqLimit.getCapability());\r\n        }\r\n        if (limit == Integer.MAX_VALUE) {\r\n            iter.remove();\r\n        }\r\n    }\r\n    requestLimitsToUpdate.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "addOutstandingRequestOnResync",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void addOutstandingRequestOnResync()\n{\r\n    for (Map<String, Map<Resource, ResourceRequest>> rr : remoteRequestsTable.values()) {\r\n        for (Map<Resource, ResourceRequest> capabalities : rr.values()) {\r\n            for (ResourceRequest request : capabalities.values()) {\r\n                addResourceRequestToAsk(request);\r\n            }\r\n        }\r\n    }\r\n    if (!ignoreBlacklisting.get()) {\r\n        blacklistAdditions.addAll(blacklistedNodes);\r\n    }\r\n    if (!pendingRelease.isEmpty()) {\r\n        release.addAll(pendingRelease);\r\n    }\r\n    requestLimitsToUpdate.addAll(requestLimits.keySet());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "computeIgnoreBlacklisting",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void computeIgnoreBlacklisting()\n{\r\n    if (!nodeBlacklistingEnabled) {\r\n        return;\r\n    }\r\n    if (blacklistDisablePercent != -1 && (blacklistedNodeCount != blacklistedNodes.size() || clusterNmCount != lastClusterNmCount)) {\r\n        blacklistedNodeCount = blacklistedNodes.size();\r\n        if (clusterNmCount == 0) {\r\n            LOG.info(\"KnownNode Count at 0. Not computing ignoreBlacklisting\");\r\n            return;\r\n        }\r\n        int val = (int) ((float) blacklistedNodes.size() / clusterNmCount * 100);\r\n        if (val >= blacklistDisablePercent) {\r\n            if (ignoreBlacklisting.compareAndSet(false, true)) {\r\n                LOG.info(\"Ignore blacklisting set to true. Known: \" + clusterNmCount + \", Blacklisted: \" + blacklistedNodeCount + \", \" + val + \"%\");\r\n                blacklistAdditions.clear();\r\n                blacklistRemovals.addAll(blacklistedNodes);\r\n            }\r\n        } else {\r\n            if (ignoreBlacklisting.compareAndSet(true, false)) {\r\n                LOG.info(\"Ignore blacklisting set to false. Known: \" + clusterNmCount + \", Blacklisted: \" + blacklistedNodeCount + \", \" + val + \"%\");\r\n                blacklistAdditions.addAll(blacklistedNodes);\r\n                blacklistRemovals.clear();\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "containerFailedOnHost",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void containerFailedOnHost(String hostName)\n{\r\n    if (!nodeBlacklistingEnabled) {\r\n        return;\r\n    }\r\n    if (blacklistedNodes.contains(hostName)) {\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"Host \" + hostName + \" is already blacklisted.\");\r\n        }\r\n        return;\r\n    }\r\n    Integer failures = nodeFailures.remove(hostName);\r\n    failures = failures == null ? Integer.valueOf(0) : failures;\r\n    failures++;\r\n    LOG.info(failures + \" failures on node \" + hostName);\r\n    if (failures >= maxTaskFailuresPerNode) {\r\n        blacklistedNodes.add(hostName);\r\n        if (!ignoreBlacklisting.get()) {\r\n            blacklistAdditions.add(hostName);\r\n        }\r\n        LOG.info(\"Blacklisted host \" + hostName);\r\n        for (Map<String, Map<Resource, ResourceRequest>> remoteRequests : remoteRequestsTable.values()) {\r\n            boolean foundAll = true;\r\n            Map<Resource, ResourceRequest> reqMap = remoteRequests.get(hostName);\r\n            if (reqMap != null) {\r\n                for (ResourceRequest req : reqMap.values()) {\r\n                    if (!ask.remove(req)) {\r\n                        foundAll = false;\r\n                        ResourceRequest zeroedRequest = ResourceRequest.newInstance(req.getPriority(), req.getResourceName(), req.getCapability(), req.getNumContainers(), req.getRelaxLocality());\r\n                        zeroedRequest.setNumContainers(0);\r\n                        addResourceRequestToAsk(zeroedRequest);\r\n                    }\r\n                }\r\n                if (foundAll) {\r\n                    remoteRequests.remove(hostName);\r\n                }\r\n            }\r\n        }\r\n    } else {\r\n        nodeFailures.put(hostName, failures);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getAvailableResources",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Resource getAvailableResources()\n{\r\n    return availableResources == null ? Resources.none() : availableResources;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "addContainerReq",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addContainerReq(ContainerRequest req)\n{\r\n    for (String host : req.hosts) {\r\n        if (!isNodeBlacklisted(host)) {\r\n            addResourceRequest(req.priority, host, req.capability, null);\r\n        }\r\n    }\r\n    for (String rack : req.racks) {\r\n        addResourceRequest(req.priority, rack, req.capability, null);\r\n    }\r\n    addResourceRequest(req.priority, ResourceRequest.ANY, req.capability, req.nodeLabelExpression);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "decContainerReq",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void decContainerReq(ContainerRequest req)\n{\r\n    for (String hostName : req.hosts) {\r\n        decResourceRequest(req.priority, hostName, req.capability);\r\n    }\r\n    for (String rack : req.racks) {\r\n        decResourceRequest(req.priority, rack, req.capability);\r\n    }\r\n    decResourceRequest(req.priority, ResourceRequest.ANY, req.capability);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "addOpportunisticResourceRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addOpportunisticResourceRequest(Priority priority, Resource capability)\n{\r\n    addResourceRequest(priority, ResourceRequest.ANY, capability, null, ExecutionType.OPPORTUNISTIC);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "addResourceRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addResourceRequest(Priority priority, String resourceName, Resource capability, String nodeLabelExpression)\n{\r\n    addResourceRequest(priority, resourceName, capability, nodeLabelExpression, ExecutionType.GUARANTEED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "addResourceRequest",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void addResourceRequest(Priority priority, String resourceName, Resource capability, String nodeLabelExpression, ExecutionType executionType)\n{\r\n    Map<String, Map<Resource, ResourceRequest>> remoteRequests = this.remoteRequestsTable.get(priority);\r\n    if (remoteRequests == null) {\r\n        remoteRequests = new HashMap<String, Map<Resource, ResourceRequest>>();\r\n        this.remoteRequestsTable.put(priority, remoteRequests);\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"Added priority=\" + priority);\r\n        }\r\n    }\r\n    Map<Resource, ResourceRequest> reqMap = remoteRequests.get(resourceName);\r\n    if (reqMap == null) {\r\n        reqMap = new HashMap<Resource, ResourceRequest>();\r\n        remoteRequests.put(resourceName, reqMap);\r\n    }\r\n    ResourceRequest remoteRequest = reqMap.get(capability);\r\n    if (remoteRequest == null) {\r\n        remoteRequest = recordFactory.newRecordInstance(ResourceRequest.class);\r\n        remoteRequest.setPriority(priority);\r\n        remoteRequest.setResourceName(resourceName);\r\n        remoteRequest.setCapability(capability);\r\n        remoteRequest.setNumContainers(0);\r\n        remoteRequest.setNodeLabelExpression(nodeLabelExpression);\r\n        remoteRequest.setExecutionTypeRequest(ExecutionTypeRequest.newInstance(executionType, true));\r\n        reqMap.put(capability, remoteRequest);\r\n    }\r\n    remoteRequest.setNumContainers(remoteRequest.getNumContainers() + 1);\r\n    addResourceRequestToAsk(remoteRequest);\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"addResourceRequest:\" + \" applicationId=\" + applicationId.getId() + \" priority=\" + priority.getPriority() + \" resourceName=\" + resourceName + \" numContainers=\" + remoteRequest.getNumContainers() + \" #asks=\" + ask.size());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "decResourceRequest",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void decResourceRequest(Priority priority, String resourceName, Resource capability)\n{\r\n    Map<String, Map<Resource, ResourceRequest>> remoteRequests = this.remoteRequestsTable.get(priority);\r\n    Map<Resource, ResourceRequest> reqMap = remoteRequests.get(resourceName);\r\n    if (reqMap == null) {\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"Not decrementing resource as \" + resourceName + \" is not present in request table\");\r\n        }\r\n        return;\r\n    }\r\n    ResourceRequest remoteRequest = reqMap.get(capability);\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"BEFORE decResourceRequest:\" + \" applicationId=\" + applicationId.getId() + \" priority=\" + priority.getPriority() + \" resourceName=\" + resourceName + \" numContainers=\" + remoteRequest.getNumContainers() + \" #asks=\" + ask.size());\r\n    }\r\n    if (remoteRequest.getNumContainers() > 0) {\r\n        remoteRequest.setNumContainers(remoteRequest.getNumContainers() - 1);\r\n    }\r\n    if (remoteRequest.getNumContainers() == 0) {\r\n        reqMap.remove(capability);\r\n        if (reqMap.size() == 0) {\r\n            remoteRequests.remove(resourceName);\r\n        }\r\n        if (remoteRequests.size() == 0) {\r\n            remoteRequestsTable.remove(priority);\r\n        }\r\n    }\r\n    addResourceRequestToAsk(remoteRequest);\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"AFTER decResourceRequest:\" + \" applicationId=\" + applicationId.getId() + \" priority=\" + priority.getPriority() + \" resourceName=\" + resourceName + \" numContainers=\" + remoteRequest.getNumContainers() + \" #asks=\" + ask.size());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "addResourceRequestToAsk",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addResourceRequestToAsk(ResourceRequest remoteRequest)\n{\r\n    ask.remove(remoteRequest);\r\n    ask.add(remoteRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "release",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void release(ContainerId containerId)\n{\r\n    release.add(containerId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "isNodeBlacklisted",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isNodeBlacklisted(String hostname)\n{\r\n    if (!nodeBlacklistingEnabled || ignoreBlacklisting.get()) {\r\n        return false;\r\n    }\r\n    return blacklistedNodes.contains(hostname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getFilteredContainerRequest",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ContainerRequest getFilteredContainerRequest(ContainerRequest orig)\n{\r\n    ArrayList<String> newHosts = new ArrayList<String>();\r\n    for (String host : orig.hosts) {\r\n        if (!isNodeBlacklisted(host)) {\r\n            newHosts.add(host);\r\n        }\r\n    }\r\n    String[] hosts = newHosts.toArray(new String[newHosts.size()]);\r\n    ContainerRequest newReq = new ContainerRequest(orig.attemptID, orig.capability, hosts, orig.racks, orig.priority, orig.nodeLabelExpression);\r\n    return newReq;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "setRequestLimit",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setRequestLimit(Priority priority, Resource capability, int limit)\n{\r\n    if (limit < 0) {\r\n        limit = Integer.MAX_VALUE;\r\n    }\r\n    ResourceRequest newReqLimit = ResourceRequest.newInstance(priority, ResourceRequest.ANY, capability, limit);\r\n    ResourceRequest oldReqLimit = requestLimits.put(newReqLimit, newReqLimit);\r\n    if (oldReqLimit == null || oldReqLimit.getNumContainers() < limit) {\r\n        requestLimitsToUpdate.add(newReqLimit);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getBlacklistedNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<String> getBlacklistedNodes()\n{\r\n    return blacklistedNodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getAsk",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<ResourceRequest> getAsk()\n{\r\n    return ask;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getKerberosInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "KerberosInfo getKerberosInfo(Class<?> protocol, Configuration conf)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getTokenInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TokenInfo getTokenInfo(Class<?> protocol, Configuration conf)\n{\r\n    if (!protocol.equals(MRClientProtocolPB.class)) {\r\n        return null;\r\n    }\r\n    return new TokenInfo() {\r\n\r\n        @Override\r\n        public Class<? extends Annotation> annotationType() {\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Class<? extends TokenSelector<? extends TokenIdentifier>> value() {\r\n            return ClientToAMTokenSelector.class;\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "serviceInit",
  "errType" : [ "IOException", "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 44,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    String jobId = TypeConverter.fromYarn(context.getApplicationID()).toString();\r\n    String stagingDirStr = null;\r\n    String doneDirStr = null;\r\n    String userDoneDirStr = null;\r\n    try {\r\n        stagingDirStr = JobHistoryUtils.getConfiguredHistoryStagingDirPrefix(conf, jobId);\r\n        doneDirStr = JobHistoryUtils.getConfiguredHistoryIntermediateDoneDirPrefix(conf);\r\n        userDoneDirStr = JobHistoryUtils.getHistoryIntermediateDoneDirForUser(conf);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Failed while getting the configured log directories\", e);\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n    try {\r\n        stagingDirPath = FileContext.getFileContext(conf).makeQualified(new Path(stagingDirStr));\r\n        stagingDirFS = FileSystem.get(stagingDirPath.toUri(), conf);\r\n        mkdir(stagingDirFS, stagingDirPath, new FsPermission(JobHistoryUtils.HISTORY_STAGING_DIR_PERMISSIONS));\r\n    } catch (IOException e) {\r\n        LOG.error(\"Failed while checking for/creating  history staging path: [\" + stagingDirPath + \"]\", e);\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n    Path doneDirPath = null;\r\n    try {\r\n        doneDirPath = FileContext.getFileContext(conf).makeQualified(new Path(doneDirStr));\r\n        doneDirFS = FileSystem.get(doneDirPath.toUri(), conf);\r\n        if (!doneDirFS.exists(doneDirPath)) {\r\n            if (JobHistoryUtils.shouldCreateNonUserDirectory(conf)) {\r\n                LOG.info(\"Creating intermediate history logDir: [\" + doneDirPath + \"] + based on conf. Should ideally be created by the JobHistoryServer: \" + MRJobConfig.MR_AM_CREATE_JH_INTERMEDIATE_BASE_DIR);\r\n                mkdir(doneDirFS, doneDirPath, new FsPermission(JobHistoryUtils.HISTORY_INTERMEDIATE_DONE_DIR_PERMISSIONS.toShort()));\r\n            } else {\r\n                String message = \"Not creating intermediate history logDir: [\" + doneDirPath + \"] based on conf: \" + MRJobConfig.MR_AM_CREATE_JH_INTERMEDIATE_BASE_DIR + \". Either set to true or pre-create this directory with\" + \" appropriate permissions\";\r\n                LOG.error(message);\r\n                throw new YarnRuntimeException(message);\r\n            }\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Failed checking for the existence of history intermediate \" + \"done directory: [\" + doneDirPath + \"]\");\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n    try {\r\n        doneDirPrefixPath = FileContext.getFileContext(conf).makeQualified(new Path(userDoneDirStr));\r\n        mkdir(doneDirFS, doneDirPrefixPath, JobHistoryUtils.getConfiguredHistoryIntermediateUserDoneDirPermissions(conf));\r\n    } catch (IOException e) {\r\n        LOG.error(\"Error creating user intermediate history done directory: [ \" + doneDirPrefixPath + \"]\", e);\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n    maxUnflushedCompletionEvents = conf.getInt(MRJobConfig.MR_AM_HISTORY_MAX_UNFLUSHED_COMPLETE_EVENTS, MRJobConfig.DEFAULT_MR_AM_HISTORY_MAX_UNFLUSHED_COMPLETE_EVENTS);\r\n    postJobCompletionMultiplier = conf.getInt(MRJobConfig.MR_AM_HISTORY_JOB_COMPLETE_UNFLUSHED_MULTIPLIER, MRJobConfig.DEFAULT_MR_AM_HISTORY_JOB_COMPLETE_UNFLUSHED_MULTIPLIER);\r\n    flushTimeout = conf.getLong(MRJobConfig.MR_AM_HISTORY_COMPLETE_EVENT_FLUSH_TIMEOUT_MS, MRJobConfig.DEFAULT_MR_AM_HISTORY_COMPLETE_EVENT_FLUSH_TIMEOUT_MS);\r\n    minQueueSizeForBatchingFlushes = conf.getInt(MRJobConfig.MR_AM_HISTORY_USE_BATCHED_FLUSH_QUEUE_SIZE_THRESHOLD, MRJobConfig.DEFAULT_MR_AM_HISTORY_USE_BATCHED_FLUSH_QUEUE_SIZE_THRESHOLD);\r\n    if (conf.getBoolean(MRJobConfig.MAPREDUCE_JOB_EMIT_TIMELINE_DATA, MRJobConfig.DEFAULT_MAPREDUCE_JOB_EMIT_TIMELINE_DATA)) {\r\n        LOG.info(\"Emitting job history data to the timeline service is enabled\");\r\n        if (YarnConfiguration.timelineServiceEnabled(conf)) {\r\n            boolean timelineServiceV2Enabled = YarnConfiguration.timelineServiceV2Enabled(conf);\r\n            if (timelineServiceV2Enabled) {\r\n                timelineV2Client = ((MRAppMaster.RunningAppContext) context).getTimelineV2Client();\r\n                timelineV2Client.init(conf);\r\n            } else {\r\n                timelineClient = ((MRAppMaster.RunningAppContext) context).getTimelineClient();\r\n                timelineClient.init(conf);\r\n            }\r\n            handleTimelineEvent = true;\r\n            LOG.info(\"Timeline service is enabled; version: \" + YarnConfiguration.getTimelineServiceVersion(conf));\r\n        } else {\r\n            LOG.info(\"Timeline service is not enabled\");\r\n        }\r\n    } else {\r\n        LOG.info(\"Emitting job history data to the timeline server is not \" + \"enabled\");\r\n    }\r\n    String jhistFormat = conf.get(JHAdminConfig.MR_HS_JHIST_FORMAT, JHAdminConfig.DEFAULT_MR_HS_JHIST_FORMAT);\r\n    if (jhistFormat.equals(\"json\")) {\r\n        jhistMode = EventWriter.WriteMode.JSON;\r\n    } else if (jhistFormat.equals(\"binary\")) {\r\n        jhistMode = EventWriter.WriteMode.BINARY;\r\n    } else {\r\n        LOG.warn(\"Unrecognized value '\" + jhistFormat + \"' for property \" + JHAdminConfig.MR_HS_JHIST_FORMAT + \".  Valid values are \" + \"'json' or 'binary'.  Falling back to default value '\" + JHAdminConfig.DEFAULT_MR_HS_JHIST_FORMAT + \"'.\");\r\n    }\r\n    if (handleTimelineEvent) {\r\n        atsEventDispatcher = createDispatcher();\r\n        EventHandler<JobHistoryEvent> timelineEventHandler = new ForwardingEventHandler();\r\n        atsEventDispatcher.register(EventType.class, timelineEventHandler);\r\n        atsEventDispatcher.setDrainEventsOnStop();\r\n        atsEventDispatcher.init(conf);\r\n    }\r\n    super.serviceInit(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "createDispatcher",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AsyncDispatcher createDispatcher()\n{\r\n    return new AsyncDispatcher(\"Job ATS Event Dispatcher\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "mkdir",
  "errType" : [ "FileAlreadyExistsException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void mkdir(FileSystem fs, Path path, FsPermission fsp) throws IOException\n{\r\n    if (!fs.exists(path)) {\r\n        try {\r\n            fs.mkdirs(path, fsp);\r\n            FileStatus fsStatus = fs.getFileStatus(path);\r\n            LOG.info(\"Perms after creating \" + fsStatus.getPermission().toShort() + \", Expected: \" + fsp.toShort());\r\n            if (fsStatus.getPermission().toShort() != fsp.toShort()) {\r\n                LOG.info(\"Explicitly setting permissions to : \" + fsp.toShort() + \", \" + fsp);\r\n                fs.setPermission(path, fsp);\r\n            }\r\n        } catch (FileAlreadyExistsException e) {\r\n            LOG.info(\"Directory: [\" + path + \"] already exists.\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "serviceStart",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    if (timelineClient != null) {\r\n        timelineClient.start();\r\n    } else if (timelineV2Client != null) {\r\n        timelineV2Client.start();\r\n    }\r\n    eventHandlingThread = new Thread(new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            JobHistoryEvent event = null;\r\n            while (!stopped && !Thread.currentThread().isInterrupted()) {\r\n                if (eventCounter != 0 && eventCounter % 1000 == 0) {\r\n                    eventCounter = 0;\r\n                    LOG.info(\"Size of the JobHistory event queue is \" + eventQueue.size());\r\n                } else {\r\n                    eventCounter++;\r\n                }\r\n                try {\r\n                    event = eventQueue.take();\r\n                } catch (InterruptedException e) {\r\n                    LOG.info(\"EventQueue take interrupted. Returning\");\r\n                    return;\r\n                }\r\n                synchronized (lock) {\r\n                    boolean isInterrupted = Thread.interrupted();\r\n                    handleEvent(event);\r\n                    if (isInterrupted) {\r\n                        LOG.debug(\"Event handling interrupted\");\r\n                        Thread.currentThread().interrupt();\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }, \"eventHandlingThread\");\r\n    eventHandlingThread.start();\r\n    if (handleTimelineEvent) {\r\n        atsEventDispatcher.start();\r\n    }\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "serviceStop",
  "errType" : [ "InterruptedException", "IOException", "IOException" ],
  "containingMethodsNum" : 46,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    LOG.info(\"Stopping JobHistoryEventHandler. \" + \"Size of the outstanding queue size is \" + eventQueue.size());\r\n    stopped = true;\r\n    synchronized (lock) {\r\n        if (eventHandlingThread != null) {\r\n            LOG.debug(\"Interrupting Event Handling thread\");\r\n            eventHandlingThread.interrupt();\r\n        } else {\r\n            LOG.debug(\"Null event handling thread\");\r\n        }\r\n    }\r\n    try {\r\n        if (eventHandlingThread != null) {\r\n            LOG.debug(\"Waiting for Event Handling thread to complete\");\r\n            eventHandlingThread.join();\r\n        }\r\n    } catch (InterruptedException ie) {\r\n        LOG.info(\"Interrupted Exception while stopping\", ie);\r\n    }\r\n    for (MetaInfo mi : fileMap.values()) {\r\n        try {\r\n            if (LOG.isDebugEnabled()) {\r\n                LOG.debug(\"Shutting down timer for \" + mi);\r\n            }\r\n            mi.shutDownTimer();\r\n        } catch (IOException e) {\r\n            LOG.info(\"Exception while canceling delayed flush timer. \" + \"Likely caused by a failed flush \" + e.getMessage());\r\n        }\r\n    }\r\n    Iterator<JobHistoryEvent> it = eventQueue.iterator();\r\n    while (it.hasNext()) {\r\n        JobHistoryEvent ev = it.next();\r\n        LOG.info(\"In stop, writing event \" + ev.getType());\r\n        handleEvent(ev);\r\n    }\r\n    if (forceJobCompletion) {\r\n        for (Map.Entry<JobId, MetaInfo> jobIt : fileMap.entrySet()) {\r\n            JobId toClose = jobIt.getKey();\r\n            MetaInfo mi = jobIt.getValue();\r\n            if (mi != null && mi.isWriterActive()) {\r\n                LOG.warn(\"Found jobId \" + toClose + \" to have not been closed. Will close\");\r\n                final Job job = context.getJob(toClose);\r\n                int successfulMaps = job.getCompletedMaps() - job.getFailedMaps() - job.getKilledMaps();\r\n                int successfulReduces = job.getCompletedReduces() - job.getFailedReduces() - job.getKilledReduces();\r\n                JobUnsuccessfulCompletionEvent jucEvent = new JobUnsuccessfulCompletionEvent(TypeConverter.fromYarn(toClose), System.currentTimeMillis(), successfulMaps, successfulReduces, job.getFailedMaps(), job.getFailedReduces(), job.getKilledMaps(), job.getKilledReduces(), createJobStateForJobUnsuccessfulCompletionEvent(mi.getForcedJobStateOnShutDown()), job.getDiagnostics());\r\n                JobHistoryEvent jfEvent = new JobHistoryEvent(toClose, jucEvent);\r\n                handleEvent(jfEvent);\r\n            }\r\n        }\r\n    }\r\n    for (MetaInfo mi : fileMap.values()) {\r\n        try {\r\n            mi.closeWriter();\r\n        } catch (IOException e) {\r\n            LOG.info(\"Exception while closing file \" + e.getMessage());\r\n        }\r\n    }\r\n    if (handleTimelineEvent && atsEventDispatcher != null) {\r\n        atsEventDispatcher.stop();\r\n    }\r\n    if (timelineClient != null) {\r\n        timelineClient.stop();\r\n    } else if (timelineV2Client != null) {\r\n        timelineV2Client.stop();\r\n    }\r\n    LOG.info(\"Stopped JobHistoryEventHandler. super.stop()\");\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "createEventWriter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EventWriter createEventWriter(Path historyFilePath) throws IOException\n{\r\n    FSDataOutputStream out = stagingDirFS.create(historyFilePath, true);\r\n    return new EventWriter(out, this.jhistMode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setupEventWriter",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void setupEventWriter(JobId jobId, AMStartedEvent amStartedEvent) throws IOException\n{\r\n    if (stagingDirPath == null) {\r\n        LOG.error(\"Log Directory is null, returning\");\r\n        throw new IOException(\"Missing Log Directory for History\");\r\n    }\r\n    MetaInfo oldFi = fileMap.get(jobId);\r\n    Configuration conf = getConfig();\r\n    Path historyFile = JobHistoryUtils.getStagingJobHistoryFile(stagingDirPath, jobId, startCount);\r\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    if (user == null) {\r\n        throw new IOException(\"User is null while setting up jobhistory eventwriter\");\r\n    }\r\n    String jobName = context.getJob(jobId).getName();\r\n    EventWriter writer = (oldFi == null) ? null : oldFi.writer;\r\n    Path logDirConfPath = JobHistoryUtils.getStagingConfFile(stagingDirPath, jobId, startCount);\r\n    if (writer == null) {\r\n        try {\r\n            writer = createEventWriter(historyFile);\r\n            LOG.info(\"Event Writer setup for JobId: \" + jobId + \", File: \" + historyFile);\r\n        } catch (IOException ioe) {\r\n            LOG.info(\"Could not create log file: [\" + historyFile + \"] + for job \" + \"[\" + jobName + \"]\");\r\n            throw ioe;\r\n        }\r\n        if (conf != null) {\r\n            if (logDirConfPath != null) {\r\n                Configuration redactedConf = new Configuration(conf);\r\n                MRJobConfUtil.redact(redactedConf);\r\n                try (FSDataOutputStream jobFileOut = stagingDirFS.create(logDirConfPath, true)) {\r\n                    redactedConf.writeXml(jobFileOut);\r\n                } catch (IOException e) {\r\n                    LOG.info(\"Failed to write the job configuration file\", e);\r\n                    throw e;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    String queueName = JobConf.DEFAULT_QUEUE_NAME;\r\n    if (conf != null) {\r\n        queueName = conf.get(MRJobConfig.QUEUE_NAME, JobConf.DEFAULT_QUEUE_NAME);\r\n    }\r\n    MetaInfo fi = new MetaInfo(historyFile, logDirConfPath, writer, user, jobName, jobId, amStartedEvent.getForcedJobStateOnShutDown(), queueName);\r\n    fi.getJobSummary().setJobId(jobId);\r\n    fi.getJobSummary().setJobLaunchTime(amStartedEvent.getStartTime());\r\n    fi.getJobSummary().setJobSubmitTime(amStartedEvent.getSubmitTime());\r\n    fi.getJobIndexInfo().setJobStartTime(amStartedEvent.getStartTime());\r\n    fi.getJobIndexInfo().setSubmitTime(amStartedEvent.getSubmitTime());\r\n    fileMap.put(jobId, fi);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "closeWriter",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void closeWriter(JobId id) throws IOException\n{\r\n    try {\r\n        final MetaInfo mi = fileMap.get(id);\r\n        if (mi != null) {\r\n            mi.closeWriter();\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Error closing writer for JobID: \" + id);\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handle",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void handle(JobHistoryEvent event)\n{\r\n    try {\r\n        if (isJobCompletionEvent(event.getHistoryEvent())) {\r\n            maxUnflushedCompletionEvents = maxUnflushedCompletionEvents * postJobCompletionMultiplier;\r\n        }\r\n        eventQueue.put(event);\r\n        if (handleTimelineEvent) {\r\n            atsEventDispatcher.getEventHandler().handle(event);\r\n        }\r\n    } catch (InterruptedException e) {\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "isJobCompletionEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isJobCompletionEvent(HistoryEvent historyEvent)\n{\r\n    if (EnumSet.of(EventType.JOB_FINISHED, EventType.JOB_FAILED, EventType.JOB_KILLED).contains(historyEvent.getEventType())) {\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleEvent",
  "errType" : [ "IOException", "IOException", "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 47,
  "sourceCodeText" : "void handleEvent(JobHistoryEvent event)\n{\r\n    synchronized (lock) {\r\n        if (event.getHistoryEvent().getEventType() == EventType.AM_STARTED) {\r\n            try {\r\n                AMStartedEvent amStartedEvent = (AMStartedEvent) event.getHistoryEvent();\r\n                setupEventWriter(event.getJobID(), amStartedEvent);\r\n            } catch (IOException ioe) {\r\n                LOG.error(\"Error JobHistoryEventHandler in handleEvent: \" + event, ioe);\r\n                throw new YarnRuntimeException(ioe);\r\n            }\r\n        }\r\n        MetaInfo mi = fileMap.get(event.getJobID());\r\n        try {\r\n            HistoryEvent historyEvent = event.getHistoryEvent();\r\n            if (!(historyEvent instanceof NormalizedResourceEvent)) {\r\n                mi.writeEvent(historyEvent);\r\n            }\r\n            processEventForJobSummary(event.getHistoryEvent(), mi.getJobSummary(), event.getJobID());\r\n            if (LOG.isDebugEnabled()) {\r\n                LOG.debug(\"In HistoryEventHandler \" + event.getHistoryEvent().getEventType());\r\n            }\r\n        } catch (IOException e) {\r\n            LOG.error(\"Error writing History Event: \" + event.getHistoryEvent(), e);\r\n            throw new YarnRuntimeException(e);\r\n        }\r\n        if (event.getHistoryEvent().getEventType() == EventType.JOB_SUBMITTED) {\r\n            JobSubmittedEvent jobSubmittedEvent = (JobSubmittedEvent) event.getHistoryEvent();\r\n            mi.getJobIndexInfo().setSubmitTime(jobSubmittedEvent.getSubmitTime());\r\n            mi.getJobIndexInfo().setQueueName(jobSubmittedEvent.getJobQueueName());\r\n        }\r\n        if (event.getHistoryEvent().getEventType() == EventType.JOB_INITED) {\r\n            JobInitedEvent jie = (JobInitedEvent) event.getHistoryEvent();\r\n            mi.getJobIndexInfo().setJobStartTime(jie.getLaunchTime());\r\n        }\r\n        if (event.getHistoryEvent().getEventType() == EventType.JOB_QUEUE_CHANGED) {\r\n            JobQueueChangeEvent jQueueEvent = (JobQueueChangeEvent) event.getHistoryEvent();\r\n            mi.getJobIndexInfo().setQueueName(jQueueEvent.getJobQueueName());\r\n        }\r\n        if (event.getHistoryEvent().getEventType() == EventType.JOB_FINISHED) {\r\n            try {\r\n                JobFinishedEvent jFinishedEvent = (JobFinishedEvent) event.getHistoryEvent();\r\n                mi.getJobIndexInfo().setFinishTime(jFinishedEvent.getFinishTime());\r\n                mi.getJobIndexInfo().setNumMaps(jFinishedEvent.getSucceededMaps());\r\n                mi.getJobIndexInfo().setNumReduces(jFinishedEvent.getSucceededReduces());\r\n                mi.getJobIndexInfo().setJobStatus(JobState.SUCCEEDED.toString());\r\n                closeEventWriter(event.getJobID());\r\n                processDoneFiles(event.getJobID());\r\n            } catch (IOException e) {\r\n                throw new YarnRuntimeException(e);\r\n            }\r\n        }\r\n        if (event.getHistoryEvent().getEventType() == EventType.JOB_ERROR) {\r\n            try {\r\n                JobUnsuccessfulCompletionEvent jucEvent = (JobUnsuccessfulCompletionEvent) event.getHistoryEvent();\r\n                mi.getJobIndexInfo().setFinishTime(jucEvent.getFinishTime());\r\n                mi.getJobIndexInfo().setNumMaps(jucEvent.getSucceededMaps());\r\n                mi.getJobIndexInfo().setNumReduces(jucEvent.getSucceededReduces());\r\n                mi.getJobIndexInfo().setJobStatus(jucEvent.getStatus());\r\n                closeEventWriter(event.getJobID());\r\n                if (context.isLastAMRetry())\r\n                    processDoneFiles(event.getJobID());\r\n            } catch (IOException e) {\r\n                throw new YarnRuntimeException(e);\r\n            }\r\n        }\r\n        if (event.getHistoryEvent().getEventType() == EventType.JOB_FAILED || event.getHistoryEvent().getEventType() == EventType.JOB_KILLED) {\r\n            try {\r\n                JobUnsuccessfulCompletionEvent jucEvent = (JobUnsuccessfulCompletionEvent) event.getHistoryEvent();\r\n                mi.getJobIndexInfo().setFinishTime(jucEvent.getFinishTime());\r\n                mi.getJobIndexInfo().setNumMaps(jucEvent.getSucceededMaps());\r\n                mi.getJobIndexInfo().setNumReduces(jucEvent.getSucceededReduces());\r\n                mi.getJobIndexInfo().setJobStatus(jucEvent.getStatus());\r\n                closeEventWriter(event.getJobID());\r\n                processDoneFiles(event.getJobID());\r\n            } catch (IOException e) {\r\n                throw new YarnRuntimeException(e);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 5,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void handleTimelineEvent(JobHistoryEvent event)\n{\r\n    HistoryEvent historyEvent = event.getHistoryEvent();\r\n    if (handleTimelineEvent) {\r\n        if (timelineV2Client != null) {\r\n            processEventForNewTimelineService(historyEvent, event.getJobID(), event.getTimestamp());\r\n        } else if (timelineClient != null) {\r\n            processEventForTimelineServer(historyEvent, event.getJobID(), event.getTimestamp());\r\n        }\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"In HistoryEventHandler, handle timelineEvent:\" + event.getHistoryEvent().getEventType());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "processEventForJobSummary",
  "errType" : null,
  "containingMethodsNum" : 40,
  "sourceCodeText" : "void processEventForJobSummary(HistoryEvent event, JobSummary summary, JobId jobId)\n{\r\n    switch(event.getEventType()) {\r\n        case JOB_SUBMITTED:\r\n            JobSubmittedEvent jse = (JobSubmittedEvent) event;\r\n            summary.setUser(jse.getUserName());\r\n            summary.setQueue(jse.getJobQueueName());\r\n            summary.setJobSubmitTime(jse.getSubmitTime());\r\n            summary.setJobName(jse.getJobName());\r\n            break;\r\n        case NORMALIZED_RESOURCE:\r\n            NormalizedResourceEvent normalizedResourceEvent = (NormalizedResourceEvent) event;\r\n            if (normalizedResourceEvent.getTaskType() == TaskType.MAP) {\r\n                summary.setResourcesPerMap((int) normalizedResourceEvent.getMemory());\r\n            } else if (normalizedResourceEvent.getTaskType() == TaskType.REDUCE) {\r\n                summary.setResourcesPerReduce((int) normalizedResourceEvent.getMemory());\r\n            }\r\n            break;\r\n        case JOB_INITED:\r\n            JobInitedEvent jie = (JobInitedEvent) event;\r\n            summary.setJobLaunchTime(jie.getLaunchTime());\r\n            break;\r\n        case MAP_ATTEMPT_STARTED:\r\n            TaskAttemptStartedEvent mtase = (TaskAttemptStartedEvent) event;\r\n            if (summary.getFirstMapTaskLaunchTime() == 0)\r\n                summary.setFirstMapTaskLaunchTime(mtase.getStartTime());\r\n            break;\r\n        case REDUCE_ATTEMPT_STARTED:\r\n            TaskAttemptStartedEvent rtase = (TaskAttemptStartedEvent) event;\r\n            if (summary.getFirstReduceTaskLaunchTime() == 0)\r\n                summary.setFirstReduceTaskLaunchTime(rtase.getStartTime());\r\n            break;\r\n        case JOB_FINISHED:\r\n            JobFinishedEvent jfe = (JobFinishedEvent) event;\r\n            summary.setJobFinishTime(jfe.getFinishTime());\r\n            summary.setNumSucceededMaps(jfe.getSucceededMaps());\r\n            summary.setNumFailedMaps(jfe.getFailedMaps());\r\n            summary.setNumSucceededReduces(jfe.getSucceededReduces());\r\n            summary.setNumFailedReduces(jfe.getFailedReduces());\r\n            summary.setNumKilledMaps(jfe.getKilledMaps());\r\n            summary.setNumKilledReduces(jfe.getKilledReduces());\r\n            if (summary.getJobStatus() == null)\r\n                summary.setJobStatus(org.apache.hadoop.mapreduce.JobStatus.State.SUCCEEDED.toString());\r\n            setSummarySlotSeconds(summary, jfe.getTotalCounters());\r\n            break;\r\n        case JOB_FAILED:\r\n        case JOB_KILLED:\r\n            Job job = context.getJob(jobId);\r\n            JobUnsuccessfulCompletionEvent juce = (JobUnsuccessfulCompletionEvent) event;\r\n            int successfulMaps = job.getCompletedMaps() - job.getFailedMaps() - job.getKilledMaps();\r\n            int successfulReduces = job.getCompletedReduces() - job.getFailedReduces() - job.getKilledReduces();\r\n            summary.setJobStatus(juce.getStatus());\r\n            summary.setNumSucceededMaps(successfulMaps);\r\n            summary.setNumSucceededReduces(successfulReduces);\r\n            summary.setNumFailedMaps(job.getFailedMaps());\r\n            summary.setNumFailedReduces(job.getFailedReduces());\r\n            summary.setJobFinishTime(juce.getFinishTime());\r\n            summary.setNumKilledMaps(juce.getKilledMaps());\r\n            summary.setNumKilledReduces(juce.getKilledReduces());\r\n            setSummarySlotSeconds(summary, context.getJob(jobId).getAllCounters());\r\n            break;\r\n        default:\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "processEventForTimelineServer",
  "errType" : [ "YarnException|IOException|ClientHandlerException" ],
  "containingMethodsNum" : 189,
  "sourceCodeText" : "void processEventForTimelineServer(HistoryEvent event, JobId jobId, long timestamp)\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setEventType(StringUtils.toUpperCase(event.getEventType().name()));\r\n    tEvent.setTimestamp(timestamp);\r\n    TimelineEntity tEntity = new TimelineEntity();\r\n    switch(event.getEventType()) {\r\n        case JOB_SUBMITTED:\r\n            JobSubmittedEvent jse = (JobSubmittedEvent) event;\r\n            tEvent.addEventInfo(\"SUBMIT_TIME\", jse.getSubmitTime());\r\n            tEvent.addEventInfo(\"QUEUE_NAME\", jse.getJobQueueName());\r\n            tEvent.addEventInfo(\"JOB_NAME\", jse.getJobName());\r\n            tEvent.addEventInfo(\"USER_NAME\", jse.getUserName());\r\n            tEvent.addEventInfo(\"JOB_CONF_PATH\", jse.getJobConfPath());\r\n            tEvent.addEventInfo(\"ACLS\", jse.getJobAcls());\r\n            tEvent.addEventInfo(\"JOB_QUEUE_NAME\", jse.getJobQueueName());\r\n            tEvent.addEventInfo(\"WORKFLOW_ID\", jse.getWorkflowId());\r\n            tEvent.addEventInfo(\"WORKFLOW_NAME\", jse.getWorkflowName());\r\n            tEvent.addEventInfo(\"WORKFLOW_NAME_NAME\", jse.getWorkflowNodeName());\r\n            tEvent.addEventInfo(\"WORKFLOW_ADJACENCIES\", jse.getWorkflowAdjacencies());\r\n            tEvent.addEventInfo(\"WORKFLOW_TAGS\", jse.getWorkflowTags());\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(jobId.toString());\r\n            tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\r\n            break;\r\n        case JOB_STATUS_CHANGED:\r\n            JobStatusChangedEvent jsce = (JobStatusChangedEvent) event;\r\n            tEvent.addEventInfo(\"STATUS\", jsce.getStatus());\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(jobId.toString());\r\n            tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\r\n            break;\r\n        case JOB_INFO_CHANGED:\r\n            JobInfoChangeEvent jice = (JobInfoChangeEvent) event;\r\n            tEvent.addEventInfo(\"SUBMIT_TIME\", jice.getSubmitTime());\r\n            tEvent.addEventInfo(\"LAUNCH_TIME\", jice.getLaunchTime());\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(jobId.toString());\r\n            tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\r\n            break;\r\n        case JOB_INITED:\r\n            JobInitedEvent jie = (JobInitedEvent) event;\r\n            tEvent.addEventInfo(\"START_TIME\", jie.getLaunchTime());\r\n            tEvent.addEventInfo(\"STATUS\", jie.getStatus());\r\n            tEvent.addEventInfo(\"TOTAL_MAPS\", jie.getTotalMaps());\r\n            tEvent.addEventInfo(\"TOTAL_REDUCES\", jie.getTotalReduces());\r\n            tEvent.addEventInfo(\"UBERIZED\", jie.getUberized());\r\n            tEntity.setStartTime(jie.getLaunchTime());\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(jobId.toString());\r\n            tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\r\n            break;\r\n        case JOB_PRIORITY_CHANGED:\r\n            JobPriorityChangeEvent jpce = (JobPriorityChangeEvent) event;\r\n            tEvent.addEventInfo(\"PRIORITY\", jpce.getPriority().toString());\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(jobId.toString());\r\n            tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\r\n            break;\r\n        case JOB_QUEUE_CHANGED:\r\n            JobQueueChangeEvent jqe = (JobQueueChangeEvent) event;\r\n            tEvent.addEventInfo(\"QUEUE_NAMES\", jqe.getJobQueueName());\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(jobId.toString());\r\n            tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\r\n            break;\r\n        case JOB_FAILED:\r\n        case JOB_KILLED:\r\n        case JOB_ERROR:\r\n            JobUnsuccessfulCompletionEvent juce = (JobUnsuccessfulCompletionEvent) event;\r\n            tEvent.addEventInfo(\"FINISH_TIME\", juce.getFinishTime());\r\n            tEvent.addEventInfo(\"NUM_MAPS\", juce.getSucceededMaps() + juce.getFailedMaps() + juce.getKilledMaps());\r\n            tEvent.addEventInfo(\"NUM_REDUCES\", juce.getSucceededReduces() + juce.getFailedReduces() + juce.getKilledReduces());\r\n            tEvent.addEventInfo(\"JOB_STATUS\", juce.getStatus());\r\n            tEvent.addEventInfo(\"DIAGNOSTICS\", juce.getDiagnostics());\r\n            tEvent.addEventInfo(\"SUCCESSFUL_MAPS\", juce.getSucceededMaps());\r\n            tEvent.addEventInfo(\"SUCCESSFUL_REDUCES\", juce.getSucceededReduces());\r\n            tEvent.addEventInfo(\"FAILED_MAPS\", juce.getFailedMaps());\r\n            tEvent.addEventInfo(\"FAILED_REDUCES\", juce.getFailedReduces());\r\n            tEvent.addEventInfo(\"KILLED_MAPS\", juce.getKilledMaps());\r\n            tEvent.addEventInfo(\"KILLED_REDUCES\", juce.getKilledReduces());\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(jobId.toString());\r\n            tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\r\n            break;\r\n        case JOB_FINISHED:\r\n            JobFinishedEvent jfe = (JobFinishedEvent) event;\r\n            tEvent.addEventInfo(\"FINISH_TIME\", jfe.getFinishTime());\r\n            tEvent.addEventInfo(\"NUM_MAPS\", jfe.getSucceededMaps() + jfe.getFailedMaps() + jfe.getKilledMaps());\r\n            tEvent.addEventInfo(\"NUM_REDUCES\", jfe.getSucceededReduces() + jfe.getFailedReduces() + jfe.getKilledReduces());\r\n            tEvent.addEventInfo(\"FAILED_MAPS\", jfe.getFailedMaps());\r\n            tEvent.addEventInfo(\"FAILED_REDUCES\", jfe.getFailedReduces());\r\n            tEvent.addEventInfo(\"SUCCESSFUL_MAPS\", jfe.getSucceededMaps());\r\n            tEvent.addEventInfo(\"SUCCESSFUL_REDUCES\", jfe.getSucceededReduces());\r\n            tEvent.addEventInfo(\"KILLED_MAPS\", jfe.getKilledMaps());\r\n            tEvent.addEventInfo(\"KILLED_REDUCES\", jfe.getKilledReduces());\r\n            tEvent.addEventInfo(\"MAP_COUNTERS_GROUPS\", JobHistoryEventUtils.countersToJSON(jfe.getMapCounters()));\r\n            tEvent.addEventInfo(\"REDUCE_COUNTERS_GROUPS\", JobHistoryEventUtils.countersToJSON(jfe.getReduceCounters()));\r\n            tEvent.addEventInfo(\"TOTAL_COUNTERS_GROUPS\", JobHistoryEventUtils.countersToJSON(jfe.getTotalCounters()));\r\n            tEvent.addEventInfo(\"JOB_STATUS\", JobState.SUCCEEDED.toString());\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(jobId.toString());\r\n            tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\r\n            break;\r\n        case TASK_STARTED:\r\n            TaskStartedEvent tse = (TaskStartedEvent) event;\r\n            tEvent.addEventInfo(\"TASK_TYPE\", tse.getTaskType().toString());\r\n            tEvent.addEventInfo(\"START_TIME\", tse.getStartTime());\r\n            tEvent.addEventInfo(\"SPLIT_LOCATIONS\", tse.getSplitLocations());\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(tse.getTaskId().toString());\r\n            tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\r\n            tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\r\n            break;\r\n        case TASK_FAILED:\r\n            TaskFailedEvent tfe = (TaskFailedEvent) event;\r\n            tEvent.addEventInfo(\"TASK_TYPE\", tfe.getTaskType().toString());\r\n            tEvent.addEventInfo(\"STATUS\", TaskStatus.State.FAILED.toString());\r\n            tEvent.addEventInfo(\"FINISH_TIME\", tfe.getFinishTime());\r\n            tEvent.addEventInfo(\"ERROR\", tfe.getError());\r\n            tEvent.addEventInfo(\"FAILED_ATTEMPT_ID\", tfe.getFailedAttemptID() == null ? \"\" : tfe.getFailedAttemptID().toString());\r\n            tEvent.addEventInfo(\"COUNTERS_GROUPS\", JobHistoryEventUtils.countersToJSON(tfe.getCounters()));\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(tfe.getTaskId().toString());\r\n            tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\r\n            tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\r\n            break;\r\n        case TASK_UPDATED:\r\n            TaskUpdatedEvent tue = (TaskUpdatedEvent) event;\r\n            tEvent.addEventInfo(\"FINISH_TIME\", tue.getFinishTime());\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(tue.getTaskId().toString());\r\n            tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\r\n            tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\r\n            break;\r\n        case TASK_FINISHED:\r\n            TaskFinishedEvent tfe2 = (TaskFinishedEvent) event;\r\n            tEvent.addEventInfo(\"TASK_TYPE\", tfe2.getTaskType().toString());\r\n            tEvent.addEventInfo(\"COUNTERS_GROUPS\", JobHistoryEventUtils.countersToJSON(tfe2.getCounters()));\r\n            tEvent.addEventInfo(\"FINISH_TIME\", tfe2.getFinishTime());\r\n            tEvent.addEventInfo(\"STATUS\", TaskStatus.State.SUCCEEDED.toString());\r\n            tEvent.addEventInfo(\"SUCCESSFUL_TASK_ATTEMPT_ID\", tfe2.getSuccessfulTaskAttemptId() == null ? \"\" : tfe2.getSuccessfulTaskAttemptId().toString());\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(tfe2.getTaskId().toString());\r\n            tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\r\n            tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\r\n            break;\r\n        case MAP_ATTEMPT_STARTED:\r\n        case CLEANUP_ATTEMPT_STARTED:\r\n        case REDUCE_ATTEMPT_STARTED:\r\n        case SETUP_ATTEMPT_STARTED:\r\n            TaskAttemptStartedEvent tase = (TaskAttemptStartedEvent) event;\r\n            tEvent.addEventInfo(\"TASK_TYPE\", tase.getTaskType().toString());\r\n            tEvent.addEventInfo(\"TASK_ATTEMPT_ID\", tase.getTaskAttemptId().toString());\r\n            tEvent.addEventInfo(\"START_TIME\", tase.getStartTime());\r\n            tEvent.addEventInfo(\"HTTP_PORT\", tase.getHttpPort());\r\n            tEvent.addEventInfo(\"TRACKER_NAME\", tase.getTrackerName());\r\n            tEvent.addEventInfo(\"SHUFFLE_PORT\", tase.getShufflePort());\r\n            tEvent.addEventInfo(\"CONTAINER_ID\", tase.getContainerId() == null ? \"\" : tase.getContainerId().toString());\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(tase.getTaskId().toString());\r\n            tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\r\n            tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\r\n            break;\r\n        case MAP_ATTEMPT_FAILED:\r\n        case CLEANUP_ATTEMPT_FAILED:\r\n        case REDUCE_ATTEMPT_FAILED:\r\n        case SETUP_ATTEMPT_FAILED:\r\n        case MAP_ATTEMPT_KILLED:\r\n        case CLEANUP_ATTEMPT_KILLED:\r\n        case REDUCE_ATTEMPT_KILLED:\r\n        case SETUP_ATTEMPT_KILLED:\r\n            TaskAttemptUnsuccessfulCompletionEvent tauce = (TaskAttemptUnsuccessfulCompletionEvent) event;\r\n            tEvent.addEventInfo(\"TASK_TYPE\", tauce.getTaskType().toString());\r\n            tEvent.addEventInfo(\"TASK_ATTEMPT_ID\", tauce.getTaskAttemptId() == null ? \"\" : tauce.getTaskAttemptId().toString());\r\n            tEvent.addEventInfo(\"FINISH_TIME\", tauce.getFinishTime());\r\n            tEvent.addEventInfo(\"ERROR\", tauce.getError());\r\n            tEvent.addEventInfo(\"STATUS\", tauce.getTaskStatus());\r\n            tEvent.addEventInfo(\"HOSTNAME\", tauce.getHostname());\r\n            tEvent.addEventInfo(\"PORT\", tauce.getPort());\r\n            tEvent.addEventInfo(\"RACK_NAME\", tauce.getRackName());\r\n            tEvent.addEventInfo(\"SHUFFLE_FINISH_TIME\", tauce.getFinishTime());\r\n            tEvent.addEventInfo(\"SORT_FINISH_TIME\", tauce.getFinishTime());\r\n            tEvent.addEventInfo(\"MAP_FINISH_TIME\", tauce.getFinishTime());\r\n            tEvent.addEventInfo(\"COUNTERS_GROUPS\", JobHistoryEventUtils.countersToJSON(tauce.getCounters()));\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(tauce.getTaskId().toString());\r\n            tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\r\n            tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\r\n            break;\r\n        case MAP_ATTEMPT_FINISHED:\r\n            MapAttemptFinishedEvent mafe = (MapAttemptFinishedEvent) event;\r\n            tEvent.addEventInfo(\"TASK_TYPE\", mafe.getTaskType().toString());\r\n            tEvent.addEventInfo(\"FINISH_TIME\", mafe.getFinishTime());\r\n            tEvent.addEventInfo(\"STATUS\", mafe.getTaskStatus());\r\n            tEvent.addEventInfo(\"STATE\", mafe.getState());\r\n            tEvent.addEventInfo(\"MAP_FINISH_TIME\", mafe.getMapFinishTime());\r\n            tEvent.addEventInfo(\"COUNTERS_GROUPS\", JobHistoryEventUtils.countersToJSON(mafe.getCounters()));\r\n            tEvent.addEventInfo(\"HOSTNAME\", mafe.getHostname());\r\n            tEvent.addEventInfo(\"PORT\", mafe.getPort());\r\n            tEvent.addEventInfo(\"RACK_NAME\", mafe.getRackName());\r\n            tEvent.addEventInfo(\"ATTEMPT_ID\", mafe.getAttemptId() == null ? \"\" : mafe.getAttemptId().toString());\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(mafe.getTaskId().toString());\r\n            tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\r\n            tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\r\n            break;\r\n        case REDUCE_ATTEMPT_FINISHED:\r\n            ReduceAttemptFinishedEvent rafe = (ReduceAttemptFinishedEvent) event;\r\n            tEvent.addEventInfo(\"TASK_TYPE\", rafe.getTaskType().toString());\r\n            tEvent.addEventInfo(\"ATTEMPT_ID\", rafe.getAttemptId() == null ? \"\" : rafe.getAttemptId().toString());\r\n            tEvent.addEventInfo(\"FINISH_TIME\", rafe.getFinishTime());\r\n            tEvent.addEventInfo(\"STATUS\", rafe.getTaskStatus());\r\n            tEvent.addEventInfo(\"STATE\", rafe.getState());\r\n            tEvent.addEventInfo(\"SHUFFLE_FINISH_TIME\", rafe.getShuffleFinishTime());\r\n            tEvent.addEventInfo(\"SORT_FINISH_TIME\", rafe.getSortFinishTime());\r\n            tEvent.addEventInfo(\"COUNTERS_GROUPS\", JobHistoryEventUtils.countersToJSON(rafe.getCounters()));\r\n            tEvent.addEventInfo(\"HOSTNAME\", rafe.getHostname());\r\n            tEvent.addEventInfo(\"PORT\", rafe.getPort());\r\n            tEvent.addEventInfo(\"RACK_NAME\", rafe.getRackName());\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(rafe.getTaskId().toString());\r\n            tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\r\n            tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\r\n            break;\r\n        case SETUP_ATTEMPT_FINISHED:\r\n        case CLEANUP_ATTEMPT_FINISHED:\r\n            TaskAttemptFinishedEvent tafe = (TaskAttemptFinishedEvent) event;\r\n            tEvent.addEventInfo(\"TASK_TYPE\", tafe.getTaskType().toString());\r\n            tEvent.addEventInfo(\"ATTEMPT_ID\", tafe.getAttemptId() == null ? \"\" : tafe.getAttemptId().toString());\r\n            tEvent.addEventInfo(\"FINISH_TIME\", tafe.getFinishTime());\r\n            tEvent.addEventInfo(\"STATUS\", tafe.getTaskStatus());\r\n            tEvent.addEventInfo(\"STATE\", tafe.getState());\r\n            tEvent.addEventInfo(\"COUNTERS_GROUPS\", JobHistoryEventUtils.countersToJSON(tafe.getCounters()));\r\n            tEvent.addEventInfo(\"HOSTNAME\", tafe.getHostname());\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(tafe.getTaskId().toString());\r\n            tEntity.setEntityType(MAPREDUCE_TASK_ENTITY_TYPE);\r\n            tEntity.addRelatedEntity(MAPREDUCE_JOB_ENTITY_TYPE, jobId.toString());\r\n            break;\r\n        case AM_STARTED:\r\n            AMStartedEvent ase = (AMStartedEvent) event;\r\n            tEvent.addEventInfo(\"APPLICATION_ATTEMPT_ID\", ase.getAppAttemptId() == null ? \"\" : ase.getAppAttemptId().toString());\r\n            tEvent.addEventInfo(\"CONTAINER_ID\", ase.getContainerId() == null ? \"\" : ase.getContainerId().toString());\r\n            tEvent.addEventInfo(\"NODE_MANAGER_HOST\", ase.getNodeManagerHost());\r\n            tEvent.addEventInfo(\"NODE_MANAGER_PORT\", ase.getNodeManagerPort());\r\n            tEvent.addEventInfo(\"NODE_MANAGER_HTTP_PORT\", ase.getNodeManagerHttpPort());\r\n            tEvent.addEventInfo(\"START_TIME\", ase.getStartTime());\r\n            tEvent.addEventInfo(\"SUBMIT_TIME\", ase.getSubmitTime());\r\n            tEntity.addEvent(tEvent);\r\n            tEntity.setEntityId(jobId.toString());\r\n            tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);\r\n            break;\r\n        default:\r\n            break;\r\n    }\r\n    try {\r\n        TimelinePutResponse response = timelineClient.putEntities(tEntity);\r\n        List<TimelinePutResponse.TimelinePutError> errors = response.getErrors();\r\n        if (errors.size() == 0) {\r\n            if (LOG.isDebugEnabled()) {\r\n                LOG.debug(\"Timeline entities are successfully put in event \" + event.getEventType());\r\n            }\r\n        } else {\r\n            for (TimelinePutResponse.TimelinePutError error : errors) {\r\n                LOG.error(\"Error when publishing entity [\" + error.getEntityType() + \",\" + error.getEntityId() + \"], server side error code: \" + error.getErrorCode());\r\n            }\r\n        }\r\n    } catch (YarnException | IOException | ClientHandlerException ex) {\r\n        LOG.error(\"Error putting entity \" + tEntity.getEntityId() + \" to Timeline\" + \"Server\", ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "createJobEntity",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity createJobEntity(HistoryEvent event, long timestamp, JobId jobId, String entityType, boolean setCreatedTime)\n{\r\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity entity = createBaseEntity(event, timestamp, entityType, setCreatedTime);\r\n    entity.setId(jobId.toString());\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "createJobEntity",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity createJobEntity(JobId jobId)\n{\r\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity entity = new org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity();\r\n    entity.setId(jobId.toString());\r\n    entity.setType(MAPREDUCE_JOB_ENTITY_TYPE);\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "createAppEntityWithJobMetrics",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity createAppEntityWithJobMetrics(HistoryEvent event, JobId jobId)\n{\r\n    ApplicationEntity entity = new ApplicationEntity();\r\n    entity.setId(jobId.getAppId().toString());\r\n    entity.setMetrics(event.getTimelineMetrics());\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "createBaseEntity",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity createBaseEntity(HistoryEvent event, long timestamp, String entityType, boolean setCreatedTime)\n{\r\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent tEvent = event.toTimelineEvent();\r\n    tEvent.setTimestamp(timestamp);\r\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity entity = new org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity();\r\n    entity.addEvent(tEvent);\r\n    entity.setType(entityType);\r\n    if (setCreatedTime) {\r\n        entity.setCreatedTime(timestamp);\r\n    }\r\n    Set<TimelineMetric> timelineMetrics = event.getTimelineMetrics();\r\n    if (timelineMetrics != null) {\r\n        entity.setMetrics(timelineMetrics);\r\n    }\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "createTaskEntity",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity createTaskEntity(HistoryEvent event, long timestamp, String taskId, String entityType, String relatedJobEntity, JobId jobId, boolean setCreatedTime, long taskIdPrefix)\n{\r\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity entity = createBaseEntity(event, timestamp, entityType, setCreatedTime);\r\n    entity.setId(taskId);\r\n    if (event.getEventType() == EventType.TASK_STARTED) {\r\n        entity.addInfo(\"TASK_TYPE\", ((TaskStartedEvent) event).getTaskType().toString());\r\n    }\r\n    entity.addIsRelatedToEntity(relatedJobEntity, jobId.toString());\r\n    entity.setIdPrefix(taskIdPrefix);\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "createTaskAttemptEntity",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity createTaskAttemptEntity(HistoryEvent event, long timestamp, String taskAttemptId, String entityType, String relatedTaskEntity, String taskId, boolean setCreatedTime, long taskAttemptIdPrefix)\n{\r\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity entity = createBaseEntity(event, timestamp, entityType, setCreatedTime);\r\n    entity.setId(taskAttemptId);\r\n    entity.addIsRelatedToEntity(relatedTaskEntity, taskId);\r\n    entity.setIdPrefix(taskAttemptIdPrefix);\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "publishConfigsOnJobSubmittedEvent",
  "errType" : [ "IOException|YarnException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void publishConfigsOnJobSubmittedEvent(JobSubmittedEvent event, JobId jobId)\n{\r\n    if (event.getJobConf() == null) {\r\n        return;\r\n    }\r\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity jobEntityForConfigs = createJobEntity(jobId);\r\n    ApplicationEntity appEntityForConfigs = new ApplicationEntity();\r\n    String appId = jobId.getAppId().toString();\r\n    appEntityForConfigs.setId(appId);\r\n    try {\r\n        int configSize = 0;\r\n        for (Map.Entry<String, String> entry : event.getJobConf()) {\r\n            int size = entry.getKey().length() + entry.getValue().length();\r\n            configSize += size;\r\n            if (configSize > JobHistoryEventUtils.ATS_CONFIG_PUBLISH_SIZE_BYTES) {\r\n                if (jobEntityForConfigs.getConfigs().size() > 0) {\r\n                    timelineV2Client.putEntities(jobEntityForConfigs);\r\n                    timelineV2Client.putEntities(appEntityForConfigs);\r\n                    jobEntityForConfigs = createJobEntity(jobId);\r\n                    appEntityForConfigs = new ApplicationEntity();\r\n                    appEntityForConfigs.setId(appId);\r\n                }\r\n                configSize = size;\r\n            }\r\n            jobEntityForConfigs.addConfig(entry.getKey(), entry.getValue());\r\n            appEntityForConfigs.addConfig(entry.getKey(), entry.getValue());\r\n        }\r\n        if (configSize > 0) {\r\n            timelineV2Client.putEntities(jobEntityForConfigs);\r\n            timelineV2Client.putEntities(appEntityForConfigs);\r\n        }\r\n    } catch (IOException | YarnException e) {\r\n        LOG.error(\"Exception while publishing configs on JOB_SUBMITTED Event \" + \" for the job : \" + jobId, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "processEventForNewTimelineService",
  "errType" : [ "IOException|YarnException" ],
  "containingMethodsNum" : 36,
  "sourceCodeText" : "void processEventForNewTimelineService(HistoryEvent event, JobId jobId, long timestamp)\n{\r\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity tEntity = null;\r\n    String taskId = null;\r\n    String taskAttemptId = null;\r\n    boolean setCreatedTime = false;\r\n    long taskIdPrefix = 0;\r\n    long taskAttemptIdPrefix = 0;\r\n    switch(event.getEventType()) {\r\n        case JOB_SUBMITTED:\r\n            setCreatedTime = true;\r\n            break;\r\n        case JOB_STATUS_CHANGED:\r\n        case JOB_INFO_CHANGED:\r\n        case JOB_INITED:\r\n        case JOB_PRIORITY_CHANGED:\r\n        case JOB_QUEUE_CHANGED:\r\n        case JOB_FAILED:\r\n        case JOB_KILLED:\r\n        case JOB_ERROR:\r\n        case JOB_FINISHED:\r\n        case AM_STARTED:\r\n        case NORMALIZED_RESOURCE:\r\n            break;\r\n        case TASK_STARTED:\r\n            setCreatedTime = true;\r\n            taskId = ((TaskStartedEvent) event).getTaskId().toString();\r\n            taskIdPrefix = TimelineServiceHelper.invertLong(((TaskStartedEvent) event).getStartTime());\r\n            break;\r\n        case TASK_FAILED:\r\n            taskId = ((TaskFailedEvent) event).getTaskId().toString();\r\n            taskIdPrefix = TimelineServiceHelper.invertLong(((TaskFailedEvent) event).getStartTime());\r\n            break;\r\n        case TASK_UPDATED:\r\n            taskId = ((TaskUpdatedEvent) event).getTaskId().toString();\r\n            break;\r\n        case TASK_FINISHED:\r\n            taskId = ((TaskFinishedEvent) event).getTaskId().toString();\r\n            taskIdPrefix = TimelineServiceHelper.invertLong(((TaskFinishedEvent) event).getStartTime());\r\n            break;\r\n        case MAP_ATTEMPT_STARTED:\r\n        case REDUCE_ATTEMPT_STARTED:\r\n            setCreatedTime = true;\r\n            taskId = ((TaskAttemptStartedEvent) event).getTaskId().toString();\r\n            taskAttemptId = ((TaskAttemptStartedEvent) event).getTaskAttemptId().toString();\r\n            taskAttemptIdPrefix = TimelineServiceHelper.invertLong(((TaskAttemptStartedEvent) event).getStartTime());\r\n            break;\r\n        case CLEANUP_ATTEMPT_STARTED:\r\n        case SETUP_ATTEMPT_STARTED:\r\n            taskId = ((TaskAttemptStartedEvent) event).getTaskId().toString();\r\n            taskAttemptId = ((TaskAttemptStartedEvent) event).getTaskAttemptId().toString();\r\n            break;\r\n        case MAP_ATTEMPT_FAILED:\r\n        case CLEANUP_ATTEMPT_FAILED:\r\n        case REDUCE_ATTEMPT_FAILED:\r\n        case SETUP_ATTEMPT_FAILED:\r\n        case MAP_ATTEMPT_KILLED:\r\n        case CLEANUP_ATTEMPT_KILLED:\r\n        case REDUCE_ATTEMPT_KILLED:\r\n        case SETUP_ATTEMPT_KILLED:\r\n            taskId = ((TaskAttemptUnsuccessfulCompletionEvent) event).getTaskId().toString();\r\n            taskAttemptId = ((TaskAttemptUnsuccessfulCompletionEvent) event).getTaskAttemptId().toString();\r\n            taskAttemptIdPrefix = TimelineServiceHelper.invertLong(((TaskAttemptUnsuccessfulCompletionEvent) event).getStartTime());\r\n            break;\r\n        case MAP_ATTEMPT_FINISHED:\r\n            taskId = ((MapAttemptFinishedEvent) event).getTaskId().toString();\r\n            taskAttemptId = ((MapAttemptFinishedEvent) event).getAttemptId().toString();\r\n            taskAttemptIdPrefix = TimelineServiceHelper.invertLong(((MapAttemptFinishedEvent) event).getStartTime());\r\n            break;\r\n        case REDUCE_ATTEMPT_FINISHED:\r\n            taskId = ((ReduceAttemptFinishedEvent) event).getTaskId().toString();\r\n            taskAttemptId = ((ReduceAttemptFinishedEvent) event).getAttemptId().toString();\r\n            taskAttemptIdPrefix = TimelineServiceHelper.invertLong(((ReduceAttemptFinishedEvent) event).getStartTime());\r\n            break;\r\n        case SETUP_ATTEMPT_FINISHED:\r\n        case CLEANUP_ATTEMPT_FINISHED:\r\n            taskId = ((TaskAttemptFinishedEvent) event).getTaskId().toString();\r\n            taskAttemptId = ((TaskAttemptFinishedEvent) event).getAttemptId().toString();\r\n            break;\r\n        default:\r\n            LOG.warn(\"EventType: \" + event.getEventType() + \" cannot be recognized\" + \" and handled by timeline service.\");\r\n            return;\r\n    }\r\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity appEntityWithJobMetrics = null;\r\n    if (taskId == null) {\r\n        tEntity = createJobEntity(event, timestamp, jobId, MAPREDUCE_JOB_ENTITY_TYPE, setCreatedTime);\r\n        if (event.getEventType() == EventType.JOB_FINISHED && event.getTimelineMetrics() != null) {\r\n            appEntityWithJobMetrics = createAppEntityWithJobMetrics(event, jobId);\r\n        }\r\n    } else {\r\n        if (taskAttemptId == null) {\r\n            tEntity = createTaskEntity(event, timestamp, taskId, MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE, jobId, setCreatedTime, taskIdPrefix);\r\n        } else {\r\n            tEntity = createTaskAttemptEntity(event, timestamp, taskAttemptId, MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE, taskId, setCreatedTime, taskAttemptIdPrefix);\r\n        }\r\n    }\r\n    try {\r\n        if (appEntityWithJobMetrics == null) {\r\n            timelineV2Client.putEntitiesAsync(tEntity);\r\n        } else {\r\n            timelineV2Client.putEntities(tEntity, appEntityWithJobMetrics);\r\n        }\r\n    } catch (IOException | YarnException e) {\r\n        LOG.error(\"Failed to process Event \" + event.getEventType() + \" for the job : \" + jobId, e);\r\n        return;\r\n    }\r\n    if (event.getEventType() == EventType.JOB_SUBMITTED) {\r\n        publishConfigsOnJobSubmittedEvent((JobSubmittedEvent) event, jobId);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setSummarySlotSeconds",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setSummarySlotSeconds(JobSummary summary, Counters allCounters)\n{\r\n    Counter slotMillisMapCounter = allCounters.findCounter(JobCounter.SLOTS_MILLIS_MAPS);\r\n    if (slotMillisMapCounter != null) {\r\n        summary.setMapSlotSeconds(slotMillisMapCounter.getValue() / 1000);\r\n    }\r\n    Counter slotMillisReduceCounter = allCounters.findCounter(JobCounter.SLOTS_MILLIS_REDUCES);\r\n    if (slotMillisReduceCounter != null) {\r\n        summary.setReduceSlotSeconds(slotMillisReduceCounter.getValue() / 1000);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "closeEventWriter",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void closeEventWriter(JobId jobId) throws IOException\n{\r\n    final MetaInfo mi = fileMap.get(jobId);\r\n    if (mi == null) {\r\n        throw new IOException(\"No MetaInfo found for JobId: [\" + jobId + \"]\");\r\n    }\r\n    if (!mi.isWriterActive()) {\r\n        throw new IOException(\"Inactive Writer: Likely received multiple JobFinished / \" + \"JobUnsuccessful events for JobId: [\" + jobId + \"]\");\r\n    }\r\n    try {\r\n        mi.closeWriter();\r\n    } catch (IOException e) {\r\n        LOG.error(\"Error closing writer for JobID: \" + jobId);\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "processDoneFiles",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 32,
  "sourceCodeText" : "void processDoneFiles(JobId jobId) throws IOException\n{\r\n    final MetaInfo mi = fileMap.get(jobId);\r\n    if (mi == null) {\r\n        throw new IOException(\"No MetaInfo found for JobId: [\" + jobId + \"]\");\r\n    }\r\n    if (mi.getHistoryFile() == null) {\r\n        LOG.warn(\"No file for job-history with \" + jobId + \" found in cache!\");\r\n    }\r\n    if (mi.getConfFile() == null) {\r\n        LOG.warn(\"No file for jobconf with \" + jobId + \" found in cache!\");\r\n    }\r\n    Path qualifiedSummaryDoneFile = null;\r\n    FSDataOutputStream summaryFileOut = null;\r\n    try {\r\n        String doneSummaryFileName = getTempFileName(JobHistoryUtils.getIntermediateSummaryFileName(jobId));\r\n        qualifiedSummaryDoneFile = doneDirFS.makeQualified(new Path(doneDirPrefixPath, doneSummaryFileName));\r\n        summaryFileOut = doneDirFS.create(qualifiedSummaryDoneFile, true);\r\n        summaryFileOut.writeUTF(mi.getJobSummary().getJobSummaryString());\r\n        summaryFileOut.close();\r\n        doneDirFS.setPermission(qualifiedSummaryDoneFile, new FsPermission(JobHistoryUtils.HISTORY_INTERMEDIATE_FILE_PERMISSIONS));\r\n    } catch (IOException e) {\r\n        LOG.info(\"Unable to write out JobSummaryInfo to [\" + qualifiedSummaryDoneFile + \"]\", e);\r\n        throw e;\r\n    }\r\n    try {\r\n        Path qualifiedDoneFile = null;\r\n        if (mi.getHistoryFile() != null) {\r\n            Path historyFile = mi.getHistoryFile();\r\n            Path qualifiedLogFile = stagingDirFS.makeQualified(historyFile);\r\n            int jobNameLimit = getConfig().getInt(JHAdminConfig.MR_HS_JOBNAME_LIMIT, JHAdminConfig.DEFAULT_MR_HS_JOBNAME_LIMIT);\r\n            String doneJobHistoryFileName = getTempFileName(FileNameIndexUtils.getDoneFileName(mi.getJobIndexInfo(), jobNameLimit));\r\n            qualifiedDoneFile = doneDirFS.makeQualified(new Path(doneDirPrefixPath, doneJobHistoryFileName));\r\n            if (moveToDoneNow(qualifiedLogFile, qualifiedDoneFile)) {\r\n                String historyUrl = MRWebAppUtil.getApplicationWebURLOnJHSWithScheme(getConfig(), context.getApplicationID());\r\n                context.setHistoryUrl(historyUrl);\r\n                LOG.info(\"Set historyUrl to \" + historyUrl);\r\n            }\r\n        }\r\n        Path qualifiedConfDoneFile = null;\r\n        if (mi.getConfFile() != null) {\r\n            Path confFile = mi.getConfFile();\r\n            Path qualifiedConfFile = stagingDirFS.makeQualified(confFile);\r\n            String doneConfFileName = getTempFileName(JobHistoryUtils.getIntermediateConfFileName(jobId));\r\n            qualifiedConfDoneFile = doneDirFS.makeQualified(new Path(doneDirPrefixPath, doneConfFileName));\r\n            moveToDoneNow(qualifiedConfFile, qualifiedConfDoneFile);\r\n        }\r\n        moveTmpToDone(qualifiedSummaryDoneFile);\r\n        moveTmpToDone(qualifiedConfDoneFile);\r\n        moveTmpToDone(qualifiedDoneFile);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Error closing writer for JobID: \" + jobId);\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "moveTmpToDone",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void moveTmpToDone(Path tmpPath) throws IOException\n{\r\n    if (tmpPath != null) {\r\n        String tmpFileName = tmpPath.getName();\r\n        String fileName = getFileNameFromTmpFN(tmpFileName);\r\n        Path path = new Path(tmpPath.getParent(), fileName);\r\n        doneDirFS.rename(tmpPath, path);\r\n        LOG.info(\"Moved tmp to done: \" + tmpPath + \" to \" + path);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "moveToDoneNow",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean moveToDoneNow(Path fromPath, Path toPath) throws IOException\n{\r\n    boolean success = false;\r\n    if (stagingDirFS.exists(fromPath)) {\r\n        LOG.info(\"Copying \" + fromPath.toString() + \" to \" + toPath.toString());\r\n        doneDirFS.delete(toPath, true);\r\n        boolean copied = FileUtil.copy(stagingDirFS, fromPath, doneDirFS, toPath, false, getConfig());\r\n        doneDirFS.setPermission(toPath, new FsPermission(JobHistoryUtils.HISTORY_INTERMEDIATE_FILE_PERMISSIONS));\r\n        if (copied) {\r\n            LOG.info(\"Copied from: \" + fromPath.toString() + \" to done location: \" + toPath.toString());\r\n            success = true;\r\n        } else {\r\n            LOG.info(\"Copy failed from: \" + fromPath.toString() + \" to done location: \" + toPath.toString());\r\n        }\r\n    }\r\n    return success;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTempFileName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTempFileName(String srcFile)\n{\r\n    return srcFile + \"_tmp\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFileNameFromTmpFN",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getFileNameFromTmpFN(String tmpFileName)\n{\r\n    return tmpFileName.substring(0, tmpFileName.length() - 4);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setForcejobCompletion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setForcejobCompletion(boolean forceJobCompletion)\n{\r\n    this.forceJobCompletion = forceJobCompletion;\r\n    LOG.info(\"JobHistoryEventHandler notified that forceJobCompletion is \" + forceJobCompletion);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "createJobStateForJobUnsuccessfulCompletionEvent",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String createJobStateForJobUnsuccessfulCompletionEvent(String forcedJobStateOnShutDown)\n{\r\n    if (forcedJobStateOnShutDown == null || forcedJobStateOnShutDown.isEmpty()) {\r\n        return JobState.KILLED.toString();\r\n    } else if (forcedJobStateOnShutDown.equals(JobStateInternal.ERROR.toString()) || forcedJobStateOnShutDown.equals(JobStateInternal.FAILED.toString())) {\r\n        return JobState.FAILED.toString();\r\n    } else if (forcedJobStateOnShutDown.equals(JobStateInternal.SUCCEEDED.toString())) {\r\n        return JobState.SUCCEEDED.toString();\r\n    }\r\n    return JobState.KILLED.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFlushTimerStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getFlushTimerStatus()\n{\r\n    return isTimerActive;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobId getJobId()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobId(JobId jobId)\n{\r\n    this.jobId = jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobSubmitTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getJobSubmitTime()\n{\r\n    return jobSubmitTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setJobSubmitTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobSubmitTime(long jobSubmitTime)\n{\r\n    this.jobSubmitTime = jobSubmitTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobLaunchTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getJobLaunchTime()\n{\r\n    return jobLaunchTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setJobLaunchTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobLaunchTime(long jobLaunchTime)\n{\r\n    this.jobLaunchTime = jobLaunchTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFirstMapTaskLaunchTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFirstMapTaskLaunchTime()\n{\r\n    return firstMapTaskLaunchTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setFirstMapTaskLaunchTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFirstMapTaskLaunchTime(long firstMapTaskLaunchTime)\n{\r\n    this.firstMapTaskLaunchTime = firstMapTaskLaunchTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFirstReduceTaskLaunchTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFirstReduceTaskLaunchTime()\n{\r\n    return firstReduceTaskLaunchTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setFirstReduceTaskLaunchTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFirstReduceTaskLaunchTime(long firstReduceTaskLaunchTime)\n{\r\n    this.firstReduceTaskLaunchTime = firstReduceTaskLaunchTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getJobFinishTime()\n{\r\n    return jobFinishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setJobFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobFinishTime(long jobFinishTime)\n{\r\n    this.jobFinishTime = jobFinishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getNumSucceededMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumSucceededMaps()\n{\r\n    return numSucceededMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setNumSucceededMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumSucceededMaps(int numSucceededMaps)\n{\r\n    this.numSucceededMaps = numSucceededMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getNumFailedMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumFailedMaps()\n{\r\n    return numFailedMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setNumFailedMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumFailedMaps(int numFailedMaps)\n{\r\n    this.numFailedMaps = numFailedMaps;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getKilledMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getKilledMaps()\n{\r\n    return numKilledMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setNumKilledMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumKilledMaps(int numKilledMaps)\n{\r\n    this.numKilledMaps = numKilledMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getKilledReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getKilledReduces()\n{\r\n    return numKilledReduces;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setNumKilledReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumKilledReduces(int numKilledReduces)\n{\r\n    this.numKilledReduces = numKilledReduces;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getResourcesPerMap",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getResourcesPerMap()\n{\r\n    return resourcesPerMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setResourcesPerMap",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setResourcesPerMap(int resourcesPerMap)\n{\r\n    this.resourcesPerMap = resourcesPerMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getNumSucceededReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumSucceededReduces()\n{\r\n    return numSucceededReduces;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setNumSucceededReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumSucceededReduces(int numSucceededReduces)\n{\r\n    this.numSucceededReduces = numSucceededReduces;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getNumFailedReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumFailedReduces()\n{\r\n    return numFailedReduces;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setNumFailedReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumFailedReduces(int numFailedReduces)\n{\r\n    this.numFailedReduces = numFailedReduces;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getResourcesPerReduce",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getResourcesPerReduce()\n{\r\n    return this.resourcesPerReduce;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setResourcesPerReduce",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setResourcesPerReduce(int resourcesPerReduce)\n{\r\n    this.resourcesPerReduce = resourcesPerReduce;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUser()\n{\r\n    return user;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setUser",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setUser(String user)\n{\r\n    this.user = user;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getQueue()\n{\r\n    return queue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setQueue(String queue)\n{\r\n    this.queue = queue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobStatus()\n{\r\n    return jobStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setJobStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobStatus(String jobStatus)\n{\r\n    this.jobStatus = jobStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getMapSlotSeconds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMapSlotSeconds()\n{\r\n    return mapSlotSeconds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setMapSlotSeconds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMapSlotSeconds(long mapSlotSeconds)\n{\r\n    this.mapSlotSeconds = mapSlotSeconds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getReduceSlotSeconds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getReduceSlotSeconds()\n{\r\n    return reduceSlotSeconds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setReduceSlotSeconds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setReduceSlotSeconds(long reduceSlotSeconds)\n{\r\n    this.reduceSlotSeconds = reduceSlotSeconds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobName()\n{\r\n    return jobName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setJobName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobName(String jobName)\n{\r\n    this.jobName = jobName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobSummaryString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getJobSummaryString()\n{\r\n    SummaryBuilder summary = new SummaryBuilder().add(\"jobId\", jobId).add(\"submitTime\", jobSubmitTime).add(\"launchTime\", jobLaunchTime).add(\"firstMapTaskLaunchTime\", firstMapTaskLaunchTime).add(\"firstReduceTaskLaunchTime\", firstReduceTaskLaunchTime).add(\"finishTime\", jobFinishTime).add(\"resourcesPerMap\", resourcesPerMap).add(\"resourcesPerReduce\", resourcesPerReduce).add(\"numMaps\", numSucceededMaps + numFailedMaps + numKilledMaps).add(\"numReduces\", numSucceededReduces + numFailedReduces + numKilledReduces).add(\"succededMaps\", numSucceededMaps).add(\"succeededReduces\", numSucceededReduces).add(\"failedMaps\", numFailedMaps).add(\"failedReduces\", numFailedReduces).add(\"killedMaps\", numKilledMaps).add(\"killedReduces\", numKilledReduces).add(\"user\", user).add(\"queue\", queue).add(\"status\", jobStatus).add(\"mapSlotSeconds\", mapSlotSeconds).add(\"reduceSlotSeconds\", reduceSlotSeconds).add(\"jobName\", jobName);\r\n    return summary.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void add(TaskAttemptInfo taskattemptInfo)\n{\r\n    taskAttempts.add(taskattemptInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<TaskAttemptInfo> getTaskAttempts()\n{\r\n    return taskAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp\\dao",
  "methodName" : "getBlacklistedNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<String> getBlacklistedNodes()\n{\r\n    return blacklistedNodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    commonPreHead(html);\r\n    String tid = $(TASK_ID);\r\n    String activeNav = \"3\";\r\n    if (tid == null || tid.isEmpty()) {\r\n        activeNav = \"2\";\r\n    }\r\n    set(initID(ACCORDION, \"nav\"), \"{autoHeight:false, active:\" + activeNav + \"}\");\r\n    set(DATATABLES_ID, \"singleCounter\");\r\n    set(initID(DATATABLES, \"singleCounter\"), counterTableInit());\r\n    setTableStyles(html, \"singleCounter\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "counterTableInit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String counterTableInit()\n{\r\n    return tableInit().append(\",aoColumnDefs:[\").append(\"{'sType':'title-numeric', 'aTargets': [ 1 ] }\").append(\"]}\").toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return SingleCounterBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getContMgrAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getContMgrAddress()\n{\r\n    return contMgrAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void init(AppContext context)\n{\r\n    dispatcher = context.getEventHandler();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "preempt",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void preempt(Context ctxt, PreemptionMessage preemptionRequests)\n{\r\n    StrictPreemptionContract strictContract = preemptionRequests.getStrictContract();\r\n    if (strictContract != null) {\r\n        for (PreemptionContainer c : strictContract.getContainers()) {\r\n            killContainer(ctxt, c);\r\n        }\r\n    }\r\n    PreemptionContract contract = preemptionRequests.getContract();\r\n    if (contract != null) {\r\n        for (PreemptionContainer c : contract.getContainers()) {\r\n            killContainer(ctxt, c);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "killContainer",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void killContainer(Context ctxt, PreemptionContainer c)\n{\r\n    ContainerId reqCont = c.getId();\r\n    TaskAttemptId reqTask = ctxt.getTaskAttempt(reqCont);\r\n    LOG.info(\"Evicting \" + reqTask);\r\n    dispatcher.handle(new TaskAttemptEvent(reqTask, TaskAttemptEventType.TA_KILL));\r\n    JobCounterUpdateEvent jce = new JobCounterUpdateEvent(reqTask.getTaskId().getJobId());\r\n    jce.addCounterUpdate(JobCounter.TASKS_REQ_PREEMPT, 1);\r\n    dispatcher.handle(jce);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "handleFailedContainer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void handleFailedContainer(TaskAttemptId attemptID)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "isPreempted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isPreempted(TaskAttemptId yarnAttemptID)\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "reportSuccessfulPreemption",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reportSuccessfulPreemption(TaskAttemptId taskAttemptID)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "getCheckpointID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskCheckpointID getCheckpointID(TaskId taskId)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "setCheckpointID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCheckpointID(TaskId taskId, TaskCheckpointID cid)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm\\preemption",
  "methodName" : "handleCompletedContainer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void handleCompletedContainer(TaskAttemptId attemptID)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobId getJobId()\n{\r\n    return jobID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "createContainerRequestEventForFailedContainer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ContainerRequestEvent createContainerRequestEventForFailedContainer(TaskAttemptId attemptID, Resource capability)\n{\r\n    return new ContainerRequestEvent(attemptID, capability);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getCapability",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Resource getCapability()\n{\r\n    return capability;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getHosts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] getHosts()\n{\r\n    return hosts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getRacks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] getRacks()\n{\r\n    return racks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getEarlierAttemptFailed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getEarlierAttemptFailed()\n{\r\n    return earlierAttemptFailed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\event",
  "methodName" : "getUpdatedNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<NodeReport> getUpdatedNodes()\n{\r\n    return updatedNodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
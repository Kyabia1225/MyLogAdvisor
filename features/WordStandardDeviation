[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "readAndCalcStdDev",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "double readAndCalcStdDev(Path path, Configuration conf) throws IOException\n{\r\n    FileSystem fs = FileSystem.get(conf);\r\n    Path file = new Path(path, \"part-r-00000\");\r\n    if (!fs.exists(file))\r\n        throw new IOException(\"Output not found!\");\r\n    double stddev = 0;\r\n    BufferedReader br = null;\r\n    try {\r\n        br = new BufferedReader(new InputStreamReader(fs.open(file), Charsets.UTF_8));\r\n        long count = 0;\r\n        long length = 0;\r\n        long square = 0;\r\n        String line;\r\n        while ((line = br.readLine()) != null) {\r\n            StringTokenizer st = new StringTokenizer(line);\r\n            String type = st.nextToken();\r\n            if (type.equals(COUNT.toString())) {\r\n                String countLit = st.nextToken();\r\n                count = Long.parseLong(countLit);\r\n            } else if (type.equals(LENGTH.toString())) {\r\n                String lengthLit = st.nextToken();\r\n                length = Long.parseLong(lengthLit);\r\n            } else if (type.equals(SQUARE.toString())) {\r\n                String squareLit = st.nextToken();\r\n                square = Long.parseLong(squareLit);\r\n            }\r\n        }\r\n        double mean = (((double) length) / ((double) count));\r\n        mean = Math.pow(mean, 2.0);\r\n        double term = (((double) square / ((double) count)));\r\n        stddev = Math.sqrt((term - mean));\r\n        System.out.println(\"The standard deviation is: \" + stddev);\r\n    } finally {\r\n        if (br != null) {\r\n            br.close();\r\n        }\r\n    }\r\n    return stddev;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    ToolRunner.run(new Configuration(), new WordStandardDeviation(), args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length != 2) {\r\n        System.err.println(\"Usage: wordstddev <in> <out>\");\r\n        return 0;\r\n    }\r\n    Configuration conf = getConf();\r\n    Job job = Job.getInstance(conf, \"word stddev\");\r\n    job.setJarByClass(WordStandardDeviation.class);\r\n    job.setMapperClass(WordStandardDeviationMapper.class);\r\n    job.setCombinerClass(WordStandardDeviationReducer.class);\r\n    job.setReducerClass(WordStandardDeviationReducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(LongWritable.class);\r\n    FileInputFormat.addInputPath(job, new Path(args[0]));\r\n    Path outputpath = new Path(args[1]);\r\n    FileOutputFormat.setOutputPath(job, outputpath);\r\n    boolean result = job.waitForCompletion(true);\r\n    stddev = readAndCalcStdDev(outputpath, conf);\r\n    return (result ? 0 : 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "getStandardDeviation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getStandardDeviation()\n{\r\n    return stddev;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length != DistSum.Parameters.COUNT + 1)\r\n        return Util.printUsage(args, getClass().getName() + \" <b> \" + Parameters.LIST + \"\\n  <b> The number of bits to skip, i.e. compute the (b+1)th position.\" + Parameters.DESCRIPTION);\r\n    int i = 0;\r\n    final long b = Util.string2long(args[i++]);\r\n    final DistSum.Parameters parameters = DistSum.Parameters.parse(args, i);\r\n    if (b < 0)\r\n        throw new IllegalArgumentException(\"b = \" + b + \" < 0\");\r\n    Util.printBitSkipped(b);\r\n    Util.out.println(parameters);\r\n    Util.out.println();\r\n    final DistSum distsum = new DistSum();\r\n    distsum.setConf(getConf());\r\n    distsum.setParameters(parameters);\r\n    final boolean isVerbose = getConf().getBoolean(Parser.VERBOSE_PROPERTY, false);\r\n    final Map<Parameter, List<TaskResult>> existings = new Parser(isVerbose).parse(parameters.localDir.getPath(), null);\r\n    Parser.combine(existings);\r\n    for (List<TaskResult> tr : existings.values()) Collections.sort(tr);\r\n    Util.out.println();\r\n    final Map<Bellard.Parameter, Bellard.Sum> sums = Bellard.getSums(b, parameters.nJobs, existings);\r\n    Util.out.println();\r\n    execute(distsum, sums);\r\n    final double pi = Bellard.computePi(b, sums);\r\n    Util.printBitSkipped(b);\r\n    Util.out.println(Util.pi2string(pi, Bellard.bit2terms(b)));\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "execute",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void execute(DistSum distsum, final Map<Bellard.Parameter, Bellard.Sum> sums) throws Exception\n{\r\n    final List<Computation> computations = new ArrayList<Computation>();\r\n    int i = 0;\r\n    for (Bellard.Parameter p : Bellard.Parameter.values()) for (Summation s : sums.get(p)) if (s.getValue() == null)\r\n        computations.add(distsum.new Computation(i++, p.toString(), s));\r\n    if (computations.isEmpty())\r\n        Util.out.println(\"No computation\");\r\n    else {\r\n        timer.tick(\"execute \" + computations.size() + \" computation(s)\");\r\n        Util.execute(distsum.getParameters().nThreads, computations);\r\n        timer.tick(\"done\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    System.exit(ToolRunner.run(null, new DistBbp(), args));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "mod",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long mod(long e, long n)\n{\r\n    final int HALF = (63 - Long.numberOfLeadingZeros(n)) >> 1;\r\n    final int FULL = HALF << 1;\r\n    final long ONES = (1 << HALF) - 1;\r\n    long r = 2;\r\n    for (long mask = Long.highestOneBit(e) >> 1; mask > 0; mask >>= 1) {\r\n        if (r <= MAX_SQRT_LONG) {\r\n            r *= r;\r\n            if (r >= n)\r\n                r %= n;\r\n        } else {\r\n            final long high = r >>> HALF;\r\n            final long low = r &= ONES;\r\n            r *= r;\r\n            if (r >= n)\r\n                r %= n;\r\n            if (high != 0) {\r\n                long s = high * high;\r\n                if (s >= n)\r\n                    s %= n;\r\n                for (int i = 0; i < FULL; i++) if ((s <<= 1) >= n)\r\n                    s -= n;\r\n                if (low == 0)\r\n                    r = s;\r\n                else {\r\n                    long t = high * low;\r\n                    if (t >= n)\r\n                        t %= n;\r\n                    for (int i = -1; i < HALF; i++) if ((t <<= 1) >= n)\r\n                        t -= n;\r\n                    r += s;\r\n                    if (r >= n)\r\n                        r -= n;\r\n                    r += t;\r\n                    if (r >= n)\r\n                        r -= n;\r\n                }\r\n            }\r\n        }\r\n        if ((e & mask) != 0) {\r\n            r <<= 1;\r\n            if (r >= n)\r\n                r -= n;\r\n        }\r\n    }\r\n    return r;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "addMod",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double addMod(double x, final double a)\n{\r\n    x += a;\r\n    return x >= 1 ? x - 1 : x < 0 ? x + 1 : x;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "modInverse",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long modInverse(final long x, final long y)\n{\r\n    if (x == 1)\r\n        return 1;\r\n    long a = 1;\r\n    long b = 0;\r\n    long c = x;\r\n    long u = 0;\r\n    long v = 1;\r\n    long w = y;\r\n    for (; ; ) {\r\n        {\r\n            final long q = w / c;\r\n            w -= q * c;\r\n            u -= q * a;\r\n            if (w == 1)\r\n                return u > 0 ? u : u + y;\r\n            v -= q * b;\r\n        }\r\n        {\r\n            final long q = c / w;\r\n            c -= q * w;\r\n            a -= q * u;\r\n            if (c == 1)\r\n                return a > 0 ? a : a + y;\r\n            b -= q * v;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "usage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void usage() throws IOException\n{\r\n    System.err.println(\"terasum <out-dir> <report-dir>\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    Job job = Job.getInstance(getConf());\r\n    if (args.length != 2) {\r\n        usage();\r\n        return 2;\r\n    }\r\n    TeraInputFormat.setInputPaths(job, new Path(args[0]));\r\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\r\n    job.setJobName(\"TeraSum\");\r\n    job.setJarByClass(TeraChecksum.class);\r\n    job.setMapperClass(ChecksumMapper.class);\r\n    job.setReducerClass(ChecksumReducer.class);\r\n    job.setOutputKeyClass(NullWritable.class);\r\n    job.setOutputValueClass(Unsigned16.class);\r\n    job.setNumReduceTasks(1);\r\n    job.setInputFormatClass(TeraInputFormat.class);\r\n    return job.waitForCompletion(true) ? 0 : 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new TeraChecksum(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "print",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void print(PrintWriter out, Iterator<T> iterator, String prefix, String format, int elementsPerGroup, int groupsPerLine)\n{\r\n    final StringBuilder sb = new StringBuilder(\"\\n\");\r\n    for (int i = 0; i < prefix.length(); i++) sb.append(\" \");\r\n    final String spaces = sb.toString();\r\n    out.print(\"\\n\" + prefix);\r\n    for (int i = 0; iterator.hasNext(); i++) {\r\n        if (i > 0 && i % elementsPerGroup == 0)\r\n            out.print((i / elementsPerGroup) % groupsPerLine == 0 ? spaces : \" \");\r\n        out.print(String.format(format, iterator.next()));\r\n    }\r\n    out.println();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "createJob",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "Job createJob(String name, Configuration conf) throws IOException\n{\r\n    final Job job = Job.getInstance(conf, NAME + \"_\" + name);\r\n    final Configuration jobconf = job.getConfiguration();\r\n    job.setJarByClass(BaileyBorweinPlouffe.class);\r\n    job.setMapperClass(BbpMapper.class);\r\n    job.setMapOutputKeyClass(LongWritable.class);\r\n    job.setMapOutputValueClass(BytesWritable.class);\r\n    job.setReducerClass(BbpReducer.class);\r\n    job.setOutputKeyClass(LongWritable.class);\r\n    job.setOutputValueClass(BytesWritable.class);\r\n    job.setNumReduceTasks(1);\r\n    job.setInputFormatClass(BbpInputFormat.class);\r\n    jobconf.setLong(MRJobConfig.TASK_TIMEOUT, 0);\r\n    jobconf.setBoolean(MRJobConfig.MAP_SPECULATIVE, false);\r\n    jobconf.setBoolean(MRJobConfig.REDUCE_SPECULATIVE, false);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "compute",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void compute(int startDigit, int nDigits, int nMaps, String workingDir, Configuration conf, PrintStream out) throws IOException\n{\r\n    final String name = startDigit + \"_\" + nDigits;\r\n    out.println(\"Working Directory = \" + workingDir);\r\n    out.println();\r\n    final FileSystem fs = FileSystem.get(conf);\r\n    final Path dir = fs.makeQualified(new Path(workingDir));\r\n    if (fs.exists(dir)) {\r\n        throw new IOException(\"Working directory \" + dir + \" already exists.  Please remove it first.\");\r\n    } else if (!fs.mkdirs(dir)) {\r\n        throw new IOException(\"Cannot create working directory \" + dir);\r\n    }\r\n    out.println(\"Start Digit      = \" + startDigit);\r\n    out.println(\"Number of Digits = \" + nDigits);\r\n    out.println(\"Number of Maps   = \" + nMaps);\r\n    final Job job = createJob(name, conf);\r\n    final Path hexfile = new Path(dir, \"pi_\" + name + \".hex\");\r\n    FileOutputFormat.setOutputPath(job, new Path(dir, \"out\"));\r\n    job.getConfiguration().set(WORKING_DIR_PROPERTY, dir.toString());\r\n    job.getConfiguration().set(HEX_FILE_PROPERTY, hexfile.toString());\r\n    job.getConfiguration().setInt(DIGIT_START_PROPERTY, startDigit);\r\n    job.getConfiguration().setInt(DIGIT_SIZE_PROPERTY, nDigits);\r\n    job.getConfiguration().setInt(DIGIT_PARTS_PROPERTY, nMaps);\r\n    out.println(\"\\nStarting Job ...\");\r\n    final long startTime = Time.monotonicNow();\r\n    try {\r\n        if (!job.waitForCompletion(true)) {\r\n            out.println(\"Job failed.\");\r\n            System.exit(1);\r\n        }\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    } finally {\r\n        final double duration = (Time.monotonicNow() - startTime) / 1000.0;\r\n        out.println(\"Duration is \" + duration + \" seconds.\");\r\n    }\r\n    out.println(\"Output file: \" + hexfile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int run(String[] args) throws IOException\n{\r\n    if (args.length != 4) {\r\n        System.err.println(\"Usage: bbp \" + \" <startDigit> <nDigits> <nMaps> <workingDir>\");\r\n        ToolRunner.printGenericCommandUsage(System.err);\r\n        return -1;\r\n    }\r\n    final int startDigit = Integer.parseInt(args[0]);\r\n    final int nDigits = Integer.parseInt(args[1]);\r\n    final int nMaps = Integer.parseInt(args[2]);\r\n    final String workingDir = args[3];\r\n    if (startDigit <= 0) {\r\n        throw new IllegalArgumentException(\"startDigit = \" + startDigit + \" <= 0\");\r\n    } else if (nDigits <= 0) {\r\n        throw new IllegalArgumentException(\"nDigits = \" + nDigits + \" <= 0\");\r\n    } else if (nDigits % BBP_HEX_DIGITS != 0) {\r\n        throw new IllegalArgumentException(\"nDigits = \" + nDigits + \" is not a multiple of \" + BBP_HEX_DIGITS);\r\n    } else if (nDigits - 1L + startDigit > IMPLEMENTATION_LIMIT + BBP_HEX_DIGITS) {\r\n        throw new UnsupportedOperationException(\"nDigits - 1 + startDigit = \" + (nDigits - 1L + startDigit) + \" > IMPLEMENTATION_LIMIT + BBP_HEX_DIGITS,\" + \", where IMPLEMENTATION_LIMIT=\" + IMPLEMENTATION_LIMIT + \"and BBP_HEX_DIGITS=\" + BBP_HEX_DIGITS);\r\n    } else if (nMaps <= 0) {\r\n        throw new IllegalArgumentException(\"nMaps = \" + nMaps + \" <= 0\");\r\n    }\r\n    compute(startDigit, nDigits, nMaps, workingDir, getConf(), System.out);\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    System.exit(ToolRunner.run(null, new BaileyBorweinPlouffe(), argv));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "hexDigits",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long hexDigits(final long d)\n{\r\n    if (d < 0) {\r\n        throw new IllegalArgumentException(\"d = \" + d + \" < 0\");\r\n    } else if (d > IMPLEMENTATION_LIMIT) {\r\n        throw new IllegalArgumentException(\"d = \" + d + \" > IMPLEMENTATION_LIMIT = \" + IMPLEMENTATION_LIMIT);\r\n    }\r\n    final double s1 = sum(1, d);\r\n    final double s4 = sum(4, d);\r\n    final double s5 = sum(5, d);\r\n    final double s6 = sum(6, d);\r\n    double pi = s1 + s1;\r\n    if (pi >= 1)\r\n        pi--;\r\n    pi *= 2;\r\n    if (pi >= 1)\r\n        pi--;\r\n    pi -= s4;\r\n    if (pi < 0)\r\n        pi++;\r\n    pi -= s4;\r\n    if (pi < 0)\r\n        pi++;\r\n    pi -= s5;\r\n    if (pi < 0)\r\n        pi++;\r\n    pi -= s6;\r\n    if (pi < 0)\r\n        pi++;\r\n    return (long) (pi * BBP_MULTIPLIER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "sum",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double sum(final long j, final long d)\n{\r\n    long k = j == 1 ? 1 : 0;\r\n    double s = 0;\r\n    if (k <= d) {\r\n        s = 1.0 / ((d << 3) | j);\r\n        for (; k < d; k++) {\r\n            final long n = (k << 3) | j;\r\n            s += mod((d - k) << 2, n) * 1.0 / n;\r\n            if (s >= 1)\r\n                s--;\r\n        }\r\n        k++;\r\n    }\r\n    if (k >= 1L << (ACCURACY_BIT - 7))\r\n        return s;\r\n    for (; ; k++) {\r\n        final long n = (k << 3) | j;\r\n        final long shift = (k - d) << 2;\r\n        if (ACCURACY_BIT <= shift || 1L << (ACCURACY_BIT - shift) < n) {\r\n            return s;\r\n        }\r\n        s += 1.0 / (n << shift);\r\n        if (s >= 1)\r\n            s--;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "mod",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long mod(final long e, final long n)\n{\r\n    long mask = (e & 0xFFFFFFFF00000000L) == 0 ? 0x00000000FFFFFFFFL : 0xFFFFFFFF00000000L;\r\n    mask &= (e & 0xFFFF0000FFFF0000L & mask) == 0 ? 0x0000FFFF0000FFFFL : 0xFFFF0000FFFF0000L;\r\n    mask &= (e & 0xFF00FF00FF00FF00L & mask) == 0 ? 0x00FF00FF00FF00FFL : 0xFF00FF00FF00FF00L;\r\n    mask &= (e & 0xF0F0F0F0F0F0F0F0L & mask) == 0 ? 0x0F0F0F0F0F0F0F0FL : 0xF0F0F0F0F0F0F0F0L;\r\n    mask &= (e & 0xCCCCCCCCCCCCCCCCL & mask) == 0 ? 0x3333333333333333L : 0xCCCCCCCCCCCCCCCCL;\r\n    mask &= (e & 0xAAAAAAAAAAAAAAAAL & mask) == 0 ? 0x5555555555555555L : 0xAAAAAAAAAAAAAAAAL;\r\n    long r = 2;\r\n    for (mask >>= 1; mask > 0; mask >>= 1) {\r\n        r *= r;\r\n        r %= n;\r\n        if ((e & mask) != 0) {\r\n            r += r;\r\n            if (r >= n)\r\n                r -= n;\r\n        }\r\n    }\r\n    return r;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "partition",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int[] partition(final int offset, final int size, final int nParts)\n{\r\n    final int[] parts = new int[nParts];\r\n    final long total = workload(offset, size);\r\n    final int remainder = offset % 4;\r\n    parts[0] = offset;\r\n    for (int i = 1; i < nParts; i++) {\r\n        final long target = offset + i * (total / nParts) + i * (total % nParts) / nParts;\r\n        int low = parts[i - 1];\r\n        int high = offset + size;\r\n        for (; high > low + 4; ) {\r\n            final int mid = (high + low - 2 * remainder) / 8 * 4 + remainder;\r\n            final long midvalue = workload(mid);\r\n            if (midvalue == target)\r\n                high = low = mid;\r\n            else if (midvalue > target)\r\n                high = mid;\r\n            else\r\n                low = mid;\r\n        }\r\n        parts[i] = high == low ? high : workload(high) - target > target - workload(low) ? low : high;\r\n    }\r\n    return parts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "workload",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long workload(final long n)\n{\r\n    if (n < 0) {\r\n        throw new IllegalArgumentException(\"n = \" + n + \" < 0\");\r\n    } else if (n > MAX_N) {\r\n        throw new IllegalArgumentException(\"n = \" + n + \" > MAX_N = \" + MAX_N);\r\n    }\r\n    return (n & 1L) == 0L ? (n >> 1) * (n + 1) : n * ((n + 1) >> 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "workload",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long workload(long offset, long size)\n{\r\n    return workload(offset + size) - workload(offset);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "readFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<String> readFile(String filename) throws IOException\n{\r\n    List<String> result = new ArrayList<String>(10000);\r\n    try (BufferedReader in = new BufferedReader(new InputStreamReader(new FileInputStream(filename), Charsets.UTF_8))) {\r\n        String line = in.readLine();\r\n        while (line != null) {\r\n            result.add(line);\r\n            line = in.readLine();\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "pickBestHost",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Host pickBestHost()\n{\r\n    Host result = null;\r\n    int splits = Integer.MAX_VALUE;\r\n    for (Host host : hosts) {\r\n        if (host.splits.size() < splits) {\r\n            result = host;\r\n            splits = host.splits.size();\r\n        }\r\n    }\r\n    if (result != null) {\r\n        hosts.remove(result);\r\n        LOG.debug(\"picking \" + result);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "pickBestSplits",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void pickBestSplits(Host host)\n{\r\n    int tasksToPick = Math.min(slotsPerHost, (int) Math.ceil((double) remainingSplits / hosts.size()));\r\n    Split[] best = new Split[tasksToPick];\r\n    for (Split cur : host.splits) {\r\n        LOG.debug(\"  examine: \" + cur.filename + \" \" + cur.locations.size());\r\n        int i = 0;\r\n        while (i < tasksToPick && best[i] != null && best[i].locations.size() <= cur.locations.size()) {\r\n            i += 1;\r\n        }\r\n        if (i < tasksToPick) {\r\n            for (int j = tasksToPick - 1; j > i; --j) {\r\n                best[j] = best[j - 1];\r\n            }\r\n            best[i] = cur;\r\n        }\r\n    }\r\n    for (int i = 0; i < tasksToPick; ++i) {\r\n        if (best[i] != null) {\r\n            LOG.debug(\" best: \" + best[i].filename);\r\n            for (Host other : best[i].locations) {\r\n                other.splits.remove(best[i]);\r\n            }\r\n            best[i].locations.clear();\r\n            best[i].locations.add(host);\r\n            best[i].isAssigned = true;\r\n            remainingSplits -= 1;\r\n        }\r\n    }\r\n    for (Split cur : host.splits) {\r\n        if (!cur.isAssigned) {\r\n            cur.locations.remove(host);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "solve",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void solve() throws IOException\n{\r\n    Host host = pickBestHost();\r\n    while (host != null) {\r\n        pickBestSplits(host);\r\n        host = pickBestHost();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "getNewFileSplits",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "List<InputSplit> getNewFileSplits() throws IOException\n{\r\n    solve();\r\n    FileSplit[] result = new FileSplit[realSplits.length];\r\n    int left = 0;\r\n    int right = realSplits.length - 1;\r\n    for (int i = 0; i < splits.length; ++i) {\r\n        if (splits[i].isAssigned) {\r\n            String[] newLocations = { splits[i].locations.get(0).hostname };\r\n            realSplits[i] = new FileSplit(realSplits[i].getPath(), realSplits[i].getStart(), realSplits[i].getLength(), newLocations);\r\n            result[left++] = realSplits[i];\r\n        } else {\r\n            result[right--] = realSplits[i];\r\n        }\r\n    }\r\n    List<InputSplit> ret = new ArrayList<InputSplit>();\r\n    for (FileSplit fs : result) {\r\n        ret.add(fs);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    TeraScheduler problem = new TeraScheduler(\"block-loc.txt\", \"nodes\");\r\n    for (Host host : problem.hosts) {\r\n        System.out.println(host);\r\n    }\r\n    LOG.info(\"starting solve\");\r\n    problem.solve();\r\n    List<Split> leftOvers = new ArrayList<Split>();\r\n    for (int i = 0; i < problem.splits.length; ++i) {\r\n        if (problem.splits[i].isAssigned) {\r\n            System.out.println(\"sched: \" + problem.splits[i]);\r\n        } else {\r\n            leftOvers.add(problem.splits[i]);\r\n        }\r\n    }\r\n    for (Split cur : leftOvers) {\r\n        System.out.println(\"left: \" + cur);\r\n    }\r\n    System.out.println(\"left over: \" + leftOvers.size());\r\n    LOG.info(\"done\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "initializePieces",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void initializePieces()\n{\r\n    pieces.add(new Piece(\"x\", \" x /xxx/ x \", false, oneRotation));\r\n    pieces.add(new Piece(\"v\", \"x  /x  /xxx\", false, fourRotations));\r\n    pieces.add(new Piece(\"t\", \"xxx/ x / x \", false, fourRotations));\r\n    pieces.add(new Piece(\"w\", \"  x/ xx/xx \", false, fourRotations));\r\n    pieces.add(new Piece(\"u\", \"x x/xxx\", false, fourRotations));\r\n    pieces.add(new Piece(\"i\", \"xxxxx\", false, twoRotations));\r\n    pieces.add(new Piece(\"f\", \" xx/xx / x \", false, fourRotations));\r\n    pieces.add(new Piece(\"p\", \"xx/xx/x \", false, fourRotations));\r\n    pieces.add(new Piece(\"z\", \"xx / x / xx\", false, twoRotations));\r\n    pieces.add(new Piece(\"n\", \"xx  / xxx\", false, fourRotations));\r\n    pieces.add(new Piece(\"y\", \"  x /xxxx\", false, fourRotations));\r\n    pieces.add(new Piece(\"l\", \"   x/xxxx\", false, fourRotations));\r\n    pieces.add(new Piece(\"F\", \"xx / xx/ x \", false, fourRotations));\r\n    pieces.add(new Piece(\"P\", \"xx/xx/ x\", false, fourRotations));\r\n    pieces.add(new Piece(\"Z\", \" xx/ x /xx \", false, twoRotations));\r\n    pieces.add(new Piece(\"N\", \"  xx/xxx \", false, fourRotations));\r\n    pieces.add(new Piece(\"Y\", \" x  /xxxx\", false, fourRotations));\r\n    pieces.add(new Piece(\"L\", \"x   /xxxx\", false, fourRotations));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    Pentomino model = new OneSidedPentomino(3, 30);\r\n    int solutions = model.solve();\r\n    System.out.println(solutions + \" solutions found.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "startHsqldbServer",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void startHsqldbServer()\n{\r\n    server = new Server();\r\n    server.setDatabasePath(0, System.getProperty(\"test.build.data\", \"/tmp\") + \"/URLAccess\");\r\n    server.setDatabaseName(0, \"URLAccess\");\r\n    server.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "createConnection",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createConnection(String driverClassName, String url) throws Exception\n{\r\n    if (StringUtils.toLowerCase(driverClassName).contains(\"oracle\")) {\r\n        isOracle = true;\r\n    }\r\n    Class.forName(driverClassName);\r\n    connection = DriverManager.getConnection(url);\r\n    connection.setAutoCommit(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "shutdown",
  "errType" : [ "Throwable", "Throwable" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void shutdown()\n{\r\n    try {\r\n        connection.commit();\r\n        connection.close();\r\n    } catch (Throwable ex) {\r\n        LOG.warn(\"Exception occurred while closing connection :\" + StringUtils.stringifyException(ex));\r\n    } finally {\r\n        try {\r\n            if (server != null) {\r\n                server.shutdown();\r\n            }\r\n        } catch (Throwable ex) {\r\n            LOG.warn(\"Exception occurred while shutting down HSQLDB :\" + StringUtils.stringifyException(ex));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void initialize(String driverClassName, String url) throws Exception\n{\r\n    if (!this.initialized) {\r\n        if (driverClassName.equals(DRIVER_CLASS)) {\r\n            startHsqldbServer();\r\n        }\r\n        createConnection(driverClassName, url);\r\n        dropTables();\r\n        createTables();\r\n        populateAccess();\r\n        this.initialized = true;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "dropTables",
  "errType" : [ "SQLException", "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void dropTables()\n{\r\n    String dropAccess = \"DROP TABLE HAccess\";\r\n    String dropPageview = \"DROP TABLE Pageview\";\r\n    Statement st = null;\r\n    try {\r\n        st = connection.createStatement();\r\n        st.executeUpdate(dropAccess);\r\n        st.executeUpdate(dropPageview);\r\n        connection.commit();\r\n        st.close();\r\n    } catch (SQLException ex) {\r\n        try {\r\n            if (st != null) {\r\n                st.close();\r\n            }\r\n        } catch (Exception e) {\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "createTables",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createTables() throws SQLException\n{\r\n    String dataType = \"BIGINT NOT NULL\";\r\n    if (isOracle) {\r\n        dataType = \"NUMBER(19) NOT NULL\";\r\n    }\r\n    String createAccess = \"CREATE TABLE \" + \"HAccess(url      VARCHAR(100) NOT NULL,\" + \" referrer VARCHAR(100),\" + \" time     \" + dataType + \", \" + \" PRIMARY KEY (url, time))\";\r\n    String createPageview = \"CREATE TABLE \" + \"Pageview(url      VARCHAR(100) NOT NULL,\" + \" pageview     \" + dataType + \", \" + \" PRIMARY KEY (url))\";\r\n    Statement st = connection.createStatement();\r\n    try {\r\n        st.executeUpdate(createAccess);\r\n        st.executeUpdate(createPageview);\r\n        connection.commit();\r\n    } finally {\r\n        st.close();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "populateAccess",
  "errType" : [ "SQLException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void populateAccess() throws SQLException\n{\r\n    PreparedStatement statement = null;\r\n    try {\r\n        statement = connection.prepareStatement(\"INSERT INTO HAccess(url, referrer, time)\" + \" VALUES (?, ?, ?)\");\r\n        Random random = new Random();\r\n        int time = random.nextInt(50) + 50;\r\n        final int PROBABILITY_PRECISION = 100;\r\n        final int NEW_PAGE_PROBABILITY = 15;\r\n        String[] pages = { \"/a\", \"/b\", \"/c\", \"/d\", \"/e\", \"/f\", \"/g\", \"/h\", \"/i\", \"/j\" };\r\n        int[][] linkMatrix = { { 1, 5, 7 }, { 0, 7, 4, 6 }, { 0, 1, 7, 8 }, { 0, 2, 4, 6, 7, 9 }, { 0, 1 }, { 0, 3, 5, 9 }, { 0 }, { 0, 1, 3 }, { 0, 2, 6 }, { 0, 2, 6 } };\r\n        int currentPage = random.nextInt(pages.length);\r\n        String referrer = null;\r\n        for (int i = 0; i < time; i++) {\r\n            statement.setString(1, pages[currentPage]);\r\n            statement.setString(2, referrer);\r\n            statement.setLong(3, i);\r\n            statement.execute();\r\n            int action = random.nextInt(PROBABILITY_PRECISION);\r\n            if (action < NEW_PAGE_PROBABILITY) {\r\n                currentPage = random.nextInt(pages.length);\r\n                referrer = null;\r\n            } else {\r\n                referrer = pages[currentPage];\r\n                action = random.nextInt(linkMatrix[currentPage].length);\r\n                currentPage = linkMatrix[currentPage][action];\r\n            }\r\n        }\r\n        connection.commit();\r\n    } catch (SQLException ex) {\r\n        connection.rollback();\r\n        throw ex;\r\n    } finally {\r\n        if (statement != null) {\r\n            statement.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "verify",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "boolean verify() throws SQLException\n{\r\n    String countAccessQuery = \"SELECT COUNT(*) FROM HAccess\";\r\n    String sumPageviewQuery = \"SELECT SUM(pageview) FROM Pageview\";\r\n    Statement st = null;\r\n    ResultSet rs = null;\r\n    try {\r\n        st = connection.createStatement();\r\n        rs = st.executeQuery(countAccessQuery);\r\n        rs.next();\r\n        long totalPageview = rs.getLong(1);\r\n        rs = st.executeQuery(sumPageviewQuery);\r\n        rs.next();\r\n        long sumPageview = rs.getLong(1);\r\n        LOG.info(\"totalPageview=\" + totalPageview);\r\n        LOG.info(\"sumPageview=\" + sumPageview);\r\n        return totalPageview == sumPageview && totalPageview != 0;\r\n    } finally {\r\n        if (st != null)\r\n            st.close();\r\n        if (rs != null)\r\n            rs.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    String driverClassName = DRIVER_CLASS;\r\n    String url = DB_URL;\r\n    if (args.length > 1) {\r\n        driverClassName = args[0];\r\n        url = args[1];\r\n    }\r\n    initialize(driverClassName, url);\r\n    Configuration conf = getConf();\r\n    DBConfiguration.configureDB(conf, driverClassName, url);\r\n    Job job = Job.getInstance(conf);\r\n    job.setJobName(\"Count Pageviews of URLs\");\r\n    job.setJarByClass(DBCountPageView.class);\r\n    job.setMapperClass(PageviewMapper.class);\r\n    job.setCombinerClass(LongSumReducer.class);\r\n    job.setReducerClass(PageviewReducer.class);\r\n    DBInputFormat.setInput(job, AccessRecord.class, \"HAccess\", null, \"url\", AccessFieldNames);\r\n    DBOutputFormat.setOutput(job, \"Pageview\", PageviewFieldNames);\r\n    job.setMapOutputKeyClass(Text.class);\r\n    job.setMapOutputValueClass(LongWritable.class);\r\n    job.setOutputKeyClass(PageviewRecord.class);\r\n    job.setOutputValueClass(NullWritable.class);\r\n    int ret;\r\n    try {\r\n        ret = job.waitForCompletion(true) ? 0 : 1;\r\n        boolean correct = verify();\r\n        if (!correct) {\r\n            throw new RuntimeException(\"Evaluation was not correct!\");\r\n        }\r\n    } finally {\r\n        shutdown();\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int ret = ToolRunner.run(new DBCountPageView(), args);\r\n    System.exit(ret);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "println",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void println(String s)\n{\r\n    if (isVerbose)\r\n        Util.out.println(s);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "parseLine",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void parseLine(final String line, Map<Parameter, List<TaskResult>> m)\n{\r\n    final Map.Entry<String, TaskResult> e = DistSum.string2TaskResult(line);\r\n    if (e != null) {\r\n        final List<TaskResult> sums = m.get(Parameter.get(e.getKey()));\r\n        if (sums == null)\r\n            throw new IllegalArgumentException(\"sums == null, line=\" + line + \", e=\" + e);\r\n        sums.add(e.getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "parse",
  "errType" : [ "RuntimeException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void parse(File f, Map<Parameter, List<TaskResult>> sums) throws IOException\n{\r\n    if (f.isDirectory()) {\r\n        println(\"Process directory \" + f);\r\n        File[] files = f.listFiles();\r\n        if (files != null) {\r\n            for (File child : files) {\r\n                parse(child, sums);\r\n            }\r\n        }\r\n    } else if (f.getName().endsWith(\".txt\")) {\r\n        println(\"Parse file \" + f);\r\n        final Map<Parameter, List<TaskResult>> m = new TreeMap<Parameter, List<TaskResult>>();\r\n        for (Parameter p : Parameter.values()) m.put(p, new ArrayList<TaskResult>());\r\n        final BufferedReader in = new BufferedReader(new InputStreamReader(new FileInputStream(f), Charsets.UTF_8));\r\n        try {\r\n            for (String line; (line = in.readLine()) != null; ) try {\r\n                parseLine(line, m);\r\n            } catch (RuntimeException e) {\r\n                Util.err.println(\"line = \" + line);\r\n                throw e;\r\n            }\r\n        } finally {\r\n            in.close();\r\n        }\r\n        for (Parameter p : Parameter.values()) {\r\n            final List<TaskResult> combined = Util.combine(m.get(p));\r\n            if (!combined.isEmpty()) {\r\n                println(p + \" (size=\" + combined.size() + \"):\");\r\n                for (TaskResult r : combined) println(\"  \" + r);\r\n            }\r\n            sums.get(p).addAll(m.get(p));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Map<Parameter, List<TaskResult>> parse(String f) throws IOException\n{\r\n    final Map<Parameter, List<TaskResult>> m = new TreeMap<Parameter, List<TaskResult>>();\r\n    for (Parameter p : Parameter.values()) m.put(p, new ArrayList<TaskResult>());\r\n    parse(new File(f), m);\r\n    for (Parameter p : Parameter.values()) m.put(p, m.get(p));\r\n    return m;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "Map<Parameter, List<TaskResult>> parse(String inputpath, String outputdir) throws IOException\n{\r\n    Util.out.print(\"\\nParsing \" + inputpath + \" ... \");\r\n    Util.out.flush();\r\n    final Map<Parameter, List<TaskResult>> parsed = parse(inputpath);\r\n    Util.out.println(\"DONE\");\r\n    if (outputdir != null) {\r\n        Util.out.print(\"\\nWriting to \" + outputdir + \" ...\");\r\n        Util.out.flush();\r\n        for (Parameter p : Parameter.values()) {\r\n            final List<TaskResult> results = parsed.get(p);\r\n            Collections.sort(results);\r\n            final PrintWriter out = new PrintWriter(new OutputStreamWriter(new FileOutputStream(new File(outputdir, p + \".txt\")), Charsets.UTF_8), true);\r\n            try {\r\n                for (int i = 0; i < results.size(); i++) out.println(DistSum.taskResult2string(p + \".\" + i, results.get(i)));\r\n            } finally {\r\n                out.close();\r\n            }\r\n        }\r\n        Util.out.println(\"DONE\");\r\n    }\r\n    return parsed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "combine",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Map<Parameter, T> combine(Map<Parameter, List<T>> m)\n{\r\n    final Map<Parameter, T> combined = new TreeMap<Parameter, T>();\r\n    for (Parameter p : Parameter.values()) {\r\n        final List<T> results = Util.combine(m.get(p));\r\n        Util.out.format(\"%-6s => \", p);\r\n        if (results.size() != 1)\r\n            Util.out.println(results.toString().replace(\", \", \",\\n           \"));\r\n        else {\r\n            final T r = results.get(0);\r\n            combined.put(p, r);\r\n            Util.out.println(r);\r\n        }\r\n    }\r\n    return combined;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    if (args.length < 2 || args.length > 3)\r\n        Util.printUsage(args, Parser.class.getName() + \" <b> <inputpath> [<outputdir>]\");\r\n    int i = 0;\r\n    final long b = Util.string2long(args[i++]);\r\n    final String inputpath = args[i++];\r\n    final String outputdir = args.length >= 3 ? args[i++] : null;\r\n    final Map<Parameter, List<TaskResult>> parsed = new Parser(true).parse(inputpath, outputdir);\r\n    final Map<Parameter, TaskResult> combined = combine(parsed);\r\n    long duration = 0;\r\n    for (TaskResult r : combined.values()) duration += r.getDuration();\r\n    final double pi = Bellard.computePi(b, combined);\r\n    Util.printBitSkipped(b);\r\n    Util.out.println(Util.pi2string(pi, Bellard.bit2terms(b)));\r\n    Util.out.println(\"cpu time = \" + Util.millis2String(duration));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "stringifySolution",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String stringifySolution(int size, List<List<ColumnName>> solution)\n{\r\n    int[][] picture = new int[size][size];\r\n    StringBuffer result = new StringBuffer();\r\n    for (List<ColumnName> row : solution) {\r\n        int x = -1;\r\n        int y = -1;\r\n        int num = -1;\r\n        for (ColumnName item : row) {\r\n            if (item instanceof ColumnConstraint) {\r\n                x = ((ColumnConstraint) item).column;\r\n                num = ((ColumnConstraint) item).num;\r\n            } else if (item instanceof RowConstraint) {\r\n                y = ((RowConstraint) item).row;\r\n            }\r\n        }\r\n        picture[y][x] = num;\r\n    }\r\n    for (int y = 0; y < size; ++y) {\r\n        for (int x = 0; x < size; ++x) {\r\n            result.append(picture[y][x]);\r\n            result.append(\" \");\r\n        }\r\n        result.append(\"\\n\");\r\n    }\r\n    return result.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "generateRow",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean[] generateRow(boolean[] rowValues, int x, int y, int num)\n{\r\n    for (int i = 0; i < rowValues.length; ++i) {\r\n        rowValues[i] = false;\r\n    }\r\n    int xBox = x / squareXSize;\r\n    int yBox = y / squareYSize;\r\n    rowValues[x * size + num - 1] = true;\r\n    rowValues[size * size + y * size + num - 1] = true;\r\n    rowValues[2 * size * size + (xBox * squareXSize + yBox) * size + num - 1] = true;\r\n    rowValues[3 * size * size + size * x + y] = true;\r\n    return rowValues;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "makeModel",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "DancingLinks<ColumnName> makeModel()\n{\r\n    DancingLinks<ColumnName> model = new DancingLinks<ColumnName>();\r\n    for (int x = 0; x < size; ++x) {\r\n        for (int num = 1; num <= size; ++num) {\r\n            model.addColumn(new ColumnConstraint(num, x));\r\n        }\r\n    }\r\n    for (int y = 0; y < size; ++y) {\r\n        for (int num = 1; num <= size; ++num) {\r\n            model.addColumn(new RowConstraint(num, y));\r\n        }\r\n    }\r\n    for (int x = 0; x < squareYSize; ++x) {\r\n        for (int y = 0; y < squareXSize; ++y) {\r\n            for (int num = 1; num <= size; ++num) {\r\n                model.addColumn(new SquareConstraint(num, x, y));\r\n            }\r\n        }\r\n    }\r\n    for (int x = 0; x < size; ++x) {\r\n        for (int y = 0; y < size; ++y) {\r\n            model.addColumn(new CellConstraint(x, y));\r\n        }\r\n    }\r\n    boolean[] rowValues = new boolean[size * size * 4];\r\n    for (int x = 0; x < size; ++x) {\r\n        for (int y = 0; y < size; ++y) {\r\n            if (board[y][x] == -1) {\r\n                for (int num = 1; num <= size; ++num) {\r\n                    model.addRow(generateRow(rowValues, x, y, num));\r\n                }\r\n            } else {\r\n                model.addRow(generateRow(rowValues, x, y, board[y][x]));\r\n            }\r\n        }\r\n    }\r\n    return model;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "solve",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void solve()\n{\r\n    DancingLinks<ColumnName> model = makeModel();\r\n    int results = model.solve(new SolutionPrinter(size));\r\n    System.out.println(\"Found \" + results + \" solutions\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    if (args.length == 0) {\r\n        System.out.println(\"Include a puzzle on the command line.\");\r\n    }\r\n    for (int i = 0; i < args.length; ++i) {\r\n        Sudoku problem = new Sudoku(new FileInputStream(args[i]));\r\n        System.out.println(\"Solving \" + args[i]);\r\n        problem.solve();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "generateRecord",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void generateRecord(byte[] recBuf, Unsigned16 rand, Unsigned16 recordNumber)\n{\r\n    for (int i = 0; i < 10; ++i) {\r\n        recBuf[i] = rand.getByte(i);\r\n    }\r\n    recBuf[10] = 0x00;\r\n    recBuf[11] = 0x11;\r\n    for (int i = 0; i < 32; i++) {\r\n        recBuf[12 + i] = (byte) recordNumber.getHexDigit(i);\r\n    }\r\n    recBuf[44] = (byte) 0x88;\r\n    recBuf[45] = (byte) 0x99;\r\n    recBuf[46] = (byte) 0xAA;\r\n    recBuf[47] = (byte) 0xBB;\r\n    for (int i = 0; i < 12; ++i) {\r\n        recBuf[48 + i * 4] = recBuf[49 + i * 4] = recBuf[50 + i * 4] = recBuf[51 + i * 4] = (byte) rand.getHexDigit(20 + i);\r\n    }\r\n    recBuf[96] = (byte) 0xCC;\r\n    recBuf[97] = (byte) 0xDD;\r\n    recBuf[98] = (byte) 0xEE;\r\n    recBuf[99] = (byte) 0xFF;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "makeBigInteger",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BigInteger makeBigInteger(long x)\n{\r\n    byte[] data = new byte[8];\r\n    for (int i = 0; i < 8; ++i) {\r\n        data[i] = (byte) (x >>> (56 - 8 * i));\r\n    }\r\n    return new BigInteger(1, data);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "generateAsciiRecord",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void generateAsciiRecord(byte[] recBuf, Unsigned16 rand, Unsigned16 recordNumber)\n{\r\n    long temp = rand.getHigh8();\r\n    if (temp < 0) {\r\n        BigInteger bigTemp = makeBigInteger(temp);\r\n        recBuf[0] = (byte) (' ' + (bigTemp.mod(NINETY_FIVE).longValue()));\r\n        temp = bigTemp.divide(NINETY_FIVE).longValue();\r\n    } else {\r\n        recBuf[0] = (byte) (' ' + (temp % 95));\r\n        temp /= 95;\r\n    }\r\n    for (int i = 1; i < 8; ++i) {\r\n        recBuf[i] = (byte) (' ' + (temp % 95));\r\n        temp /= 95;\r\n    }\r\n    temp = rand.getLow8();\r\n    if (temp < 0) {\r\n        BigInteger bigTemp = makeBigInteger(temp);\r\n        recBuf[8] = (byte) (' ' + (bigTemp.mod(NINETY_FIVE).longValue()));\r\n        temp = bigTemp.divide(NINETY_FIVE).longValue();\r\n    } else {\r\n        recBuf[8] = (byte) (' ' + (temp % 95));\r\n        temp /= 95;\r\n    }\r\n    recBuf[9] = (byte) (' ' + (temp % 95));\r\n    recBuf[10] = ' ';\r\n    recBuf[11] = ' ';\r\n    for (int i = 0; i < 32; i++) {\r\n        recBuf[12 + i] = (byte) recordNumber.getHexDigit(i);\r\n    }\r\n    recBuf[44] = ' ';\r\n    recBuf[45] = ' ';\r\n    for (int i = 0; i < 13; ++i) {\r\n        recBuf[46 + i * 4] = recBuf[47 + i * 4] = recBuf[48 + i * 4] = recBuf[49 + i * 4] = (byte) rand.getHexDigit(19 + i);\r\n    }\r\n    recBuf[98] = '\\r';\r\n    recBuf[99] = '\\n';\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "usage",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void usage()\n{\r\n    PrintStream out = System.out;\r\n    out.println(\"usage: gensort [-a] [-c] [-bSTARTING_REC_NUM] NUM_RECS FILE_NAME\");\r\n    out.println(\"-a        Generate ascii records required for PennySort or JouleSort.\");\r\n    out.println(\"          These records are also an alternative input for the other\");\r\n    out.println(\"          sort benchmarks.  Without this flag, binary records will be\");\r\n    out.println(\"          generated that contain the highest density of randomness in\");\r\n    out.println(\"          the 10-byte key.\");\r\n    out.println(\"-c        Calculate the sum of the crc32 checksums of each of the\");\r\n    out.println(\"          generated records and send it to standard error.\");\r\n    out.println(\"-bN       Set the beginning record generated to N. By default the\");\r\n    out.println(\"          first record generated is record 0.\");\r\n    out.println(\"NUM_RECS  The number of sequential records to generate.\");\r\n    out.println(\"FILE_NAME The name of the file to write the records to.\\n\");\r\n    out.println(\"Example 1 - to generate 1000000 ascii records starting at record 0 to\");\r\n    out.println(\"the file named \\\"pennyinput\\\":\");\r\n    out.println(\"    gensort -a 1000000 pennyinput\\n\");\r\n    out.println(\"Example 2 - to generate 1000 binary records beginning with record 2000\");\r\n    out.println(\"to the file named \\\"partition2\\\":\");\r\n    out.println(\"    gensort -b2000 1000 partition2\");\r\n    System.exit(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "outputRecords",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void outputRecords(OutputStream out, boolean useAscii, Unsigned16 firstRecordNumber, Unsigned16 recordsToGenerate, Unsigned16 checksum) throws IOException\n{\r\n    byte[] row = new byte[100];\r\n    Unsigned16 recordNumber = new Unsigned16(firstRecordNumber);\r\n    Unsigned16 lastRecordNumber = new Unsigned16(firstRecordNumber);\r\n    Checksum crc = new PureJavaCrc32();\r\n    Unsigned16 tmp = new Unsigned16();\r\n    lastRecordNumber.add(recordsToGenerate);\r\n    Unsigned16 ONE = new Unsigned16(1);\r\n    Unsigned16 rand = Random16.skipAhead(firstRecordNumber);\r\n    while (!recordNumber.equals(lastRecordNumber)) {\r\n        Random16.nextRand(rand);\r\n        if (useAscii) {\r\n            generateAsciiRecord(row, rand, recordNumber);\r\n        } else {\r\n            generateRecord(row, rand, recordNumber);\r\n        }\r\n        if (checksum != null) {\r\n            crc.reset();\r\n            crc.update(row, 0, row.length);\r\n            tmp.set(crc.getValue());\r\n            checksum.add(tmp);\r\n        }\r\n        recordNumber.add(ONE);\r\n        out.write(row);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    Unsigned16 startingRecord = new Unsigned16();\r\n    Unsigned16 numberOfRecords;\r\n    OutputStream out;\r\n    boolean useAscii = false;\r\n    Unsigned16 checksum = null;\r\n    int i;\r\n    for (i = 0; i < args.length; ++i) {\r\n        String arg = args[i];\r\n        int argLength = arg.length();\r\n        if (argLength >= 1 && arg.charAt(0) == '-') {\r\n            if (argLength < 2) {\r\n                usage();\r\n            }\r\n            switch(arg.charAt(1)) {\r\n                case 'a':\r\n                    useAscii = true;\r\n                    break;\r\n                case 'b':\r\n                    startingRecord = Unsigned16.fromDecimal(arg.substring(2));\r\n                    break;\r\n                case 'c':\r\n                    checksum = new Unsigned16();\r\n                    break;\r\n                default:\r\n                    usage();\r\n            }\r\n        } else {\r\n            break;\r\n        }\r\n    }\r\n    if (args.length - i != 2) {\r\n        usage();\r\n    }\r\n    numberOfRecords = Unsigned16.fromDecimal(args[i]);\r\n    out = new FileOutputStream(args[i + 1]);\r\n    outputRecords(out, useAscii, startingRecord, numberOfRecords, checksum);\r\n    out.close();\r\n    if (checksum != null) {\r\n        System.out.println(checksum);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length == 0) {\r\n        System.out.println(\"Usage: writer <out-dir>\");\r\n        ToolRunner.printGenericCommandUsage(System.out);\r\n        return 2;\r\n    }\r\n    Path outDir = new Path(args[0]);\r\n    Configuration conf = getConf();\r\n    JobClient client = new JobClient(conf);\r\n    ClusterStatus cluster = client.getClusterStatus();\r\n    int numMapsPerHost = conf.getInt(MAPS_PER_HOST, 10);\r\n    long numBytesToWritePerMap = conf.getLong(BYTES_PER_MAP, 1 * 1024 * 1024 * 1024);\r\n    if (numBytesToWritePerMap == 0) {\r\n        System.err.println(\"Cannot have\" + BYTES_PER_MAP + \" set to 0\");\r\n        return -2;\r\n    }\r\n    long totalBytesToWrite = conf.getLong(TOTAL_BYTES, numMapsPerHost * numBytesToWritePerMap * cluster.getTaskTrackers());\r\n    int numMaps = (int) (totalBytesToWrite / numBytesToWritePerMap);\r\n    if (numMaps == 0 && totalBytesToWrite > 0) {\r\n        numMaps = 1;\r\n        conf.setLong(BYTES_PER_MAP, totalBytesToWrite);\r\n    }\r\n    conf.setInt(MRJobConfig.NUM_MAPS, numMaps);\r\n    Job job = Job.getInstance(conf);\r\n    job.setJarByClass(RandomWriter.class);\r\n    job.setJobName(\"random-writer\");\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    job.setOutputKeyClass(BytesWritable.class);\r\n    job.setOutputValueClass(BytesWritable.class);\r\n    job.setInputFormatClass(RandomInputFormat.class);\r\n    job.setMapperClass(RandomMapper.class);\r\n    job.setReducerClass(Reducer.class);\r\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\r\n    System.out.println(\"Running \" + numMaps + \" maps.\");\r\n    job.setNumReduceTasks(0);\r\n    Date startTime = new Date();\r\n    System.out.println(\"Job started: \" + startTime);\r\n    int ret = job.waitForCompletion(true) ? 0 : 1;\r\n    Date endTime = new Date();\r\n    System.out.println(\"Job ended: \" + endTime);\r\n    System.out.println(\"The job took \" + (endTime.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new RandomWriter(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "set",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LongLong set(long d0, long d1)\n{\r\n    this.d0 = d0;\r\n    this.d1 = d1;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "and",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long and(long mask)\n{\r\n    return d0 & mask;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "shiftRight",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long shiftRight(int n)\n{\r\n    return (d1 << (BITS_PER_LONG - n)) + (d0 >>> n);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "plusEqual",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LongLong plusEqual(LongLong that)\n{\r\n    this.d0 += that.d0;\r\n    this.d1 += that.d1;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "toBigInteger",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BigInteger toBigInteger()\n{\r\n    return BigInteger.valueOf(d1).shiftLeft(BITS_PER_LONG).add(BigInteger.valueOf(d0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    final int remainder = BITS_PER_LONG % 4;\r\n    return String.format(\"%x*2^%d + %016x\", d1 << remainder, BITS_PER_LONG - remainder, d0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "multiplication",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LongLong multiplication(final LongLong r, final long a, final long b)\n{\r\n    final long a_lower = a & LOWER_MASK;\r\n    final long a_upper = (a & UPPER_MASK) >> MID;\r\n    final long b_lower = b & LOWER_MASK;\r\n    final long b_upper = (b & UPPER_MASK) >> MID;\r\n    final long tmp = a_lower * b_upper + a_upper * b_lower;\r\n    r.d0 = a_lower * b_lower + ((tmp << MID) & FULL_MASK);\r\n    r.d1 = a_upper * b_upper + (tmp >> MID);\r\n    return r;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "getElement",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Summation getElement()\n{\r\n    return sigma;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "getDuration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDuration()\n{\r\n    return duration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int compareTo(TaskResult that)\n{\r\n    return this.sigma.compareTo(that.sigma);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (this == obj)\r\n        return true;\r\n    else if (obj instanceof TaskResult) {\r\n        final TaskResult that = (TaskResult) obj;\r\n        return this.compareTo(that) == 0;\r\n    }\r\n    throw new IllegalArgumentException(obj == null ? \"obj == null\" : \"obj.getClass()=\" + obj.getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int hashCode()\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "combine",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskResult combine(TaskResult that)\n{\r\n    final Summation s = sigma.combine(that.sigma);\r\n    return s == null ? null : new TaskResult(s, this.duration + that.duration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    sigma = SummationWritable.read(in);\r\n    duration = in.readLong();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    SummationWritable.write(sigma, out);\r\n    out.writeLong(duration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"sigma=\" + sigma + \", duration=\" + duration + \"(\" + Util.millis2String(duration) + \")\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "valueOf",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "TaskResult valueOf(String s)\n{\r\n    int i = 0;\r\n    int j = s.indexOf(\", duration=\");\r\n    if (j < 0)\r\n        throw new IllegalArgumentException(\"i=\" + i + \", j=\" + j + \" < 0, s=\" + s);\r\n    final Summation sigma = Summation.valueOf(Util.parseStringVariable(\"sigma\", s.substring(i, j)));\r\n    i = j + 2;\r\n    j = s.indexOf(\"(\", i);\r\n    if (j < 0)\r\n        throw new IllegalArgumentException(\"i=\" + i + \", j=\" + j + \" < 0, s=\" + s);\r\n    final long duration = Util.parseLongVariable(\"duration\", s.substring(i, j));\r\n    return new TaskResult(sigma, duration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "stringifySolution",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String stringifySolution(int width, int height, List<List<ColumnName>> solution)\n{\r\n    String[][] picture = new String[height][width];\r\n    StringBuffer result = new StringBuffer();\r\n    for (List<ColumnName> row : solution) {\r\n        Piece piece = null;\r\n        for (ColumnName item : row) {\r\n            if (item instanceof Piece) {\r\n                piece = (Piece) item;\r\n                break;\r\n            }\r\n        }\r\n        if (piece == null) {\r\n            continue;\r\n        }\r\n        for (ColumnName item : row) {\r\n            if (item instanceof Point) {\r\n                Point p = (Point) item;\r\n                picture[p.y][p.x] = piece.getName();\r\n            }\r\n        }\r\n    }\r\n    for (int y = 0; y < picture.length; ++y) {\r\n        for (int x = 0; x < picture[y].length; ++x) {\r\n            result.append(picture[y][x]);\r\n        }\r\n        result.append(\"\\n\");\r\n    }\r\n    return result.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "getCategory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SolutionCategory getCategory(List<List<ColumnName>> names)\n{\r\n    Piece xPiece = null;\r\n    for (Piece p : pieces) {\r\n        if (\"x\".equals(p.name)) {\r\n            xPiece = p;\r\n            break;\r\n        }\r\n    }\r\n    for (List<ColumnName> row : names) {\r\n        if (row.contains(xPiece)) {\r\n            int low_x = width;\r\n            int high_x = 0;\r\n            int low_y = height;\r\n            int high_y = 0;\r\n            for (ColumnName col : row) {\r\n                if (col instanceof Point) {\r\n                    int x = ((Point) col).x;\r\n                    int y = ((Point) col).y;\r\n                    if (x < low_x) {\r\n                        low_x = x;\r\n                    }\r\n                    if (x > high_x) {\r\n                        high_x = x;\r\n                    }\r\n                    if (y < low_y) {\r\n                        low_y = y;\r\n                    }\r\n                    if (y > high_y) {\r\n                        high_y = y;\r\n                    }\r\n                }\r\n            }\r\n            boolean mid_x = (low_x + high_x == width - 1);\r\n            boolean mid_y = (low_y + high_y == height - 1);\r\n            if (mid_x && mid_y) {\r\n                return SolutionCategory.CENTER;\r\n            } else if (mid_x) {\r\n                return SolutionCategory.MID_X;\r\n            } else if (mid_y) {\r\n                return SolutionCategory.MID_Y;\r\n            }\r\n            break;\r\n        }\r\n    }\r\n    return SolutionCategory.UPPER_LEFT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "initializePieces",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void initializePieces()\n{\r\n    pieces.add(new Piece(\"x\", \" x /xxx/ x \", false, oneRotation));\r\n    pieces.add(new Piece(\"v\", \"x  /x  /xxx\", false, fourRotations));\r\n    pieces.add(new Piece(\"t\", \"xxx/ x / x \", false, fourRotations));\r\n    pieces.add(new Piece(\"w\", \"  x/ xx/xx \", false, fourRotations));\r\n    pieces.add(new Piece(\"u\", \"x x/xxx\", false, fourRotations));\r\n    pieces.add(new Piece(\"i\", \"xxxxx\", false, twoRotations));\r\n    pieces.add(new Piece(\"f\", \" xx/xx / x \", true, fourRotations));\r\n    pieces.add(new Piece(\"p\", \"xx/xx/x \", true, fourRotations));\r\n    pieces.add(new Piece(\"z\", \"xx / x / xx\", true, twoRotations));\r\n    pieces.add(new Piece(\"n\", \"xx  / xxx\", true, fourRotations));\r\n    pieces.add(new Piece(\"y\", \"  x /xxxx\", true, fourRotations));\r\n    pieces.add(new Piece(\"l\", \"   x/xxxx\", true, fourRotations));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "isSide",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSide(int offset, int shapeSize, int board)\n{\r\n    return 2 * offset + shapeSize <= board;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "generateRows",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void generateRows(DancingLinks dancer, Piece piece, int width, int height, boolean flip, boolean[] row, boolean upperLeft)\n{\r\n    int[] rotations = piece.getRotations();\r\n    for (int rotIndex = 0; rotIndex < rotations.length; ++rotIndex) {\r\n        boolean[][] shape = piece.getShape(flip, rotations[rotIndex]);\r\n        for (int x = 0; x < width; ++x) {\r\n            for (int y = 0; y < height; ++y) {\r\n                if (y + shape.length <= height && x + shape[0].length <= width && (!upperLeft || (isSide(x, shape[0].length, width) && isSide(y, shape.length, height)))) {\r\n                    for (int idx = 0; idx < width * height; ++idx) {\r\n                        row[idx] = false;\r\n                    }\r\n                    for (int subY = 0; subY < shape.length; ++subY) {\r\n                        for (int subX = 0; subX < shape[0].length; ++subX) {\r\n                            row[(y + subY) * width + x + subX] = shape[subY][subX];\r\n                        }\r\n                    }\r\n                    dancer.addRow(row);\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void initialize(int width, int height)\n{\r\n    this.width = width;\r\n    this.height = height;\r\n    for (int y = 0; y < height; ++y) {\r\n        for (int x = 0; x < width; ++x) {\r\n            dancer.addColumn(new Point(x, y));\r\n        }\r\n    }\r\n    int pieceBase = dancer.getNumberColumns();\r\n    for (Piece p : pieces) {\r\n        dancer.addColumn(p);\r\n    }\r\n    boolean[] row = new boolean[dancer.getNumberColumns()];\r\n    for (int idx = 0; idx < pieces.size(); ++idx) {\r\n        Piece piece = pieces.get(idx);\r\n        row[idx + pieceBase] = true;\r\n        generateRows(dancer, piece, width, height, false, row, idx == 0);\r\n        if (piece.getFlippable()) {\r\n            generateRows(dancer, piece, width, height, true, row, idx == 0);\r\n        }\r\n        row[idx + pieceBase] = false;\r\n    }\r\n    printer = new SolutionPrinter(width, height);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<int[]> getSplits(int depth)\n{\r\n    return dancer.split(depth);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "solve",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int solve(int[] split)\n{\r\n    return dancer.solve(split, printer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "solve",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int solve()\n{\r\n    return dancer.solve(printer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "setPrinter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setPrinter(DancingLinks.SolutionAcceptor<ColumnName> printer)\n{\r\n    this.printer = printer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    int width = 6;\r\n    int height = 10;\r\n    Pentomino model = new Pentomino(width, height);\r\n    List splits = model.getSplits(2);\r\n    for (Iterator splitItr = splits.iterator(); splitItr.hasNext(); ) {\r\n        int[] choices = (int[]) splitItr.next();\r\n        System.out.print(\"split:\");\r\n        for (int i = 0; i < choices.length; ++i) {\r\n            System.out.print(\" \" + choices[i]);\r\n        }\r\n        System.out.println();\r\n        System.out.println(model.solve(choices) + \" solutions found.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\r\n    if (otherArgs.length < 2) {\r\n        System.err.println(\"Usage: wordcount <in> [<in>...] <out>\");\r\n        System.exit(2);\r\n    }\r\n    Job job = Job.getInstance(conf, \"word count\");\r\n    job.setJarByClass(WordCount.class);\r\n    job.setMapperClass(TokenizerMapper.class);\r\n    job.setCombinerClass(IntSumReducer.class);\r\n    job.setReducerClass(IntSumReducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    for (int i = 0; i < otherArgs.length - 1; ++i) {\r\n        FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\r\n    }\r\n    FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1]));\r\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "setFinalSync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFinalSync(JobContext job, boolean newValue)\n{\r\n    job.getConfiguration().setBoolean(TeraSortConfigKeys.FINAL_SYNC_ATTRIBUTE.key(), newValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "getFinalSync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getFinalSync(JobContext job)\n{\r\n    return job.getConfiguration().getBoolean(TeraSortConfigKeys.FINAL_SYNC_ATTRIBUTE.key(), TeraSortConfigKeys.DEFAULT_FINAL_SYNC_ATTRIBUTE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "checkOutputSpecs",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void checkOutputSpecs(JobContext job) throws InvalidJobConfException, IOException\n{\r\n    Path outDir = getOutputPath(job);\r\n    if (outDir == null) {\r\n        throw new InvalidJobConfException(\"Output directory not set in JobConf.\");\r\n    }\r\n    final Configuration jobConf = job.getConfiguration();\r\n    TokenCache.obtainTokensForNamenodes(job.getCredentials(), new Path[] { outDir }, jobConf);\r\n    final FileSystem fs = outDir.getFileSystem(jobConf);\r\n    try {\r\n        final FileStatus[] outDirKids = fs.listStatus(outDir);\r\n        boolean empty = false;\r\n        if (outDirKids != null && outDirKids.length == 1) {\r\n            final FileStatus st = outDirKids[0];\r\n            final String fname = st.getPath().getName();\r\n            empty = !st.isDirectory() && TeraInputFormat.PARTITION_FILENAME.equals(fname);\r\n        }\r\n        if (TeraSort.getUseSimplePartitioner(job) || !empty) {\r\n            throw new FileAlreadyExistsException(\"Output directory \" + outDir + \" already exists\");\r\n        }\r\n    } catch (FileNotFoundException ignored) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RecordWriter<Text, Text> getRecordWriter(TaskAttemptContext job) throws IOException\n{\r\n    Path file = getDefaultWorkFile(job, \"\");\r\n    FileSystem fs = file.getFileSystem(job.getConfiguration());\r\n    FSDataOutputStream fileOut = fs.create(file);\r\n    return new TeraRecordWriter(fileOut, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "textifyBytes",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String textifyBytes(Text t)\n{\r\n    BytesWritable b = new BytesWritable();\r\n    b.set(t.getBytes(), 0, t.getLength());\r\n    return b.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "usage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void usage() throws IOException\n{\r\n    System.err.println(\"teravalidate <out-dir> <report-dir>\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    Job job = Job.getInstance(getConf());\r\n    if (args.length != 2) {\r\n        usage();\r\n        return 1;\r\n    }\r\n    TeraInputFormat.setInputPaths(job, new Path(args[0]));\r\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\r\n    job.setJobName(\"TeraValidate\");\r\n    job.setJarByClass(TeraValidate.class);\r\n    job.setMapperClass(ValidateMapper.class);\r\n    job.setReducerClass(ValidateReducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setNumReduceTasks(1);\r\n    FileInputFormat.setMinInputSplitSize(job, Long.MAX_VALUE);\r\n    job.setInputFormatClass(TeraInputFormat.class);\r\n    return job.waitForCompletion(true) ? 0 : 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new TeraValidate(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    Job job = ValueAggregatorJob.createValueAggregatorJob(args, new Class[] { AggregateWordHistogramPlugin.class });\r\n    job.setJarByClass(AggregateWordCount.class);\r\n    int ret = job.waitForCompletion(true) ? 0 : 1;\r\n    System.exit(ret);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (o instanceof Unsigned16) {\r\n        Unsigned16 other = (Unsigned16) o;\r\n        return other.hi8 == hi8 && other.lo8 == lo8;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int hashCode()\n{\r\n    return (int) lo8;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "set",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void set(String s) throws NumberFormatException\n{\r\n    hi8 = 0;\r\n    lo8 = 0;\r\n    final long lastDigit = 0xfl << 60;\r\n    for (int i = 0; i < s.length(); ++i) {\r\n        int digit = getHexDigit(s.charAt(i));\r\n        if ((lastDigit & hi8) != 0) {\r\n            throw new NumberFormatException(s + \" overflowed 16 bytes\");\r\n        }\r\n        hi8 <<= 4;\r\n        hi8 |= (lo8 & lastDigit) >>> 60;\r\n        lo8 <<= 4;\r\n        lo8 |= digit;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "set",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void set(long l)\n{\r\n    lo8 = l;\r\n    hi8 = 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "getHexDigit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getHexDigit(char ch) throws NumberFormatException\n{\r\n    if (ch >= '0' && ch <= '9') {\r\n        return ch - '0';\r\n    }\r\n    if (ch >= 'a' && ch <= 'f') {\r\n        return ch - 'a' + 10;\r\n    }\r\n    if (ch >= 'A' && ch <= 'F') {\r\n        return ch - 'A' + 10;\r\n    }\r\n    throw new NumberFormatException(ch + \" is not a valid hex digit\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "fromDecimal",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Unsigned16 fromDecimal(String s) throws NumberFormatException\n{\r\n    Unsigned16 result = new Unsigned16();\r\n    Unsigned16 tmp = new Unsigned16();\r\n    for (int i = 0; i < s.length(); i++) {\r\n        char ch = s.charAt(i);\r\n        if (ch < '0' || ch > '9') {\r\n            throw new NumberFormatException(ch + \" not a valid decimal digit\");\r\n        }\r\n        int digit = ch - '0';\r\n        result.multiply(TEN);\r\n        tmp.set(digit);\r\n        result.add(tmp);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String toString()\n{\r\n    if (hi8 == 0) {\r\n        return Long.toHexString(lo8);\r\n    } else {\r\n        StringBuilder result = new StringBuilder();\r\n        result.append(Long.toHexString(hi8));\r\n        String loString = Long.toHexString(lo8);\r\n        for (int i = loString.length(); i < 16; ++i) {\r\n            result.append('0');\r\n        }\r\n        result.append(loString);\r\n        return result.toString();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "getByte",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte getByte(int b)\n{\r\n    if (b >= 0 && b < 16) {\r\n        if (b < 8) {\r\n            return (byte) (hi8 >> (56 - 8 * b));\r\n        } else {\r\n            return (byte) (lo8 >> (120 - 8 * b));\r\n        }\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "getHexDigit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "char getHexDigit(int p)\n{\r\n    byte digit = getByte(p / 2);\r\n    if (p % 2 == 0) {\r\n        digit >>>= 4;\r\n    }\r\n    digit &= 0xf;\r\n    if (digit < 10) {\r\n        return (char) ('0' + digit);\r\n    } else {\r\n        return (char) ('A' + digit - 10);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "getHigh8",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getHigh8()\n{\r\n    return hi8;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "getLow8",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLow8()\n{\r\n    return lo8;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "multiply",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void multiply(Unsigned16 b)\n{\r\n    long[] left = new long[4];\r\n    left[0] = lo8 & 0xffffffffl;\r\n    left[1] = lo8 >>> 32;\r\n    left[2] = hi8 & 0xffffffffl;\r\n    left[3] = hi8 >>> 32;\r\n    long[] right = new long[5];\r\n    right[0] = b.lo8 & 0x7fffffffl;\r\n    right[1] = (b.lo8 >>> 31) & 0x7fffffffl;\r\n    right[2] = (b.lo8 >>> 62) + ((b.hi8 & 0x1fffffffl) << 2);\r\n    right[3] = (b.hi8 >>> 29) & 0x7fffffffl;\r\n    right[4] = (b.hi8 >>> 60);\r\n    set(0);\r\n    Unsigned16 tmp = new Unsigned16();\r\n    for (int l = 0; l < 4; ++l) {\r\n        for (int r = 0; r < 5; ++r) {\r\n            long prod = left[l] * right[r];\r\n            if (prod != 0) {\r\n                int off = l * 32 + r * 31;\r\n                tmp.set(prod);\r\n                tmp.shiftLeft(off);\r\n                add(tmp);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void add(Unsigned16 b)\n{\r\n    long sumHi;\r\n    long sumLo;\r\n    long reshibit, hibit0, hibit1;\r\n    sumHi = hi8 + b.hi8;\r\n    hibit0 = (lo8 & 0x8000000000000000L);\r\n    hibit1 = (b.lo8 & 0x8000000000000000L);\r\n    sumLo = lo8 + b.lo8;\r\n    reshibit = (sumLo & 0x8000000000000000L);\r\n    if ((hibit0 & hibit1) != 0 | ((hibit0 ^ hibit1) != 0 && reshibit == 0))\r\n        sumHi++;\r\n    hi8 = sumHi;\r\n    lo8 = sumLo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "shiftLeft",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void shiftLeft(int bits)\n{\r\n    if (bits != 0) {\r\n        if (bits < 64) {\r\n            hi8 <<= bits;\r\n            hi8 |= (lo8 >>> (64 - bits));\r\n            lo8 <<= bits;\r\n        } else if (bits < 128) {\r\n            hi8 = lo8 << (bits - 64);\r\n            lo8 = 0;\r\n        } else {\r\n            hi8 = 0;\r\n            lo8 = 0;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    hi8 = in.readLong();\r\n    lo8 = in.readLong();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    out.writeLong(hi8);\r\n    out.writeLong(lo8);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length < 3) {\r\n        System.out.println(\"Grep <inDir> <outDir> <regex> [<group>]\");\r\n        ToolRunner.printGenericCommandUsage(System.out);\r\n        return 2;\r\n    }\r\n    Path tempDir = new Path(\"grep-temp-\" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));\r\n    Configuration conf = getConf();\r\n    conf.set(RegexMapper.PATTERN, args[2]);\r\n    if (args.length == 4)\r\n        conf.set(RegexMapper.GROUP, args[3]);\r\n    Job grepJob = Job.getInstance(conf);\r\n    try {\r\n        grepJob.setJobName(\"grep-search\");\r\n        grepJob.setJarByClass(Grep.class);\r\n        FileInputFormat.setInputPaths(grepJob, args[0]);\r\n        grepJob.setMapperClass(RegexMapper.class);\r\n        grepJob.setCombinerClass(LongSumReducer.class);\r\n        grepJob.setReducerClass(LongSumReducer.class);\r\n        FileOutputFormat.setOutputPath(grepJob, tempDir);\r\n        grepJob.setOutputFormatClass(SequenceFileOutputFormat.class);\r\n        grepJob.setOutputKeyClass(Text.class);\r\n        grepJob.setOutputValueClass(LongWritable.class);\r\n        grepJob.waitForCompletion(true);\r\n        Job sortJob = Job.getInstance(conf);\r\n        sortJob.setJobName(\"grep-sort\");\r\n        sortJob.setJarByClass(Grep.class);\r\n        FileInputFormat.setInputPaths(sortJob, tempDir);\r\n        sortJob.setInputFormatClass(SequenceFileInputFormat.class);\r\n        sortJob.setMapperClass(InverseMapper.class);\r\n        sortJob.setNumReduceTasks(1);\r\n        FileOutputFormat.setOutputPath(sortJob, new Path(args[1]));\r\n        sortJob.setSortComparatorClass(LongWritable.DecreasingComparator.class);\r\n        sortJob.waitForCompletion(true);\r\n    } finally {\r\n        FileSystem.get(conf).delete(tempDir, true);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new Grep(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    Job job = ValueAggregatorJob.createValueAggregatorJob(args, new Class[] { WordCountPlugInClass.class });\r\n    job.setJarByClass(AggregateWordCount.class);\r\n    int ret = job.waitForCompletion(true) ? 0 : 1;\r\n    System.exit(ret);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void main(String[] argv)\n{\r\n    int exitCode = -1;\r\n    ProgramDriver pgd = new ProgramDriver();\r\n    try {\r\n        pgd.addClass(\"wordcount\", WordCount.class, \"A map/reduce program that counts the words in the input files.\");\r\n        pgd.addClass(\"wordmean\", WordMean.class, \"A map/reduce program that counts the average length of the words in the input files.\");\r\n        pgd.addClass(\"wordmedian\", WordMedian.class, \"A map/reduce program that counts the median length of the words in the input files.\");\r\n        pgd.addClass(\"wordstandarddeviation\", WordStandardDeviation.class, \"A map/reduce program that counts the standard deviation of the length of the words in the input files.\");\r\n        pgd.addClass(\"aggregatewordcount\", AggregateWordCount.class, \"An Aggregate based map/reduce program that counts the words in the input files.\");\r\n        pgd.addClass(\"aggregatewordhist\", AggregateWordHistogram.class, \"An Aggregate based map/reduce program that computes the histogram of the words in the input files.\");\r\n        pgd.addClass(\"grep\", Grep.class, \"A map/reduce program that counts the matches of a regex in the input.\");\r\n        pgd.addClass(\"randomwriter\", RandomWriter.class, \"A map/reduce program that writes 10GB of random data per node.\");\r\n        pgd.addClass(\"randomtextwriter\", RandomTextWriter.class, \"A map/reduce program that writes 10GB of random textual data per node.\");\r\n        pgd.addClass(\"sort\", Sort.class, \"A map/reduce program that sorts the data written by the random writer.\");\r\n        pgd.addClass(\"pi\", QuasiMonteCarlo.class, QuasiMonteCarlo.DESCRIPTION);\r\n        pgd.addClass(\"bbp\", BaileyBorweinPlouffe.class, BaileyBorweinPlouffe.DESCRIPTION);\r\n        pgd.addClass(\"distbbp\", DistBbp.class, DistBbp.DESCRIPTION);\r\n        pgd.addClass(\"pentomino\", DistributedPentomino.class, \"A map/reduce tile laying program to find solutions to pentomino problems.\");\r\n        pgd.addClass(\"secondarysort\", SecondarySort.class, \"An example defining a secondary sort to the reduce.\");\r\n        pgd.addClass(\"sudoku\", Sudoku.class, \"A sudoku solver.\");\r\n        pgd.addClass(\"join\", Join.class, \"A job that effects a join over sorted, equally partitioned datasets\");\r\n        pgd.addClass(\"multifilewc\", MultiFileWordCount.class, \"A job that counts words from several files.\");\r\n        pgd.addClass(\"dbcount\", DBCountPageView.class, \"An example job that count the pageview counts from a database.\");\r\n        pgd.addClass(\"teragen\", TeraGen.class, \"Generate data for the terasort\");\r\n        pgd.addClass(\"terasort\", TeraSort.class, \"Run the terasort\");\r\n        pgd.addClass(\"teravalidate\", TeraValidate.class, \"Checking results of terasort\");\r\n        exitCode = pgd.run(argv);\r\n    } catch (Throwable e) {\r\n        e.printStackTrace();\r\n    }\r\n    System.exit(exitCode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "getParameters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Parameters getParameters()\n{\r\n    return parameters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "setParameters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setParameters(Parameters p)\n{\r\n    parameters = p;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "createJob",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Job createJob(String name, Summation sigma) throws IOException\n{\r\n    final Job job = Job.getInstance(getConf(), parameters.remoteDir + \"/\" + name);\r\n    final Configuration jobconf = job.getConfiguration();\r\n    job.setJarByClass(DistSum.class);\r\n    jobconf.setInt(N_PARTS, parameters.nParts);\r\n    SummationWritable.write(sigma, DistSum.class, jobconf);\r\n    jobconf.setLong(MRJobConfig.TASK_TIMEOUT, 0);\r\n    jobconf.setBoolean(MRJobConfig.MAP_SPECULATIVE, false);\r\n    jobconf.setBoolean(MRJobConfig.REDUCE_SPECULATIVE, false);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "compute",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void compute(final String name, Summation sigma) throws IOException\n{\r\n    if (sigma.getValue() != null)\r\n        throw new IOException(\"sigma.getValue() != null, sigma=\" + sigma);\r\n    final FileSystem fs = FileSystem.get(getConf());\r\n    final Path dir = fs.makeQualified(new Path(parameters.remoteDir, name));\r\n    if (!Util.createNonexistingDirectory(fs, dir))\r\n        return;\r\n    final Job job = createJob(name, sigma);\r\n    final Path outdir = new Path(dir, \"out\");\r\n    FileOutputFormat.setOutputPath(job, outdir);\r\n    final String startmessage = \"steps/parts = \" + sigma.E.getSteps() + \"/\" + parameters.nParts + \" = \" + Util.long2string(sigma.E.getSteps() / parameters.nParts);\r\n    Util.runJob(name, job, parameters.machine, startmessage, timer);\r\n    final List<TaskResult> results = Util.readJobOutputs(fs, outdir);\r\n    Util.writeResults(name, results, fs, parameters.remoteDir);\r\n    fs.delete(dir, true);\r\n    final List<TaskResult> combined = Util.combine(results);\r\n    final PrintWriter out = Util.createWriter(parameters.localDir, name);\r\n    try {\r\n        for (TaskResult r : combined) {\r\n            final String s = taskResult2string(name, r);\r\n            out.println(s);\r\n            out.flush();\r\n            Util.out.println(s);\r\n        }\r\n    } finally {\r\n        out.close();\r\n    }\r\n    if (combined.size() == 1) {\r\n        final Summation s = combined.get(0).getElement();\r\n        if (sigma.contains(s) && s.contains(sigma))\r\n            sigma.setValue(s.getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "taskResult2string",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String taskResult2string(String name, TaskResult result)\n{\r\n    return NAME + \" \" + name + \"> \" + result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "string2TaskResult",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Map.Entry<String, TaskResult> string2TaskResult(final String s)\n{\r\n    int j = s.indexOf(NAME);\r\n    if (j == 0) {\r\n        int i = j + NAME.length() + 1;\r\n        j = s.indexOf(\"> \", i);\r\n        final String key = s.substring(i, j);\r\n        final TaskResult value = TaskResult.valueOf(s.substring(j + 2));\r\n        return new Map.Entry<String, TaskResult>() {\r\n\r\n            @Override\r\n            public String getKey() {\r\n                return key;\r\n            }\r\n\r\n            @Override\r\n            public TaskResult getValue() {\r\n                return value;\r\n            }\r\n\r\n            @Override\r\n            public TaskResult setValue(TaskResult value) {\r\n                throw new UnsupportedOperationException();\r\n            }\r\n        };\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "execute",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Summation execute(String name, Summation sigma)\n{\r\n    final Summation[] summations = sigma.partition(parameters.nJobs);\r\n    final List<Computation> computations = new ArrayList<Computation>();\r\n    for (int i = 0; i < summations.length; i++) computations.add(new Computation(i, name, summations[i]));\r\n    try {\r\n        Util.execute(parameters.nThreads, computations);\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n    final List<Summation> combined = Util.combine(Arrays.asList(summations));\r\n    return combined.size() == 1 ? combined.get(0) : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length != Parameters.COUNT + 2)\r\n        return Util.printUsage(args, getClass().getName() + \" <name> <sigma> \" + Parameters.LIST + \"\\n  <name> The name.\" + \"\\n  <sigma> The summation.\" + Parameters.DESCRIPTION);\r\n    int i = 0;\r\n    final String name = args[i++];\r\n    final Summation sigma = Summation.valueOf(args[i++]);\r\n    setParameters(DistSum.Parameters.parse(args, i));\r\n    Util.out.println();\r\n    Util.out.println(\"name  = \" + name);\r\n    Util.out.println(\"sigma = \" + sigma);\r\n    Util.out.println(parameters);\r\n    Util.out.println();\r\n    final Summation result = execute(name, sigma);\r\n    if (result.equals(sigma)) {\r\n        sigma.setValue(result.getValue());\r\n        timer.tick(\"\\n\\nDONE\\n\\nsigma=\" + sigma);\r\n        return 0;\r\n    } else {\r\n        timer.tick(\"\\n\\nDONE WITH ERROR\\n\\nresult=\" + result);\r\n        return 1;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    System.exit(ToolRunner.run(null, new DistSum(), args));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\r\n    if (otherArgs.length != 2) {\r\n        System.err.println(\"Usage: secondarysort <in> <out>\");\r\n        System.exit(2);\r\n    }\r\n    Job job = Job.getInstance(conf, \"secondary sort\");\r\n    job.setJarByClass(SecondarySort.class);\r\n    job.setMapperClass(MapClass.class);\r\n    job.setReducerClass(Reduce.class);\r\n    job.setPartitionerClass(FirstPartitioner.class);\r\n    job.setGroupingComparatorClass(FirstGroupingComparator.class);\r\n    job.setMapOutputKeyClass(IntPair.class);\r\n    job.setMapOutputValueClass(IntWritable.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    FileInputFormat.addInputPath(job, new Path(otherArgs[0]));\r\n    FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));\r\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "createInputDirectory",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "long createInputDirectory(FileSystem fs, Path dir, Pentomino pent, int depth) throws IOException\n{\r\n    fs.mkdirs(dir);\r\n    List<int[]> splits = pent.getSplits(depth);\r\n    Path input = new Path(dir, \"part1\");\r\n    PrintWriter file = new PrintWriter(new OutputStreamWriter(new BufferedOutputStream(fs.create(input), 64 * 1024), Charsets.UTF_8));\r\n    for (int[] prefix : splits) {\r\n        for (int i = 0; i < prefix.length; ++i) {\r\n            if (i != 0) {\r\n                file.print(',');\r\n            }\r\n            file.print(prefix[i]);\r\n        }\r\n        file.print('\\n');\r\n    }\r\n    file.close();\r\n    return fs.getFileStatus(input).getLen();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new DistributedPentomino(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 34,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    Configuration conf = getConf();\r\n    if (args.length == 0) {\r\n        System.out.println(\"Usage: pentomino <output> [-depth #] [-height #] [-width #]\");\r\n        ToolRunner.printGenericCommandUsage(System.out);\r\n        return 2;\r\n    }\r\n    int width = conf.getInt(Pentomino.WIDTH, PENT_WIDTH);\r\n    int height = conf.getInt(Pentomino.HEIGHT, PENT_HEIGHT);\r\n    int depth = conf.getInt(Pentomino.DEPTH, PENT_DEPTH);\r\n    for (int i = 0; i < args.length; i++) {\r\n        if (args[i].equalsIgnoreCase(\"-depth\")) {\r\n            depth = Integer.parseInt(args[++i].trim());\r\n        } else if (args[i].equalsIgnoreCase(\"-height\")) {\r\n            height = Integer.parseInt(args[++i].trim());\r\n        } else if (args[i].equalsIgnoreCase(\"-width\")) {\r\n            width = Integer.parseInt(args[++i].trim());\r\n        }\r\n    }\r\n    conf.setInt(Pentomino.WIDTH, width);\r\n    conf.setInt(Pentomino.HEIGHT, height);\r\n    conf.setInt(Pentomino.DEPTH, depth);\r\n    Class<? extends Pentomino> pentClass = conf.getClass(Pentomino.CLASS, OneSidedPentomino.class, Pentomino.class);\r\n    int numMaps = conf.getInt(MRJobConfig.NUM_MAPS, DEFAULT_MAPS);\r\n    Path output = new Path(args[0]);\r\n    Path input = new Path(output + \"_input\");\r\n    FileSystem fileSys = FileSystem.get(conf);\r\n    try {\r\n        Job job = Job.getInstance(conf);\r\n        FileInputFormat.setInputPaths(job, input);\r\n        FileOutputFormat.setOutputPath(job, output);\r\n        job.setJarByClass(PentMap.class);\r\n        job.setJobName(\"dancingElephant\");\r\n        Pentomino pent = ReflectionUtils.newInstance(pentClass, conf);\r\n        pent.initialize(width, height);\r\n        long inputSize = createInputDirectory(fileSys, input, pent, depth);\r\n        FileInputFormat.setMaxInputSplitSize(job, (inputSize / numMaps));\r\n        job.setOutputKeyClass(Text.class);\r\n        job.setOutputValueClass(Text.class);\r\n        job.setMapperClass(PentMap.class);\r\n        job.setReducerClass(Reducer.class);\r\n        job.setNumReduceTasks(1);\r\n        return (job.waitForCompletion(true) ? 0 : 1);\r\n    } finally {\r\n        fileSys.delete(input, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "skipAhead",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Unsigned16 skipAhead(Unsigned16 advance)\n{\r\n    Unsigned16 result = new Unsigned16();\r\n    long bit_map;\r\n    bit_map = advance.getLow8();\r\n    for (int i = 0; bit_map != 0 && i < 64; i++) {\r\n        if ((bit_map & (1L << i)) != 0) {\r\n            result.multiply(genArray[i].a);\r\n            result.add(genArray[i].c);\r\n            bit_map &= ~(1L << i);\r\n        }\r\n    }\r\n    bit_map = advance.getHigh8();\r\n    for (int i = 0; bit_map != 0 && i < 64; i++) {\r\n        if ((bit_map & (1L << i)) != 0) {\r\n            result.multiply(genArray[i + 64].a);\r\n            result.add(genArray[i + 64].c);\r\n            bit_map &= ~(1L << i);\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "nextRand",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void nextRand(Unsigned16 rand)\n{\r\n    rand.multiply(genArray[0].a);\r\n    rand.add(genArray[0].c);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int printUsage()\n{\r\n    System.out.println(\"randomtextwriter \" + \"[-outFormat <output format class>] \" + \"<output>\");\r\n    ToolRunner.printGenericCommandUsage(System.out);\r\n    return 2;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "run",
  "errType" : [ "ArrayIndexOutOfBoundsException" ],
  "containingMethodsNum" : 29,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length == 0) {\r\n        return printUsage();\r\n    }\r\n    Configuration conf = getConf();\r\n    JobClient client = new JobClient(conf);\r\n    ClusterStatus cluster = client.getClusterStatus();\r\n    int numMapsPerHost = conf.getInt(MAPS_PER_HOST, 10);\r\n    long numBytesToWritePerMap = conf.getLong(BYTES_PER_MAP, 1 * 1024 * 1024 * 1024);\r\n    if (numBytesToWritePerMap == 0) {\r\n        System.err.println(\"Cannot have \" + BYTES_PER_MAP + \" set to 0\");\r\n        return -2;\r\n    }\r\n    long totalBytesToWrite = conf.getLong(TOTAL_BYTES, numMapsPerHost * numBytesToWritePerMap * cluster.getTaskTrackers());\r\n    int numMaps = (int) (totalBytesToWrite / numBytesToWritePerMap);\r\n    if (numMaps == 0 && totalBytesToWrite > 0) {\r\n        numMaps = 1;\r\n        conf.setLong(BYTES_PER_MAP, totalBytesToWrite);\r\n    }\r\n    conf.setInt(MRJobConfig.NUM_MAPS, numMaps);\r\n    Job job = Job.getInstance(conf);\r\n    job.setJarByClass(RandomTextWriter.class);\r\n    job.setJobName(\"random-text-writer\");\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setInputFormatClass(RandomWriter.RandomInputFormat.class);\r\n    job.setMapperClass(RandomTextMapper.class);\r\n    Class<? extends OutputFormat> outputFormatClass = SequenceFileOutputFormat.class;\r\n    List<String> otherArgs = new ArrayList<String>();\r\n    for (int i = 0; i < args.length; ++i) {\r\n        try {\r\n            if (\"-outFormat\".equals(args[i])) {\r\n                outputFormatClass = Class.forName(args[++i]).asSubclass(OutputFormat.class);\r\n            } else {\r\n                otherArgs.add(args[i]);\r\n            }\r\n        } catch (ArrayIndexOutOfBoundsException except) {\r\n            System.out.println(\"ERROR: Required parameter missing from \" + args[i - 1]);\r\n            return printUsage();\r\n        }\r\n    }\r\n    job.setOutputFormatClass(outputFormatClass);\r\n    FileOutputFormat.setOutputPath(job, new Path(otherArgs.get(0)));\r\n    System.out.println(\"Running \" + numMaps + \" maps.\");\r\n    job.setNumReduceTasks(0);\r\n    Date startTime = new Date();\r\n    System.out.println(\"Job started: \" + startTime);\r\n    int ret = job.waitForCompletion(true) ? 0 : 1;\r\n    Date endTime = new Date();\r\n    System.out.println(\"Job ended: \" + endTime);\r\n    System.out.println(\"The job took \" + (endTime.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new RandomTextWriter(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "getNumberOfRows",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumberOfRows(JobContext job)\n{\r\n    return job.getConfiguration().getLong(TeraSortConfigKeys.NUM_ROWS.key(), TeraSortConfigKeys.DEFAULT_NUM_ROWS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "setNumberOfRows",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumberOfRows(Job job, long numRows)\n{\r\n    job.getConfiguration().setLong(TeraSortConfigKeys.NUM_ROWS.key(), numRows);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "usage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void usage() throws IOException\n{\r\n    System.err.println(\"teragen <num rows> <output dir>\");\r\n    System.err.println(\"If you want to generate data and store them as \" + \"erasure code striping file, just make sure that the parent dir \" + \"of <output dir> has erasure code policy set\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "parseHumanLong",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long parseHumanLong(String str)\n{\r\n    char tail = str.charAt(str.length() - 1);\r\n    long base = 1;\r\n    switch(tail) {\r\n        case 't':\r\n            base *= 1000 * 1000 * 1000 * 1000;\r\n            break;\r\n        case 'b':\r\n            base *= 1000 * 1000 * 1000;\r\n            break;\r\n        case 'm':\r\n            base *= 1000 * 1000;\r\n            break;\r\n        case 'k':\r\n            base *= 1000;\r\n            break;\r\n        default:\r\n    }\r\n    if (base != 1) {\r\n        str = str.substring(0, str.length() - 1);\r\n    }\r\n    return Long.parseLong(str) * base;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "int run(String[] args) throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    Job job = Job.getInstance(getConf());\r\n    if (args.length != 2) {\r\n        usage();\r\n        return 2;\r\n    }\r\n    setNumberOfRows(job, parseHumanLong(args[0]));\r\n    Path outputDir = new Path(args[1]);\r\n    FileOutputFormat.setOutputPath(job, outputDir);\r\n    job.setJobName(\"TeraGen\");\r\n    job.setJarByClass(TeraGen.class);\r\n    job.setMapperClass(SortGenMapper.class);\r\n    job.setNumReduceTasks(0);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setInputFormatClass(RangeInputFormat.class);\r\n    job.setOutputFormatClass(TeraOutputFormat.class);\r\n    return job.waitForCompletion(true) ? 0 : 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new TeraGen(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "writePartitionFile",
  "errType" : [ "IOException", "InterruptedException", "InterruptedException" ],
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void writePartitionFile(final JobContext job, Path partFile) throws Throwable\n{\r\n    long t1 = System.currentTimeMillis();\r\n    Configuration conf = job.getConfiguration();\r\n    final TeraInputFormat inFormat = new TeraInputFormat();\r\n    final TextSampler sampler = new TextSampler();\r\n    int partitions = job.getNumReduceTasks();\r\n    long sampleSize = conf.getLong(TeraSortConfigKeys.SAMPLE_SIZE.key(), TeraSortConfigKeys.DEFAULT_SAMPLE_SIZE);\r\n    final List<InputSplit> splits = inFormat.getSplits(job);\r\n    long t2 = System.currentTimeMillis();\r\n    System.out.println(\"Computing input splits took \" + (t2 - t1) + \"ms\");\r\n    int samples = Math.min(conf.getInt(TeraSortConfigKeys.NUM_PARTITIONS.key(), TeraSortConfigKeys.DEFAULT_NUM_PARTITIONS), splits.size());\r\n    System.out.println(\"Sampling \" + samples + \" splits of \" + splits.size());\r\n    final long recordsPerSample = sampleSize / samples;\r\n    final int sampleStep = splits.size() / samples;\r\n    Thread[] samplerReader = new Thread[samples];\r\n    SamplerThreadGroup threadGroup = new SamplerThreadGroup(\"Sampler Reader Thread Group\");\r\n    for (int i = 0; i < samples; ++i) {\r\n        final int idx = i;\r\n        samplerReader[i] = new Thread(threadGroup, \"Sampler Reader \" + idx) {\r\n\r\n            {\r\n                setDaemon(true);\r\n            }\r\n\r\n            public void run() {\r\n                long records = 0;\r\n                try {\r\n                    TaskAttemptContext context = new TaskAttemptContextImpl(job.getConfiguration(), new TaskAttemptID());\r\n                    RecordReader<Text, Text> reader = inFormat.createRecordReader(splits.get(sampleStep * idx), context);\r\n                    reader.initialize(splits.get(sampleStep * idx), context);\r\n                    while (reader.nextKeyValue()) {\r\n                        sampler.addKey(new Text(reader.getCurrentKey()));\r\n                        records += 1;\r\n                        if (recordsPerSample <= records) {\r\n                            break;\r\n                        }\r\n                    }\r\n                } catch (IOException ie) {\r\n                    System.err.println(\"Got an exception while reading splits \" + StringUtils.stringifyException(ie));\r\n                    throw new RuntimeException(ie);\r\n                } catch (InterruptedException e) {\r\n                }\r\n            }\r\n        };\r\n        samplerReader[i].start();\r\n    }\r\n    FileSystem outFs = partFile.getFileSystem(conf);\r\n    DataOutputStream writer = outFs.create(partFile, true, 64 * 1024, (short) 10, outFs.getDefaultBlockSize(partFile));\r\n    for (int i = 0; i < samples; i++) {\r\n        try {\r\n            samplerReader[i].join();\r\n            if (threadGroup.getThrowable() != null) {\r\n                throw threadGroup.getThrowable();\r\n            }\r\n        } catch (InterruptedException e) {\r\n        }\r\n    }\r\n    for (Text split : sampler.createPartitions(partitions)) {\r\n        split.write(writer);\r\n    }\r\n    writer.close();\r\n    long t3 = System.currentTimeMillis();\r\n    System.out.println(\"Computing parititions took \" + (t3 - t2) + \"ms\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<Text, Text> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException\n{\r\n    return new TeraRecordReader();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "List<InputSplit> getSplits(JobContext job) throws IOException\n{\r\n    if (job == lastContext) {\r\n        return lastResult;\r\n    }\r\n    long t1, t2, t3;\r\n    t1 = System.currentTimeMillis();\r\n    lastContext = job;\r\n    lastResult = super.getSplits(job);\r\n    t2 = System.currentTimeMillis();\r\n    System.out.println(\"Spent \" + (t2 - t1) + \"ms computing base-splits.\");\r\n    if (job.getConfiguration().getBoolean(TeraSortConfigKeys.USE_TERA_SCHEDULER.key(), TeraSortConfigKeys.DEFAULT_USE_TERA_SCHEDULER)) {\r\n        TeraScheduler scheduler = new TeraScheduler(lastResult.toArray(new FileSplit[0]), job.getConfiguration());\r\n        lastResult = scheduler.getNewFileSplits();\r\n        t3 = System.currentTimeMillis();\r\n        System.out.println(\"Spent \" + (t3 - t2) + \"ms computing TeraScheduler splits.\");\r\n    }\r\n    return lastResult;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "set",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Montgomery set(long n)\n{\r\n    if (n % 2 != 1)\r\n        throw new IllegalArgumentException(\"n % 2 != 1, n=\" + n);\r\n    N = n;\r\n    R = Long.highestOneBit(n) << 1;\r\n    N_I = R - Modular.modInverse(N, R);\r\n    R_1 = R - 1;\r\n    s = Long.numberOfTrailingZeros(R);\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "mod",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long mod(final long y)\n{\r\n    long p = R - N;\r\n    long x = p << 1;\r\n    if (x >= N)\r\n        x -= N;\r\n    for (long mask = Long.highestOneBit(y); mask > 0; mask >>>= 1) {\r\n        p = product.m(p, p);\r\n        if ((mask & y) != 0)\r\n            p = product.m(p, x);\r\n    }\r\n    return product.m(p, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "readAndFindMedian",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "double readAndFindMedian(String path, int medianIndex1, int medianIndex2, Configuration conf) throws IOException\n{\r\n    FileSystem fs = FileSystem.get(conf);\r\n    Path file = new Path(path, \"part-r-00000\");\r\n    if (!fs.exists(file))\r\n        throw new IOException(\"Output not found!\");\r\n    BufferedReader br = null;\r\n    try {\r\n        br = new BufferedReader(new InputStreamReader(fs.open(file), Charsets.UTF_8));\r\n        int num = 0;\r\n        String line;\r\n        while ((line = br.readLine()) != null) {\r\n            StringTokenizer st = new StringTokenizer(line);\r\n            String currLen = st.nextToken();\r\n            String lengthFreq = st.nextToken();\r\n            int prevNum = num;\r\n            num += Integer.parseInt(lengthFreq);\r\n            if (medianIndex2 >= prevNum && medianIndex1 <= num) {\r\n                System.out.println(\"The median is: \" + currLen);\r\n                br.close();\r\n                return Double.parseDouble(currLen);\r\n            } else if (medianIndex2 >= prevNum && medianIndex1 < num) {\r\n                String nextCurrLen = st.nextToken();\r\n                double theMedian = (Integer.parseInt(currLen) + Integer.parseInt(nextCurrLen)) / 2.0;\r\n                System.out.println(\"The median is: \" + theMedian);\r\n                br.close();\r\n                return theMedian;\r\n            }\r\n        }\r\n    } finally {\r\n        if (br != null) {\r\n            br.close();\r\n        }\r\n    }\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    ToolRunner.run(new Configuration(), new WordMedian(), args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\r\n    if (otherArgs.length != 2) {\r\n        System.err.println(\"Usage: wordmedian <in> <out>\");\r\n        return 0;\r\n    }\r\n    setConf(conf);\r\n    Job job = Job.getInstance(conf, \"word median\");\r\n    job.setJarByClass(WordMedian.class);\r\n    job.setMapperClass(WordMedianMapper.class);\r\n    job.setCombinerClass(WordMedianReducer.class);\r\n    job.setReducerClass(WordMedianReducer.class);\r\n    job.setOutputKeyClass(IntWritable.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    FileInputFormat.addInputPath(job, new Path(otherArgs[0]));\r\n    FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));\r\n    boolean result = job.waitForCompletion(true);\r\n    long totalWords = job.getCounters().getGroup(TaskCounter.class.getCanonicalName()).findCounter(\"MAP_OUTPUT_RECORDS\", \"Map output records\").getValue();\r\n    int medianIndex1 = (int) Math.ceil((totalWords / 2.0));\r\n    int medianIndex2 = (int) Math.floor((totalWords / 2.0));\r\n    median = readAndFindMedian(args[1], medianIndex1, medianIndex2, conf);\r\n    return (result ? 0 : 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "getMedian",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getMedian()\n{\r\n    return median;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "addColumn",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addColumn(ColumnName name, boolean primary)\n{\r\n    ColumnHeader<ColumnName> top = new ColumnHeader<ColumnName>(name, 0);\r\n    top.up = top;\r\n    top.down = top;\r\n    if (primary) {\r\n        Node<ColumnName> tail = head.left;\r\n        tail.right = top;\r\n        top.left = tail;\r\n        top.right = head;\r\n        head.left = top;\r\n    } else {\r\n        top.left = top;\r\n        top.right = top;\r\n    }\r\n    columns.add(top);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "addColumn",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addColumn(ColumnName name)\n{\r\n    addColumn(name, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "getNumberColumns",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumberColumns()\n{\r\n    return columns.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "getColumnName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getColumnName(int index)\n{\r\n    return columns.get(index).name.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "addRow",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addRow(boolean[] values)\n{\r\n    Node<ColumnName> prev = null;\r\n    for (int i = 0; i < values.length; ++i) {\r\n        if (values[i]) {\r\n            ColumnHeader<ColumnName> top = columns.get(i);\r\n            top.size += 1;\r\n            Node<ColumnName> bottom = top.up;\r\n            Node<ColumnName> node = new Node<ColumnName>(null, null, bottom, top, top);\r\n            bottom.down = node;\r\n            top.up = node;\r\n            if (prev != null) {\r\n                Node<ColumnName> front = prev.right;\r\n                node.left = prev;\r\n                node.right = front;\r\n                prev.right = node;\r\n                front.left = node;\r\n            } else {\r\n                node.left = node;\r\n                node.right = node;\r\n            }\r\n            prev = node;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "findBestColumn",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ColumnHeader<ColumnName> findBestColumn()\n{\r\n    int lowSize = Integer.MAX_VALUE;\r\n    ColumnHeader<ColumnName> result = null;\r\n    ColumnHeader<ColumnName> current = (ColumnHeader<ColumnName>) head.right;\r\n    while (current != head) {\r\n        if (current.size < lowSize) {\r\n            lowSize = current.size;\r\n            result = current;\r\n        }\r\n        current = (ColumnHeader<ColumnName>) current.right;\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "coverColumn",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void coverColumn(ColumnHeader<ColumnName> col)\n{\r\n    LOG.debug(\"cover \" + col.head.name);\r\n    col.right.left = col.left;\r\n    col.left.right = col.right;\r\n    Node<ColumnName> row = col.down;\r\n    while (row != col) {\r\n        Node<ColumnName> node = row.right;\r\n        while (node != row) {\r\n            node.down.up = node.up;\r\n            node.up.down = node.down;\r\n            node.head.size -= 1;\r\n            node = node.right;\r\n        }\r\n        row = row.down;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "uncoverColumn",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void uncoverColumn(ColumnHeader<ColumnName> col)\n{\r\n    LOG.debug(\"uncover \" + col.head.name);\r\n    Node<ColumnName> row = col.up;\r\n    while (row != col) {\r\n        Node<ColumnName> node = row.left;\r\n        while (node != row) {\r\n            node.head.size += 1;\r\n            node.down.up = node;\r\n            node.up.down = node;\r\n            node = node.left;\r\n        }\r\n        row = row.up;\r\n    }\r\n    col.right.left = col;\r\n    col.left.right = col;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "getRowName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<ColumnName> getRowName(Node<ColumnName> row)\n{\r\n    List<ColumnName> result = new ArrayList<ColumnName>();\r\n    result.add(row.head.name);\r\n    Node<ColumnName> node = row.right;\r\n    while (node != row) {\r\n        result.add(node.head.name);\r\n        node = node.right;\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "search",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "int search(List<Node<ColumnName>> partial, SolutionAcceptor<ColumnName> output)\n{\r\n    int results = 0;\r\n    if (head.right == head) {\r\n        List<List<ColumnName>> result = new ArrayList<List<ColumnName>>(partial.size());\r\n        for (Node<ColumnName> row : partial) {\r\n            result.add(getRowName(row));\r\n        }\r\n        output.solution(result);\r\n        results += 1;\r\n    } else {\r\n        ColumnHeader<ColumnName> col = findBestColumn();\r\n        if (col.size > 0) {\r\n            coverColumn(col);\r\n            Node<ColumnName> row = col.down;\r\n            while (row != col) {\r\n                partial.add(row);\r\n                Node<ColumnName> node = row.right;\r\n                while (node != row) {\r\n                    coverColumn(node.head);\r\n                    node = node.right;\r\n                }\r\n                results += search(partial, output);\r\n                partial.remove(partial.size() - 1);\r\n                node = row.left;\r\n                while (node != row) {\r\n                    uncoverColumn(node.head);\r\n                    node = node.left;\r\n                }\r\n                row = row.down;\r\n            }\r\n            uncoverColumn(col);\r\n        }\r\n    }\r\n    return results;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "searchPrefixes",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void searchPrefixes(int depth, int[] choices, List<int[]> prefixes)\n{\r\n    if (depth == 0) {\r\n        prefixes.add(choices.clone());\r\n    } else {\r\n        ColumnHeader<ColumnName> col = findBestColumn();\r\n        if (col.size > 0) {\r\n            coverColumn(col);\r\n            Node<ColumnName> row = col.down;\r\n            int rowId = 0;\r\n            while (row != col) {\r\n                Node<ColumnName> node = row.right;\r\n                while (node != row) {\r\n                    coverColumn(node.head);\r\n                    node = node.right;\r\n                }\r\n                choices[choices.length - depth] = rowId;\r\n                searchPrefixes(depth - 1, choices, prefixes);\r\n                node = row.left;\r\n                while (node != row) {\r\n                    uncoverColumn(node.head);\r\n                    node = node.left;\r\n                }\r\n                row = row.down;\r\n                rowId += 1;\r\n            }\r\n            uncoverColumn(col);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "split",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<int[]> split(int depth)\n{\r\n    int[] choices = new int[depth];\r\n    List<int[]> result = new ArrayList<int[]>(100000);\r\n    searchPrefixes(depth, choices, result);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "advance",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Node<ColumnName> advance(int goalRow)\n{\r\n    ColumnHeader<ColumnName> col = findBestColumn();\r\n    if (col.size > 0) {\r\n        coverColumn(col);\r\n        Node<ColumnName> row = col.down;\r\n        int id = 0;\r\n        while (row != col) {\r\n            if (id == goalRow) {\r\n                Node<ColumnName> node = row.right;\r\n                while (node != row) {\r\n                    coverColumn(node.head);\r\n                    node = node.right;\r\n                }\r\n                return row;\r\n            }\r\n            id += 1;\r\n            row = row.down;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "rollback",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void rollback(Node<ColumnName> row)\n{\r\n    Node<ColumnName> node = row.left;\r\n    while (node != row) {\r\n        uncoverColumn(node.head);\r\n        node = node.left;\r\n    }\r\n    uncoverColumn(row.head);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "solve",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int solve(int[] prefix, SolutionAcceptor<ColumnName> output)\n{\r\n    List<Node<ColumnName>> choices = new ArrayList<Node<ColumnName>>();\r\n    for (int i = 0; i < prefix.length; ++i) {\r\n        choices.add(advance(prefix[i]));\r\n    }\r\n    int result = search(choices, output);\r\n    for (int i = prefix.length - 1; i >= 0; --i) {\r\n        rollback(choices.get(i));\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\dancing",
  "methodName" : "solve",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int solve(SolutionAcceptor<ColumnName> output)\n{\r\n    return search(new ArrayList<Node<ColumnName>>(), output);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (this == obj)\r\n        return true;\r\n    else if (obj != null && obj instanceof ArithmeticProgression) {\r\n        final ArithmeticProgression that = (ArithmeticProgression) obj;\r\n        if (this.symbol != that.symbol)\r\n            throw new IllegalArgumentException(\"this.symbol != that.symbol, this=\" + this + \", that=\" + that);\r\n        return this.value == that.value && this.delta == that.delta && this.limit == that.limit;\r\n    }\r\n    throw new IllegalArgumentException(obj == null ? \"obj == null\" : \"obj.getClass()=\" + obj.getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int hashCode()\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int compareTo(ArithmeticProgression that)\n{\r\n    if (this.symbol != that.symbol)\r\n        throw new IllegalArgumentException(\"this.symbol != that.symbol, this=\" + this + \", that=\" + that);\r\n    if (this.delta != that.delta)\r\n        throw new IllegalArgumentException(\"this.delta != that.delta, this=\" + this + \", that=\" + that);\r\n    final long d = this.limit - that.limit;\r\n    return d > 0 ? 1 : d == 0 ? 0 : -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "contains",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean contains(ArithmeticProgression that)\n{\r\n    if (this.symbol != that.symbol)\r\n        throw new IllegalArgumentException(\"this.symbol != that.symbol, this=\" + this + \", that=\" + that);\r\n    if (this.delta == that.delta) {\r\n        if (this.value == that.value)\r\n            return this.getSteps() >= that.getSteps();\r\n        else if (this.delta < 0)\r\n            return this.value > that.value && this.limit <= that.limit;\r\n        else if (this.delta > 0)\r\n            return this.value < that.value && this.limit >= that.limit;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "skip",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long skip(long steps)\n{\r\n    if (steps < 0)\r\n        throw new IllegalArgumentException(\"steps < 0, steps=\" + steps);\r\n    return value + steps * delta;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "getSteps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSteps()\n{\r\n    return (limit - value) / delta;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return symbol + \":value=\" + value + \",delta=\" + delta + \",limit=\" + limit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "valueOf",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ArithmeticProgression valueOf(final String s)\n{\r\n    int i = 2;\r\n    int j = s.indexOf(\",delta=\");\r\n    final long value = Util.parseLongVariable(\"value\", s.substring(2, j));\r\n    i = j + 1;\r\n    j = s.indexOf(\",limit=\");\r\n    final long delta = Util.parseLongVariable(\"delta\", s.substring(i, j));\r\n    i = j + 1;\r\n    final long limit = Util.parseLongVariable(\"limit\", s.substring(i));\r\n    return new ArithmeticProgression(s.charAt(0), value, delta, limit);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "millis2String",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "String millis2String(long n)\n{\r\n    if (n < 0)\r\n        return \"-\" + millis2String(-n);\r\n    else if (n < 1000)\r\n        return n + \"ms\";\r\n    final StringBuilder b = new StringBuilder();\r\n    final int millis = (int) (n % 1000L);\r\n    if (millis != 0)\r\n        b.append(String.format(\".%03d\", millis));\r\n    if ((n /= 1000) < 60)\r\n        return b.insert(0, n).append(\"s\").toString();\r\n    b.insert(0, String.format(\":%02d\", (int) (n % 60L)));\r\n    if ((n /= 60) < 60)\r\n        return b.insert(0, n).toString();\r\n    b.insert(0, String.format(\":%02d\", (int) (n % 60L)));\r\n    if ((n /= 60) < 24)\r\n        return b.insert(0, n).toString();\r\n    b.insert(0, n % 24L);\r\n    final int days = (int) ((n /= 24) % 365L);\r\n    b.insert(0, days == 1 ? \" day \" : \" days \").insert(0, days);\r\n    if ((n /= 365L) > 0)\r\n        b.insert(0, n == 1 ? \" year \" : \" years \").insert(0, n);\r\n    return b.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "string2long",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long string2long(String s)\n{\r\n    return Long.parseLong(s.trim().replace(\",\", \"\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "long2string",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String long2string(long n)\n{\r\n    if (n < 0)\r\n        return \"-\" + long2string(-n);\r\n    final StringBuilder b = new StringBuilder();\r\n    for (; n >= 1000; n = n / 1000) b.insert(0, String.format(\",%03d\", n % 1000));\r\n    return n + b.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "parseLongVariable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long parseLongVariable(final String name, final String s)\n{\r\n    return string2long(parseStringVariable(name, s));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "parseStringVariable",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String parseStringVariable(final String name, final String s)\n{\r\n    if (!s.startsWith(name + '='))\r\n        throw new IllegalArgumentException(\"!s.startsWith(name + '='), name=\" + name + \", s=\" + s);\r\n    return s.substring(name.length() + 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "execute",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void execute(int nThreads, List<E> callables) throws InterruptedException, ExecutionException\n{\r\n    final ExecutorService executor = HadoopExecutors.newFixedThreadPool(nThreads);\r\n    final List<Future<T>> futures = executor.invokeAll(callables);\r\n    for (Future<T> f : futures) f.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int printUsage(String[] args, String usage)\n{\r\n    err.println(\"args = \" + Arrays.asList(args));\r\n    err.println();\r\n    err.println(\"Usage: java \" + usage);\r\n    err.println();\r\n    ToolRunner.printGenericCommandUsage(err);\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "combine",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "List<T> combine(Collection<T> items)\n{\r\n    final List<T> sorted = new ArrayList<T>(items);\r\n    if (sorted.size() <= 1)\r\n        return sorted;\r\n    Collections.sort(sorted);\r\n    final List<T> combined = new ArrayList<T>(items.size());\r\n    T prev = sorted.get(0);\r\n    for (int i = 1; i < sorted.size(); i++) {\r\n        final T curr = sorted.get(i);\r\n        final T c = curr.combine(prev);\r\n        if (c != null)\r\n            prev = c;\r\n        else {\r\n            combined.add(prev);\r\n            prev = curr;\r\n        }\r\n    }\r\n    combined.add(prev);\r\n    return combined;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "checkDirectory",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkDirectory(File dir)\n{\r\n    if (!dir.exists())\r\n        if (!dir.mkdirs())\r\n            throw new IllegalArgumentException(\"!dir.mkdirs(), dir=\" + dir);\r\n    if (!dir.isDirectory())\r\n        throw new IllegalArgumentException(\"dir (=\" + dir + \") is not a directory.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "createWriter",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "PrintWriter createWriter(File dir, String prefix) throws IOException\n{\r\n    checkDirectory(dir);\r\n    SimpleDateFormat dateFormat = new SimpleDateFormat(\"-yyyyMMdd-HHmmssSSS\");\r\n    for (; ; ) {\r\n        final File f = new File(dir, prefix + dateFormat.format(new Date(System.currentTimeMillis())) + \".txt\");\r\n        if (!f.exists())\r\n            return new PrintWriter(new OutputStreamWriter(new FileOutputStream(f), Charsets.UTF_8));\r\n        try {\r\n            Thread.sleep(10);\r\n        } catch (InterruptedException e) {\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "printBitSkipped",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void printBitSkipped(final long b)\n{\r\n    out.println();\r\n    out.println(\"b = \" + long2string(b) + \" (\" + (b < 2 ? \"bit\" : \"bits\") + \" skipped)\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "pi2string",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String pi2string(final double pi, final long terms)\n{\r\n    final long value = (long) (pi * (1L << DOUBLE_PRECISION));\r\n    final int acc_bit = accuracy(terms, false);\r\n    final int acc_hex = acc_bit / 4;\r\n    final int shift = DOUBLE_PRECISION - acc_bit;\r\n    return String.format(\"%0\" + acc_hex + \"X %0\" + (13 - acc_hex) + \"X (%d hex digits)\", value >> shift, value & ((1 << shift) - 1), acc_hex);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "accuracy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int accuracy(final long terms, boolean print)\n{\r\n    final double error = terms <= 0 ? 2 : (Math.log(terms) / Math.log(2)) / 2;\r\n    final int bits = MACHEPS_EXPONENT - (int) Math.ceil(error);\r\n    if (print)\r\n        out.println(\"accuracy: bits=\" + bits + \", terms=\" + long2string(terms) + \", error exponent=\" + error);\r\n    return bits - bits % 4;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "runJob",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void runJob(String name, Job job, Machine machine, String startmessage, Util.Timer timer)\n{\r\n    JOB_SEMAPHORE.acquireUninterruptibly();\r\n    Long starttime = null;\r\n    try {\r\n        try {\r\n            starttime = timer.tick(\"starting \" + name + \" ...\\n  \" + startmessage);\r\n            machine.init(job);\r\n            job.submit();\r\n            final long sleeptime = 1000L * job.getConfiguration().getInt(JOB_SEPARATION_PROPERTY, 10);\r\n            if (sleeptime > 0) {\r\n                Util.out.println(name + \"> sleep(\" + Util.millis2String(sleeptime) + \")\");\r\n                Thread.sleep(sleeptime);\r\n            }\r\n        } finally {\r\n            JOB_SEMAPHORE.release();\r\n        }\r\n        if (!job.waitForCompletion(false))\r\n            throw new RuntimeException(name + \" failed.\");\r\n    } catch (Exception e) {\r\n        throw e instanceof RuntimeException ? (RuntimeException) e : new RuntimeException(e);\r\n    } finally {\r\n        if (starttime != null)\r\n            timer.tick(name + \"> timetaken=\" + Util.millis2String(timer.tick() - starttime));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "readJobOutputs",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<TaskResult> readJobOutputs(FileSystem fs, Path outdir) throws IOException\n{\r\n    final List<TaskResult> results = new ArrayList<TaskResult>();\r\n    for (FileStatus status : fs.listStatus(outdir)) {\r\n        if (status.getPath().getName().startsWith(\"part-\")) {\r\n            final BufferedReader in = new BufferedReader(new InputStreamReader(fs.open(status.getPath()), Charsets.UTF_8));\r\n            try {\r\n                for (String line; (line = in.readLine()) != null; ) results.add(TaskResult.valueOf(line));\r\n            } finally {\r\n                in.close();\r\n            }\r\n        }\r\n    }\r\n    if (results.isEmpty())\r\n        throw new IOException(\"Output not found\");\r\n    return results;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "writeResults",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeResults(String name, List<TaskResult> results, FileSystem fs, String dir) throws IOException\n{\r\n    final Path outfile = new Path(dir, name + \".txt\");\r\n    Util.out.println(name + \"> writing results to \" + outfile);\r\n    final PrintWriter out = new PrintWriter(new OutputStreamWriter(fs.create(outfile), Charsets.UTF_8), true);\r\n    try {\r\n        for (TaskResult r : results) out.println(r);\r\n    } finally {\r\n        out.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "createNonexistingDirectory",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean createNonexistingDirectory(FileSystem fs, Path dir) throws IOException\n{\r\n    if (fs.exists(dir)) {\r\n        Util.err.println(\"dir (= \" + dir + \") already exists.\");\r\n        return false;\r\n    } else if (!fs.mkdirs(dir)) {\r\n        throw new IOException(\"Cannot create working directory \" + dir);\r\n    }\r\n    fs.setPermission(dir, new FsPermission((short) 0777));\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int printUsage()\n{\r\n    System.out.println(\"join [-r <reduces>] \" + \"[-inFormat <input format class>] \" + \"[-outFormat <output format class>] \" + \"[-outKey <output key class>] \" + \"[-outValue <output value class>] \" + \"[-joinOp <inner|outer|override>] \" + \"[input]* <input> <output>\");\r\n    ToolRunner.printGenericCommandUsage(System.out);\r\n    return 2;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "run",
  "errType" : [ "NumberFormatException", "ArrayIndexOutOfBoundsException" ],
  "containingMethodsNum" : 43,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    Configuration conf = getConf();\r\n    JobClient client = new JobClient(conf);\r\n    ClusterStatus cluster = client.getClusterStatus();\r\n    int num_reduces = (int) (cluster.getMaxReduceTasks() * 0.9);\r\n    String join_reduces = conf.get(REDUCES_PER_HOST);\r\n    if (join_reduces != null) {\r\n        num_reduces = cluster.getTaskTrackers() * Integer.parseInt(join_reduces);\r\n    }\r\n    Job job = Job.getInstance(conf);\r\n    job.setJobName(\"join\");\r\n    job.setJarByClass(Sort.class);\r\n    job.setMapperClass(Mapper.class);\r\n    job.setReducerClass(Reducer.class);\r\n    Class<? extends InputFormat> inputFormatClass = SequenceFileInputFormat.class;\r\n    Class<? extends OutputFormat> outputFormatClass = SequenceFileOutputFormat.class;\r\n    Class<? extends WritableComparable> outputKeyClass = BytesWritable.class;\r\n    Class<? extends Writable> outputValueClass = TupleWritable.class;\r\n    String op = \"inner\";\r\n    List<String> otherArgs = new ArrayList<String>();\r\n    for (int i = 0; i < args.length; ++i) {\r\n        try {\r\n            if (\"-r\".equals(args[i])) {\r\n                num_reduces = Integer.parseInt(args[++i]);\r\n            } else if (\"-inFormat\".equals(args[i])) {\r\n                inputFormatClass = Class.forName(args[++i]).asSubclass(InputFormat.class);\r\n            } else if (\"-outFormat\".equals(args[i])) {\r\n                outputFormatClass = Class.forName(args[++i]).asSubclass(OutputFormat.class);\r\n            } else if (\"-outKey\".equals(args[i])) {\r\n                outputKeyClass = Class.forName(args[++i]).asSubclass(WritableComparable.class);\r\n            } else if (\"-outValue\".equals(args[i])) {\r\n                outputValueClass = Class.forName(args[++i]).asSubclass(Writable.class);\r\n            } else if (\"-joinOp\".equals(args[i])) {\r\n                op = args[++i];\r\n            } else {\r\n                otherArgs.add(args[i]);\r\n            }\r\n        } catch (NumberFormatException except) {\r\n            System.out.println(\"ERROR: Integer expected instead of \" + args[i]);\r\n            return printUsage();\r\n        } catch (ArrayIndexOutOfBoundsException except) {\r\n            System.out.println(\"ERROR: Required parameter missing from \" + args[i - 1]);\r\n            return printUsage();\r\n        }\r\n    }\r\n    job.setNumReduceTasks(num_reduces);\r\n    if (otherArgs.size() < 2) {\r\n        System.out.println(\"ERROR: Wrong number of parameters: \");\r\n        return printUsage();\r\n    }\r\n    FileOutputFormat.setOutputPath(job, new Path(otherArgs.remove(otherArgs.size() - 1)));\r\n    List<Path> plist = new ArrayList<Path>(otherArgs.size());\r\n    for (String s : otherArgs) {\r\n        plist.add(new Path(s));\r\n    }\r\n    job.setInputFormatClass(CompositeInputFormat.class);\r\n    job.getConfiguration().set(CompositeInputFormat.JOIN_EXPR, CompositeInputFormat.compose(op, inputFormatClass, plist.toArray(new Path[0])));\r\n    job.setOutputFormatClass(outputFormatClass);\r\n    job.setOutputKeyClass(outputKeyClass);\r\n    job.setOutputValueClass(outputValueClass);\r\n    Date startTime = new Date();\r\n    System.out.println(\"Job started: \" + startTime);\r\n    int ret = job.waitForCompletion(true) ? 0 : 1;\r\n    Date end_time = new Date();\r\n    System.out.println(\"Job ended: \" + end_time);\r\n    System.out.println(\"The job took \" + (end_time.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new Join(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "getUseSimplePartitioner",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getUseSimplePartitioner(JobContext job)\n{\r\n    return job.getConfiguration().getBoolean(TeraSortConfigKeys.USE_SIMPLE_PARTITIONER.key(), TeraSortConfigKeys.DEFAULT_USE_SIMPLE_PARTITIONER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "setUseSimplePartitioner",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUseSimplePartitioner(Job job, boolean value)\n{\r\n    job.getConfiguration().setBoolean(TeraSortConfigKeys.USE_SIMPLE_PARTITIONER.key(), value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "getOutputReplication",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getOutputReplication(JobContext job)\n{\r\n    return job.getConfiguration().getInt(TeraSortConfigKeys.OUTPUT_REPLICATION.key(), TeraSortConfigKeys.DEFAULT_OUTPUT_REPLICATION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "setOutputReplication",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOutputReplication(Job job, int value)\n{\r\n    job.getConfiguration().setInt(TeraSortConfigKeys.OUTPUT_REPLICATION.key(), value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "usage",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void usage() throws IOException\n{\r\n    System.err.println(\"Usage: terasort [-Dproperty=value] <in> <out>\");\r\n    System.err.println(\"TeraSort configurations are:\");\r\n    for (TeraSortConfigKeys teraSortConfigKeys : TeraSortConfigKeys.values()) {\r\n        System.err.println(teraSortConfigKeys.toString());\r\n    }\r\n    System.err.println(\"If you want to store the output data as \" + \"erasure code striping file, just make sure that the parent dir \" + \"of <out> has erasure code policy set\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "run",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length != 2) {\r\n        usage();\r\n        return 2;\r\n    }\r\n    LOG.info(\"starting\");\r\n    Job job = Job.getInstance(getConf());\r\n    Path inputDir = new Path(args[0]);\r\n    Path outputDir = new Path(args[1]);\r\n    boolean useSimplePartitioner = getUseSimplePartitioner(job);\r\n    TeraInputFormat.setInputPaths(job, inputDir);\r\n    FileOutputFormat.setOutputPath(job, outputDir);\r\n    job.setJobName(\"TeraSort\");\r\n    job.setJarByClass(TeraSort.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setInputFormatClass(TeraInputFormat.class);\r\n    job.setOutputFormatClass(TeraOutputFormat.class);\r\n    if (useSimplePartitioner) {\r\n        job.setPartitionerClass(SimplePartitioner.class);\r\n    } else {\r\n        long start = System.currentTimeMillis();\r\n        Path partitionFile = new Path(outputDir, TeraInputFormat.PARTITION_FILENAME);\r\n        URI partitionUri = new URI(partitionFile.toString() + \"#\" + TeraInputFormat.PARTITION_FILENAME);\r\n        try {\r\n            TeraInputFormat.writePartitionFile(job, partitionFile);\r\n        } catch (Throwable e) {\r\n            LOG.error(\"{}\", e.getMessage(), e);\r\n            return -1;\r\n        }\r\n        job.addCacheFile(partitionUri);\r\n        long end = System.currentTimeMillis();\r\n        System.out.println(\"Spent \" + (end - start) + \"ms computing partitions.\");\r\n        job.setPartitionerClass(TotalOrderPartitioner.class);\r\n    }\r\n    job.getConfiguration().setInt(\"dfs.replication\", getOutputReplication(job));\r\n    int ret = job.waitForCompletion(true) ? 0 : 1;\r\n    LOG.info(\"done\");\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\terasort",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new TeraSort(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void printUsage()\n{\r\n    System.out.println(\"Usage : multifilewc <input_dir> <output>\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length < 2) {\r\n        printUsage();\r\n        return 2;\r\n    }\r\n    Job job = Job.getInstance(getConf());\r\n    job.setJobName(\"MultiFileWordCount\");\r\n    job.setJarByClass(MultiFileWordCount.class);\r\n    job.setInputFormatClass(MyInputFormat.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    job.setMapperClass(MapClass.class);\r\n    job.setCombinerClass(IntSumReducer.class);\r\n    job.setReducerClass(IntSumReducer.class);\r\n    FileInputFormat.addInputPaths(job, args[0]);\r\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\r\n    return job.waitForCompletion(true) ? 0 : 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int ret = ToolRunner.run(new MultiFileWordCount(), args);\r\n    System.exit(ret);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "estimatePi",
  "errType" : null,
  "containingMethodsNum" : 34,
  "sourceCodeText" : "BigDecimal estimatePi(int numMaps, long numPoints, Path tmpDir, Configuration conf) throws IOException, ClassNotFoundException, InterruptedException\n{\r\n    Job job = Job.getInstance(conf);\r\n    job.setJobName(QuasiMonteCarlo.class.getSimpleName());\r\n    job.setJarByClass(QuasiMonteCarlo.class);\r\n    job.setInputFormatClass(SequenceFileInputFormat.class);\r\n    job.setOutputKeyClass(BooleanWritable.class);\r\n    job.setOutputValueClass(LongWritable.class);\r\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\r\n    job.setMapperClass(QmcMapper.class);\r\n    job.setReducerClass(QmcReducer.class);\r\n    job.setNumReduceTasks(1);\r\n    job.setSpeculativeExecution(false);\r\n    final Path inDir = new Path(tmpDir, \"in\");\r\n    final Path outDir = new Path(tmpDir, \"out\");\r\n    FileInputFormat.setInputPaths(job, inDir);\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    final FileSystem fs = FileSystem.get(conf);\r\n    if (fs.exists(tmpDir)) {\r\n        throw new IOException(\"Tmp directory \" + fs.makeQualified(tmpDir) + \" already exists.  Please remove it first.\");\r\n    }\r\n    if (!fs.mkdirs(inDir)) {\r\n        throw new IOException(\"Cannot create input directory \" + inDir);\r\n    }\r\n    try {\r\n        for (int i = 0; i < numMaps; ++i) {\r\n            final Path file = new Path(inDir, \"part\" + i);\r\n            final LongWritable offset = new LongWritable(i * numPoints);\r\n            final LongWritable size = new LongWritable(numPoints);\r\n            final SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, file, LongWritable.class, LongWritable.class, CompressionType.NONE);\r\n            try {\r\n                writer.append(offset, size);\r\n            } finally {\r\n                writer.close();\r\n            }\r\n            System.out.println(\"Wrote input for Map #\" + i);\r\n        }\r\n        System.out.println(\"Starting Job\");\r\n        final long startTime = Time.monotonicNow();\r\n        job.waitForCompletion(true);\r\n        if (!job.isSuccessful()) {\r\n            System.out.println(\"Job \" + job.getJobID() + \" failed!\");\r\n            System.exit(1);\r\n        }\r\n        final double duration = (Time.monotonicNow() - startTime) / 1000.0;\r\n        System.out.println(\"Job Finished in \" + duration + \" seconds\");\r\n        Path inFile = new Path(outDir, \"reduce-out\");\r\n        LongWritable numInside = new LongWritable();\r\n        LongWritable numOutside = new LongWritable();\r\n        SequenceFile.Reader reader = new SequenceFile.Reader(fs, inFile, conf);\r\n        try {\r\n            reader.next(numInside, numOutside);\r\n        } finally {\r\n            reader.close();\r\n        }\r\n        final BigDecimal numTotal = BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));\r\n        return BigDecimal.valueOf(4).setScale(20).multiply(BigDecimal.valueOf(numInside.get())).divide(numTotal, RoundingMode.HALF_UP);\r\n    } finally {\r\n        fs.delete(tmpDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length != 2) {\r\n        System.err.println(\"Usage: \" + getClass().getName() + \" <nMaps> <nSamples>\");\r\n        ToolRunner.printGenericCommandUsage(System.err);\r\n        return 2;\r\n    }\r\n    final int nMaps = Integer.parseInt(args[0]);\r\n    final long nSamples = Long.parseLong(args[1]);\r\n    long now = System.currentTimeMillis();\r\n    int rand = new Random().nextInt(Integer.MAX_VALUE);\r\n    final Path tmpDir = new Path(TMP_DIR_PREFIX + \"_\" + now + \"_\" + rand);\r\n    System.out.println(\"Number of Maps  = \" + nMaps);\r\n    System.out.println(\"Samples per Map = \" + nSamples);\r\n    System.out.println(\"Estimated value of Pi is \" + estimatePi(nMaps, nSamples, tmpDir, getConf()));\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    System.exit(ToolRunner.run(null, new QuasiMonteCarlo(), argv));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return getClass().getSimpleName() + sigma;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "getElement",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Summation getElement()\n{\r\n    return sigma;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Summation read(Class<?> clazz, Configuration conf)\n{\r\n    return Summation.valueOf(conf.get(clazz.getSimpleName() + \".sigma\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(Summation sigma, Class<?> clazz, Configuration conf)\n{\r\n    conf.set(clazz.getSimpleName() + \".sigma\", sigma.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Summation read(DataInput in) throws IOException\n{\r\n    final SummationWritable s = new SummationWritable();\r\n    s.readFields(in);\r\n    return s.getElement();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    final ArithmeticProgression N = ArithmeticProgressionWritable.read(in);\r\n    final ArithmeticProgression E = ArithmeticProgressionWritable.read(in);\r\n    sigma = new Summation(N, E);\r\n    if (in.readBoolean()) {\r\n        sigma.setValue(in.readDouble());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void write(Summation sigma, DataOutput out) throws IOException\n{\r\n    ArithmeticProgressionWritable.write(sigma.N, out);\r\n    ArithmeticProgressionWritable.write(sigma.E, out);\r\n    final Double v = sigma.getValue();\r\n    if (v == null)\r\n        out.writeBoolean(false);\r\n    else {\r\n        out.writeBoolean(true);\r\n        out.writeDouble(v);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    write(sigma, out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int compareTo(SummationWritable that)\n{\r\n    return this.sigma.compareTo(that.sigma);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (this == obj)\r\n        return true;\r\n    else if (obj != null && obj instanceof SummationWritable) {\r\n        final SummationWritable that = (SummationWritable) obj;\r\n        return this.compareTo(that) == 0;\r\n    }\r\n    throw new IllegalArgumentException(obj == null ? \"obj == null\" : \"obj.getClass()=\" + obj.getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int hashCode()\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "getSums",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<Parameter, Sum> getSums(long b, int partsPerSum, Map<Parameter, List<T>> existing)\n{\r\n    final Map<Parameter, Sum> sums = new TreeMap<Parameter, Sum>();\r\n    for (Parameter p : Parameter.values()) {\r\n        final Sum s = new Sum(b, p, partsPerSum, existing.get(p));\r\n        Util.out.println(\"put \" + s);\r\n        sums.put(p, s);\r\n    }\r\n    return sums;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "computePi",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "double computePi(final long b, Map<Parameter, T> results)\n{\r\n    if (results.size() != Parameter.values().length)\r\n        throw new IllegalArgumentException(\"m.size() != Parameter.values().length\" + \", m.size()=\" + results.size() + \"\\n  m=\" + results);\r\n    double pi = 0;\r\n    for (Parameter p : Parameter.values()) {\r\n        final Summation sigma = results.get(p).getElement();\r\n        final Sum s = new Sum(b, p, 1, null);\r\n        s.setValue(sigma);\r\n        pi = Modular.addMod(pi, s.getValue());\r\n    }\r\n    return pi;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "computePi",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "double computePi(final long b)\n{\r\n    double pi = 0;\r\n    for (Parameter p : Parameter.values()) pi = Modular.addMod(pi, new Sum(b, p, 1, null).getValue());\r\n    return pi;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "bit2terms",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long bit2terms(long b)\n{\r\n    return 7 * (b / 10);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "computePi",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void computePi(Util.Timer t, long b)\n{\r\n    t.tick(Util.pi2string(computePi(b), bit2terms(b)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    final Util.Timer t = new Util.Timer(false);\r\n    computePi(t, 0);\r\n    computePi(t, 1);\r\n    computePi(t, 2);\r\n    computePi(t, 3);\r\n    computePi(t, 4);\r\n    Util.printBitSkipped(1008);\r\n    computePi(t, 1008);\r\n    computePi(t, 1012);\r\n    long b = 10;\r\n    for (int i = 0; i < 7; i++) {\r\n        Util.printBitSkipped(b);\r\n        computePi(t, b - 4);\r\n        computePi(t, b);\r\n        computePi(t, b + 4);\r\n        b *= 10;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int printUsage()\n{\r\n    System.out.println(\"sort [-r <reduces>] \" + \"[-inFormat <input format class>] \" + \"[-outFormat <output format class>] \" + \"[-outKey <output key class>] \" + \"[-outValue <output value class>] \" + \"[-totalOrder <pcnt> <num samples> <max splits>] \" + \"<input> <output>\");\r\n    ToolRunner.printGenericCommandUsage(System.out);\r\n    return 2;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "run",
  "errType" : [ "NumberFormatException", "ArrayIndexOutOfBoundsException" ],
  "containingMethodsNum" : 54,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    Configuration conf = getConf();\r\n    JobClient client = new JobClient(conf);\r\n    ClusterStatus cluster = client.getClusterStatus();\r\n    int num_reduces = (int) (cluster.getMaxReduceTasks() * 0.9);\r\n    String sort_reduces = conf.get(REDUCES_PER_HOST);\r\n    if (sort_reduces != null) {\r\n        num_reduces = cluster.getTaskTrackers() * Integer.parseInt(sort_reduces);\r\n    }\r\n    Class<? extends InputFormat> inputFormatClass = SequenceFileInputFormat.class;\r\n    Class<? extends OutputFormat> outputFormatClass = SequenceFileOutputFormat.class;\r\n    Class<? extends WritableComparable> outputKeyClass = BytesWritable.class;\r\n    Class<? extends Writable> outputValueClass = BytesWritable.class;\r\n    List<String> otherArgs = new ArrayList<String>();\r\n    InputSampler.Sampler<K, V> sampler = null;\r\n    for (int i = 0; i < args.length; ++i) {\r\n        try {\r\n            if (\"-r\".equals(args[i])) {\r\n                num_reduces = Integer.parseInt(args[++i]);\r\n            } else if (\"-inFormat\".equals(args[i])) {\r\n                inputFormatClass = Class.forName(args[++i]).asSubclass(InputFormat.class);\r\n            } else if (\"-outFormat\".equals(args[i])) {\r\n                outputFormatClass = Class.forName(args[++i]).asSubclass(OutputFormat.class);\r\n            } else if (\"-outKey\".equals(args[i])) {\r\n                outputKeyClass = Class.forName(args[++i]).asSubclass(WritableComparable.class);\r\n            } else if (\"-outValue\".equals(args[i])) {\r\n                outputValueClass = Class.forName(args[++i]).asSubclass(Writable.class);\r\n            } else if (\"-totalOrder\".equals(args[i])) {\r\n                double pcnt = Double.parseDouble(args[++i]);\r\n                int numSamples = Integer.parseInt(args[++i]);\r\n                int maxSplits = Integer.parseInt(args[++i]);\r\n                if (0 >= maxSplits)\r\n                    maxSplits = Integer.MAX_VALUE;\r\n                sampler = new InputSampler.RandomSampler<K, V>(pcnt, numSamples, maxSplits);\r\n            } else {\r\n                otherArgs.add(args[i]);\r\n            }\r\n        } catch (NumberFormatException except) {\r\n            System.out.println(\"ERROR: Integer expected instead of \" + args[i]);\r\n            return printUsage();\r\n        } catch (ArrayIndexOutOfBoundsException except) {\r\n            System.out.println(\"ERROR: Required parameter missing from \" + args[i - 1]);\r\n            return printUsage();\r\n        }\r\n    }\r\n    job = Job.getInstance(conf);\r\n    job.setJobName(\"sorter\");\r\n    job.setJarByClass(Sort.class);\r\n    job.setMapperClass(Mapper.class);\r\n    job.setReducerClass(Reducer.class);\r\n    job.setNumReduceTasks(num_reduces);\r\n    job.setInputFormatClass(inputFormatClass);\r\n    job.setOutputFormatClass(outputFormatClass);\r\n    job.setOutputKeyClass(outputKeyClass);\r\n    job.setOutputValueClass(outputValueClass);\r\n    if (otherArgs.size() != 2) {\r\n        System.out.println(\"ERROR: Wrong number of parameters: \" + otherArgs.size() + \" instead of 2.\");\r\n        return printUsage();\r\n    }\r\n    FileInputFormat.setInputPaths(job, otherArgs.get(0));\r\n    FileOutputFormat.setOutputPath(job, new Path(otherArgs.get(1)));\r\n    if (sampler != null) {\r\n        System.out.println(\"Sampling input to effect total-order sort...\");\r\n        job.setPartitionerClass(TotalOrderPartitioner.class);\r\n        Path inputDir = FileInputFormat.getInputPaths(job)[0];\r\n        FileSystem fs = inputDir.getFileSystem(conf);\r\n        inputDir = inputDir.makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n        Path partitionFile = new Path(inputDir, \"_sortPartitioning\");\r\n        TotalOrderPartitioner.setPartitionFile(conf, partitionFile);\r\n        InputSampler.<K, V>writePartitionFile(job, sampler);\r\n        URI partitionUri = new URI(partitionFile.toString() + \"#\" + \"_sortPartitioning\");\r\n        job.addCacheFile(partitionUri);\r\n    }\r\n    System.out.println(\"Running on \" + cluster.getTaskTrackers() + \" nodes to sort from \" + FileInputFormat.getInputPaths(job)[0] + \" into \" + FileOutputFormat.getOutputPath(job) + \" with \" + num_reduces + \" reduces.\");\r\n    Date startTime = new Date();\r\n    System.out.println(\"Job started: \" + startTime);\r\n    int ret = job.waitForCompletion(true) ? 0 : 1;\r\n    Date end_time = new Date();\r\n    System.out.println(\"Job ended: \" + end_time);\r\n    System.out.println(\"The job took \" + (end_time.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new Sort(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "getResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Job getResult()\n{\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "readAndCalcMean",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "double readAndCalcMean(Path path, Configuration conf) throws IOException\n{\r\n    FileSystem fs = FileSystem.get(conf);\r\n    Path file = new Path(path, \"part-r-00000\");\r\n    if (!fs.exists(file))\r\n        throw new IOException(\"Output not found!\");\r\n    BufferedReader br = null;\r\n    try {\r\n        br = new BufferedReader(new InputStreamReader(fs.open(file), Charsets.UTF_8));\r\n        long count = 0;\r\n        long length = 0;\r\n        String line;\r\n        while ((line = br.readLine()) != null) {\r\n            StringTokenizer st = new StringTokenizer(line);\r\n            String type = st.nextToken();\r\n            if (type.equals(COUNT.toString())) {\r\n                String countLit = st.nextToken();\r\n                count = Long.parseLong(countLit);\r\n            } else if (type.equals(LENGTH.toString())) {\r\n                String lengthLit = st.nextToken();\r\n                length = Long.parseLong(lengthLit);\r\n            }\r\n        }\r\n        double theMean = (((double) length) / ((double) count));\r\n        System.out.println(\"The mean is: \" + theMean);\r\n        return theMean;\r\n    } finally {\r\n        if (br != null) {\r\n            br.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    ToolRunner.run(new Configuration(), new WordMean(), args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length != 2) {\r\n        System.err.println(\"Usage: wordmean <in> <out>\");\r\n        return 0;\r\n    }\r\n    Configuration conf = getConf();\r\n    Job job = Job.getInstance(conf, \"word mean\");\r\n    job.setJarByClass(WordMean.class);\r\n    job.setMapperClass(WordMeanMapper.class);\r\n    job.setCombinerClass(WordMeanReducer.class);\r\n    job.setReducerClass(WordMeanReducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(LongWritable.class);\r\n    FileInputFormat.addInputPath(job, new Path(args[0]));\r\n    Path outputpath = new Path(args[1]);\r\n    FileOutputFormat.setOutputPath(job, outputpath);\r\n    boolean result = job.waitForCompletion(true);\r\n    mean = readAndCalcMean(outputpath, conf);\r\n    return (result ? 0 : 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples",
  "methodName" : "getMean",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getMean()\n{\r\n    return mean;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "getElement",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Summation getElement()\n{\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "getSteps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getSteps()\n{\r\n    return E.getSteps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Double getValue()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "setValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setValue(double v)\n{\r\n    this.value = v;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"[\" + N + \"; \" + E + (value == null ? \"]\" : \"]value=\" + Double.doubleToLongBits(value));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (obj == this)\r\n        return true;\r\n    if (obj != null && obj instanceof Summation) {\r\n        final Summation that = (Summation) obj;\r\n        return this.N.equals(that.N) && this.E.equals(that.E);\r\n    }\r\n    throw new IllegalArgumentException(obj == null ? \"obj == null\" : \"obj.getClass()=\" + obj.getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int hashCode()\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "valueOf",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Summation valueOf(final String s)\n{\r\n    int i = 1;\r\n    int j = s.indexOf(\"; \", i);\r\n    if (j < 0)\r\n        throw new IllegalArgumentException(\"i=\" + i + \", j=\" + j + \" < 0, s=\" + s);\r\n    final ArithmeticProgression N = ArithmeticProgression.valueOf(s.substring(i, j));\r\n    i = j + 2;\r\n    j = s.indexOf(\"]\", i);\r\n    if (j < 0)\r\n        throw new IllegalArgumentException(\"i=\" + i + \", j=\" + j + \" < 0, s=\" + s);\r\n    final ArithmeticProgression E = ArithmeticProgression.valueOf(s.substring(i, j));\r\n    final Summation sigma = new Summation(N, E);\r\n    i = j + 1;\r\n    if (s.length() > i) {\r\n        final String value = Util.parseStringVariable(\"value\", s.substring(i));\r\n        sigma.setValue(value.indexOf('.') < 0 ? Double.longBitsToDouble(Long.parseLong(value)) : Double.parseDouble(value));\r\n    }\r\n    return sigma;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "compute",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "double compute()\n{\r\n    if (value == null)\r\n        value = N.limit <= MAX_MODULAR ? compute_modular() : compute_montgomery();\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "compute_modular",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double compute_modular()\n{\r\n    long e = E.value;\r\n    long n = N.value;\r\n    double s = 0;\r\n    for (; e > E.limit; e += E.delta) {\r\n        s = Modular.addMod(s, Modular.mod(e, n) / (double) n);\r\n        n += N.delta;\r\n    }\r\n    return s;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "compute_montgomery",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double compute_montgomery()\n{\r\n    long e = E.value;\r\n    long n = N.value;\r\n    double s = 0;\r\n    for (; e > E.limit; e += E.delta) {\r\n        s = Modular.addMod(s, montgomery.set(n).mod(e) / (double) n);\r\n        n += N.delta;\r\n    }\r\n    return s;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int compareTo(Summation that)\n{\r\n    final int de = this.E.compareTo(that.E);\r\n    if (de != 0)\r\n        return de;\r\n    return this.N.compareTo(that.N);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "combine",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Summation combine(Summation that)\n{\r\n    if (this.N.delta != that.N.delta || this.E.delta != that.E.delta)\r\n        throw new IllegalArgumentException(\"this.N.delta != that.N.delta || this.E.delta != that.E.delta\" + \",\\n  this=\" + this + \",\\n  that=\" + that);\r\n    if (this.E.limit == that.E.value && this.N.limit == that.N.value) {\r\n        final double v = Modular.addMod(this.value, that.value);\r\n        final Summation s = new Summation(new ArithmeticProgression(N.symbol, N.value, N.delta, that.N.limit), new ArithmeticProgression(E.symbol, E.value, E.delta, that.E.limit));\r\n        s.setValue(v);\r\n        return s;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "remainingTerms",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "List<Summation> remainingTerms(List<T> sorted)\n{\r\n    final List<Summation> results = new ArrayList<Summation>();\r\n    Summation remaining = this;\r\n    if (sorted != null)\r\n        for (Container<Summation> c : sorted) {\r\n            final Summation sigma = c.getElement();\r\n            if (!remaining.contains(sigma))\r\n                throw new IllegalArgumentException(\"!remaining.contains(s),\" + \"\\n  remaining = \" + remaining + \"\\n  s         = \" + sigma + \"\\n  this      = \" + this + \"\\n  sorted    = \" + sorted);\r\n            final Summation s = new Summation(sigma.N.limit, N.delta, remaining.N.limit, sigma.E.limit, E.delta, remaining.E.limit);\r\n            if (s.getSteps() > 0)\r\n                results.add(s);\r\n            remaining = new Summation(remaining.N.value, N.delta, sigma.N.value, remaining.E.value, E.delta, sigma.E.value);\r\n        }\r\n    if (remaining.getSteps() > 0)\r\n        results.add(remaining);\r\n    return results;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "contains",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean contains(Summation that)\n{\r\n    return this.N.contains(that.N) && this.E.contains(that.E);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-examples\\src\\main\\java\\org\\apache\\hadoop\\examples\\pi\\math",
  "methodName" : "partition",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Summation[] partition(final int nParts)\n{\r\n    final Summation[] parts = new Summation[nParts];\r\n    final long steps = (E.limit - E.value) / E.delta + 1;\r\n    long prevN = N.value;\r\n    long prevE = E.value;\r\n    for (int i = 1; i < parts.length; i++) {\r\n        final long k = (i * steps) / parts.length;\r\n        final long currN = N.skip(k);\r\n        final long currE = E.skip(k);\r\n        parts[i - 1] = new Summation(new ArithmeticProgression(N.symbol, prevN, N.delta, currN), new ArithmeticProgression(E.symbol, prevE, E.delta, currE));\r\n        prevN = currN;\r\n        prevE = currE;\r\n    }\r\n    parts[parts.length - 1] = new Summation(new ArithmeticProgression(N.symbol, prevN, N.delta, N.limit), new ArithmeticProgression(E.symbol, prevE, E.delta, E.limit));\r\n    return parts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
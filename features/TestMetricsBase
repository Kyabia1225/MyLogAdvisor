[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "setupBase",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void setupBase() throws Exception\n{\r\n    if (router == null) {\r\n        routerConfig = new RouterConfigBuilder().stateStore().metrics().http().build();\r\n        router = new Router();\r\n        router.init(routerConfig);\r\n        router.setRouterId(\"routerId\");\r\n        router.start();\r\n        stateStore = router.getStateStore();\r\n        membershipStore = stateStore.getRegisteredRecordStore(MembershipStore.class);\r\n        routerStore = stateStore.getRegisteredRecordStore(RouterStore.class);\r\n        waitStateStore(stateStore, 10000);\r\n        createFixtures();\r\n        stateStore.refreshCaches(true);\r\n        Thread.sleep(1000);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "tearDownBase",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDownBase() throws IOException\n{\r\n    if (router != null) {\r\n        router.stop();\r\n        router.close();\r\n        router = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "createFixtures",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void createFixtures() throws IOException\n{\r\n    clearAllRecords(stateStore);\r\n    nameservices = new ArrayList<>();\r\n    nameservices.add(NAMESERVICES[0]);\r\n    nameservices.add(NAMESERVICES[1]);\r\n    activeMemberships = new ArrayList<>();\r\n    standbyMemberships = new ArrayList<>();\r\n    for (String nameservice : nameservices) {\r\n        MembershipState namenode1 = createMockRegistrationForNamenode(nameservice, NAMENODES[0], FederationNamenodeServiceState.ACTIVE);\r\n        NamenodeHeartbeatRequest request1 = NamenodeHeartbeatRequest.newInstance(namenode1);\r\n        assertTrue(membershipStore.namenodeHeartbeat(request1).getResult());\r\n        activeMemberships.add(namenode1);\r\n        MembershipState namenode2 = createMockRegistrationForNamenode(nameservice, NAMENODES[1], FederationNamenodeServiceState.STANDBY);\r\n        NamenodeHeartbeatRequest request2 = NamenodeHeartbeatRequest.newInstance(namenode2);\r\n        assertTrue(membershipStore.namenodeHeartbeat(request2).getResult());\r\n        standbyMemberships.add(namenode2);\r\n    }\r\n    mockMountTable = createMockMountTable(nameservices);\r\n    synchronizeRecords(stateStore, mockMountTable, MountTable.class);\r\n    long t1 = Time.now();\r\n    mockRouters = new ArrayList<>();\r\n    RouterState router1 = RouterState.newInstance(\"router1\", t1, RouterServiceState.RUNNING);\r\n    router1.setStateStoreVersion(StateStoreVersion.newInstance(t1 - 1000, t1 - 2000));\r\n    RouterHeartbeatRequest heartbeatRequest = RouterHeartbeatRequest.newInstance(router1);\r\n    assertTrue(routerStore.routerHeartbeat(heartbeatRequest).getStatus());\r\n    GetRouterRegistrationRequest getRequest = GetRouterRegistrationRequest.newInstance(\"router1\");\r\n    GetRouterRegistrationResponse getResponse = routerStore.getRouterRegistration(getRequest);\r\n    RouterState routerState1 = getResponse.getRouter();\r\n    mockRouters.add(routerState1);\r\n    long t2 = Time.now();\r\n    RouterState router2 = RouterState.newInstance(\"router2\", t2, RouterServiceState.RUNNING);\r\n    router2.setStateStoreVersion(StateStoreVersion.newInstance(t2 - 6000, t2 - 7000));\r\n    heartbeatRequest.setRouter(router2);\r\n    assertTrue(routerStore.routerHeartbeat(heartbeatRequest).getStatus());\r\n    getRequest.setRouterId(\"router2\");\r\n    getResponse = routerStore.getRouterRegistration(getRequest);\r\n    RouterState routerState2 = getResponse.getRouter();\r\n    mockRouters.add(routerState2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRouter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Router getRouter()\n{\r\n    return router;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getMockMountTable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<MountTable> getMockMountTable()\n{\r\n    return mockMountTable;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getActiveMemberships",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<MembershipState> getActiveMemberships()\n{\r\n    return activeMemberships;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getStandbyMemberships",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<MembershipState> getStandbyMemberships()\n{\r\n    return standbyMemberships;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNameservices",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<String> getNameservices()\n{\r\n    return nameservices;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getMockRouters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<RouterState> getMockRouters()\n{\r\n    return mockRouters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getStateStore",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StateStoreService getStateStore()\n{\r\n    return stateStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testObserverMetrics",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testObserverMetrics() throws Exception\n{\r\n    mockObserver();\r\n    RBFMetrics metrics = router.getMetrics();\r\n    String jsonString = metrics.getNameservices();\r\n    JSONObject jsonObject = new JSONObject(jsonString);\r\n    Map<String, String> map = getNameserviceStateMap(jsonObject);\r\n    assertTrue(\"Cannot find ns0 in: \" + jsonString, map.containsKey(\"ns0\"));\r\n    assertEquals(\"OBSERVER\", map.get(\"ns0\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNameserviceStateMap",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Map<String, String> getNameserviceStateMap(JSONObject jsonObject) throws JSONException\n{\r\n    Map<String, String> map = new TreeMap<>();\r\n    Iterator<?> keys = jsonObject.keys();\r\n    while (keys.hasNext()) {\r\n        String key = (String) keys.next();\r\n        JSONObject json = jsonObject.getJSONObject(key);\r\n        String nsId = json.getString(\"nameserviceId\");\r\n        String state = json.getString(\"state\");\r\n        map.put(nsId, state);\r\n    }\r\n    return map;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "mockObserver",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void mockObserver() throws IOException\n{\r\n    String ns = \"ns0\";\r\n    String nn = \"nn0\";\r\n    createRegistration(ns, nn, ROUTERS[1], FederationNamenodeServiceState.OBSERVER);\r\n    assertTrue(stateStore.loadCache(MembershipStore.class, true));\r\n    membershipStore.loadCache(true);\r\n    MembershipNamenodeResolver resolver = (MembershipNamenodeResolver) router.getNamenodeResolver();\r\n    resolver.loadCache(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "createRegistration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "MembershipState createRegistration(String ns, String nn, String routerId, FederationNamenodeServiceState state) throws IOException\n{\r\n    MembershipState record = MembershipState.newInstance(routerId, ns, nn, \"testcluster\", \"testblock-\" + ns, \"testrpc-\" + ns + nn, \"testservice-\" + ns + nn, \"testlifeline-\" + ns + nn, \"http\", \"testweb-\" + ns + nn, state, false);\r\n    NamenodeHeartbeatRequest request = NamenodeHeartbeatRequest.newInstance(record);\r\n    NamenodeHeartbeatResponse response = membershipStore.namenodeHeartbeat(request);\r\n    assertTrue(response.getResult());\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "refreshNamenodeRegistration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean refreshNamenodeRegistration(NamenodeHeartbeatRequest request) throws IOException\n{\r\n    boolean result = membershipStore.namenodeHeartbeat(request).getResult();\r\n    membershipStore.loadCache(true);\r\n    MembershipNamenodeResolver resolver = (MembershipNamenodeResolver) router.getNamenodeResolver();\r\n    resolver.loadCache(true);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, String> getMountTableEntry(String subcluster, String path)\n{\r\n    Map<String, String> ret = new HashMap<>();\r\n    ret.put(subcluster, path);\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "setupMountTable",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void setupMountTable() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(FEDERATION_MOUNT_TABLE_MAX_CACHE_SIZE, TEST_MAX_CACHE_SIZE);\r\n    conf.setStrings(DFS_ROUTER_DEFAULT_NAMESERVICE, \"0\");\r\n    mountTable = new MountTableResolver(conf);\r\n    Map<String, String> map = getMountTableEntry(\"1\", \"/\");\r\n    mountTable.addEntry(MountTable.newInstance(\"/\", map));\r\n    map = getMountTableEntry(\"2\", \"/\");\r\n    mountTable.addEntry(MountTable.newInstance(\"/tmp\", map));\r\n    map = getMountTableEntry(\"3\", \"/user\");\r\n    mountTable.addEntry(MountTable.newInstance(\"/user\", map));\r\n    map = getMountTableEntry(\"2\", \"/bin\");\r\n    mountTable.addEntry(MountTable.newInstance(\"/usr/bin\", map));\r\n    map = getMountTableEntry(\"2\", \"/user/test\");\r\n    mountTable.addEntry(MountTable.newInstance(\"/user/a\", map));\r\n    map = getMountTableEntry(\"4\", \"/user/file1.txt\");\r\n    mountTable.addEntry(MountTable.newInstance(\"/user/b/file1.txt\", map));\r\n    map = getMountTableEntry(\"1\", \"/user/test\");\r\n    mountTable.addEntry(MountTable.newInstance(\"/user/a/demo/test/a\", map));\r\n    map = getMountTableEntry(\"3\", \"/user/test\");\r\n    mountTable.addEntry(MountTable.newInstance(\"/user/a/demo/test/b\", map));\r\n    map = getMountTableEntry(\"2\", \"/tmp\");\r\n    MountTable readOnlyEntry = MountTable.newInstance(\"/readonly\", map);\r\n    readOnlyEntry.setReadOnly(true);\r\n    mountTable.addEntry(readOnlyEntry);\r\n    map = getMountTableEntry(\"5\", \"/dest1\");\r\n    map.put(\"6\", \"/dest2\");\r\n    MountTable multiEntry = MountTable.newInstance(\"/multi\", map);\r\n    mountTable.addEntry(multiEntry);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    setupMountTable();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testDestination",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testDestination() throws IOException\n{\r\n    assertEquals(\"1->/tesfile1.txt\", mountTable.getDestinationForPath(\"/tesfile1.txt\").toString());\r\n    assertEquals(\"3->/user/testfile2.txt\", mountTable.getDestinationForPath(\"/user/testfile2.txt\").toString());\r\n    assertEquals(\"2->/user/test/testfile3.txt\", mountTable.getDestinationForPath(\"/user/a/testfile3.txt\").toString());\r\n    assertEquals(\"3->/user/b/testfile4.txt\", mountTable.getDestinationForPath(\"/user/b/testfile4.txt\").toString());\r\n    assertEquals(\"1->/share/file5.txt\", mountTable.getDestinationForPath(\"/share/file5.txt\").toString());\r\n    assertEquals(\"2->/bin/file7.txt\", mountTable.getDestinationForPath(\"/usr/bin/file7.txt\").toString());\r\n    assertEquals(\"1->/usr/file8.txt\", mountTable.getDestinationForPath(\"/usr/file8.txt\").toString());\r\n    assertEquals(\"2->/user/test/demo/file9.txt\", mountTable.getDestinationForPath(\"/user/a/demo/file9.txt\").toString());\r\n    assertEquals(\"3->/user/testfolder\", mountTable.getDestinationForPath(\"/user/testfolder\").toString());\r\n    assertEquals(\"2->/user/test/b\", mountTable.getDestinationForPath(\"/user/a/b\").toString());\r\n    assertEquals(\"3->/user/test/a\", mountTable.getDestinationForPath(\"/user/test/a\").toString());\r\n    assertEquals(\"2->/tmp/tesfile1.txt\", mountTable.getDestinationForPath(\"/readonly/tesfile1.txt\").toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testDestinationOfConsecutiveSlash",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testDestinationOfConsecutiveSlash() throws IOException\n{\r\n    assertEquals(\"1->/tesfile1.txt\", mountTable.getDestinationForPath(\"//tesfile1.txt///\").toString());\r\n    assertEquals(\"3->/user/testfile2.txt\", mountTable.getDestinationForPath(\"/user///testfile2.txt\").toString());\r\n    assertEquals(\"2->/user/test/testfile3.txt\", mountTable.getDestinationForPath(\"///user/a/testfile3.txt\").toString());\r\n    assertEquals(\"3->/user/b/testfile4.txt\", mountTable.getDestinationForPath(\"/user/b/testfile4.txt//\").toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testDefaultNameServiceEnable",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testDefaultNameServiceEnable() throws IOException\n{\r\n    assertTrue(mountTable.isDefaultNSEnable());\r\n    mountTable.setDefaultNameService(\"3\");\r\n    mountTable.removeEntry(\"/\");\r\n    assertEquals(\"3->/unknown\", mountTable.getDestinationForPath(\"/unknown\").toString());\r\n    Map<String, String> map = getMountTableEntry(\"4\", \"/unknown\");\r\n    mountTable.addEntry(MountTable.newInstance(\"/unknown\", map));\r\n    mountTable.setDefaultNSEnable(false);\r\n    assertFalse(mountTable.isDefaultNSEnable());\r\n    assertEquals(\"4->/unknown\", mountTable.getDestinationForPath(\"/unknown\").toString());\r\n    try {\r\n        mountTable.getDestinationForPath(\"/\");\r\n        fail(\"The getDestinationForPath call should fail.\");\r\n    } catch (IOException ioe) {\r\n        GenericTestUtils.assertExceptionContains(\"the default nameservice is disabled to read or write\", ioe);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testMuiltipleDestinations",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testMuiltipleDestinations() throws IOException\n{\r\n    try {\r\n        mountTable.getDestinationForPath(\"/multi\");\r\n        fail(\"The getDestinationForPath call should fail.\");\r\n    } catch (IOException ioe) {\r\n        GenericTestUtils.assertExceptionContains(\"MountTableResolver should not resolve multiple destinations\", ioe);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "compareLists",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void compareLists(List<String> list1, String[] list2)\n{\r\n    assertEquals(list1.size(), list2.length);\r\n    for (String item : list2) {\r\n        assertTrue(list1.contains(item));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testGetMountPoint",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testGetMountPoint() throws IOException\n{\r\n    MountTable mtEntry;\r\n    mtEntry = mountTable.getMountPoint(\"/\");\r\n    assertEquals(\"/\", mtEntry.getSourcePath());\r\n    mtEntry = mountTable.getMountPoint(\"/user\");\r\n    assertEquals(\"/user\", mtEntry.getSourcePath());\r\n    mtEntry = mountTable.getMountPoint(\"/user/a\");\r\n    assertEquals(\"/user/a\", mtEntry.getSourcePath());\r\n    mtEntry = mountTable.getMountPoint(\"/user/a/\");\r\n    assertEquals(\"/user/a\", mtEntry.getSourcePath());\r\n    mtEntry = mountTable.getMountPoint(\"/user/a/11\");\r\n    assertEquals(\"/user/a\", mtEntry.getSourcePath());\r\n    mtEntry = mountTable.getMountPoint(\"/user/a1\");\r\n    assertEquals(\"/user\", mtEntry.getSourcePath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testGetMountPointOfConsecutiveSlashes",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testGetMountPointOfConsecutiveSlashes() throws IOException\n{\r\n    MountTable mtEntry;\r\n    mtEntry = mountTable.getMountPoint(\"///\");\r\n    assertEquals(\"/\", mtEntry.getSourcePath());\r\n    mtEntry = mountTable.getMountPoint(\"///user//\");\r\n    assertEquals(\"/user\", mtEntry.getSourcePath());\r\n    mtEntry = mountTable.getMountPoint(\"/user///a\");\r\n    assertEquals(\"/user/a\", mtEntry.getSourcePath());\r\n    mtEntry = mountTable.getMountPoint(\"/user/a////\");\r\n    assertEquals(\"/user/a\", mtEntry.getSourcePath());\r\n    mtEntry = mountTable.getMountPoint(\"///user/a/11//\");\r\n    assertEquals(\"/user/a\", mtEntry.getSourcePath());\r\n    mtEntry = mountTable.getMountPoint(\"/user///a1///\");\r\n    assertEquals(\"/user\", mtEntry.getSourcePath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testTrailingSlashInInputPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTrailingSlashInInputPath() throws IOException\n{\r\n    getMountPoints(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testGetMountPoints",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetMountPoints() throws IOException\n{\r\n    getMountPoints(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getMountPoints",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void getMountPoints(boolean trailingSlash) throws IOException\n{\r\n    List<String> mounts = mountTable.getMountPoints(\"/\");\r\n    assertEquals(5, mounts.size());\r\n    compareLists(mounts, new String[] { \"tmp\", \"user\", \"usr\", \"readonly\", \"multi\" });\r\n    String path = trailingSlash ? \"/user/\" : \"/user\";\r\n    mounts = mountTable.getMountPoints(path);\r\n    assertEquals(2, mounts.size());\r\n    compareLists(mounts, new String[] { \"a\", \"b\" });\r\n    path = trailingSlash ? \"/user/a/\" : \"/user/a\";\r\n    mounts = mountTable.getMountPoints(path);\r\n    assertEquals(1, mounts.size());\r\n    compareLists(mounts, new String[] { \"demo\" });\r\n    path = trailingSlash ? \"/user/a/demo/\" : \"/user/a/demo\";\r\n    mounts = mountTable.getMountPoints(path);\r\n    assertEquals(1, mounts.size());\r\n    compareLists(mounts, new String[] { \"test\" });\r\n    path = trailingSlash ? \"/user/a/demo/test/\" : \"/user/a/demo/test\";\r\n    mounts = mountTable.getMountPoints(path);\r\n    assertEquals(2, mounts.size());\r\n    compareLists(mounts, new String[] { \"a\", \"b\" });\r\n    path = trailingSlash ? \"/tmp/\" : \"/tmp\";\r\n    mounts = mountTable.getMountPoints(path);\r\n    assertEquals(0, mounts.size());\r\n    path = trailingSlash ? \"/t/\" : \"/t\";\r\n    mounts = mountTable.getMountPoints(path);\r\n    assertNull(mounts);\r\n    path = trailingSlash ? \"/unknownpath/\" : \"/unknownpath\";\r\n    mounts = mountTable.getMountPoints(path);\r\n    assertNull(mounts);\r\n    path = trailingSlash ? \"/multi/\" : \"/multi\";\r\n    mounts = mountTable.getMountPoints(path);\r\n    assertEquals(0, mounts.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testSuccessiveSlashesInInputPath",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSuccessiveSlashesInInputPath() throws IOException\n{\r\n    List<String> mounts = mountTable.getMountPoints(\"///\");\r\n    assertEquals(5, mounts.size());\r\n    compareLists(mounts, new String[] { \"tmp\", \"user\", \"usr\", \"readonly\", \"multi\" });\r\n    String path = \"///user//\";\r\n    mounts = mountTable.getMountPoints(path);\r\n    assertEquals(2, mounts.size());\r\n    compareLists(mounts, new String[] { \"a\", \"b\" });\r\n    path = \"/user///a\";\r\n    mounts = mountTable.getMountPoints(path);\r\n    assertEquals(1, mounts.size());\r\n    compareLists(mounts, new String[] { \"demo\" });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "compareRecords",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void compareRecords(List<MountTable> list1, String[] list2)\n{\r\n    assertEquals(list1.size(), list2.length);\r\n    for (String item : list2) {\r\n        for (MountTable record : list1) {\r\n            if (record.getSourcePath().equals(item)) {\r\n                return;\r\n            }\r\n        }\r\n    }\r\n    fail();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testGetMounts",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testGetMounts() throws IOException\n{\r\n    List<MountTable> records = mountTable.getMounts(\"/\");\r\n    assertEquals(10, records.size());\r\n    compareRecords(records, new String[] { \"/\", \"/tmp\", \"/user\", \"/usr/bin\", \"user/a\", \"/user/a/demo/a\", \"/user/a/demo/b\", \"/user/b/file1.txt\", \"readonly\", \"multi\" });\r\n    records = mountTable.getMounts(\"/user\");\r\n    assertEquals(5, records.size());\r\n    compareRecords(records, new String[] { \"/user\", \"/user/a/demo/a\", \"/user/a/demo/b\", \"user/a\", \"/user/b/file1.txt\" });\r\n    records = mountTable.getMounts(\"/user/a\");\r\n    assertEquals(3, records.size());\r\n    compareRecords(records, new String[] { \"/user/a/demo/a\", \"/user/a/demo/b\", \"/user/a\" });\r\n    records = mountTable.getMounts(\"/tmp\");\r\n    assertEquals(1, records.size());\r\n    compareRecords(records, new String[] { \"/tmp\" });\r\n    records = mountTable.getMounts(\"/readonly\");\r\n    assertEquals(1, records.size());\r\n    compareRecords(records, new String[] { \"/readonly\" });\r\n    assertTrue(records.get(0).isReadOnly());\r\n    records = mountTable.getMounts(\"/multi\");\r\n    assertEquals(1, records.size());\r\n    compareRecords(records, new String[] { \"/multi\" });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testGetMountsOfConsecutiveSlashes",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGetMountsOfConsecutiveSlashes() throws IOException\n{\r\n    List<MountTable> records = mountTable.getMounts(\"///\");\r\n    assertEquals(10, records.size());\r\n    compareRecords(records, new String[] { \"/\", \"/tmp\", \"/user\", \"/usr/bin\", \"user/a\", \"/user/a/demo/a\", \"/user/a/demo/b\", \"/user/b/file1.txt\", \"readonly\", \"multi\" });\r\n    records = mountTable.getMounts(\"/user///\");\r\n    assertEquals(5, records.size());\r\n    compareRecords(records, new String[] { \"/user\", \"/user/a/demo/a\", \"/user/a/demo/b\", \"user/a\", \"/user/b/file1.txt\" });\r\n    records = mountTable.getMounts(\"///user///a\");\r\n    assertEquals(3, records.size());\r\n    compareRecords(records, new String[] { \"/user/a/demo/a\", \"/user/a/demo/b\", \"/user/a\" });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testRemoveSubTree",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRemoveSubTree() throws UnsupportedOperationException, IOException\n{\r\n    compareLists(mountTable.getMountPoints(\"/\"), new String[] { \"user\", \"usr\", \"tmp\", \"readonly\", \"multi\" });\r\n    assertEquals(\"2\", mountTable.getDestinationForPath(\"/tmp/testfile.txt\").getDefaultLocation().getNameserviceId());\r\n    mountTable.removeEntry(\"/tmp\");\r\n    compareLists(mountTable.getMountPoints(\"/\"), new String[] { \"user\", \"usr\", \"readonly\", \"multi\" });\r\n    assertEquals(\"1\", mountTable.getDestinationForPath(\"/tmp/testfile.txt\").getDefaultLocation().getNameserviceId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testRemoveVirtualNode",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testRemoveVirtualNode() throws UnsupportedOperationException, IOException\n{\r\n    compareLists(mountTable.getMountPoints(\"/\"), new String[] { \"user\", \"usr\", \"tmp\", \"readonly\", \"multi\" });\r\n    assertEquals(\"1\", mountTable.getDestinationForPath(\"/usr/testfile.txt\").getDefaultLocation().getNameserviceId());\r\n    mountTable.removeEntry(\"/usr\");\r\n    compareLists(mountTable.getMountPoints(\"/\"), new String[] { \"user\", \"usr\", \"tmp\", \"readonly\", \"multi\" });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testRemoveLeafNode",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRemoveLeafNode() throws UnsupportedOperationException, IOException\n{\r\n    assertEquals(\"1\", mountTable.getDestinationForPath(\"/user/a/demo/test/a\").getDefaultLocation().getNameserviceId());\r\n    mountTable.removeEntry(\"/user/a/demo/test/a\");\r\n    assertEquals(\"2\", mountTable.getDestinationForPath(\"/user/a/demo/test/a\").getDefaultLocation().getNameserviceId());\r\n    compareLists(mountTable.getMountPoints(\"/user/a\"), new String[] { \"demo\" });\r\n    assertEquals(\"3\", mountTable.getDestinationForPath(\"/user/a/demo/test/b\").getDefaultLocation().getNameserviceId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testRefreshEntries",
  "errType" : [ "AssertionError" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testRefreshEntries() throws UnsupportedOperationException, IOException\n{\r\n    testDestination();\r\n    assertEquals(10, mountTable.getMounts(\"/\").size());\r\n    List<MountTable> records = new ArrayList<>();\r\n    Map<String, String> map1 = getMountTableEntry(\"1\", \"/\");\r\n    records.add(MountTable.newInstance(\"/1\", map1));\r\n    Map<String, String> map2 = getMountTableEntry(\"2\", \"/\");\r\n    records.add(MountTable.newInstance(\"/2\", map2));\r\n    mountTable.refreshEntries(records);\r\n    PathLocation destination1 = mountTable.getDestinationForPath(\"/1\");\r\n    RemoteLocation defaultLoc1 = destination1.getDefaultLocation();\r\n    assertEquals(\"1\", defaultLoc1.getNameserviceId());\r\n    PathLocation destination2 = mountTable.getDestinationForPath(\"/2\");\r\n    RemoteLocation defaultLoc2 = destination2.getDefaultLocation();\r\n    assertEquals(\"2\", defaultLoc2.getNameserviceId());\r\n    assertEquals(2, mountTable.getMounts(\"/\").size());\r\n    boolean assertionThrown = false;\r\n    try {\r\n        testDestination();\r\n        fail();\r\n    } catch (AssertionError e) {\r\n        assertionThrown = true;\r\n    }\r\n    assertTrue(assertionThrown);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testMountTableScalability",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testMountTableScalability() throws IOException\n{\r\n    List<MountTable> emptyList = new ArrayList<>();\r\n    mountTable.refreshEntries(emptyList);\r\n    for (int i = 0; i < 100000; i++) {\r\n        Map<String, String> map = getMountTableEntry(\"1\", \"/\" + i);\r\n        MountTable record = MountTable.newInstance(\"/\" + i, map);\r\n        mountTable.addEntry(record);\r\n        if (i % 10000 == 0) {\r\n            LOG.info(\"Adding flat mount record {}: {}\", i, record);\r\n        }\r\n    }\r\n    assertEquals(100000, mountTable.getMountPoints(\"/\").size());\r\n    assertEquals(100000, mountTable.getMounts(\"/\").size());\r\n    mountTable.refreshEntries(emptyList);\r\n    String parent = \"/\";\r\n    for (int i = 0; i < 1000; i++) {\r\n        final int index = i;\r\n        Map<String, String> map = getMountTableEntry(\"1\", \"/\" + index);\r\n        if (i > 0) {\r\n            parent = parent + \"/\";\r\n        }\r\n        parent = parent + i;\r\n        MountTable record = MountTable.newInstance(parent, map);\r\n        mountTable.addEntry(record);\r\n    }\r\n    assertEquals(1, mountTable.getMountPoints(\"/\").size());\r\n    assertEquals(1000, mountTable.getMounts(\"/\").size());\r\n    mountTable.refreshEntries(emptyList);\r\n    Random rand = new Random();\r\n    parent = \"/\" + Integer.toString(rand.nextInt());\r\n    int numRootTrees = 1;\r\n    for (int i = 0; i < 100000; i++) {\r\n        final int index = i;\r\n        Map<String, String> map = getMountTableEntry(\"1\", \"/\" + index);\r\n        parent = parent + \"/\" + i;\r\n        if (parent.length() > 2000) {\r\n            parent = \"/\" + Integer.toString(rand.nextInt());\r\n            numRootTrees++;\r\n        }\r\n        MountTable record = MountTable.newInstance(parent, map);\r\n        mountTable.addEntry(record);\r\n    }\r\n    assertEquals(numRootTrees, mountTable.getMountPoints(\"/\").size());\r\n    assertEquals(100000, mountTable.getMounts(\"/\").size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testUpdate",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testUpdate() throws IOException\n{\r\n    Map<String, String> map = getMountTableEntry(\"1\", \"/\");\r\n    mountTable.addEntry(MountTable.newInstance(\"/testupdate\", map));\r\n    MountTable entry = mountTable.getMountPoint(\"/testupdate\");\r\n    List<RemoteLocation> dests = entry.getDestinations();\r\n    assertEquals(1, dests.size());\r\n    RemoteLocation dest = dests.get(0);\r\n    assertEquals(\"1\", dest.getNameserviceId());\r\n    Collection<MountTable> entries = Collections.singletonList(MountTable.newInstance(\"/testupdate\", getMountTableEntry(\"2\", \"/\")));\r\n    mountTable.refreshEntries(entries);\r\n    MountTable entry1 = mountTable.getMountPoint(\"/testupdate\");\r\n    List<RemoteLocation> dests1 = entry1.getDestinations();\r\n    assertEquals(1, dests1.size());\r\n    RemoteLocation dest1 = dests1.get(0);\r\n    assertEquals(\"2\", dest1.getNameserviceId());\r\n    mountTable.removeEntry(\"/testupdate\");\r\n    MountTable entry2 = mountTable.getMountPoint(\"/testupdate\");\r\n    assertNull(entry2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testDisableLocalCache",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testDisableLocalCache() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(FEDERATION_MOUNT_TABLE_CACHE_ENABLE, false);\r\n    conf.setStrings(DFS_ROUTER_DEFAULT_NAMESERVICE, \"0\");\r\n    MountTableResolver tmpMountTable = new MountTableResolver(conf);\r\n    Map<String, String> map = getMountTableEntry(\"1\", \"/\");\r\n    tmpMountTable.addEntry(MountTable.newInstance(\"/\", map));\r\n    map = getMountTableEntry(\"2\", \"/tmp\");\r\n    tmpMountTable.addEntry(MountTable.newInstance(\"/tmp\", map));\r\n    try {\r\n        tmpMountTable.getCacheSize();\r\n        fail(\"getCacheSize call should fail.\");\r\n    } catch (IOException e) {\r\n        GenericTestUtils.assertExceptionContains(\"localCache is null\", e);\r\n    }\r\n    assertEquals(\"2->/tmp/tesfile1.txt\", tmpMountTable.getDestinationForPath(\"/tmp/tesfile1.txt\").toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testCacheCleaning",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCacheCleaning() throws Exception\n{\r\n    for (int i = 0; i < 1000; i++) {\r\n        String filename = String.format(\"/user/a/file-%04d.txt\", i);\r\n        mountTable.getDestinationForPath(filename);\r\n    }\r\n    long cacheSize = mountTable.getCacheSize();\r\n    assertTrue(cacheSize <= TEST_MAX_CACHE_SIZE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testLocationCache",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testLocationCache() throws Exception\n{\r\n    List<MountTable> entries = new ArrayList<>();\r\n    Map<String, String> map1 = getMountTableEntry(\"1\", \"/testlocationcache\");\r\n    MountTable entry1 = MountTable.newInstance(\"/testlocationcache\", map1);\r\n    entries.add(entry1);\r\n    Map<String, String> map2 = getMountTableEntry(\"2\", \"/anothertestlocationcache\");\r\n    MountTable entry2 = MountTable.newInstance(\"/anothertestlocationcache\", map2);\r\n    entries.add(entry2);\r\n    mountTable.refreshEntries(entries);\r\n    assertEquals(\"1->/testlocationcache\", mountTable.getDestinationForPath(\"/testlocationcache\").toString());\r\n    assertEquals(\"2->/anothertestlocationcache\", mountTable.getDestinationForPath(\"/anothertestlocationcache\").toString());\r\n    entries.remove(entry1);\r\n    mountTable.refreshEntries(entries);\r\n    assertEquals(\"0->/testlocationcache\", mountTable.getDestinationForPath(\"/testlocationcache\").toString());\r\n    Map<String, String> map3 = getMountTableEntry(\"3\", \"/testlocationcache\");\r\n    MountTable entry3 = MountTable.newInstance(\"/testlocationcache\", map3);\r\n    entries.add(entry3);\r\n    mountTable.refreshEntries(entries);\r\n    assertEquals(\"3->/testlocationcache\", mountTable.getDestinationForPath(\"/testlocationcache\").toString());\r\n    mountTable.removeEntry(\"/testlocationcache\");\r\n    mountTable.removeEntry(\"/anothertestlocationcache\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testInvalidateCache",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testInvalidateCache() throws Exception\n{\r\n    Map<String, String> map1 = getMountTableEntry(\"1\", \"/\");\r\n    MountTable entry1 = MountTable.newInstance(\"/\", map1);\r\n    mountTable.addEntry(entry1);\r\n    assertEquals(\"1->/\", mountTable.getDestinationForPath(\"/\").toString());\r\n    assertEquals(\"1->/testInvalidateCache/foo\", mountTable.getDestinationForPath(\"/testInvalidateCache/foo\").toString());\r\n    Map<String, String> map2 = getMountTableEntry(\"2\", \"/testInvalidateCache\");\r\n    MountTable entry2 = MountTable.newInstance(\"/testInvalidateCache\", map2);\r\n    mountTable.addEntry(entry2);\r\n    assertEquals(\"2->/testInvalidateCache\", mountTable.getDestinationForPath(\"/testInvalidateCache\").toString());\r\n    assertEquals(\"2->/testInvalidateCache/foo\", mountTable.getDestinationForPath(\"/testInvalidateCache/foo\").toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "createRecord",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "MembershipState createRecord() throws IOException\n{\r\n    MembershipState record = MembershipState.newInstance(ROUTER, NAMESERVICE, NAMENODE, CLUSTER_ID, BLOCKPOOL_ID, RPC_ADDRESS, SERVICE_ADDRESS, LIFELINE_ADDRESS, SCHEME, WEB_ADDRESS, STATE, SAFE_MODE);\r\n    record.setDateCreated(DATE_CREATED);\r\n    record.setDateModified(DATE_MODIFIED);\r\n    MembershipStats stats = MembershipStats.newInstance();\r\n    stats.setNumOfBlocks(NUM_BLOCKS);\r\n    stats.setNumOfFiles(NUM_FILES);\r\n    stats.setNumOfActiveDatanodes(NUM_ACTIVE);\r\n    stats.setNumOfDeadDatanodes(NUM_DEAD);\r\n    stats.setNumOfStaleDatanodes(NUM_STALE);\r\n    stats.setNumOfDecommissioningDatanodes(NUM_DECOM);\r\n    stats.setNumOfDecomActiveDatanodes(NUM_DECOM_ACTIVE);\r\n    stats.setNumOfDecomDeadDatanodes(NUM_DECOM_DEAD);\r\n    stats.setNumOfInMaintenanceLiveDataNodes(NUM_MAIN_LIVE);\r\n    stats.setNumOfInMaintenanceDeadDataNodes(NUM_MAIN_DEAD);\r\n    stats.setNumOfEnteringMaintenanceDataNodes(NUM_ENTER_MAIN);\r\n    stats.setNumOfBlocksMissing(NUM_BLOCK_MISSING);\r\n    stats.setTotalSpace(TOTAL_SPACE);\r\n    stats.setAvailableSpace(AVAILABLE_SPACE);\r\n    stats.setCorruptFilesCount(CORRUPT_FILES_COUNT);\r\n    stats.setScheduledReplicationBlocks(SCHEDULED_REPLICATION_BLOCKS);\r\n    stats.setNumberOfMissingBlocksWithReplicationFactorOne(MISSING_BLOCK_WITH_REPLICATION_ONE);\r\n    stats.setHighestPriorityLowRedundancyReplicatedBlocks(HIGHEST_PRIORITY_LOW_REDUNDANCY_REPL_BLOCK);\r\n    stats.setHighestPriorityLowRedundancyECBlocks(HIGHEST_PRIORITY_LOW_REDUNDANCY_EC_BLOCK);\r\n    record.setStats(stats);\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "validateRecord",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void validateRecord(MembershipState record) throws IOException\n{\r\n    assertEquals(ROUTER, record.getRouterId());\r\n    assertEquals(NAMESERVICE, record.getNameserviceId());\r\n    assertEquals(CLUSTER_ID, record.getClusterId());\r\n    assertEquals(BLOCKPOOL_ID, record.getBlockPoolId());\r\n    assertEquals(RPC_ADDRESS, record.getRpcAddress());\r\n    assertEquals(SCHEME, record.getWebScheme());\r\n    assertEquals(WEB_ADDRESS, record.getWebAddress());\r\n    assertEquals(STATE, record.getState());\r\n    assertEquals(SAFE_MODE, record.getIsSafeMode());\r\n    assertEquals(DATE_CREATED, record.getDateCreated());\r\n    assertEquals(DATE_MODIFIED, record.getDateModified());\r\n    MembershipStats stats = record.getStats();\r\n    assertEquals(NUM_BLOCKS, stats.getNumOfBlocks());\r\n    assertEquals(NUM_FILES, stats.getNumOfFiles());\r\n    assertEquals(NUM_ACTIVE, stats.getNumOfActiveDatanodes());\r\n    assertEquals(NUM_DEAD, stats.getNumOfDeadDatanodes());\r\n    assertEquals(NUM_STALE, stats.getNumOfStaleDatanodes());\r\n    assertEquals(NUM_DECOM, stats.getNumOfDecommissioningDatanodes());\r\n    assertEquals(NUM_DECOM_ACTIVE, stats.getNumOfDecomActiveDatanodes());\r\n    assertEquals(NUM_DECOM_DEAD, stats.getNumOfDecomDeadDatanodes());\r\n    assertEquals(NUM_MAIN_LIVE, stats.getNumOfInMaintenanceLiveDataNodes());\r\n    assertEquals(NUM_MAIN_DEAD, stats.getNumOfInMaintenanceDeadDataNodes());\r\n    assertEquals(NUM_ENTER_MAIN, stats.getNumOfEnteringMaintenanceDataNodes());\r\n    assertEquals(TOTAL_SPACE, stats.getTotalSpace());\r\n    assertEquals(AVAILABLE_SPACE, stats.getAvailableSpace());\r\n    assertEquals(CORRUPT_FILES_COUNT, stats.getCorruptFilesCount());\r\n    assertEquals(SCHEDULED_REPLICATION_BLOCKS, stats.getScheduledReplicationBlocks());\r\n    assertEquals(MISSING_BLOCK_WITH_REPLICATION_ONE, stats.getNumberOfMissingBlocksWithReplicationFactorOne());\r\n    assertEquals(HIGHEST_PRIORITY_LOW_REDUNDANCY_REPL_BLOCK, stats.getHighestPriorityLowRedundancyReplicatedBlocks());\r\n    assertEquals(HIGHEST_PRIORITY_LOW_REDUNDANCY_EC_BLOCK, stats.getHighestPriorityLowRedundancyECBlocks());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "testGetterSetter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetterSetter() throws IOException\n{\r\n    MembershipState record = createRecord();\r\n    validateRecord(record);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "testSerialization",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSerialization() throws IOException\n{\r\n    MembershipState record = createRecord();\r\n    StateStoreSerializer serializer = StateStoreSerializer.getSerializer();\r\n    String serializedString = serializer.serializeString(record);\r\n    MembershipState newRecord = serializer.deserialize(serializedString, MembershipState.class);\r\n    validateRecord(newRecord);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testClusterStatsJMX",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testClusterStatsJMX() throws MalformedObjectNameException, IOException\n{\r\n    FederationMBean federationBean = getBean(FEDERATION_BEAN, FederationMBean.class);\r\n    validateClusterStatsFederationBean(federationBean);\r\n    testCapacity(federationBean);\r\n    RouterMBean routerBean = getBean(ROUTER_BEAN, RouterMBean.class);\r\n    validateClusterStatsRouterBean(routerBean);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testClusterStatsDataSource",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testClusterStatsDataSource() throws IOException\n{\r\n    RBFMetrics metrics = getRouter().getMetrics();\r\n    validateClusterStatsFederationBean(metrics);\r\n    validateClusterStatsRouterBean(metrics);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testMountTableStatsDataSource",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testMountTableStatsDataSource() throws IOException, JSONException\n{\r\n    RBFMetrics metrics = getRouter().getMetrics();\r\n    String jsonString = metrics.getMountTable();\r\n    JSONArray jsonArray = new JSONArray(jsonString);\r\n    assertEquals(jsonArray.length(), getMockMountTable().size());\r\n    int match = 0;\r\n    for (int i = 0; i < jsonArray.length(); i++) {\r\n        JSONObject json = jsonArray.getJSONObject(i);\r\n        String src = json.getString(\"sourcePath\");\r\n        for (MountTable entry : getMockMountTable()) {\r\n            if (entry.getSourcePath().equals(src)) {\r\n                assertEquals(entry.getDefaultLocation().getNameserviceId(), json.getString(\"nameserviceId\"));\r\n                assertEquals(entry.getDefaultLocation().getDest(), json.getString(\"path\"));\r\n                assertEquals(entry.getOwnerName(), json.getString(\"ownerName\"));\r\n                assertEquals(entry.getGroupName(), json.getString(\"groupName\"));\r\n                assertEquals(entry.getMode().toString(), json.getString(\"mode\"));\r\n                assertEquals(entry.getQuota().toString(), json.getString(\"quota\"));\r\n                assertNotNullAndNotEmpty(json.getString(\"dateCreated\"));\r\n                assertNotNullAndNotEmpty(json.getString(\"dateModified\"));\r\n                match++;\r\n            }\r\n        }\r\n    }\r\n    assertEquals(match, getMockMountTable().size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "findMockNamenode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "MembershipState findMockNamenode(String nsId, String nnId)\n{\r\n    @SuppressWarnings(\"unchecked\")\r\n    List<MembershipState> namenodes = ListUtils.union(getActiveMemberships(), getStandbyMemberships());\r\n    for (MembershipState nn : namenodes) {\r\n        if (nn.getNamenodeId().equals(nnId) && nn.getNameserviceId().equals(nsId)) {\r\n            return nn;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testNamenodeStatsDataSource",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testNamenodeStatsDataSource() throws IOException, JSONException\n{\r\n    RBFMetrics metrics = getRouter().getMetrics();\r\n    String jsonString = metrics.getNamenodes();\r\n    JSONObject jsonObject = new JSONObject(jsonString);\r\n    Iterator<?> keys = jsonObject.keys();\r\n    int nnsFound = 0;\r\n    while (keys.hasNext()) {\r\n        JSONObject json = jsonObject.getJSONObject((String) keys.next());\r\n        String nameserviceId = json.getString(\"nameserviceId\");\r\n        String namenodeId = json.getString(\"namenodeId\");\r\n        MembershipState mockEntry = this.findMockNamenode(nameserviceId, namenodeId);\r\n        assertNotNull(mockEntry);\r\n        assertEquals(json.getString(\"state\"), mockEntry.getState().toString());\r\n        MembershipStats stats = mockEntry.getStats();\r\n        assertEquals(json.getLong(\"numOfActiveDatanodes\"), stats.getNumOfActiveDatanodes());\r\n        assertEquals(json.getLong(\"numOfDeadDatanodes\"), stats.getNumOfDeadDatanodes());\r\n        assertEquals(json.getLong(\"numOfStaleDatanodes\"), stats.getNumOfStaleDatanodes());\r\n        assertEquals(json.getLong(\"numOfDecommissioningDatanodes\"), stats.getNumOfDecommissioningDatanodes());\r\n        assertEquals(json.getLong(\"numOfDecomActiveDatanodes\"), stats.getNumOfDecomActiveDatanodes());\r\n        assertEquals(json.getLong(\"numOfDecomDeadDatanodes\"), stats.getNumOfDecomDeadDatanodes());\r\n        assertEquals(json.getLong(\"numOfInMaintenanceLiveDataNodes\"), stats.getNumOfInMaintenanceLiveDataNodes());\r\n        assertEquals(json.getLong(\"numOfInMaintenanceDeadDataNodes\"), stats.getNumOfInMaintenanceDeadDataNodes());\r\n        assertEquals(json.getLong(\"numOfEnteringMaintenanceDataNodes\"), stats.getNumOfEnteringMaintenanceDataNodes());\r\n        assertEquals(json.getLong(\"numOfBlocks\"), stats.getNumOfBlocks());\r\n        assertEquals(json.getString(\"rpcAddress\"), mockEntry.getRpcAddress());\r\n        assertEquals(json.getString(\"webScheme\"), mockEntry.getWebScheme());\r\n        assertEquals(json.getString(\"webAddress\"), mockEntry.getWebAddress());\r\n        nnsFound++;\r\n    }\r\n    assertEquals(getActiveMemberships().size() + getStandbyMemberships().size(), nnsFound);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testNameserviceStatsDataSource",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testNameserviceStatsDataSource() throws IOException, JSONException\n{\r\n    RBFMetrics metrics = getRouter().getMetrics();\r\n    String jsonString = metrics.getNameservices();\r\n    JSONObject jsonObject = new JSONObject(jsonString);\r\n    Iterator<?> keys = jsonObject.keys();\r\n    int nameservicesFound = 0;\r\n    while (keys.hasNext()) {\r\n        JSONObject json = jsonObject.getJSONObject((String) keys.next());\r\n        String nameserviceId = json.getString(\"nameserviceId\");\r\n        String namenodeId = json.getString(\"namenodeId\");\r\n        MembershipState mockEntry = this.findMockNamenode(nameserviceId, namenodeId);\r\n        assertNotNull(mockEntry);\r\n        assertEquals(mockEntry.getState().toString(), json.getString(\"state\"));\r\n        assertEquals(\"ACTIVE\", json.getString(\"state\"));\r\n        MembershipStats stats = mockEntry.getStats();\r\n        assertEquals(stats.getNumOfFiles(), json.getLong(\"numOfFiles\"));\r\n        assertEquals(stats.getTotalSpace(), json.getLong(\"totalSpace\"));\r\n        assertEquals(stats.getAvailableSpace(), json.getLong(\"availableSpace\"));\r\n        assertEquals(stats.getNumOfBlocksMissing(), json.getLong(\"numOfBlocksMissing\"));\r\n        assertEquals(stats.getNumOfActiveDatanodes(), json.getLong(\"numOfActiveDatanodes\"));\r\n        assertEquals(stats.getNumOfDeadDatanodes(), json.getLong(\"numOfDeadDatanodes\"));\r\n        assertEquals(stats.getNumOfStaleDatanodes(), json.getLong(\"numOfStaleDatanodes\"));\r\n        assertEquals(stats.getNumOfDecommissioningDatanodes(), json.getLong(\"numOfDecommissioningDatanodes\"));\r\n        assertEquals(stats.getNumOfDecomActiveDatanodes(), json.getLong(\"numOfDecomActiveDatanodes\"));\r\n        assertEquals(stats.getNumOfDecomDeadDatanodes(), json.getLong(\"numOfDecomDeadDatanodes\"));\r\n        assertEquals(stats.getNumOfInMaintenanceLiveDataNodes(), json.getLong(\"numOfInMaintenanceLiveDataNodes\"));\r\n        assertEquals(stats.getNumOfInMaintenanceDeadDataNodes(), json.getLong(\"numOfInMaintenanceDeadDataNodes\"));\r\n        assertEquals(stats.getNumOfStaleDatanodes(), json.getLong(\"numOfEnteringMaintenanceDataNodes\"));\r\n        assertEquals(stats.getProvidedSpace(), json.getLong(\"providedSpace\"));\r\n        assertEquals(stats.getPendingSPSPaths(), json.getInt(\"pendingSPSPaths\"));\r\n        nameservicesFound++;\r\n    }\r\n    assertEquals(getNameservices().size(), nameservicesFound);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testRouterStatsDataSource",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testRouterStatsDataSource() throws IOException, JSONException\n{\r\n    RBFMetrics metrics = getRouter().getMetrics();\r\n    String jsonString = metrics.getRouters();\r\n    JSONObject jsonObject = new JSONObject(jsonString);\r\n    Iterator<?> keys = jsonObject.keys();\r\n    int routersFound = 0;\r\n    while (keys.hasNext()) {\r\n        JSONObject json = jsonObject.getJSONObject((String) keys.next());\r\n        String address = json.getString(\"address\");\r\n        assertNotNullAndNotEmpty(address);\r\n        RouterState router = findMockRouter(address);\r\n        assertNotNull(router);\r\n        assertEquals(router.getStatus().toString(), json.getString(\"status\"));\r\n        assertEquals(router.getCompileInfo(), json.getString(\"compileInfo\"));\r\n        assertEquals(router.getVersion(), json.getString(\"version\"));\r\n        assertEquals(router.getDateStarted(), json.getLong(\"dateStarted\"));\r\n        assertEquals(router.getDateCreated(), json.getLong(\"dateCreated\"));\r\n        assertEquals(router.getDateModified(), json.getLong(\"dateModified\"));\r\n        StateStoreVersion version = router.getStateStoreVersion();\r\n        assertEquals(RBFMetrics.getDateString(version.getMembershipVersion()), json.get(\"lastMembershipUpdate\"));\r\n        assertEquals(RBFMetrics.getDateString(version.getMountTableVersion()), json.get(\"lastMountTableUpdate\"));\r\n        assertEquals(version.getMembershipVersion(), json.get(\"membershipVersion\"));\r\n        assertEquals(version.getMountTableVersion(), json.get(\"mountTableVersion\"));\r\n        routersFound++;\r\n    }\r\n    assertEquals(getMockRouters().size(), routersFound);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "assertNotNullAndNotEmpty",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertNotNullAndNotEmpty(String field)\n{\r\n    assertNotNull(field);\r\n    assertTrue(field.length() > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "findMockRouter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RouterState findMockRouter(String routerId)\n{\r\n    for (RouterState router : getMockRouters()) {\r\n        if (router.getAddress().equals(routerId)) {\r\n            return router;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "validateClusterStatsFederationBean",
  "errType" : null,
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void validateClusterStatsFederationBean(FederationMBean bean)\n{\r\n    long numBlocks = 0;\r\n    long numLive = 0;\r\n    long numDead = 0;\r\n    long numStale = 0;\r\n    long numDecom = 0;\r\n    long numDecomLive = 0;\r\n    long numDecomDead = 0;\r\n    long numInMaintenanceLive = 0;\r\n    long numInMaintenanceDead = 0;\r\n    long numEnteringMaintenance = 0;\r\n    int numCorruptsFilesCount = 0;\r\n    long scheduledReplicationBlocks = 0;\r\n    long numberOfMissingBlocksWithReplicationFactorOne = 0;\r\n    long highestPriorityLowRedundancyReplicatedBlocks = 0;\r\n    long highestPriorityLowRedundancyECBlocks = 0;\r\n    long numFiles = 0;\r\n    int pendingSPSPaths = 0;\r\n    for (MembershipState mock : getActiveMemberships()) {\r\n        MembershipStats stats = mock.getStats();\r\n        numBlocks += stats.getNumOfBlocks();\r\n        numLive += stats.getNumOfActiveDatanodes();\r\n        numDead += stats.getNumOfDeadDatanodes();\r\n        numStale += stats.getNumOfStaleDatanodes();\r\n        numDecom += stats.getNumOfDecommissioningDatanodes();\r\n        numDecomLive += stats.getNumOfDecomActiveDatanodes();\r\n        numDecomDead += stats.getNumOfDecomDeadDatanodes();\r\n        numInMaintenanceLive += stats.getNumOfInMaintenanceLiveDataNodes();\r\n        numInMaintenanceDead += stats.getNumOfInMaintenanceLiveDataNodes();\r\n        numEnteringMaintenance += stats.getNumOfEnteringMaintenanceDataNodes();\r\n        numCorruptsFilesCount += stats.getCorruptFilesCount();\r\n        scheduledReplicationBlocks += stats.getScheduledReplicationBlocks();\r\n        numberOfMissingBlocksWithReplicationFactorOne += stats.getNumberOfMissingBlocksWithReplicationFactorOne();\r\n        highestPriorityLowRedundancyReplicatedBlocks += stats.getHighestPriorityLowRedundancyReplicatedBlocks();\r\n        highestPriorityLowRedundancyECBlocks += stats.getHighestPriorityLowRedundancyECBlocks();\r\n        pendingSPSPaths += stats.getPendingSPSPaths();\r\n    }\r\n    assertEquals(numBlocks, bean.getNumBlocks());\r\n    assertEquals(numLive, bean.getNumLiveNodes());\r\n    assertEquals(numDead, bean.getNumDeadNodes());\r\n    assertEquals(numStale, bean.getNumStaleNodes());\r\n    assertEquals(numDecom, bean.getNumDecommissioningNodes());\r\n    assertEquals(numDecomLive, bean.getNumDecomLiveNodes());\r\n    assertEquals(numDecomDead, bean.getNumDecomDeadNodes());\r\n    assertEquals(numInMaintenanceLive, bean.getNumInMaintenanceLiveDataNodes());\r\n    assertEquals(numInMaintenanceDead, bean.getNumInMaintenanceDeadDataNodes());\r\n    assertEquals(numEnteringMaintenance, bean.getNumEnteringMaintenanceDataNodes());\r\n    assertEquals(numFiles, bean.getNumFiles());\r\n    assertEquals(getActiveMemberships().size() + getStandbyMemberships().size(), bean.getNumNamenodes());\r\n    assertEquals(getNameservices().size(), bean.getNumNameservices());\r\n    assertEquals(numCorruptsFilesCount, bean.getCorruptFilesCount());\r\n    assertEquals(scheduledReplicationBlocks, bean.getScheduledReplicationBlocks());\r\n    assertEquals(numberOfMissingBlocksWithReplicationFactorOne, bean.getNumberOfMissingBlocksWithReplicationFactorOne());\r\n    assertEquals(highestPriorityLowRedundancyReplicatedBlocks, bean.getHighestPriorityLowRedundancyReplicatedBlocks());\r\n    assertEquals(highestPriorityLowRedundancyECBlocks, bean.getHighestPriorityLowRedundancyECBlocks());\r\n    assertEquals(pendingSPSPaths, bean.getPendingSPSPaths());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "validateClusterStatsRouterBean",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void validateClusterStatsRouterBean(RouterMBean bean)\n{\r\n    assertTrue(bean.getVersion().length() > 0);\r\n    assertTrue(bean.getCompiledDate().length() > 0);\r\n    assertTrue(bean.getCompileInfo().length() > 0);\r\n    assertTrue(bean.getRouterStarted().length() > 0);\r\n    assertTrue(bean.getHostAndPort().length() > 0);\r\n    assertFalse(bean.isSecurityEnabled());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testCapacity",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testCapacity(FederationMBean bean) throws IOException\n{\r\n    List<MembershipState> memberships = getActiveMemberships();\r\n    assertTrue(memberships.size() > 1);\r\n    BigInteger availableCapacity = BigInteger.valueOf(0);\r\n    BigInteger totalCapacity = BigInteger.valueOf(0);\r\n    BigInteger unitCapacity = BigInteger.valueOf(Long.MAX_VALUE);\r\n    for (MembershipState mock : memberships) {\r\n        MembershipStats stats = mock.getStats();\r\n        stats.setTotalSpace(Long.MAX_VALUE);\r\n        stats.setAvailableSpace(Long.MAX_VALUE);\r\n        mock.setStats(stats);\r\n        assertTrue(refreshNamenodeRegistration(NamenodeHeartbeatRequest.newInstance(mock)));\r\n        totalCapacity = totalCapacity.add(unitCapacity);\r\n        availableCapacity = availableCapacity.add(unitCapacity);\r\n    }\r\n    assertEquals(totalCapacity, bean.getTotalCapacityBigInt());\r\n    assertNotEquals(totalCapacity, BigInteger.valueOf(bean.getTotalCapacity()));\r\n    assertEquals(availableCapacity, bean.getRemainingCapacityBigInt());\r\n    assertNotEquals(availableCapacity, BigInteger.valueOf(bean.getRemainingCapacity()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "setupCluster",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setupCluster() throws Exception\n{\r\n    Configuration conf = getStateStoreConfiguration(StateStoreFileImpl.class);\r\n    getStateStore(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "startup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void startup() throws IOException\n{\r\n    removeAll(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testInsert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testInsert() throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    testInsert(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testUpdate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUpdate() throws IllegalArgumentException, ReflectiveOperationException, IOException, SecurityException\n{\r\n    testPut(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testDelete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDelete() throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    testRemove(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testFetchErrors",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFetchErrors() throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    testFetchErrors(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMetrics() throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    testMetrics(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "testHandlerAllocationEqualAssignment",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testHandlerAllocationEqualAssignment()\n{\r\n    RouterRpcFairnessPolicyController routerRpcFairnessPolicyController = getFairnessPolicyController(30);\r\n    verifyHandlerAllocation(routerRpcFairnessPolicyController);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "testHandlerAllocationWithLeftOverHandler",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testHandlerAllocationWithLeftOverHandler()\n{\r\n    RouterRpcFairnessPolicyController routerRpcFairnessPolicyController = getFairnessPolicyController(31);\r\n    assertTrue(routerRpcFairnessPolicyController.acquirePermit(CONCURRENT_NS));\r\n    verifyHandlerAllocation(routerRpcFairnessPolicyController);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "testHandlerAllocationPreconfigured",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testHandlerAllocationPreconfigured()\n{\r\n    Configuration conf = createConf(40);\r\n    conf.setInt(DFS_ROUTER_FAIR_HANDLER_COUNT_KEY_PREFIX + \"ns1\", 30);\r\n    RouterRpcFairnessPolicyController routerRpcFairnessPolicyController = FederationUtil.newFairnessPolicyController(conf);\r\n    for (int i = 0; i < 30; i++) {\r\n        assertTrue(routerRpcFairnessPolicyController.acquirePermit(\"ns1\"));\r\n    }\r\n    for (int i = 0; i < 5; i++) {\r\n        assertTrue(routerRpcFairnessPolicyController.acquirePermit(\"ns2\"));\r\n        assertTrue(routerRpcFairnessPolicyController.acquirePermit(CONCURRENT_NS));\r\n    }\r\n    assertFalse(routerRpcFairnessPolicyController.acquirePermit(\"ns1\"));\r\n    assertFalse(routerRpcFairnessPolicyController.acquirePermit(\"ns2\"));\r\n    assertFalse(routerRpcFairnessPolicyController.acquirePermit(CONCURRENT_NS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "testAllocationErrorWithZeroHandlers",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testAllocationErrorWithZeroHandlers()\n{\r\n    Configuration conf = createConf(0);\r\n    verifyInstantiationError(conf, 0, 3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "testAllocationErrorForLowDefaultHandlers",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testAllocationErrorForLowDefaultHandlers()\n{\r\n    Configuration conf = createConf(1);\r\n    verifyInstantiationError(conf, 1, 3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "testAllocationErrorForLowDefaultHandlersPerNS",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testAllocationErrorForLowDefaultHandlersPerNS()\n{\r\n    Configuration conf = createConf(1);\r\n    conf.setInt(DFS_ROUTER_FAIR_HANDLER_COUNT_KEY_PREFIX + \"concurrent\", 1);\r\n    verifyInstantiationError(conf, 1, 3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "testGetAvailableHandlerOnPerNs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetAvailableHandlerOnPerNs()\n{\r\n    RouterRpcFairnessPolicyController routerRpcFairnessPolicyController = getFairnessPolicyController(30);\r\n    assertEquals(\"{\\\"concurrent\\\":10,\\\"ns2\\\":10,\\\"ns1\\\":10}\", routerRpcFairnessPolicyController.getAvailableHandlerOnPerNs());\r\n    routerRpcFairnessPolicyController.acquirePermit(\"ns1\");\r\n    assertEquals(\"{\\\"concurrent\\\":10,\\\"ns2\\\":10,\\\"ns1\\\":9}\", routerRpcFairnessPolicyController.getAvailableHandlerOnPerNs());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "testGetAvailableHandlerOnPerNsForNoFairness",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetAvailableHandlerOnPerNsForNoFairness()\n{\r\n    Configuration conf = new Configuration();\r\n    RouterRpcFairnessPolicyController routerRpcFairnessPolicyController = FederationUtil.newFairnessPolicyController(conf);\r\n    assertEquals(\"N/A\", routerRpcFairnessPolicyController.getAvailableHandlerOnPerNs());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "testAllocationErrorForLowPreconfiguredHandlers",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testAllocationErrorForLowPreconfiguredHandlers()\n{\r\n    Configuration conf = createConf(1);\r\n    conf.setInt(DFS_ROUTER_FAIR_HANDLER_COUNT_KEY_PREFIX + \"ns1\", 2);\r\n    verifyInstantiationError(conf, 1, 4);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "testHandlerAllocationConcurrentConfigured",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testHandlerAllocationConcurrentConfigured()\n{\r\n    Configuration conf = createConf(5);\r\n    conf.setInt(DFS_ROUTER_FAIR_HANDLER_COUNT_KEY_PREFIX + \"ns1\", 1);\r\n    conf.setInt(DFS_ROUTER_FAIR_HANDLER_COUNT_KEY_PREFIX + \"ns2\", 1);\r\n    conf.setInt(DFS_ROUTER_FAIR_HANDLER_COUNT_KEY_PREFIX + \"concurrent\", 1);\r\n    RouterRpcFairnessPolicyController routerRpcFairnessPolicyController = FederationUtil.newFairnessPolicyController(conf);\r\n    assertTrue(routerRpcFairnessPolicyController.acquirePermit(\"ns1\"));\r\n    assertTrue(routerRpcFairnessPolicyController.acquirePermit(\"ns2\"));\r\n    assertFalse(routerRpcFairnessPolicyController.acquirePermit(\"ns1\"));\r\n    assertFalse(routerRpcFairnessPolicyController.acquirePermit(\"ns2\"));\r\n    for (int i = 0; i < 3; i++) {\r\n        assertTrue(routerRpcFairnessPolicyController.acquirePermit(CONCURRENT_NS));\r\n    }\r\n    assertFalse(routerRpcFairnessPolicyController.acquirePermit(CONCURRENT_NS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "verifyInstantiationError",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyInstantiationError(Configuration conf, int handlerCount, int totalDedicatedHandlers)\n{\r\n    GenericTestUtils.LogCapturer logs = GenericTestUtils.LogCapturer.captureLogs(LoggerFactory.getLogger(StaticRouterRpcFairnessPolicyController.class));\r\n    try {\r\n        FederationUtil.newFairnessPolicyController(conf);\r\n    } catch (IllegalArgumentException e) {\r\n    }\r\n    String errorMsg = String.format(StaticRouterRpcFairnessPolicyController.ERROR_MSG, handlerCount, totalDedicatedHandlers);\r\n    assertTrue(\"Should contain error message: \" + errorMsg, logs.getOutput().contains(errorMsg));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "getFairnessPolicyController",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterRpcFairnessPolicyController getFairnessPolicyController(int handlers)\n{\r\n    return FederationUtil.newFairnessPolicyController(createConf(handlers));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "verifyHandlerAllocation",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void verifyHandlerAllocation(RouterRpcFairnessPolicyController routerRpcFairnessPolicyController)\n{\r\n    for (int i = 0; i < 10; i++) {\r\n        assertTrue(routerRpcFairnessPolicyController.acquirePermit(\"ns1\"));\r\n        assertTrue(routerRpcFairnessPolicyController.acquirePermit(\"ns2\"));\r\n        assertTrue(routerRpcFairnessPolicyController.acquirePermit(CONCURRENT_NS));\r\n    }\r\n    assertFalse(routerRpcFairnessPolicyController.acquirePermit(\"ns1\"));\r\n    assertFalse(routerRpcFairnessPolicyController.acquirePermit(\"ns2\"));\r\n    assertFalse(routerRpcFairnessPolicyController.acquirePermit(CONCURRENT_NS));\r\n    routerRpcFairnessPolicyController.releasePermit(\"ns1\");\r\n    routerRpcFairnessPolicyController.releasePermit(\"ns2\");\r\n    routerRpcFairnessPolicyController.releasePermit(CONCURRENT_NS);\r\n    assertTrue(routerRpcFairnessPolicyController.acquirePermit(\"ns1\"));\r\n    assertTrue(routerRpcFairnessPolicyController.acquirePermit(\"ns2\"));\r\n    assertTrue(routerRpcFairnessPolicyController.acquirePermit(CONCURRENT_NS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "createConf",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createConf(int handlers)\n{\r\n    Configuration conf = new HdfsConfiguration();\r\n    conf.setInt(DFS_ROUTER_HANDLER_COUNT_KEY, handlers);\r\n    conf.set(DFS_ROUTER_MONITOR_NAMENODE, nameServices);\r\n    conf.setClass(RBFConfigKeys.DFS_ROUTER_FAIRNESS_POLICY_CONTROLLER_CLASS, StaticRouterRpcFairnessPolicyController.class, RouterRpcFairnessPolicyController.class);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "createIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DelegationTokenIdentifier createIdentifier()\n{\r\n    return new DelegationTokenIdentifier();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testDefaultNameserviceIsMissing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDefaultNameserviceIsMissing()\n{\r\n    Configuration conf = new Configuration();\r\n    MountTableResolver mountTable = new MountTableResolver(conf);\r\n    assertEquals(\"\", mountTable.getDefaultNamespace());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testDefaultNameserviceWithEmptyString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testDefaultNameserviceWithEmptyString()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(DFS_ROUTER_DEFAULT_NAMESERVICE, \"\");\r\n    MountTableResolver mountTable = new MountTableResolver(conf);\r\n    assertEquals(\"\", mountTable.getDefaultNamespace());\r\n    assertFalse(\"Default NS should be disabled if default NS is set empty\", mountTable.isDefaultNSEnable());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testRouterDefaultNameservice",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRouterDefaultNameservice()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(DFS_ROUTER_DEFAULT_NAMESERVICE, \"router_ns\");\r\n    MountTableResolver mountTable = new MountTableResolver(conf);\r\n    assertEquals(\"router_ns\", mountTable.getDefaultNamespace());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testRouterDefaultNameserviceDisabled",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testRouterDefaultNameserviceDisabled()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(DFS_ROUTER_DEFAULT_NAMESERVICE_ENABLE, false);\r\n    conf.set(DFS_NAMESERVICE_ID, \"ns_id\");\r\n    conf.set(DFS_NAMESERVICES, \"nss\");\r\n    MountTableResolver mountTable = new MountTableResolver(conf);\r\n    assertEquals(\"\", mountTable.getDefaultNamespace());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterWebHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterWebHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterWebHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "globalSetUp",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void globalSetUp() throws Exception\n{\r\n    cluster = new StateStoreDFSCluster(false, 1, MultipleDestinationMountTableResolver.class);\r\n    Configuration conf = new RouterConfigBuilder().stateStore().metrics().admin().rpc().quota().safemode().build();\r\n    cluster.addRouterOverrides(conf);\r\n    cluster.startRouters();\r\n    routerContext = cluster.getRandomRouter();\r\n    router = routerContext.getRouter();\r\n    stateStore = router.getStateStore();\r\n    Configuration routerConf = new Configuration();\r\n    InetSocketAddress routerSocket = router.getAdminServerAddress();\r\n    routerConf.setSocketAddr(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, routerSocket);\r\n    admin = new RouterAdmin(routerConf);\r\n    client = routerContext.getAdminClient();\r\n    ActiveNamenodeResolver membership = router.getNamenodeResolver();\r\n    membership.registerNamenode(createNamenodeReport(\"ns0\", \"nn1\", HAServiceState.ACTIVE));\r\n    membership.registerNamenode(createNamenodeReport(\"ns1\", \"nn1\", HAServiceState.ACTIVE));\r\n    stateStore.refreshCaches(true);\r\n    Quota quota = Mockito.spy(routerContext.getRouter().createRpcServer().getQuotaModule());\r\n    Mockito.doNothing().when(quota).setQuota(Mockito.anyString(), Mockito.anyLong(), Mockito.anyLong(), Mockito.any(), Mockito.anyBoolean());\r\n    Whitebox.setInternalState(routerContext.getRouter().getRpcServer(), \"quotaCall\", quota);\r\n    RouterRpcServer spyRpcServer = Mockito.spy(routerContext.getRouter().createRpcServer());\r\n    Whitebox.setInternalState(routerContext.getRouter(), \"rpcServer\", spyRpcServer);\r\n    Mockito.doReturn(null).when(spyRpcServer).getFileInfo(Mockito.anyString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDownCluster",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDownCluster()\n{\r\n    cluster.stopRouter(routerContext);\r\n    cluster.shutdown();\r\n    cluster = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown()\n{\r\n    System.setOut(OLD_OUT);\r\n    System.setErr(OLD_ERR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testAddMountTable",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testAddMountTable() throws Exception\n{\r\n    String nsId = \"ns0,ns1\";\r\n    String src = \"/test-addmounttable\";\r\n    String dest = \"/addmounttable\";\r\n    String[] argv = new String[] { \"-add\", src, nsId, dest };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(src);\r\n    GetMountTableEntriesResponse getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    MountTable mountTable = getResponse.getEntries().get(0);\r\n    List<RemoteLocation> destinations = mountTable.getDestinations();\r\n    assertEquals(2, destinations.size());\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    assertEquals(\"ns0\", destinations.get(0).getNameserviceId());\r\n    assertEquals(dest, destinations.get(0).getDest());\r\n    assertEquals(\"ns1\", destinations.get(1).getNameserviceId());\r\n    assertEquals(dest, destinations.get(1).getDest());\r\n    assertFalse(mountTable.isReadOnly());\r\n    assertFalse(mountTable.isFaultTolerant());\r\n    dest = dest + \"-new\";\r\n    argv = new String[] { \"-add\", src, nsId, dest, \"-readonly\", \"-faulttolerant\", \"-order\", \"HASH_ALL\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(4, mountTable.getDestinations().size());\r\n    RemoteLocation loc2 = mountTable.getDestinations().get(2);\r\n    assertEquals(\"ns0\", loc2.getNameserviceId());\r\n    assertEquals(dest, loc2.getDest());\r\n    RemoteLocation loc3 = mountTable.getDestinations().get(3);\r\n    assertEquals(\"ns1\", loc3.getNameserviceId());\r\n    assertEquals(dest, loc3.getDest());\r\n    assertTrue(mountTable.isReadOnly());\r\n    assertTrue(mountTable.isFaultTolerant());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testAddMountTableNotNormalized",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testAddMountTableNotNormalized() throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    String src = \"/test-addmounttable-notnormalized\";\r\n    String srcWithSlash = src + \"/\";\r\n    String dest = \"/addmounttable-notnormalized\";\r\n    String[] argv = new String[] { \"-add\", srcWithSlash, nsId, dest };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(src);\r\n    GetMountTableEntriesResponse getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    MountTable mountTable = getResponse.getEntries().get(0);\r\n    List<RemoteLocation> destinations = mountTable.getDestinations();\r\n    assertEquals(1, destinations.size());\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    assertEquals(nsId, destinations.get(0).getNameserviceId());\r\n    assertEquals(dest, destinations.get(0).getDest());\r\n    assertFalse(mountTable.isReadOnly());\r\n    assertFalse(mountTable.isFaultTolerant());\r\n    dest = dest + \"-new\";\r\n    argv = new String[] { \"-add\", srcWithSlash, nsId, dest, \"-readonly\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(2, mountTable.getDestinations().size());\r\n    assertEquals(nsId, mountTable.getDestinations().get(1).getNameserviceId());\r\n    assertEquals(dest, mountTable.getDestinations().get(1).getDest());\r\n    assertTrue(mountTable.isReadOnly());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testAddOrderMountTable",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testAddOrderMountTable() throws Exception\n{\r\n    testAddOrderMountTable(DestinationOrder.HASH);\r\n    testAddOrderMountTable(DestinationOrder.LOCAL);\r\n    testAddOrderMountTable(DestinationOrder.RANDOM);\r\n    testAddOrderMountTable(DestinationOrder.HASH_ALL);\r\n    testAddOrderMountTable(DestinationOrder.SPACE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testAddOrderErrorMsg",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAddOrderErrorMsg() throws Exception\n{\r\n    DestinationOrder order = DestinationOrder.HASH;\r\n    final String mnt = \"/newAdd1\" + order;\r\n    final String nsId = \"ns0,ns1\";\r\n    final String dest = \"/changAdd\";\r\n    String[] argv1 = new String[] { \"-add\", mnt, nsId, dest, \"-order\", order.toString() };\r\n    assertEquals(0, ToolRunner.run(admin, argv1));\r\n    String[] argv = new String[] { \"-add\", mnt, nsId, dest, \"-orde\", order.toString() };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testAddOrderMountTable",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testAddOrderMountTable(DestinationOrder order) throws Exception\n{\r\n    final String mnt = \"/\" + order;\r\n    final String nsId = \"ns0,ns1\";\r\n    final String dest = \"/\";\r\n    String[] argv = new String[] { \"-add\", mnt, nsId, dest, \"-order\", order.toString() };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    MountTableManager mountTable = client.getMountTableManager();\r\n    GetMountTableEntriesRequest request = GetMountTableEntriesRequest.newInstance(mnt);\r\n    GetMountTableEntriesResponse response = mountTable.getMountTableEntries(request);\r\n    List<MountTable> entries = response.getEntries();\r\n    assertEquals(1, entries.size());\r\n    assertEquals(2, entries.get(0).getDestinations().size());\r\n    assertEquals(order, response.getEntries().get(0).getDestOrder());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testListMountTable",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testListMountTable() throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    String src = \"/test-lsmounttable\";\r\n    String srcWithSlash = src + \"/\";\r\n    String dest = \"/lsmounttable\";\r\n    String[] argv = new String[] { \"-add\", src, nsId, dest };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    System.setOut(new PrintStream(out));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    argv = new String[] { \"-ls\", src };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    String response = out.toString();\r\n    assertTrue(\"Wrong response: \" + response, response.contains(src));\r\n    argv = new String[] { \"-ls\", srcWithSlash };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    response = out.toString();\r\n    assertTrue(\"Wrong response: \" + response, response.contains(src));\r\n    argv = new String[] { \"-ls\", srcWithSlash, \"check\", \"check2\" };\r\n    System.setErr(new PrintStream(err));\r\n    ToolRunner.run(admin, argv);\r\n    response = err.toString();\r\n    assertTrue(\"Wrong response: \" + response, response.contains(\"Too many arguments, Max=2 argument allowed\"));\r\n    out.reset();\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(\"/\");\r\n    GetMountTableEntriesResponse getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    argv = new String[] { \"-ls\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    response = out.toString();\r\n    assertTrue(\"Wrong response: \" + response, response.contains(src));\r\n    for (MountTable entry : getResponse.getEntries()) {\r\n        assertTrue(\"Wrong response: \" + response, response.contains(entry.getSourcePath()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testListWithDetails",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testListWithDetails() throws Exception\n{\r\n    String[] argv = new String[] { \"-add\", \"/testLsWithDetails\", \"ns0,ns1\", \"/dest\", \"-order\", \"HASH_ALL\", \"-readonly\", \"-faulttolerant\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    System.setOut(new PrintStream(out));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    argv = new String[] { \"-ls\", \"-d\", \"/testLsWithDetails\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    String response = out.toString();\r\n    assertTrue(response.contains(\"Read-Only\"));\r\n    assertTrue(response.contains(\"Fault-Tolerant\"));\r\n    out.reset();\r\n    argv = new String[] { \"-ls\", \"-d\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    response = out.toString();\r\n    assertTrue(\"Wrong response: \" + response, response.contains(\"Read-Only\"));\r\n    assertTrue(\"Wrong response: \" + response, response.contains(\"Fault-Tolerant\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testListNestedMountTable",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testListNestedMountTable() throws Exception\n{\r\n    String dir1 = \"/test-ls\";\r\n    String dir2 = \"/test-ls-longger\";\r\n    String[] nsIdList = { \"ns0\", \"ns1\", \"ns2\", \"ns3\", \"ns3\" };\r\n    String[] sourceList = { dir1, dir1 + \"/subdir1\", dir2, dir2 + \"/subdir1\", dir2 + \"/subdir2\" };\r\n    String[] destList = { \"/test-ls\", \"/test-ls/subdir1\", \"/ls\", \"/ls/subdir1\", \"/ls/subdir2\" };\r\n    for (int i = 0; i < nsIdList.length; i++) {\r\n        String[] argv = new String[] { \"-add\", sourceList[i], nsIdList[i], destList[i] };\r\n        assertEquals(0, ToolRunner.run(admin, argv));\r\n    }\r\n    System.setOut(new PrintStream(out));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    String[] argv = new String[] { \"-ls\", dir1 };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    String outStr = out.toString();\r\n    assertTrue(out.toString().contains(dir1 + \"/subdir1\"));\r\n    assertFalse(out.toString().contains(dir2));\r\n    argv = new String[] { \"-ls\", dir2 };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    assertTrue(out.toString().contains(dir2 + \"/subdir1\"));\r\n    assertTrue(out.toString().contains(dir2 + \"/subdir2\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRemoveMountTable",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testRemoveMountTable() throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    String src = \"/test-rmmounttable\";\r\n    String dest = \"/rmmounttable\";\r\n    String[] argv = new String[] { \"-add\", src, nsId, dest };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(src);\r\n    GetMountTableEntriesResponse getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    MountTable mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    argv = new String[] { \"-rm\", src };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    assertEquals(0, getResponse.getEntries().size());\r\n    String invalidPath = \"/invalid\";\r\n    System.setOut(new PrintStream(out));\r\n    argv = new String[] { \"-rm\", invalidPath };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    assertTrue(out.toString().contains(\"Cannot remove mount point \" + invalidPath));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMultiArgsRemoveMountTable",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testMultiArgsRemoveMountTable() throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    String src1 = \"/test-rmmounttable1\";\r\n    String src2 = \"/test-rmmounttable2\";\r\n    String dest1 = \"/rmmounttable1\";\r\n    String dest2 = \"/rmmounttable2\";\r\n    String[] argv = new String[] { \"-add\", src1, nsId, dest1 };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    argv = new String[] { \"-add\", src2, nsId, dest2 };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(src1);\r\n    GetMountTableEntriesResponse getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    MountTable mountTable = getResponse.getEntries().get(0);\r\n    getRequest = GetMountTableEntriesRequest.newInstance(src2);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    assertEquals(src1, mountTable.getSourcePath());\r\n    mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(src2, mountTable.getSourcePath());\r\n    argv = new String[] { \"-rm\", src1, src2 };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    assertEquals(0, getResponse.getEntries().size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRemoveMountTableNotNormalized",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testRemoveMountTableNotNormalized() throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    String src = \"/test-rmmounttable-notnormalized\";\r\n    String srcWithSlash = src + \"/\";\r\n    String dest = \"/rmmounttable-notnormalized\";\r\n    String[] argv = new String[] { \"-add\", src, nsId, dest };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(src);\r\n    GetMountTableEntriesResponse getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    MountTable mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    argv = new String[] { \"-rm\", srcWithSlash };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    assertEquals(0, getResponse.getEntries().size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountTableDefaultACL",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testMountTableDefaultACL() throws Exception\n{\r\n    String[] argv = new String[] { \"-add\", \"/testpath0\", \"ns0\", \"/testdir0\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(\"/testpath0\");\r\n    GetMountTableEntriesResponse getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    MountTable mountTable = getResponse.getEntries().get(0);\r\n    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n    String group = ugi.getGroups().isEmpty() ? ugi.getShortUserName() : ugi.getPrimaryGroupName();\r\n    assertEquals(ugi.getShortUserName(), mountTable.getOwnerName());\r\n    assertEquals(group, mountTable.getGroupName());\r\n    assertEquals((short) 0755, mountTable.getMode().toShort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testUpdateMountTableWithoutPermission",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testUpdateMountTableWithoutPermission() throws Exception\n{\r\n    UserGroupInformation superUser = UserGroupInformation.getCurrentUser();\r\n    String superUserName = superUser.getShortUserName();\r\n    System.setOut(new PrintStream(out));\r\n    try {\r\n        stateStore.loadCache(MountTableStoreImpl.class, true);\r\n        String[] argv = new String[] { \"-add\", \"/testpath3-1\", \"ns0\", \"/testdir3-1\", \"-owner\", superUserName, \"-group\", superUserName, \"-mode\", \"755\" };\r\n        assertEquals(0, ToolRunner.run(admin, argv));\r\n        UserGroupInformation remoteUser = UserGroupInformation.createRemoteUser(TEST_USER);\r\n        UserGroupInformation.setLoginUser(remoteUser);\r\n        stateStore.loadCache(MountTableStoreImpl.class, true);\r\n        argv = new String[] { \"-update\", \"/testpath3-1\", \"ns0\", \"/testdir3-2\", \"-owner\", TEST_USER, \"-group\", TEST_USER, \"-mode\", \"777\" };\r\n        assertEquals(\"Normal user update mount table which created by \" + \"superuser unexpected.\", -1, ToolRunner.run(admin, argv));\r\n    } finally {\r\n        UserGroupInformation.setLoginUser(superUser);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testOperateMountTableWithGroupPermission",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testOperateMountTableWithGroupPermission() throws Exception\n{\r\n    UserGroupInformation superUser = UserGroupInformation.getCurrentUser();\r\n    System.setOut(new PrintStream(out));\r\n    try {\r\n        String testUserA = \"test-user-a\";\r\n        String testUserB = \"test-user-b\";\r\n        String testUserC = \"test-user-c\";\r\n        String testUserD = \"test-user-d\";\r\n        String testGroup = \"test-group\";\r\n        stateStore.loadCache(MountTableStoreImpl.class, true);\r\n        UserGroupInformation userA = UserGroupInformation.createUserForTesting(testUserA, new String[] { testGroup });\r\n        UserGroupInformation.setLoginUser(userA);\r\n        String[] argv = new String[] { \"-add\", \"/testpath4-1\", \"ns0\", \"/testdir4-1\", \"-owner\", testUserA, \"-group\", testGroup, \"-mode\", \"775\" };\r\n        assertEquals(\"Normal user can't add mount table unexpected.\", 0, ToolRunner.run(admin, argv));\r\n        stateStore.loadCache(MountTableStoreImpl.class, true);\r\n        UserGroupInformation userB = UserGroupInformation.createUserForTesting(testUserB, new String[] { testGroup });\r\n        UserGroupInformation.setLoginUser(userB);\r\n        argv = new String[] { \"-update\", \"/testpath4-1\", \"ns0\", \"/testdir4-2\", \"-owner\", testUserA, \"-group\", testGroup, \"-mode\", \"775\" };\r\n        assertEquals(\"Another user in same group can't update mount table \" + \"unexpected.\", 0, ToolRunner.run(admin, argv));\r\n        stateStore.loadCache(MountTableStoreImpl.class, true);\r\n        UserGroupInformation userC = UserGroupInformation.createUserForTesting(testUserC, new String[] {});\r\n        UserGroupInformation.setLoginUser(userC);\r\n        argv = new String[] { \"-update\", \"/testpath4-1\", \"ns0\", \"/testdir4-3\", \"-owner\", testUserA, \"-group\", testGroup, \"-mode\", \"775\" };\r\n        assertEquals(\"Another user not in same group have no permission but \" + \"update mount table successful unexpected.\", -1, ToolRunner.run(admin, argv));\r\n        stateStore.loadCache(MountTableStoreImpl.class, true);\r\n        UserGroupInformation userD = UserGroupInformation.createUserForTesting(testUserD, new String[] { testGroup });\r\n        UserGroupInformation.setLoginUser(userD);\r\n        argv = new String[] { \"-add\", \"/testpath4-1/foo/bar\", \"ns0\", \"/testdir4-1/foo/bar\", \"-owner\", testUserD, \"-group\", testGroup, \"-mode\", \"775\" };\r\n        assertEquals(\"Normal user can't add mount table unexpected.\", 0, ToolRunner.run(admin, argv));\r\n        UserGroupInformation.setLoginUser(userC);\r\n        argv = new String[] { \"-rm\", \"/testpath4-1\" };\r\n        assertEquals(-1, ToolRunner.run(admin, argv));\r\n        UserGroupInformation.setLoginUser(userB);\r\n        assertEquals(\"Another user in same group can't remove mount table \" + \"unexpected.\", 0, ToolRunner.run(admin, argv));\r\n    } finally {\r\n        UserGroupInformation.setLoginUser(superUser);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testOperateMountTableWithSuperUserPermission",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testOperateMountTableWithSuperUserPermission() throws Exception\n{\r\n    UserGroupInformation superUser = UserGroupInformation.getCurrentUser();\r\n    System.setOut(new PrintStream(out));\r\n    try {\r\n        String testUserA = \"test-user-a\";\r\n        String testGroup = \"test-group\";\r\n        stateStore.loadCache(MountTableStoreImpl.class, true);\r\n        UserGroupInformation userA = UserGroupInformation.createUserForTesting(testUserA, new String[] { testGroup });\r\n        UserGroupInformation.setLoginUser(userA);\r\n        String[] argv = new String[] { \"-add\", \"/testpath5-1\", \"ns0\", \"/testdir5-1\", \"-owner\", testUserA, \"-group\", testGroup, \"-mode\", \"755\" };\r\n        assertEquals(0, ToolRunner.run(admin, argv));\r\n        stateStore.loadCache(MountTableStoreImpl.class, true);\r\n        UserGroupInformation.setLoginUser(superUser);\r\n        argv = new String[] { \"-update\", \"/testpath5-1\", \"ns0\", \"/testdir5-2\", \"-owner\", testUserA, \"-group\", testGroup, \"-mode\", \"755\" };\r\n        assertEquals(\"Super user can't update mount table unexpected.\", 0, ToolRunner.run(admin, argv));\r\n        argv = new String[] { \"-rm\", \"/testpath5-1\" };\r\n        assertEquals(\"Super user can't remove mount table unexpected.\", 0, ToolRunner.run(admin, argv));\r\n    } finally {\r\n        UserGroupInformation.setLoginUser(superUser);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testAddMountTableIfParentExist",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testAddMountTableIfParentExist() throws Exception\n{\r\n    UserGroupInformation superUser = UserGroupInformation.getCurrentUser();\r\n    System.setOut(new PrintStream(out));\r\n    try {\r\n        String testUserA = \"test-user-a\";\r\n        String testUserB = \"test-user-b\";\r\n        String testGroup = \"test-group\";\r\n        stateStore.loadCache(MountTableStoreImpl.class, true);\r\n        UserGroupInformation userA = UserGroupInformation.createUserForTesting(testUserA, new String[] { testGroup });\r\n        UserGroupInformation.setLoginUser(userA);\r\n        String[] argv = new String[] { \"-add\", \"/testpath6-1\", \"ns0\", \"/testdir6-1\", \"-owner\", testUserA, \"-group\", testGroup, \"-mode\", \"755\" };\r\n        assertEquals(0, ToolRunner.run(admin, argv));\r\n        UserGroupInformation userB = UserGroupInformation.createUserForTesting(testUserB, new String[] { testGroup });\r\n        UserGroupInformation.setLoginUser(userB);\r\n        argv = new String[] { \"-add\", \"/testpath6-1/parent/foo\", \"ns0\", \"/testdir6-1/parent/foo\", \"-owner\", testUserA, \"-group\", testGroup, \"-mode\", \"755\" };\r\n        assertEquals(0, ToolRunner.run(admin, argv));\r\n        argv = new String[] { \"-add\", \"/testpath6-1/foo\", \"ns0\", \"/testdir6-1/foo\", \"-owner\", testUserA, \"-group\", testGroup, \"-mode\", \"755\" };\r\n        assertEquals(-1, ToolRunner.run(admin, argv));\r\n    } finally {\r\n        UserGroupInformation.setLoginUser(superUser);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountTablePermissions",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testMountTablePermissions() throws Exception\n{\r\n    System.setOut(new PrintStream(out));\r\n    String[] argv = new String[] { \"-add\", \"/testpath2-1\", \"ns0\", \"/testdir2-1\", \"-owner\", TEST_USER, \"-group\", TEST_USER, \"-mode\", \"0455\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    String superUser = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    UserGroupInformation remoteUser = UserGroupInformation.createRemoteUser(TEST_USER);\r\n    UserGroupInformation.setLoginUser(remoteUser);\r\n    verifyExecutionResult(\"/testpath2-1\", true, -1, -1);\r\n    argv = new String[] { \"-add\", \"/testpath2-2\", \"ns0\", \"/testdir2-2\", \"-owner\", TEST_USER, \"-group\", TEST_USER, \"-mode\", \"0255\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    verifyExecutionResult(\"/testpath2-2\", false, -1, 0);\r\n    argv = new String[] { \"-add\", \"/testpath2-3\", \"ns0\", \"/testdir2-3\", \"-owner\", TEST_USER, \"-group\", TEST_USER, \"-mode\", \"0755\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    verifyExecutionResult(\"/testpath2-3\", true, 0, 0);\r\n    remoteUser = UserGroupInformation.createRemoteUser(superUser);\r\n    UserGroupInformation.setLoginUser(remoteUser);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifyExecutionResult",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void verifyExecutionResult(String mount, boolean canRead, int addCommandCode, int rmCommandCode) throws Exception\n{\r\n    String[] argv = null;\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    out.reset();\r\n    argv = new String[] { \"-ls\", mount };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    assertEquals(canRead, out.toString().contains(mount));\r\n    argv = new String[] { \"-add\", mount, \"ns0\", mount + \"newdir\" };\r\n    assertEquals(addCommandCode, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    argv = new String[] { \"-rm\", mount };\r\n    assertEquals(rmCommandCode, ToolRunner.run(admin, argv));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testInvalidArgumentMessage",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testInvalidArgumentMessage() throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    String src = \"/testSource\";\r\n    System.setOut(new PrintStream(out));\r\n    String[] argv = new String[] { \"-add\", src, nsId };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(\"Wrong message: \" + out, out.toString().contains(\"\\t[-add <source> <nameservice1, nameservice2, ...> <destination> \" + \"[-readonly] [-faulttolerant] \" + \"[-order HASH|LOCAL|RANDOM|HASH_ALL|SPACE] \" + \"-owner <owner> -group <group> -mode <mode>]\"));\r\n    out.reset();\r\n    argv = new String[] { \"-update\", src, nsId };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(\"Wrong message: \" + out, out.toString().contains(\"\\t[-update <source> [<nameservice1, nameservice2, ...> <destination>] \" + \"[-readonly true|false] [-faulttolerant true|false] \" + \"[-order HASH|LOCAL|RANDOM|HASH_ALL|SPACE] \" + \"-owner <owner> -group <group> -mode <mode>]\"));\r\n    out.reset();\r\n    argv = new String[] { \"-rm\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(out.toString().contains(\"\\t[-rm <source>]\"));\r\n    out.reset();\r\n    argv = new String[] { \"-setQuota\", src };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(out.toString().contains(\"\\t[-setQuota <path> -nsQuota <nsQuota> -ssQuota \" + \"<quota in bytes or quota size string>]\"));\r\n    out.reset();\r\n    argv = new String[] { \"-clrQuota\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(out.toString().contains(\"\\t[-clrQuota <path>]\"));\r\n    out.reset();\r\n    argv = new String[] { \"-safemode\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(out.toString().contains(\"\\t[-safemode enter | leave | get]\"));\r\n    out.reset();\r\n    argv = new String[] { \"-nameservice\", nsId };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(out.toString().contains(\"\\t[-nameservice enable | disable <nameservice>]\"));\r\n    out.reset();\r\n    argv = new String[] { \"-getDestination\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(out.toString().contains(\"\\t[-getDestination <path>]\"));\r\n    out.reset();\r\n    argv = new String[] { \"-refreshRouterArgs\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(out.toString().contains(\"\\t[-refreshRouterArgs \" + \"<host:ipc_port> <key> [arg1..argn]]\"));\r\n    out.reset();\r\n    argv = new String[] { \"-Random\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    String expected = \"Usage: hdfs dfsrouteradmin :\\n\" + \"\\t[-add <source> <nameservice1, nameservice2, ...> <destination> \" + \"[-readonly] [-faulttolerant] \" + \"[-order HASH|LOCAL|RANDOM|HASH_ALL|SPACE] \" + \"-owner <owner> -group <group> -mode <mode>]\\n\" + \"\\t[-update <source> [<nameservice1, nameservice2, ...> \" + \"<destination>] [-readonly true|false]\" + \" [-faulttolerant true|false] \" + \"[-order HASH|LOCAL|RANDOM|HASH_ALL|SPACE] \" + \"-owner <owner> -group <group> -mode <mode>]\\n\" + \"\\t[-rm <source>]\\n\" + \"\\t[-ls [-d] <path>]\\n\" + \"\\t[-getDestination <path>]\\n\" + \"\\t[-setQuota <path> -nsQuota <nsQuota> -ssQuota\" + \" <quota in bytes or quota size string>]\\n\" + \"\\t[-setStorageTypeQuota <path> -storageType <storage type>\" + \" <quota in bytes or quota size string>]\\n\" + \"\\t[-clrQuota <path>]\\n\" + \"\\t[-clrStorageTypeQuota <path>]\\n\" + \"\\t[-safemode enter | leave | get]\\n\" + \"\\t[-nameservice enable | disable <nameservice>]\\n\" + \"\\t[-getDisabledNameservices]\\n\" + \"\\t[-refresh]\\n\" + \"\\t[-refreshRouterArgs <host:ipc_port> <key> [arg1..argn]]\";\r\n    assertTrue(\"Wrong message: \" + out, out.toString().contains(expected));\r\n    out.reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testWrongArgumentsWhenSetStorageTypeQuota",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testWrongArgumentsWhenSetStorageTypeQuota() throws Exception\n{\r\n    String src = \"/type-QuotaMounttable\";\r\n    System.setErr(new PrintStream(err));\r\n    String[] argv = new String[] { \"-setStorageTypeQuota\", src, \"check\", \"c2\", \"c3\" };\r\n    ToolRunner.run(admin, argv);\r\n    assertTrue(err.toString().contains(\"Invalid argument : check\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSetStorageTypeQuota",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSetStorageTypeQuota() throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    String src = \"/type-QuotaMounttable\";\r\n    String dest = \"/type-QuotaMounttable\";\r\n    try {\r\n        addMountTable(src, nsId, dest);\r\n        MountTable mountTable = getMountTable(src).get(0);\r\n        RouterQuotaUsage quotaUsage = mountTable.getQuota();\r\n        for (StorageType t : StorageType.values()) {\r\n            assertEquals(RouterQuotaUsage.QUOTA_USAGE_COUNT_DEFAULT, quotaUsage.getTypeConsumed(t));\r\n            assertEquals(HdfsConstants.QUOTA_RESET, quotaUsage.getTypeQuota(t));\r\n        }\r\n        long ssQuota = 100;\r\n        setStorageTypeQuota(src, ssQuota, StorageType.DISK);\r\n        mountTable = getMountTable(src).get(0);\r\n        quotaUsage = mountTable.getQuota();\r\n        assertEquals(ssQuota, quotaUsage.getTypeQuota(StorageType.DISK));\r\n    } finally {\r\n        rmMountTable(src);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testClearStorageTypeQuota",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testClearStorageTypeQuota() throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    String src = \"/type-QuotaMounttable\";\r\n    String src1 = \"/type-QuotaMounttable1\";\r\n    String dest = \"/type-QuotaMounttable\";\r\n    String dest1 = \"/type-QuotaMounttable1\";\r\n    long ssQuota = 100;\r\n    try {\r\n        addMountTable(src, nsId, dest);\r\n        addMountTable(src1, nsId, dest1);\r\n        setStorageTypeQuota(src, ssQuota, StorageType.DISK);\r\n        assertEquals(ssQuota, getMountTable(src).get(0).getQuota().getTypeQuota(StorageType.DISK));\r\n        setStorageTypeQuota(src1, ssQuota, StorageType.DISK);\r\n        assertEquals(ssQuota, getMountTable(src1).get(0).getQuota().getTypeQuota(StorageType.DISK));\r\n        assertEquals(0, ToolRunner.run(admin, new String[] { \"-clrStorageTypeQuota\", src, src1 }));\r\n        stateStore.loadCache(MountTableStoreImpl.class, true);\r\n        List<MountTable> mountTables = getMountTable(\"/\");\r\n        for (int i = 0; i < 2; i++) {\r\n            MountTable mountTable = mountTables.get(i);\r\n            RouterQuotaUsage quotaUsage = mountTable.getQuota();\r\n            for (StorageType t : StorageType.values()) {\r\n                assertEquals(RouterQuotaUsage.QUOTA_USAGE_COUNT_DEFAULT, quotaUsage.getTypeConsumed(t));\r\n                assertEquals(HdfsConstants.QUOTA_RESET, quotaUsage.getTypeQuota(t));\r\n            }\r\n        }\r\n    } finally {\r\n        rmMountTable(src);\r\n        rmMountTable(src1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSetAndClearQuota",
  "errType" : null,
  "containingMethodsNum" : 53,
  "sourceCodeText" : "void testSetAndClearQuota() throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    String src = \"/test-QuotaMounttable\";\r\n    String src1 = \"/test-QuotaMounttable1\";\r\n    String dest = \"/QuotaMounttable\";\r\n    String[] argv = new String[] { \"-add\", src, nsId, dest };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(src);\r\n    GetMountTableEntriesResponse getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    MountTable mountTable = getResponse.getEntries().get(0);\r\n    RouterQuotaUsage quotaUsage = mountTable.getQuota();\r\n    assertEquals(RouterQuotaUsage.QUOTA_USAGE_COUNT_DEFAULT, quotaUsage.getFileAndDirectoryCount());\r\n    assertEquals(HdfsConstants.QUOTA_RESET, quotaUsage.getQuota());\r\n    assertEquals(RouterQuotaUsage.QUOTA_USAGE_COUNT_DEFAULT, quotaUsage.getSpaceConsumed());\r\n    assertEquals(HdfsConstants.QUOTA_RESET, quotaUsage.getSpaceQuota());\r\n    long nsQuota = 50;\r\n    long ssQuota = 100;\r\n    argv = new String[] { \"-setQuota\", src, \"-nsQuota\", String.valueOf(nsQuota), \"-ssQuota\", String.valueOf(ssQuota) };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    mountTable = getResponse.getEntries().get(0);\r\n    quotaUsage = mountTable.getQuota();\r\n    assertEquals(nsQuota, quotaUsage.getQuota());\r\n    assertEquals(ssQuota, quotaUsage.getSpaceQuota());\r\n    String newSsQuota = \"2m\";\r\n    argv = new String[] { \"-setQuota\", src, \"-ssQuota\", newSsQuota };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    mountTable = getResponse.getEntries().get(0);\r\n    quotaUsage = mountTable.getQuota();\r\n    assertEquals(nsQuota, quotaUsage.getQuota());\r\n    assertEquals(2 * 1024 * 1024, quotaUsage.getSpaceQuota());\r\n    argv = new String[] { \"-clrQuota\", src };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    mountTable = getResponse.getEntries().get(0);\r\n    quotaUsage = mountTable.getQuota();\r\n    assertEquals(HdfsConstants.QUOTA_RESET, quotaUsage.getQuota());\r\n    assertEquals(HdfsConstants.QUOTA_RESET, quotaUsage.getSpaceQuota());\r\n    String dest1 = \"/QuotaMounttable1\";\r\n    argv = new String[] { \"-add\", src1, nsId, dest1 };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    argv = new String[] { \"-setQuota\", src, \"-nsQuota\", String.valueOf(nsQuota), \"-ssQuota\", String.valueOf(ssQuota) };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    argv = new String[] { \"-setQuota\", src1, \"-nsQuota\", String.valueOf(nsQuota), \"-ssQuota\", String.valueOf(ssQuota) };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    argv = new String[] { \"-clrQuota\", src, src1 };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getRequest = GetMountTableEntriesRequest.newInstance(\"/\");\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    for (int i = 0; i < 2; i++) {\r\n        mountTable = getResponse.getEntries().get(i);\r\n        quotaUsage = mountTable.getQuota();\r\n        assertEquals(HdfsConstants.QUOTA_RESET, quotaUsage.getQuota());\r\n        assertEquals(HdfsConstants.QUOTA_RESET, quotaUsage.getSpaceQuota());\r\n    }\r\n    System.setErr(new PrintStream(err));\r\n    argv = new String[] { \"-setQuota\", src, \"check\", \"check2\" };\r\n    ToolRunner.run(admin, argv);\r\n    assertTrue(err.toString().contains(\"Invalid argument : check\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testManageSafeMode",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testManageSafeMode() throws Exception\n{\r\n    waitState(RouterServiceState.RUNNING);\r\n    assertFalse(routerContext.getRouter().getSafemodeService().isInSafeMode());\r\n    assertEquals(0, ToolRunner.run(admin, new String[] { \"-safemode\", \"enter\" }));\r\n    assertEquals(RouterServiceState.SAFEMODE, routerContext.getRouter().getRouterState());\r\n    assertTrue(routerContext.getRouter().getSafemodeService().isInSafeMode());\r\n    System.setOut(new PrintStream(out));\r\n    System.setErr(new PrintStream(err));\r\n    assertEquals(0, ToolRunner.run(admin, new String[] { \"-safemode\", \"get\" }));\r\n    assertTrue(out.toString().contains(\"true\"));\r\n    assertEquals(0, ToolRunner.run(admin, new String[] { \"-safemode\", \"leave\" }));\r\n    assertEquals(RouterServiceState.RUNNING, routerContext.getRouter().getRouterState());\r\n    assertFalse(routerContext.getRouter().getSafemodeService().isInSafeMode());\r\n    out.reset();\r\n    assertEquals(0, ToolRunner.run(admin, new String[] { \"-safemode\", \"get\" }));\r\n    assertTrue(out.toString().contains(\"false\"));\r\n    out.reset();\r\n    assertEquals(-1, ToolRunner.run(admin, new String[] { \"-safemode\", \"get\", \"-random\", \"check\" }));\r\n    assertTrue(err.toString(), err.toString().contains(\"safemode: Too many arguments, Max=1 argument allowed only\"));\r\n    err.reset();\r\n    assertEquals(-1, ToolRunner.run(admin, new String[] { \"-safemode\", \"check\" }));\r\n    assertTrue(err.toString(), err.toString().contains(\"safemode: Invalid argument: check\"));\r\n    err.reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSafeModeStatus",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testSafeModeStatus() throws Exception\n{\r\n    waitState(RouterServiceState.RUNNING);\r\n    assertFalse(routerContext.getRouter().getSafemodeService().isInSafeMode());\r\n    final RouterClientProtocol clientProtocol = routerContext.getRouter().getRpcServer().getClientProtocolModule();\r\n    assertEquals(HAServiceState.ACTIVE, clientProtocol.getHAServiceState());\r\n    assertEquals(0, ToolRunner.run(admin, new String[] { \"-safemode\", \"enter\" }));\r\n    RBFMetrics metrics = router.getMetrics();\r\n    String jsonString = metrics.getRouterStatus();\r\n    String result = router.getNamenodeMetrics().getSafemode();\r\n    assertTrue(\"Wrong safe mode message: \" + result, result.startsWith(\"Safe mode is ON.\"));\r\n    assertEquals(RouterServiceState.SAFEMODE.toString(), jsonString);\r\n    assertTrue(routerContext.getRouter().getSafemodeService().isInSafeMode());\r\n    assertEquals(HAServiceState.STANDBY, clientProtocol.getHAServiceState());\r\n    System.setOut(new PrintStream(out));\r\n    assertEquals(0, ToolRunner.run(admin, new String[] { \"-safemode\", \"leave\" }));\r\n    jsonString = metrics.getRouterStatus();\r\n    result = router.getNamenodeMetrics().getSafemode();\r\n    assertEquals(\"Wrong safe mode message: \" + result, \"\", result);\r\n    assertEquals(RouterServiceState.RUNNING.toString(), jsonString);\r\n    assertFalse(routerContext.getRouter().getSafemodeService().isInSafeMode());\r\n    assertEquals(HAServiceState.ACTIVE, clientProtocol.getHAServiceState());\r\n    out.reset();\r\n    assertEquals(0, ToolRunner.run(admin, new String[] { \"-safemode\", \"get\" }));\r\n    assertTrue(out.toString().contains(\"false\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSafeModePermission",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testSafeModePermission() throws Exception\n{\r\n    waitState(RouterServiceState.RUNNING);\r\n    assertFalse(routerContext.getRouter().getSafemodeService().isInSafeMode());\r\n    UserGroupInformation superUser = UserGroupInformation.createRemoteUser(UserGroupInformation.getCurrentUser().getShortUserName());\r\n    UserGroupInformation remoteUser = UserGroupInformation.createRemoteUser(TEST_USER);\r\n    try {\r\n        UserGroupInformation.setLoginUser(remoteUser);\r\n        assertEquals(-1, ToolRunner.run(admin, new String[] { \"-safemode\", \"enter\" }));\r\n        UserGroupInformation.setLoginUser(superUser);\r\n        assertEquals(0, ToolRunner.run(admin, new String[] { \"-safemode\", \"enter\" }));\r\n        UserGroupInformation.setLoginUser(remoteUser);\r\n        assertEquals(-1, ToolRunner.run(admin, new String[] { \"-safemode\", \"leave\" }));\r\n        UserGroupInformation.setLoginUser(superUser);\r\n        assertEquals(0, ToolRunner.run(admin, new String[] { \"-safemode\", \"leave\" }));\r\n    } finally {\r\n        UserGroupInformation.setLoginUser(superUser);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testCreateInvalidEntry",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCreateInvalidEntry() throws Exception\n{\r\n    String[] argv = new String[] { \"-add\", \"test-createInvalidEntry\", \"ns0\", \"/createInvalidEntry\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    argv = new String[] { \"-add\", \"/test-createInvalidEntry\", \"ns0\", \"createInvalidEntry\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    argv = new String[] { \"-add\", null, \"ns0\", \"/createInvalidEntry\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    argv = new String[] { \"-add\", \"/test-createInvalidEntry\", \"ns0\", null };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    argv = new String[] { \"-add\", \"\", \"ns0\", \"/createInvalidEntry\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    argv = new String[] { \"-add\", \"/test-createInvalidEntry\", null, \"/createInvalidEntry\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    argv = new String[] { \"-add\", \"/test-createInvalidEntry\", \"\", \"/createInvalidEntry\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNameserviceManager",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testNameserviceManager() throws Exception\n{\r\n    assertEquals(0, ToolRunner.run(admin, new String[] { \"-nameservice\", \"disable\", \"ns0\" }));\r\n    stateStore.loadCache(DisabledNameserviceStoreImpl.class, true);\r\n    System.setOut(new PrintStream(out));\r\n    assertEquals(0, ToolRunner.run(admin, new String[] { \"-getDisabledNameservices\" }));\r\n    assertTrue(\"ns0 should be reported: \" + out, out.toString().contains(\"ns0\"));\r\n    assertEquals(0, ToolRunner.run(admin, new String[] { \"-nameservice\", \"enable\", \"ns0\" }));\r\n    out.reset();\r\n    stateStore.loadCache(DisabledNameserviceStoreImpl.class, true);\r\n    assertEquals(0, ToolRunner.run(admin, new String[] { \"-getDisabledNameservices\" }));\r\n    assertFalse(\"ns0 should not be reported: \" + out, out.toString().contains(\"ns0\"));\r\n    System.setErr(new PrintStream(err));\r\n    assertEquals(-1, ToolRunner.run(admin, new String[] { \"-nameservice\", \"enable\" }));\r\n    String msg = \"Not enough parameters specificed for cmd -nameservice\";\r\n    assertTrue(\"Got error: \" + err.toString(), err.toString().startsWith(msg));\r\n    err.reset();\r\n    assertEquals(-1, ToolRunner.run(admin, new String[] { \"-nameservice\", \"wrong\", \"ns0\" }));\r\n    assertTrue(\"Got error: \" + err.toString(), err.toString().startsWith(\"nameservice: Unknown command: wrong\"));\r\n    err.reset();\r\n    ToolRunner.run(admin, new String[] { \"-nameservice\", \"enable\", \"ns0\", \"check\" });\r\n    assertTrue(err.toString().contains(\"Too many arguments, Max=2 arguments allowed\"));\r\n    err.reset();\r\n    ToolRunner.run(admin, new String[] { \"-getDisabledNameservices\", \"check\" });\r\n    assertTrue(err.toString().contains(\"No arguments allowed\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRefreshMountTableCache",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRefreshMountTableCache() throws Exception\n{\r\n    String src = \"/refreshMount\";\r\n    String[] argv = new String[] { \"-add\", src, \"refreshNS0\", \"/refreshDest\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    System.setOut(new PrintStream(out));\r\n    argv = new String[] { \"-refresh\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    assertTrue(out.toString().startsWith(\"Successfully updated mount table cache\"));\r\n    out.reset();\r\n    argv = new String[] { \"-ls\", src };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    assertTrue(out.toString().contains(src));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "waitState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitState(final RouterServiceState expectedState) throws Exception\n{\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            return expectedState == routerContext.getRouter().getRouterState();\r\n        }\r\n    }, 1000, 30000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testUpdateNonExistingMountTable",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testUpdateNonExistingMountTable() throws Exception\n{\r\n    System.setErr(new PrintStream(err));\r\n    String nsId = \"ns0\";\r\n    String src = \"/test-updateNonExistingMounttable\";\r\n    String dest = \"/updateNonExistingMounttable\";\r\n    String[] argv = new String[] { \"-update\", src, nsId, dest };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(err.toString(), err.toString().contains(\"update: /test-updateNonExistingMounttable doesn't exist.\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testUpdateDestinationForExistingMountTable",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testUpdateDestinationForExistingMountTable() throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    String src = \"/test-updateDestinationForExistingMountTable\";\r\n    String dest = \"/UpdateDestinationForExistingMountTable\";\r\n    String[] argv = new String[] { \"-add\", src, nsId, dest };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(src);\r\n    GetMountTableEntriesResponse getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    MountTable mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    assertEquals(nsId, mountTable.getDestinations().get(0).getNameserviceId());\r\n    assertEquals(dest, mountTable.getDestinations().get(0).getDest());\r\n    String newNsId = \"ns1\";\r\n    String newDest = \"/newDestination\";\r\n    argv = new String[] { \"-update\", src, newNsId, newDest };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    assertEquals(newNsId, mountTable.getDestinations().get(0).getNameserviceId());\r\n    assertEquals(newDest, mountTable.getDestinations().get(0).getDest());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testUpdateDestinationForExistingMountTableNotNormalized",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testUpdateDestinationForExistingMountTableNotNormalized() throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    String src = \"/test-updateDestinationForExistingMountTableNotNormalized\";\r\n    String srcWithSlash = src + \"/\";\r\n    String dest = \"/UpdateDestinationForExistingMountTableNotNormalized\";\r\n    String[] argv = new String[] { \"-add\", src, nsId, dest };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(src);\r\n    GetMountTableEntriesResponse getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    MountTable mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    assertEquals(nsId, mountTable.getDestinations().get(0).getNameserviceId());\r\n    assertEquals(dest, mountTable.getDestinations().get(0).getDest());\r\n    String newNsId = \"ns1\";\r\n    String newDest = \"/newDestination\";\r\n    argv = new String[] { \"-update\", srcWithSlash, newNsId, newDest };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    assertEquals(newNsId, mountTable.getDestinations().get(0).getNameserviceId());\r\n    assertEquals(newDest, mountTable.getDestinations().get(0).getDest());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testUpdateChangeAttributes",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testUpdateChangeAttributes() throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    String src = \"/mount\";\r\n    String dest = \"/dest\";\r\n    String[] argv = new String[] { \"-add\", src, nsId, dest, \"-readonly\", \"-order\", \"HASH_ALL\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(src);\r\n    GetMountTableEntriesResponse getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    MountTable mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    String newNsId = \"ns0\";\r\n    String newDest = \"/newDestination\";\r\n    argv = new String[] { \"-update\", src, newNsId, newDest };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    assertEquals(newNsId, mountTable.getDestinations().get(0).getNameserviceId());\r\n    assertEquals(newDest, mountTable.getDestinations().get(0).getDest());\r\n    assertTrue(mountTable.isReadOnly());\r\n    assertEquals(\"HASH_ALL\", mountTable.getDestOrder().toString());\r\n    argv = new String[] { \"-update\", src, \"-readonly\", \"false\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    assertEquals(newNsId, mountTable.getDestinations().get(0).getNameserviceId());\r\n    assertEquals(newDest, mountTable.getDestinations().get(0).getDest());\r\n    assertFalse(mountTable.isReadOnly());\r\n    assertEquals(\"HASH_ALL\", mountTable.getDestOrder().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testUpdateErrorCase",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testUpdateErrorCase() throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    String src = \"/mount\";\r\n    String dest = \"/dest\";\r\n    String[] argv = new String[] { \"-add\", src, nsId, dest, \"-readonly\", \"-order\", \"HASH_ALL\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    argv = new String[] { \"-update\", \"/noMount\", \"-readonly\", \"false\" };\r\n    System.setErr(new PrintStream(err));\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(err.toString(), err.toString().contains(\"update: /noMount doesn't exist.\"));\r\n    err.reset();\r\n    argv = new String[] { \"-update\", src, \"-readonly\", \"check\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(err.toString(), err.toString().contains(\"update: \" + \"Invalid argument: check. Please specify either true or false.\"));\r\n    err.reset();\r\n    argv = new String[] { \"-update\", src, \"ns1\", \"/tmp\", \"-faulttolerant\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(err.toString(), err.toString().contains(\"update: Unable to parse arguments:\" + \" no value provided for -faulttolerant\"));\r\n    err.reset();\r\n    argv = new String[] { \"-update\", src, \"ns1\", \"/tmp\", \"-order\", \"Invalid\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(err.toString(), err.toString().contains(\"update: Unable to parse arguments: Cannot parse order: Invalid\"));\r\n    err.reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testUpdateReadonlyUserGroupPermissionMountable",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testUpdateReadonlyUserGroupPermissionMountable() throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    String src = \"/test-updateReadonlyUserGroupPermissionMountTable\";\r\n    String dest = \"/UpdateReadonlyUserGroupPermissionMountTable\";\r\n    String[] argv = new String[] { \"-add\", src, nsId, dest };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(src);\r\n    GetMountTableEntriesResponse getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    MountTable mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    assertEquals(nsId, mountTable.getDestinations().get(0).getNameserviceId());\r\n    assertEquals(dest, mountTable.getDestinations().get(0).getDest());\r\n    assertFalse(mountTable.isReadOnly());\r\n    String testOwner = \"test_owner\";\r\n    String testGroup = \"test_group\";\r\n    argv = new String[] { \"-update\", src, nsId, dest, \"-readonly\", \"true\", \"-owner\", testOwner, \"-group\", testGroup, \"-mode\", \"0455\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    assertEquals(nsId, mountTable.getDestinations().get(0).getNameserviceId());\r\n    assertEquals(dest, mountTable.getDestinations().get(0).getDest());\r\n    assertTrue(mountTable.isReadOnly());\r\n    assertEquals(testOwner, mountTable.getOwnerName());\r\n    assertEquals(testGroup, mountTable.getGroupName());\r\n    assertEquals((short) 0455, mountTable.getMode().toShort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testUpdateReadonlyWithQuota",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testUpdateReadonlyWithQuota() throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    String src = \"/test-updateReadonlywithQuota\";\r\n    String dest = \"/UpdateReadonlywithQuota\";\r\n    String[] argv = new String[] { \"-add\", src, nsId, dest };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(src);\r\n    GetMountTableEntriesResponse getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    MountTable mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    RemoteLocation localDest = mountTable.getDestinations().get(0);\r\n    assertEquals(nsId, localDest.getNameserviceId());\r\n    assertEquals(dest, localDest.getDest());\r\n    assertFalse(mountTable.isReadOnly());\r\n    argv = new String[] { \"-update\", src, nsId, dest, \"-readonly\", \"true\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    mountTable = getResponse.getEntries().get(0);\r\n    assertTrue(mountTable.isReadOnly());\r\n    long nsQuota = 50;\r\n    long ssQuota = 100;\r\n    argv = new String[] { \"-setQuota\", src, \"-nsQuota\", String.valueOf(nsQuota), \"-ssQuota\", String.valueOf(ssQuota) };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    mountTable = getResponse.getEntries().get(0);\r\n    RouterQuotaUsage quota = mountTable.getQuota();\r\n    assertEquals(nsQuota, quota.getQuota());\r\n    assertEquals(ssQuota, quota.getSpaceQuota());\r\n    assertTrue(mountTable.isReadOnly());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testUpdateOrderMountTable",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testUpdateOrderMountTable() throws Exception\n{\r\n    testUpdateOrderMountTable(DestinationOrder.HASH);\r\n    testUpdateOrderMountTable(DestinationOrder.LOCAL);\r\n    testUpdateOrderMountTable(DestinationOrder.RANDOM);\r\n    testUpdateOrderMountTable(DestinationOrder.HASH_ALL);\r\n    testUpdateOrderMountTable(DestinationOrder.SPACE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testOrderErrorMsg",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testOrderErrorMsg() throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    DestinationOrder order = DestinationOrder.HASH;\r\n    String src = \"/testod\" + order.toString();\r\n    String dest = \"/testUpd\";\r\n    String[] argv = new String[] { \"-add\", src, nsId, dest };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(src);\r\n    GetMountTableEntriesResponse getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    MountTable mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    assertEquals(nsId, mountTable.getDestinations().get(0).getNameserviceId());\r\n    assertEquals(dest, mountTable.getDestinations().get(0).getDest());\r\n    assertEquals(DestinationOrder.HASH, mountTable.getDestOrder());\r\n    argv = new String[] { \"-update\", src, nsId, dest, \"-order\", order.toString() };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    argv = new String[] { \"-update\", src + \"a\", nsId, dest + \"a\", \"-orde\", order.toString() };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    argv = new String[] { \"-update\", src, nsId, dest, order.toString() };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testUpdateOrderMountTable",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testUpdateOrderMountTable(DestinationOrder order) throws Exception\n{\r\n    String nsId = \"ns0\";\r\n    String src = \"/test-updateOrderMountTable-\" + order.toString();\r\n    String dest = \"/UpdateOrderMountTable\";\r\n    String[] argv = new String[] { \"-add\", src, nsId, dest };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(src);\r\n    GetMountTableEntriesResponse getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    MountTable mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    assertEquals(nsId, mountTable.getDestinations().get(0).getNameserviceId());\r\n    assertEquals(dest, mountTable.getDestinations().get(0).getDest());\r\n    assertEquals(DestinationOrder.HASH, mountTable.getDestOrder());\r\n    argv = new String[] { \"-update\", src, nsId, dest, \"-order\", order.toString() };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    mountTable = getResponse.getEntries().get(0);\r\n    assertEquals(src, mountTable.getSourcePath());\r\n    assertEquals(nsId, mountTable.getDestinations().get(0).getNameserviceId());\r\n    assertEquals(dest, mountTable.getDestinations().get(0).getDest());\r\n    assertEquals(order, mountTable.getDestOrder());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetDestination",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testGetDestination() throws Exception\n{\r\n    System.setOut(new PrintStream(out));\r\n    String[] argv = new String[] { \"-getDestination\", \"/file.txt\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    assertEquals(\"Destination: ns0\" + System.lineSeparator(), out.toString());\r\n    argv = new String[] { \"-add\", \"/testGetDest\", \"ns0,ns1\", \"/testGetDestination\", \"-order\", DestinationOrder.HASH_ALL.toString() };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    MountTableResolver resolver = (MountTableResolver) router.getSubclusterResolver();\r\n    resolver.loadCache(true);\r\n    Map<String, AtomicInteger> counter = new TreeMap<>();\r\n    final Pattern p = Pattern.compile(\"Destination: (.*)\");\r\n    for (int i = 0; i < 10; i++) {\r\n        out.reset();\r\n        String filename = \"file\" + i + \".txt\";\r\n        argv = new String[] { \"-getDestination\", \"/testGetDest/\" + filename };\r\n        assertEquals(0, ToolRunner.run(admin, argv));\r\n        String outLine = out.toString();\r\n        Matcher m = p.matcher(outLine);\r\n        assertTrue(m.find());\r\n        String nsId = m.group(1);\r\n        if (counter.containsKey(nsId)) {\r\n            counter.get(nsId).getAndIncrement();\r\n        } else {\r\n            counter.put(nsId, new AtomicInteger(1));\r\n        }\r\n    }\r\n    assertEquals(\"Wrong counter size: \" + counter, 2, counter.size());\r\n    assertTrue(counter + \" should contain ns0\", counter.containsKey(\"ns0\"));\r\n    assertTrue(counter + \" should contain ns1\", counter.containsKey(\"ns1\"));\r\n    argv = new String[] { \"-getDestination\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    argv = new String[] { \"-getDestination /file1.txt /file2.txt\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testErrorFaultTolerant",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testErrorFaultTolerant() throws Exception\n{\r\n    System.setErr(new PrintStream(err));\r\n    String[] argv = new String[] { \"-add\", \"/mntft\", \"ns01\", \"/tmp\", \"-faulttolerant\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(err.toString(), err.toString().contains(\"Invalid entry, fault tolerance requires multiple destinations\"));\r\n    err.reset();\r\n    System.setErr(new PrintStream(err));\r\n    argv = new String[] { \"-add\", \"/mntft\", \"ns0,ns1\", \"/tmp\", \"-order\", \"HASH\", \"-faulttolerant\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(err.toString(), err.toString().contains(\"Invalid entry, fault tolerance only supported for ALL order\"));\r\n    err.reset();\r\n    argv = new String[] { \"-add\", \"/mntft\", \"ns0,ns1\", \"/tmp\", \"-order\", \"HASH_ALL\", \"-faulttolerant\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRefreshCallQueue",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRefreshCallQueue() throws Exception\n{\r\n    System.setOut(new PrintStream(out));\r\n    System.setErr(new PrintStream(err));\r\n    String[] argv = new String[] { \"-refreshCallQueue\" };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    assertTrue(out.toString().contains(\"Refresh call queue successfully\"));\r\n    argv = new String[] {};\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(out.toString().contains(\"-refreshCallQueue\"));\r\n    argv = new String[] { \"-refreshCallQueue\", \"redundant\" };\r\n    assertEquals(-1, ToolRunner.run(admin, argv));\r\n    assertTrue(err.toString().contains(\"No arguments allowed\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addMountTable",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addMountTable(String src, String nsId, String dst) throws Exception\n{\r\n    String[] argv = new String[] { \"-add\", src, nsId, dst };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTable",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<MountTable> getMountTable(String src) throws IOException\n{\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(src);\r\n    GetMountTableEntriesResponse getResponse = client.getMountTableManager().getMountTableEntries(getRequest);\r\n    return getResponse.getEntries();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setStorageTypeQuota",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setStorageTypeQuota(String src, long ssQuota, StorageType type) throws Exception\n{\r\n    assertEquals(0, ToolRunner.run(admin, new String[] { \"-setStorageTypeQuota\", src, \"-storageType\", type.name(), String.valueOf(ssQuota) }));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "rmMountTable",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void rmMountTable(String src) throws Exception\n{\r\n    String[] argv = new String[] { \"-rm\", src };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "globalSetUp",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void globalSetUp() throws Exception\n{\r\n    cluster = new MiniRouterDFSCluster(false, NUM_SUBCLUSTERS);\r\n    cluster.setNumDatanodesPerNameservice(NUM_DNS);\r\n    cluster.startCluster();\r\n    Configuration routerConf = new RouterConfigBuilder().metrics().rpc().quota().build();\r\n    cluster.addRouterOverrides(routerConf);\r\n    cluster.startRouters();\r\n    cluster.registerNamenodes();\r\n    cluster.waitNamenodeRegistration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testSetup",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSetup() throws Exception\n{\r\n    cluster.installMockLocations();\r\n    cluster.deleteAllFiles();\r\n    cluster.createTestDirectoriesNamenode();\r\n    Thread.sleep(100);\r\n    routerContext = cluster.getRouters().get(0);\r\n    this.routerFS = routerContext.getFileSystem();\r\n    router = routerContext.getRouter();\r\n    MockResolver resolver = (MockResolver) router.getSubclusterResolver();\r\n    resolver.addLocation(\"/\", cluster.getNameservices().get(1), \"/\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    cluster.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testGetListing",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGetListing() throws IOException\n{\r\n    routerFS.listStatus(new Path(\"/\"));\r\n    assertCounter(\"GetListingOps\", 2L, getMetrics(ROUTER_METRICS));\r\n    assertCounter(\"ConcurrentGetListingOps\", 1L, getMetrics(ROUTER_METRICS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testCreate",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCreate() throws IOException\n{\r\n    Path testFile = new Path(\"/testCreate\");\r\n    routerFS.create(testFile);\r\n    assertCounter(\"CreateOps\", 1L, getMetrics(ROUTER_METRICS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testGetServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetServerDefaults() throws IOException\n{\r\n    router.getRpcServer().getServerDefaults();\r\n    assertCounter(\"GetServerDefaultsOps\", 1L, getMetrics(ROUTER_METRICS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testSetQuota",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSetQuota() throws Exception\n{\r\n    router.getRpcServer().setQuota(\"/\", 1L, 1L, null);\r\n    assertCounter(\"SetQuotaOps\", 2L, getMetrics(ROUTER_METRICS));\r\n    assertCounter(\"ConcurrentSetQuotaOps\", 1L, getMetrics(ROUTER_METRICS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testGetQuota",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGetQuota() throws Exception\n{\r\n    router.getRpcServer().getQuotaUsage(\"/\");\r\n    assertCounter(\"GetQuotaUsageOps\", 2L, getMetrics(ROUTER_METRICS));\r\n    assertCounter(\"ConcurrentGetQuotaUsageOps\", 1L, getMetrics(ROUTER_METRICS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testRenewLease",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testRenewLease() throws Exception\n{\r\n    router.getRpcServer().renewLease(\"test\");\r\n    assertCounter(\"RenewLeaseOps\", 2L, getMetrics(ROUTER_METRICS));\r\n    assertCounter(\"ConcurrentRenewLeaseOps\", 1L, getMetrics(ROUTER_METRICS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testGetDatanodeReport",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGetDatanodeReport() throws Exception\n{\r\n    router.getRpcServer().getDatanodeReport(HdfsConstants.DatanodeReportType.LIVE);\r\n    assertCounter(\"GetDatanodeReportOps\", 2L, getMetrics(ROUTER_METRICS));\r\n    assertCounter(\"ConcurrentGetDatanodeReportOps\", 1L, getMetrics(ROUTER_METRICS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getTestDriverClass",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends StateStoreDriver> getTestDriverClass()\n{\r\n    return FEDERATION_STORE_DRIVER_CLASS_FOR_TEST;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getStateStoreConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration getStateStoreConfiguration()\n{\r\n    Class<? extends StateStoreDriver> clazz = getTestDriverClass();\r\n    return getStateStoreConfiguration(clazz);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getStateStoreConfiguration",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Configuration getStateStoreConfiguration(Class<? extends StateStoreDriver> clazz)\n{\r\n    Configuration conf = new HdfsConfiguration(false);\r\n    conf.setBoolean(DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY, true);\r\n    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, \"hdfs://test\");\r\n    conf.setClass(FEDERATION_STORE_DRIVER_CLASS, clazz, StateStoreDriver.class);\r\n    if (StateStoreFileBaseImpl.class.isAssignableFrom(clazz)) {\r\n        setFileConfiguration(conf);\r\n    }\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "newStateStore",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "StateStoreService newStateStore(Configuration configuration) throws IOException, InterruptedException\n{\r\n    StateStoreService stateStore = new StateStoreService();\r\n    assertNotNull(stateStore);\r\n    String identifier = UUID.randomUUID().toString();\r\n    stateStore.setIdentifier(identifier);\r\n    stateStore.init(configuration);\r\n    stateStore.start();\r\n    waitStateStore(stateStore, TimeUnit.SECONDS.toMillis(10));\r\n    return stateStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "waitStateStore",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void waitStateStore(StateStoreService stateStore, long timeoutMs) throws IOException, InterruptedException\n{\r\n    long startingTime = Time.monotonicNow();\r\n    while (!stateStore.isDriverReady()) {\r\n        Thread.sleep(100);\r\n        if (Time.monotonicNow() - startingTime > timeoutMs) {\r\n            throw new IOException(\"Timeout waiting for State Store to connect\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "deleteStateStore",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void deleteStateStore() throws IOException\n{\r\n    Class<? extends StateStoreDriver> driverClass = getTestDriverClass();\r\n    deleteStateStore(driverClass);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "deleteStateStore",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void deleteStateStore(Class<? extends StateStoreDriver> driverClass) throws IOException\n{\r\n    if (StateStoreFileBaseImpl.class.isAssignableFrom(driverClass)) {\r\n        String workingDirectory = System.getProperty(\"user.dir\");\r\n        File dir = new File(workingDirectory + \"/statestore\");\r\n        if (dir.exists()) {\r\n            FileUtils.cleanDirectory(dir);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "setFileConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setFileConfiguration(Configuration conf)\n{\r\n    String stateStorePath = GenericTestUtils.getRandomizedTempPath();\r\n    conf.set(FEDERATION_STORE_FILE_DIRECTORY, stateStorePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "clearAllRecords",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean clearAllRecords(StateStoreService store) throws IOException\n{\r\n    Collection<Class<? extends BaseRecord>> allRecords = store.getSupportedRecords();\r\n    for (Class<? extends BaseRecord> recordType : allRecords) {\r\n        if (!clearRecords(store, recordType)) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "clearRecords",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean clearRecords(StateStoreService store, Class<T> recordClass) throws IOException\n{\r\n    List<T> emptyList = new ArrayList<>();\r\n    if (!synchronizeRecords(store, emptyList, recordClass)) {\r\n        return false;\r\n    }\r\n    store.refreshCaches(true);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "synchronizeRecords",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean synchronizeRecords(StateStoreService stateStore, List<T> records, Class<T> clazz) throws IOException\n{\r\n    StateStoreDriver driver = stateStore.getDriver();\r\n    driver.verifyDriverReady();\r\n    if (driver.removeAll(clazz)) {\r\n        if (driver.putAll(records, true, false)) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "createMockMountTable",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<MountTable> createMockMountTable(List<String> nameservices) throws IOException\n{\r\n    List<MountTable> entries = new ArrayList<>();\r\n    for (String ns : nameservices) {\r\n        Map<String, String> destMap = new HashMap<>();\r\n        destMap.put(ns, \"/target-\" + ns);\r\n        MountTable entry = MountTable.newInstance(\"/\" + ns, destMap);\r\n        entries.add(entry);\r\n    }\r\n    return entries;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "createMockRegistrationForNamenode",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "MembershipState createMockRegistrationForNamenode(String nameserviceId, String namenodeId, FederationNamenodeServiceState state) throws IOException\n{\r\n    MembershipState entry = MembershipState.newInstance(\"routerId\", nameserviceId, namenodeId, \"clusterId\", \"test\", \"0.0.0.0:0\", \"0.0.0.0:0\", \"0.0.0.0:0\", \"http\", \"0.0.0.0:0\", state, false);\r\n    MembershipStats stats = MembershipStats.newInstance();\r\n    stats.setNumOfActiveDatanodes(100);\r\n    stats.setNumOfDeadDatanodes(10);\r\n    stats.setNumOfDecommissioningDatanodes(20);\r\n    stats.setNumOfDecomActiveDatanodes(15);\r\n    stats.setNumOfDecomDeadDatanodes(5);\r\n    stats.setNumOfBlocks(10);\r\n    stats.setPendingSPSPaths(10);\r\n    entry.setStats(stats);\r\n    return entry;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "createTestRegistration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createTestRegistration(StateStoreService stateStore) throws IOException\n{\r\n    List<MembershipState> entries = new ArrayList<MembershipState>();\r\n    for (NamenodeContext nn : this.getNamenodes()) {\r\n        MembershipState entry = createMockRegistrationForNamenode(nn.getNameserviceId(), nn.getNamenodeId(), FederationNamenodeServiceState.STANDBY);\r\n        entries.add(entry);\r\n    }\r\n    synchronizeRecords(stateStore, entries, MembershipState.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "createTestMountTable",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createTestMountTable(StateStoreService stateStore) throws IOException\n{\r\n    List<MountTable> mounts = generateMockMountTable();\r\n    synchronizeRecords(stateStore, mounts, MountTable.class);\r\n    stateStore.refreshCaches();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "generateMockMountTable",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "List<MountTable> generateMockMountTable() throws IOException\n{\r\n    List<MountTable> entries = new ArrayList<>();\r\n    for (String ns : this.getNameservices()) {\r\n        Map<String, String> destMap = new HashMap<>();\r\n        destMap.put(ns, getNamenodePathForNS(ns));\r\n        String fedPath = getFederatedPathForNS(ns);\r\n        MountTable entry = MountTable.newInstance(fedPath, destMap);\r\n        entries.add(entry);\r\n    }\r\n    Map<String, String> destMap = new HashMap<>();\r\n    String ns0 = this.getNameservices().get(0);\r\n    destMap.put(ns0, \"/\");\r\n    MountTable entry = MountTable.newInstance(\"/\", destMap);\r\n    entries.add(entry);\r\n    return entries;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getRouterClientConf",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Configuration getRouterClientConf()\n{\r\n    List<RouterContext> routers = getRouters();\r\n    Configuration clientConf = DFSTestUtil.newHAConfiguration(\"fed\");\r\n    int i = 0;\r\n    List<String> names = new ArrayList<>(routers.size());\r\n    for (RouterContext routerContext : routers) {\r\n        String name = \"r\" + i++;\r\n        clientConf.set(DFSConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY + \".fed.\" + name, \"localhost:\" + routerContext.getRpcPort());\r\n        names.add(name);\r\n    }\r\n    clientConf.set(DFSUtil.addKeySuffixes(HdfsClientConfigKeys.DFS_HA_NAMENODES_KEY_PREFIX, \"fed\"), StringUtils.join(\",\", names));\r\n    return clientConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    Configuration routerConf = new RouterConfigBuilder().stateStore().admin().quota().rpc().build();\r\n    routerConf.set(DFS_ROUTER_HTTP_ENABLE, \"true\");\r\n    Configuration hdfsConf = new Configuration(false);\r\n    clusterWithDatanodes = new StateStoreDFSCluster(false, 2, MultipleDestinationMountTableResolver.class);\r\n    clusterWithDatanodes.addNamenodeOverrides(hdfsConf);\r\n    clusterWithDatanodes.addRouterOverrides(routerConf);\r\n    clusterWithDatanodes.setNumDatanodesPerNameservice(9);\r\n    clusterWithDatanodes.setIndependentDNs();\r\n    clusterWithDatanodes.setRacks(new String[] { \"/rack1\", \"/rack1\", \"/rack1\", \"/rack2\", \"/rack2\", \"/rack2\", \"/rack3\", \"/rack3\", \"/rack3\", \"/rack4\", \"/rack4\", \"/rack4\", \"/rack5\", \"/rack5\", \"/rack5\", \"/rack6\", \"/rack6\", \"/rack6\" });\r\n    clusterWithDatanodes.startCluster();\r\n    clusterWithDatanodes.startRouters();\r\n    clusterWithDatanodes.waitClusterUp();\r\n    clusterWithDatanodes.waitActiveNamespaces();\r\n    clusterNoDatanodes = new StateStoreDFSCluster(false, 2, MultipleDestinationMountTableResolver.class);\r\n    clusterNoDatanodes.addNamenodeOverrides(hdfsConf);\r\n    clusterNoDatanodes.addRouterOverrides(routerConf);\r\n    clusterNoDatanodes.setNumDatanodesPerNameservice(0);\r\n    clusterNoDatanodes.setIndependentDNs();\r\n    clusterNoDatanodes.startCluster();\r\n    clusterNoDatanodes.startRouters();\r\n    clusterNoDatanodes.waitClusterUp();\r\n    clusterNoDatanodes.waitActiveNamespaces();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testPrintTopologyTextFormat",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testPrintTopologyTextFormat() throws Exception\n{\r\n    String httpAddress = clusterWithDatanodes.getRandomRouter().getRouter().getHttpServerAddress().toString();\r\n    URL url = new URL(\"http:/\" + httpAddress + \"/topology\");\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setReadTimeout(20000);\r\n    conn.setConnectTimeout(20000);\r\n    conn.connect();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    IOUtils.copyBytes(conn.getInputStream(), out, 4096, true);\r\n    StringBuilder sb = new StringBuilder(\"-- Network Topology -- \\n\");\r\n    sb.append(out);\r\n    sb.append(\"\\n-- Network Topology -- \");\r\n    String topology = sb.toString();\r\n    assertTrue(topology.contains(\"/ns0/rack1\"));\r\n    assertTrue(topology.contains(\"/ns0/rack2\"));\r\n    assertTrue(topology.contains(\"/ns0/rack3\"));\r\n    assertTrue(topology.contains(\"/ns1/rack4\"));\r\n    assertTrue(topology.contains(\"/ns1/rack5\"));\r\n    assertTrue(topology.contains(\"/ns1/rack6\"));\r\n    assertEquals(18, topology.split(\"127.0.0.1\").length - 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testPrintTopologyJsonFormat",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testPrintTopologyJsonFormat() throws Exception\n{\r\n    String httpAddress = clusterWithDatanodes.getRandomRouter().getRouter().getHttpServerAddress().toString();\r\n    URL url = new URL(\"http:/\" + httpAddress + \"/topology\");\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setReadTimeout(20000);\r\n    conn.setConnectTimeout(20000);\r\n    conn.setRequestProperty(\"Accept\", \"application/json\");\r\n    conn.connect();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    IOUtils.copyBytes(conn.getInputStream(), out, 4096, true);\r\n    String topology = out.toString();\r\n    JsonNode racks = new ObjectMapper().readTree(topology);\r\n    assertEquals(6, racks.size());\r\n    assertTrue(topology.contains(\"/ns0/rack1\"));\r\n    assertTrue(topology.contains(\"/ns0/rack2\"));\r\n    assertTrue(topology.contains(\"/ns0/rack3\"));\r\n    assertTrue(topology.contains(\"/ns1/rack4\"));\r\n    assertTrue(topology.contains(\"/ns1/rack5\"));\r\n    assertTrue(topology.contains(\"/ns1/rack6\"));\r\n    Iterator<JsonNode> elements = racks.elements();\r\n    int dataNodesCount = 0;\r\n    while (elements.hasNext()) {\r\n        JsonNode rack = elements.next();\r\n        Iterator<Map.Entry<String, JsonNode>> fields = rack.fields();\r\n        while (fields.hasNext()) {\r\n            dataNodesCount += fields.next().getValue().size();\r\n        }\r\n    }\r\n    assertEquals(18, dataNodesCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testPrintTopologyNoDatanodesTextFormat",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testPrintTopologyNoDatanodesTextFormat() throws Exception\n{\r\n    String httpAddress = clusterNoDatanodes.getRandomRouter().getRouter().getHttpServerAddress().toString();\r\n    URL url = new URL(\"http:/\" + httpAddress + \"/topology\");\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setReadTimeout(20000);\r\n    conn.setConnectTimeout(20000);\r\n    conn.connect();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    IOUtils.copyBytes(conn.getInputStream(), out, 4096, true);\r\n    StringBuilder sb = new StringBuilder(\"-- Network Topology -- \\n\");\r\n    sb.append(out);\r\n    sb.append(\"\\n-- Network Topology -- \");\r\n    String topology = sb.toString();\r\n    assertTrue(topology.contains(\"No DataNodes\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testPrintTopologyNoDatanodesJsonFormat",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testPrintTopologyNoDatanodesJsonFormat() throws Exception\n{\r\n    String httpAddress = clusterNoDatanodes.getRandomRouter().getRouter().getHttpServerAddress().toString();\r\n    URL url = new URL(\"http:/\" + httpAddress + \"/topology\");\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setReadTimeout(20000);\r\n    conn.setConnectTimeout(20000);\r\n    conn.setRequestProperty(\"Accept\", \"application/json\");\r\n    conn.connect();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    IOUtils.copyBytes(conn.getInputStream(), out, 4096, true);\r\n    StringBuilder sb = new StringBuilder(\"-- Network Topology -- \\n\");\r\n    sb.append(out);\r\n    sb.append(\"\\n-- Network Topology -- \");\r\n    String topology = sb.toString();\r\n    assertTrue(topology.contains(\"No DataNodes\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    cluster = new StateStoreDFSCluster(false, NUM_NAMESPACES, MultipleDestinationMountTableResolver.class);\r\n    cluster.startCluster();\r\n    Configuration routerConf = new RouterConfigBuilder().stateStore().admin().rpc().build();\r\n    cluster.addRouterOverrides(routerConf);\r\n    cluster.startRouters();\r\n    routerContext = cluster.getRandomRouter();\r\n    cluster.registerNamenodes();\r\n    cluster.waitNamenodeRegistration();\r\n    createMountTableEntry(TEST_DIR_HASH_ALL, DestinationOrder.HASH_ALL);\r\n    createMountTableEntry(TEST_DIR_RANDOM, DestinationOrder.RANDOM);\r\n    createMountTableEntry(TEST_DIR_SPACE, DestinationOrder.SPACE);\r\n    routerFs = routerContext.getFileSystem();\r\n    for (String nsId : cluster.getNameservices()) {\r\n        List<NamenodeContext> nns = cluster.getNamenodes(nsId);\r\n        for (NamenodeContext nn : nns) {\r\n            FileSystem nnFs = nn.getFileSystem();\r\n            nsFss.add(nnFs);\r\n        }\r\n    }\r\n    assertEquals(NUM_NAMESPACES, nsFss.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cleanup()\n{\r\n    cluster.shutdown();\r\n    cluster = null;\r\n    routerContext = null;\r\n    routerFs = null;\r\n    nsFss.clear();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testHashAll",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testHashAll() throws Exception\n{\r\n    testAll(TEST_DIR_HASH_ALL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRandomAll",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRandomAll() throws Exception\n{\r\n    testAll(TEST_DIR_RANDOM);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSpaceAll",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSpaceAll() throws Exception\n{\r\n    testAll(TEST_DIR_SPACE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testAll",
  "errType" : null,
  "containingMethodsNum" : 39,
  "sourceCodeText" : "void testAll(final String path) throws Exception\n{\r\n    routerFs.mkdirs(new Path(path + \"/dir0\"));\r\n    routerFs.mkdirs(new Path(path + \"/dir1\"));\r\n    routerFs.mkdirs(new Path(path + \"/dir2/dir20\"));\r\n    routerFs.mkdirs(new Path(path + \"/dir2/dir21\"));\r\n    routerFs.mkdirs(new Path(path + \"/dir2/dir22\"));\r\n    routerFs.mkdirs(new Path(path + \"/dir2/dir22/dir220\"));\r\n    routerFs.mkdirs(new Path(path + \"/dir2/dir22/dir221\"));\r\n    routerFs.mkdirs(new Path(path + \"/dir2/dir22/dir222\"));\r\n    assertDirsEverywhere(path, 9);\r\n    createTestFile(routerFs, path + \"/dir0/file1.txt\");\r\n    createTestFile(routerFs, path + \"/dir0/file2.txt\");\r\n    createTestFile(routerFs, path + \"/dir1/file2.txt\");\r\n    createTestFile(routerFs, path + \"/dir1/file3.txt\");\r\n    createTestFile(routerFs, path + \"/dir2/dir20/file4.txt\");\r\n    createTestFile(routerFs, path + \"/dir2/dir20/file5.txt\");\r\n    createTestFile(routerFs, path + \"/dir2/dir21/file6.txt\");\r\n    createTestFile(routerFs, path + \"/dir2/dir21/file7.txt\");\r\n    createTestFile(routerFs, path + \"/dir2/dir22/file8.txt\");\r\n    createTestFile(routerFs, path + \"/dir2/dir22/file9.txt\");\r\n    createTestFile(routerFs, path + \"/dir2/dir22/dir220/file10.txt\");\r\n    createTestFile(routerFs, path + \"/dir2/dir22/dir220/file11.txt\");\r\n    createTestFile(routerFs, path + \"/dir2/dir22/dir220/file12.txt\");\r\n    createTestFile(routerFs, path + \"/dir2/dir22/dir220/file13.txt\");\r\n    assertDirsEverywhere(path, 9);\r\n    assertFilesDistributed(path, 14);\r\n    String testFile = path + \"/dir2/dir22/dir220/file-append.txt\";\r\n    createTestFile(routerFs, testFile);\r\n    Path testFilePath = new Path(testFile);\r\n    assertTrue(\"Created file is too small\", routerFs.getFileStatus(testFilePath).getLen() > 50);\r\n    appendTestFile(routerFs, testFile);\r\n    assertTrue(\"Append file is too small\", routerFs.getFileStatus(testFilePath).getLen() > 110);\r\n    assertDirsEverywhere(path, 9);\r\n    assertFilesDistributed(path, 15);\r\n    routerFs.delete(new Path(path + \"/dir2/dir22/dir220\"), true);\r\n    assertDirsEverywhere(path, 8);\r\n    assertFilesDistributed(path, 10);\r\n    routerFs.delete(new Path(path + \"/dir0\"), true);\r\n    routerFs.delete(new Path(path + \"/dir1\"), true);\r\n    routerFs.delete(new Path(path + \"/dir2\"), true);\r\n    assertDirsEverywhere(path, 0);\r\n    assertFilesDistributed(path, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "assertDirsEverywhere",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void assertDirsEverywhere(String path, int expectedNumDirs) throws IOException\n{\r\n    List<FileStatus> files = listRecursive(routerFs, path);\r\n    int numDirs = 0;\r\n    for (FileStatus file : files) {\r\n        if (file.isDirectory()) {\r\n            numDirs++;\r\n            Path dirPath = file.getPath();\r\n            Path checkPath = getRelativePath(dirPath);\r\n            for (FileSystem nsFs : nsFss) {\r\n                FileStatus fileStatus1 = nsFs.getFileStatus(checkPath);\r\n                assertTrue(file + \" should be a directory\", fileStatus1.isDirectory());\r\n            }\r\n        }\r\n    }\r\n    assertEquals(expectedNumDirs, numDirs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "assertFilesDistributed",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void assertFilesDistributed(String path, int expectedNumFiles) throws IOException\n{\r\n    List<FileStatus> routerFiles = listRecursive(routerFs, path);\r\n    List<List<FileStatus>> nssFiles = new LinkedList<>();\r\n    for (FileSystem nsFs : nsFss) {\r\n        List<FileStatus> nsFiles = listRecursive(nsFs, path);\r\n        nssFiles.add(nsFiles);\r\n    }\r\n    int numRouterFiles = getNumTxtFiles(routerFiles);\r\n    assertEquals(numRouterFiles, expectedNumFiles);\r\n    List<Integer> numNsFiles = new LinkedList<>();\r\n    int sumNsFiles = 0;\r\n    for (int i = 0; i < NUM_NAMESPACES; i++) {\r\n        List<FileStatus> nsFiles = nssFiles.get(i);\r\n        int numFiles = getNumTxtFiles(nsFiles);\r\n        numNsFiles.add(numFiles);\r\n        sumNsFiles += numFiles;\r\n    }\r\n    assertEquals(numRouterFiles, sumNsFiles);\r\n    if (expectedNumFiles > 0) {\r\n        for (int numFiles : numNsFiles) {\r\n            assertTrue(\"Files not distributed: \" + numNsFiles, numFiles > 0);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createTestFile",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void createTestFile(final FileSystem fs, final String filename) throws IOException\n{\r\n    final Path path = new Path(filename);\r\n    FSDataOutputStream os = fs.create(path);\r\n    os.writeUTF(\"Test data \" + filename);\r\n    os.close();\r\n    FSDataInputStream is = fs.open(path);\r\n    String read = is.readUTF();\r\n    assertEquals(\"Test data \" + filename, read);\r\n    is.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "appendTestFile",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void appendTestFile(final FileSystem fs, final String filename) throws IOException\n{\r\n    final Path path = new Path(filename);\r\n    FSDataOutputStream os = fs.append(path);\r\n    os.writeUTF(\"Test append data \" + filename);\r\n    os.close();\r\n    FSDataInputStream is = fs.open(path);\r\n    String read = is.readUTF();\r\n    assertEquals(read, \"Test data \" + filename);\r\n    read = is.readUTF();\r\n    assertEquals(read, \"Test append data \" + filename);\r\n    is.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumTxtFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumTxtFiles(final List<FileStatus> files)\n{\r\n    int numFiles = 0;\r\n    for (FileStatus file : files) {\r\n        if (file.getPath().getName().endsWith(\".txt\")) {\r\n            numFiles++;\r\n        }\r\n    }\r\n    return numFiles;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRelativePath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getRelativePath(final Path path)\n{\r\n    URI uri = path.toUri();\r\n    String uriPath = uri.getPath();\r\n    return new Path(uriPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listRecursive",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<FileStatus> listRecursive(final FileSystem fs, final String path) throws IOException\n{\r\n    List<FileStatus> ret = new LinkedList<>();\r\n    List<Path> temp = new LinkedList<>();\r\n    temp.add(new Path(path));\r\n    while (!temp.isEmpty()) {\r\n        Path p = temp.remove(0);\r\n        for (FileStatus fileStatus : fs.listStatus(p)) {\r\n            ret.add(fileStatus);\r\n            if (fileStatus.isDirectory()) {\r\n                temp.add(fileStatus.getPath());\r\n            }\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void createMountTableEntry(final String mountPoint, final DestinationOrder order) throws Exception\n{\r\n    RouterClient admin = routerContext.getAdminClient();\r\n    MountTableManager mountTable = admin.getMountTableManager();\r\n    Map<String, String> destMap = new HashMap<>();\r\n    for (String nsId : cluster.getNameservices()) {\r\n        destMap.put(nsId, mountPoint);\r\n    }\r\n    MountTable newEntry = MountTable.newInstance(mountPoint, destMap);\r\n    newEntry.setDestOrder(order);\r\n    AddMountTableEntryRequest addRequest = AddMountTableEntryRequest.newInstance(newEntry);\r\n    AddMountTableEntryResponse addResponse = mountTable.addMountTableEntry(addRequest);\r\n    boolean created = addResponse.getStatus();\r\n    assertTrue(created);\r\n    Router router = routerContext.getRouter();\r\n    StateStoreService stateStore = router.getStateStore();\r\n    stateStore.refreshCaches(true);\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(mountPoint);\r\n    GetMountTableEntriesResponse getResponse = mountTable.getMountTableEntries(getRequest);\r\n    List<MountTable> entries = getResponse.getEntries();\r\n    assertEquals(1, entries.size());\r\n    assertEquals(mountPoint, entries.get(0).getSourcePath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "testResolverWithNoPreference",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testResolverWithNoPreference() throws IOException\n{\r\n    MultipleDestinationMountTableResolver mountTableResolver = mockAvailableSpaceResolver(1.0f);\r\n    PathLocation loc = mountTableResolver.getDestinationForPath(\"/space\");\r\n    assertEquals(\"subcluster9\", loc.getDestinations().get(0).getNameserviceId());\r\n    loc = mountTableResolver.getDestinationForPath(\"/space/subdir\");\r\n    assertEquals(\"subcluster9\", loc.getDestinations().get(0).getNameserviceId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "testResolverWithDefaultPreference",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testResolverWithDefaultPreference() throws IOException\n{\r\n    MultipleDestinationMountTableResolver mountTableResolver = mockAvailableSpaceResolver(BALANCER_PREFERENCE_DEFAULT);\r\n    int retries = 10;\r\n    int retryTimes = 0;\r\n    for (retryTimes = 0; retryTimes < retries; retryTimes++) {\r\n        PathLocation loc = mountTableResolver.getDestinationForPath(\"/space\");\r\n        if (!\"subcluster9\".equals(loc.getDestinations().get(0).getNameserviceId())) {\r\n            break;\r\n        }\r\n    }\r\n    assertNotEquals(retries, retryTimes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "mockAvailableSpaceResolver",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "MultipleDestinationMountTableResolver mockAvailableSpaceResolver(float balancerPreference) throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setFloat(BALANCER_PREFERENCE_KEY, balancerPreference);\r\n    Router router = mock(Router.class);\r\n    StateStoreService stateStore = mock(StateStoreService.class);\r\n    MembershipStore membership = mock(MembershipStore.class);\r\n    when(router.getStateStore()).thenReturn(stateStore);\r\n    when(stateStore.getRegisteredRecordStore(any(Class.class))).thenReturn(membership);\r\n    GetNamenodeRegistrationsResponse response = GetNamenodeRegistrationsResponse.newInstance();\r\n    List<MembershipState> records = new LinkedList<>();\r\n    for (int i = 0; i < SUBCLUSTER_NUM; i++) {\r\n        records.add(newMembershipState(\"subcluster\" + i, i));\r\n    }\r\n    response.setNamenodeMemberships(records);\r\n    when(membership.getNamenodeRegistrations(any(GetNamenodeRegistrationsRequest.class))).thenReturn(response);\r\n    AvailableSpaceResolver resolver = new AvailableSpaceResolver(conf, router);\r\n    MultipleDestinationMountTableResolver mountTableResolver = new MultipleDestinationMountTableResolver(conf, router);\r\n    mountTableResolver.addResolver(DestinationOrder.SPACE, resolver);\r\n    Map<String, String> destinations = new HashMap<>();\r\n    for (int i = 0; i < SUBCLUSTER_NUM; i++) {\r\n        destinations.put(\"subcluster\" + i, \"/space\");\r\n    }\r\n    MountTable spaceEntry = MountTable.newInstance(\"/space\", destinations);\r\n    spaceEntry.setDestOrder(DestinationOrder.SPACE);\r\n    mountTableResolver.addEntry(spaceEntry);\r\n    return mountTableResolver;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "newMembershipState",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "MembershipState newMembershipState(String nameservice, long availableSpace)\n{\r\n    MembershipState record = MembershipState.newInstance();\r\n    record.setNameserviceId(nameservice);\r\n    MembershipStats stats = new MembershipStatsPBImpl();\r\n    stats.setAvailableSpace(availableSpace);\r\n    record.setStats(stats);\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "testSubclusterSpaceComparator",
  "errType" : [ "IllegalArgumentException", "IllegalArgumentException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSubclusterSpaceComparator()\n{\r\n    verifyRank(0.0f, true, false);\r\n    verifyRank(1.0f, true, true);\r\n    verifyRank(0.5f, false, false);\r\n    verifyRank(BALANCER_PREFERENCE_DEFAULT, false, false);\r\n    try {\r\n        verifyRank(2.0f, false, false);\r\n        fail(\"Subcluster comparison should be failed.\");\r\n    } catch (IllegalArgumentException e) {\r\n        GenericTestUtils.assertExceptionContains(\"The balancer preference value should be in the range 0.0 - 1.0\", e);\r\n    }\r\n    try {\r\n        verifyRank(-1.0f, false, false);\r\n        fail(\"Subcluster comparison should be failed.\");\r\n    } catch (IllegalArgumentException e) {\r\n        GenericTestUtils.assertExceptionContains(\"The balancer preference value should be in the range 0.0 - 1.0\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "verifyRank",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void verifyRank(float balancerPreference, boolean shouldOrdered, boolean isDesc)\n{\r\n    List<SubclusterAvailableSpace> subclusters = new LinkedList<>();\r\n    for (int i = 0; i < SUBCLUSTER_NUM; i++) {\r\n        subclusters.add(new SubclusterAvailableSpace(\"subcluster\" + i, i));\r\n    }\r\n    if (shouldOrdered) {\r\n        Collections.shuffle(subclusters);\r\n    }\r\n    SubclusterSpaceComparator comparator = new SubclusterSpaceComparator(balancerPreference);\r\n    Collections.sort(subclusters, comparator);\r\n    int i = SUBCLUSTER_NUM - 1;\r\n    for (; i >= 0; i--) {\r\n        SubclusterAvailableSpace cluster = subclusters.get(SUBCLUSTER_NUM - 1 - i);\r\n        if (shouldOrdered) {\r\n            if (isDesc) {\r\n                assertEquals(\"subcluster\" + i, cluster.getNameserviceId());\r\n                assertEquals(i, cluster.getAvailableSpace());\r\n            } else {\r\n                assertEquals(\"subcluster\" + (SUBCLUSTER_NUM - 1 - i), cluster.getNameserviceId());\r\n                assertEquals(SUBCLUSTER_NUM - 1 - i, cluster.getAvailableSpace());\r\n            }\r\n        } else {\r\n            if (!cluster.getNameserviceId().equals(\"subcluster\" + i) && cluster.getAvailableSpace() != i) {\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    if (!shouldOrdered) {\r\n        assertNotEquals(0, i);\r\n    }\r\n    subclusters.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    cluster = new StateStoreDFSCluster(false, 2);\r\n    Configuration routerConf = new RouterConfigBuilder().stateStore().metrics().admin().rpc().build();\r\n    routerConf.setTimeDuration(RBFConfigKeys.DN_REPORT_CACHE_EXPIRE, 1, TimeUnit.SECONDS);\r\n    Configuration clientConf = new Configuration(false);\r\n    clientConf.setInt(CommonConfigurationKeys.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, 1);\r\n    clientConf.setInt(CommonConfigurationKeys.IPC_CLIENT_CONNECT_RETRY_INTERVAL_KEY, 100);\r\n    cluster.setIndependentDNs();\r\n    cluster.addRouterOverrides(routerConf);\r\n    cluster.startCluster(clientConf);\r\n    cluster.startRouters();\r\n    cluster.waitClusterUp();\r\n    nnContext1 = cluster.getNamenode(cluster.getNameservices().get(0), null);\r\n    routerContext = cluster.getRandomRouter();\r\n    resolver = (MembershipNamenodeResolver) routerContext.getRouter().getNamenodeResolver();\r\n    routerProtocol = routerContext.getClient().getNamenode();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (cluster != null) {\r\n        cluster.stopRouter(routerContext);\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRetryWhenAllNameServiceDown",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRetryWhenAllNameServiceDown() throws Exception\n{\r\n    MiniDFSCluster dfsCluster = cluster.getCluster();\r\n    dfsCluster.shutdown();\r\n    registerInvalidNameReport();\r\n    String dirPath = \"/testRetryWhenClusterisDown\";\r\n    FsPermission permission = new FsPermission(\"705\");\r\n    try {\r\n        routerProtocol.mkdirs(dirPath, permission, false);\r\n        fail(\"Should have thrown RemoteException error.\");\r\n    } catch (RemoteException e) {\r\n        String ns0 = cluster.getNameservices().get(0);\r\n        assertExceptionContains(\"No namenodes available under nameservice \" + ns0, e);\r\n    }\r\n    FederationRPCMetrics rpcMetrics = routerContext.getRouter().getRpcServer().getRPCMetrics();\r\n    assertEquals(1, rpcMetrics.getProxyOpRetries());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRetryWhenOneNameServiceDown",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRetryWhenOneNameServiceDown() throws Exception\n{\r\n    MiniDFSCluster dfsCluster = cluster.getCluster();\r\n    dfsCluster.shutdownNameNode(0);\r\n    registerInvalidNameReport();\r\n    DFSClient client = nnContext1.getClient();\r\n    routerProtocol.renewLease(client.getClientName());\r\n    FederationRPCMetrics rpcMetrics = routerContext.getRouter().getRpcServer().getRPCMetrics();\r\n    assertEquals(1, rpcMetrics.getProxyOpRetries());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "registerInvalidNameReport",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void registerInvalidNameReport() throws IOException\n{\r\n    String ns0 = cluster.getNameservices().get(0);\r\n    List<? extends FederationNamenodeContext> origin = resolver.getNamenodesForNameserviceId(ns0);\r\n    FederationNamenodeContext nnInfo = origin.get(0);\r\n    NamenodeStatusReport report = new NamenodeStatusReport(ns0, nnInfo.getNamenodeId(), nnInfo.getRpcAddress(), nnInfo.getServiceAddress(), nnInfo.getLifelineAddress(), nnInfo.getWebScheme(), nnInfo.getWebAddress());\r\n    report.setRegistrationValid(false);\r\n    assertTrue(resolver.registerNamenode(report));\r\n    resolver.loadCache(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNamenodeMetricsSlow",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testNamenodeMetricsSlow() throws Exception\n{\r\n    final Router router = routerContext.getRouter();\r\n    final NamenodeBeanMetrics metrics = router.getNamenodeMetrics();\r\n    final String jsonString0 = metrics.getLiveNodes();\r\n    assertEquals(4, getNumDatanodes(jsonString0));\r\n    assertEquals(jsonString0, metrics.getLiveNodes());\r\n    waitUpdateLiveNodes(jsonString0, metrics);\r\n    final String jsonString2 = metrics.getLiveNodes();\r\n    assertNotEquals(jsonString0, jsonString2);\r\n    assertEquals(4, getNumDatanodes(jsonString2));\r\n    MiniDFSCluster dfsCluster = cluster.getCluster();\r\n    NameNode nn0 = dfsCluster.getNameNode(0);\r\n    simulateSlowNamenode(nn0, 3);\r\n    waitUpdateLiveNodes(jsonString2, metrics);\r\n    final String jsonString3 = metrics.getLiveNodes();\r\n    assertEquals(2, getNumDatanodes(jsonString3));\r\n    NameNode nn1 = dfsCluster.getNameNode(1);\r\n    simulateSlowNamenode(nn1, 3);\r\n    waitUpdateLiveNodes(jsonString3, metrics);\r\n    final String jsonString4 = metrics.getLiveNodes();\r\n    assertEquals(0, getNumDatanodes(jsonString4));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumDatanodes",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getNumDatanodes(final String jsonString) throws JSONException\n{\r\n    JSONObject jsonObject = new JSONObject(jsonString);\r\n    if (jsonObject.length() == 0) {\r\n        return 0;\r\n    }\r\n    return jsonObject.names().length();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "waitUpdateLiveNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitUpdateLiveNodes(final String oldValue, final NamenodeBeanMetrics metrics) throws Exception\n{\r\n    waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            return !oldValue.equals(metrics.getLiveNodes());\r\n        }\r\n    }, 500, 5 * 1000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    LOG.info(\"Initialize the Mock Namenodes to monitor\");\r\n    for (String nsId : nsIds) {\r\n        nns.put(nsId, new HashMap<>());\r\n        for (String nnId : asList(\"nn0\", \"nn1\")) {\r\n            nns.get(nsId).put(nnId, new MockNamenode(nsId));\r\n        }\r\n    }\r\n    LOG.info(\"Set nn0 to active for all nameservices\");\r\n    for (Map<String, MockNamenode> nnNS : nns.values()) {\r\n        nnNS.get(\"nn0\").transitionToActive();\r\n        nnNS.get(\"nn1\").transitionToStandby();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void cleanup() throws Exception\n{\r\n    for (Map<String, MockNamenode> nnNS : nns.values()) {\r\n        for (MockNamenode nn : nnNS.values()) {\r\n            nn.stop();\r\n        }\r\n    }\r\n    nns.clear();\r\n    if (router != null) {\r\n        router.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodesConfig",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "Configuration getNamenodesConfig()\n{\r\n    final Configuration conf = new HdfsConfiguration();\r\n    conf.set(DFSConfigKeys.DFS_NAMESERVICES, StringUtils.join(\",\", nns.keySet()));\r\n    for (String nsId : nns.keySet()) {\r\n        Set<String> nnIds = nns.get(nsId).keySet();\r\n        StringBuilder sb = new StringBuilder();\r\n        sb.append(DFSConfigKeys.DFS_HA_NAMENODES_KEY_PREFIX);\r\n        sb.append(\".\").append(nsId);\r\n        conf.set(sb.toString(), StringUtils.join(\",\", nnIds));\r\n        for (String nnId : nnIds) {\r\n            final MockNamenode nn = nns.get(nsId).get(nnId);\r\n            sb = new StringBuilder();\r\n            sb.append(DFSConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY);\r\n            sb.append(\".\").append(nsId);\r\n            sb.append(\".\").append(nnId);\r\n            conf.set(sb.toString(), \"localhost:\" + nn.getRPCPort());\r\n            sb = new StringBuilder();\r\n            sb.append(DFSConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY);\r\n            sb.append(\".\").append(nsId);\r\n            sb.append(\".\").append(nnId);\r\n            conf.set(sb.toString(), \"localhost:\" + nn.getHTTPPort());\r\n        }\r\n    }\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testWebSchemeHttp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testWebSchemeHttp() throws IOException\n{\r\n    testWebScheme(HttpConfig.Policy.HTTP_ONLY, \"http\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testWebSchemeHttps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testWebSchemeHttps() throws IOException\n{\r\n    testWebScheme(HttpConfig.Policy.HTTPS_ONLY, \"https\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testWebScheme",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testWebScheme(HttpConfig.Policy httpPolicy, String expectedScheme) throws IOException\n{\r\n    Configuration nsConf = getNamenodesConfig();\r\n    Configuration stateStoreConfig = getStateStoreConfiguration();\r\n    stateStoreConfig.setClass(RBFConfigKeys.FEDERATION_NAMENODE_RESOLVER_CLIENT_CLASS, MembershipNamenodeResolver.class, ActiveNamenodeResolver.class);\r\n    stateStoreConfig.setClass(RBFConfigKeys.FEDERATION_FILE_RESOLVER_CLIENT_CLASS, MountTableResolver.class, FileSubclusterResolver.class);\r\n    Configuration routerConf = new RouterConfigBuilder(nsConf).enableLocalHeartbeat(true).heartbeat().stateStore().rpc().build();\r\n    routerConf.set(DFSConfigKeys.DFS_HTTP_POLICY_KEY, httpPolicy.name());\r\n    routerConf.set(RBFConfigKeys.DFS_ROUTER_RPC_ADDRESS_KEY, \"0.0.0.0:0\");\r\n    routerConf.set(RBFConfigKeys.DFS_ROUTER_MONITOR_NAMENODE, \"ns1.nn0,ns1.nn1\");\r\n    routerConf.addResource(stateStoreConfig);\r\n    routerConf.set(DFSConfigKeys.DFS_NAMESERVICE_ID, \"ns0\");\r\n    routerConf.set(DFSConfigKeys.DFS_HA_NAMENODE_ID_KEY, \"nn1\");\r\n    router = new Router();\r\n    router.init(routerConf);\r\n    router.start();\r\n    Collection<NamenodeHeartbeatService> heartbeatServices = router.getNamenodeHeartbeatServices();\r\n    for (NamenodeHeartbeatService service : heartbeatServices) {\r\n        service.periodicInvoke();\r\n    }\r\n    MembershipNamenodeResolver resolver = (MembershipNamenodeResolver) router.getNamenodeResolver();\r\n    resolver.loadCache(true);\r\n    final List<FederationNamenodeContext> namespaceInfo = new ArrayList<>();\r\n    for (String nsId : nns.keySet()) {\r\n        List<? extends FederationNamenodeContext> nnReports = resolver.getNamenodesForNameserviceId(nsId);\r\n        namespaceInfo.addAll(nnReports);\r\n    }\r\n    for (FederationNamenodeContext nnInfo : namespaceInfo) {\r\n        assertEquals(\"Unexpected scheme for Policy: \" + httpPolicy.name(), nnInfo.getWebScheme(), expectedScheme);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "setupCluster",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setupCluster() throws Exception\n{\r\n    Configuration conf = FederationStateStoreTestUtils.getStateStoreConfiguration(StateStoreFileSystemImpl.class);\r\n    conf.set(StateStoreFileSystemImpl.FEDERATION_STORE_FS_PATH, \"/hdfs-federation/\");\r\n    MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(conf);\r\n    builder.numDataNodes(1);\r\n    dfsCluster = builder.build();\r\n    dfsCluster.waitClusterUp();\r\n    getStateStore(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "tearDownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDownCluster()\n{\r\n    if (dfsCluster != null) {\r\n        dfsCluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "startup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void startup() throws IOException\n{\r\n    removeAll(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testInsert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testInsert() throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    testInsert(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testUpdate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUpdate() throws IllegalArgumentException, IOException, SecurityException, ReflectiveOperationException\n{\r\n    testPut(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testDelete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDelete() throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    testRemove(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testFetchErrors",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFetchErrors() throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    testFetchErrors(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMetrics() throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    testMetrics(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    cluster = new StateStoreDFSCluster(false, 2);\r\n    Configuration routerConf = new RouterConfigBuilder().stateStore().admin().quota().rpc().build();\r\n    routerConf.set(RBFConfigKeys.DFS_ROUTER_QUOTA_CACHE_UPDATE_INTERVAL, \"2s\");\r\n    Configuration hdfsConf = new Configuration(false);\r\n    hdfsConf.setInt(HdfsClientConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);\r\n    hdfsConf.setInt(HdfsClientConfigKeys.DFS_REPLICATION_KEY, 1);\r\n    cluster.addRouterOverrides(routerConf);\r\n    cluster.addNamenodeOverrides(hdfsConf);\r\n    cluster.startCluster();\r\n    cluster.startRouters();\r\n    cluster.waitClusterUp();\r\n    nnContext1 = cluster.getNamenode(cluster.getNameservices().get(0), null);\r\n    nnContext2 = cluster.getNamenode(cluster.getNameservices().get(1), null);\r\n    routerContext = cluster.getRandomRouter();\r\n    Router router = routerContext.getRouter();\r\n    resolver = (MountTableResolver) router.getSubclusterResolver();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (cluster != null) {\r\n        cluster.stopRouter(routerContext);\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNamespaceQuotaExceed",
  "errType" : [ "NSQuotaExceededException", "IOException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testNamespaceQuotaExceed() throws Exception\n{\r\n    long nsQuota = 3;\r\n    final FileSystem nnFs1 = nnContext1.getFileSystem();\r\n    final FileSystem nnFs2 = nnContext2.getFileSystem();\r\n    nnFs1.mkdirs(new Path(\"/testdir1\"));\r\n    nnFs2.mkdirs(new Path(\"/testdir2\"));\r\n    MountTable mountTable1 = MountTable.newInstance(\"/nsquota\", Collections.singletonMap(\"ns0\", \"/testdir1\"));\r\n    mountTable1.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota).build());\r\n    addMountTable(mountTable1);\r\n    MountTable mountTable2 = MountTable.newInstance(\"/nsquota/subdir\", Collections.singletonMap(\"ns1\", \"/testdir2\"));\r\n    mountTable2.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota).build());\r\n    addMountTable(mountTable2);\r\n    final FileSystem routerFs = routerContext.getFileSystem();\r\n    final List<Path> created = new ArrayList<>();\r\n    GenericTestUtils.waitFor(() -> {\r\n        boolean isNsQuotaViolated = false;\r\n        try {\r\n            Path p = new Path(\"/nsquota/\" + UUID.randomUUID());\r\n            routerFs.mkdirs(p);\r\n            created.add(p);\r\n            p = new Path(\"/nsquota/subdir/\" + UUID.randomUUID());\r\n            routerFs.mkdirs(p);\r\n            created.add(p);\r\n        } catch (NSQuotaExceededException e) {\r\n            isNsQuotaViolated = true;\r\n        } catch (IOException ignored) {\r\n        }\r\n        return isNsQuotaViolated;\r\n    }, 5000, 60000);\r\n    nnFs1.mkdirs(new Path(\"/testdir1/\" + UUID.randomUUID()));\r\n    nnFs2.mkdirs(new Path(\"/testdir2/\" + UUID.randomUUID()));\r\n    assertFalse(created.isEmpty());\r\n    for (Path src : created) {\r\n        final Path dst = new Path(src.toString() + \"-renamed\");\r\n        routerFs.rename(src, dst);\r\n        routerFs.delete(dst, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testStorageSpaceQuotaExceed",
  "errType" : [ "DSQuotaExceededException", "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testStorageSpaceQuotaExceed() throws Exception\n{\r\n    long ssQuota = 3071;\r\n    final FileSystem nnFs1 = nnContext1.getFileSystem();\r\n    final FileSystem nnFs2 = nnContext2.getFileSystem();\r\n    nnFs1.mkdirs(new Path(\"/testdir3\"));\r\n    nnFs2.mkdirs(new Path(\"/testdir4\"));\r\n    MountTable mountTable1 = MountTable.newInstance(\"/ssquota\", Collections.singletonMap(\"ns0\", \"/testdir3\"));\r\n    mountTable1.setQuota(new RouterQuotaUsage.Builder().spaceQuota(ssQuota).build());\r\n    addMountTable(mountTable1);\r\n    MountTable mountTable2 = MountTable.newInstance(\"/ssquota/subdir\", Collections.singletonMap(\"ns1\", \"/testdir4\"));\r\n    mountTable2.setQuota(new RouterQuotaUsage.Builder().spaceQuota(ssQuota).build());\r\n    addMountTable(mountTable2);\r\n    DFSClient routerClient = routerContext.getClient();\r\n    routerClient.create(\"/ssquota/file\", true).close();\r\n    routerClient.create(\"/ssquota/subdir/file\", true).close();\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            boolean isDsQuotaViolated = false;\r\n            try {\r\n                appendData(\"/ssquota/file\", routerClient, BLOCK_SIZE);\r\n                appendData(\"/ssquota/subdir/file\", routerClient, BLOCK_SIZE);\r\n            } catch (DSQuotaExceededException e) {\r\n                isDsQuotaViolated = true;\r\n            } catch (IOException ignored) {\r\n            }\r\n            return isDsQuotaViolated;\r\n        }\r\n    }, 5000, 60000);\r\n    appendData(\"/testdir3/file\", nnContext1.getClient(), BLOCK_SIZE);\r\n    appendData(\"/testdir4/file\", nnContext2.getClient(), BLOCK_SIZE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testStorageTypeQuotaExceed",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testStorageTypeQuotaExceed() throws Exception\n{\r\n    long ssQuota = BLOCK_SIZE * 3;\r\n    DFSClient routerClient = routerContext.getClient();\r\n    prepareStorageTypeQuotaTestMountTable(StorageType.DISK, BLOCK_SIZE, ssQuota * 2, ssQuota, BLOCK_SIZE + 1, BLOCK_SIZE + 1);\r\n    LambdaTestUtils.intercept(DSQuotaExceededException.class, \"The DiskSpace quota is exceeded\", \"Expect quota exceed exception.\", () -> appendData(\"/type0/file\", routerClient, BLOCK_SIZE));\r\n    LambdaTestUtils.intercept(DSQuotaExceededException.class, \"The DiskSpace quota is exceeded\", \"Expect quota exceed exception.\", () -> appendData(\"/type0/type1/file\", routerClient, BLOCK_SIZE));\r\n    LambdaTestUtils.intercept(QuotaByStorageTypeExceededException.class, \"Quota by storage type\", \"Expect quota exceed exception.\", () -> appendData(\"/type0/file\", nnContext1.getClient(), BLOCK_SIZE));\r\n    LambdaTestUtils.intercept(QuotaByStorageTypeExceededException.class, \"Quota by storage type\", \"Expect quota exceed exception.\", () -> appendData(\"/type1/file\", nnContext1.getClient(), BLOCK_SIZE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addMountTable",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean addMountTable(final MountTable entry) throws IOException\n{\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTableManager = client.getMountTableManager();\r\n    AddMountTableEntryRequest addRequest = AddMountTableEntryRequest.newInstance(entry);\r\n    AddMountTableEntryResponse addResponse = mountTableManager.addMountTableEntry(addRequest);\r\n    resolver.loadCache(true);\r\n    return addResponse.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "updateMountTable",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean updateMountTable(final MountTable entry) throws IOException\n{\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTableManager = client.getMountTableManager();\r\n    UpdateMountTableEntryRequest updateRequest = UpdateMountTableEntryRequest.newInstance(entry);\r\n    UpdateMountTableEntryResponse updateResponse = mountTableManager.updateMountTableEntry(updateRequest);\r\n    resolver.loadCache(true);\r\n    return updateResponse.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "appendData",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void appendData(String path, DFSClient client, int dataLen) throws IOException\n{\r\n    EnumSet<CreateFlag> createFlag = EnumSet.of(CreateFlag.APPEND);\r\n    HdfsDataOutputStream stream = client.append(path, 1024, createFlag, null, null);\r\n    byte[] data = new byte[dataLen];\r\n    stream.write(data);\r\n    stream.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSetQuotaToMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testSetQuotaToMountTableEntry() throws Exception\n{\r\n    long nsQuota = 10;\r\n    long ssQuota = 10240;\r\n    long diskQuota = 1024;\r\n    final FileSystem nnFs1 = nnContext1.getFileSystem();\r\n    nnFs1.mkdirs(new Path(\"/testSetQuotaToFederationPath\"));\r\n    nnFs1.mkdirs(new Path(\"/testSetQuotaToFederationPath/dir0\"));\r\n    MountTable mountTable = MountTable.newInstance(\"/setquota\", Collections.singletonMap(\"ns0\", \"/testSetQuotaToFederationPath\"));\r\n    addMountTable(mountTable);\r\n    RouterQuotaUpdateService updateService = routerContext.getRouter().getQuotaCacheUpdateService();\r\n    updateService.periodicInvoke();\r\n    final FileSystem routerFs = routerContext.getFileSystem();\r\n    LambdaTestUtils.intercept(AccessControlException.class, \"is not allowed to change quota of\", \"Expect an AccessControlException.\", () -> routerFs.setQuota(new Path(\"/setquota\"), nsQuota, ssQuota));\r\n    LambdaTestUtils.intercept(AccessControlException.class, \"is not allowed to change quota of\", \"Expect an AccessControlException.\", () -> routerFs.setQuotaByStorageType(new Path(\"/setquota\"), StorageType.DISK, diskQuota));\r\n    QuotaUsage quota = nnFs1.getQuotaUsage(new Path(\"/testSetQuotaToFederationPath\"));\r\n    assertEquals(-1, quota.getQuota());\r\n    assertEquals(-1, quota.getSpaceQuota());\r\n    routerFs.setQuota(new Path(\"/setquota/dir0\"), nsQuota, ssQuota);\r\n    routerFs.setQuotaByStorageType(new Path(\"/setquota/dir0\"), StorageType.DISK, diskQuota);\r\n    quota = nnFs1.getQuotaUsage(new Path(\"/testSetQuotaToFederationPath/dir0\"));\r\n    assertEquals(nsQuota, quota.getQuota());\r\n    assertEquals(ssQuota, quota.getSpaceQuota());\r\n    assertEquals(diskQuota, quota.getTypeQuota(StorageType.DISK));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSetQuota",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testSetQuota() throws Exception\n{\r\n    long nsQuota = 5;\r\n    long ssQuota = 100;\r\n    final FileSystem nnFs1 = nnContext1.getFileSystem();\r\n    final FileSystem nnFs2 = nnContext2.getFileSystem();\r\n    nnFs1.mkdirs(new Path(\"/testdir5\"));\r\n    nnFs2.mkdirs(new Path(\"/testdir6\"));\r\n    MountTable mountTable1 = MountTable.newInstance(\"/setquota\", Collections.singletonMap(\"ns0\", \"/testdir5\"));\r\n    mountTable1.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota).spaceQuota(ssQuota).build());\r\n    addMountTable(mountTable1);\r\n    MountTable mountTable2 = MountTable.newInstance(\"/setquota/subdir\", Collections.singletonMap(\"ns1\", \"/testdir6\"));\r\n    addMountTable(mountTable2);\r\n    RouterQuotaUpdateService updateService = routerContext.getRouter().getQuotaCacheUpdateService();\r\n    updateService.periodicInvoke();\r\n    ClientProtocol client1 = nnContext1.getClient().getNamenode();\r\n    ClientProtocol client2 = nnContext2.getClient().getNamenode();\r\n    final QuotaUsage quota1 = client1.getQuotaUsage(\"/testdir5\");\r\n    final QuotaUsage quota2 = client2.getQuotaUsage(\"/testdir6\");\r\n    assertEquals(nsQuota, quota1.getQuota());\r\n    assertEquals(ssQuota, quota1.getSpaceQuota());\r\n    assertEquals(nsQuota, quota2.getQuota());\r\n    assertEquals(ssQuota, quota2.getSpaceQuota());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testStorageTypeQuota",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testStorageTypeQuota() throws Exception\n{\r\n    long ssQuota = BLOCK_SIZE * 3;\r\n    int fileSize = BLOCK_SIZE;\r\n    prepareStorageTypeQuotaTestMountTable(StorageType.DISK, BLOCK_SIZE, ssQuota * 2, ssQuota, fileSize, fileSize);\r\n    ClientProtocol client = nnContext1.getClient().getNamenode();\r\n    QuotaUsage usage = client.getQuotaUsage(\"/type0\");\r\n    assertEquals(HdfsConstants.QUOTA_RESET, usage.getQuota());\r\n    assertEquals(HdfsConstants.QUOTA_RESET, usage.getSpaceQuota());\r\n    verifyTypeQuotaAndConsume(new long[] { -1, -1, ssQuota * 2, -1, -1, -1 }, null, usage);\r\n    usage = client.getQuotaUsage(\"/type1\");\r\n    assertEquals(HdfsConstants.QUOTA_RESET, usage.getQuota());\r\n    assertEquals(HdfsConstants.QUOTA_RESET, usage.getSpaceQuota());\r\n    verifyTypeQuotaAndConsume(new long[] { -1, -1, ssQuota, -1, -1, -1 }, null, usage);\r\n    FileSystem routerFs = routerContext.getFileSystem();\r\n    QuotaUsage u0 = routerFs.getQuotaUsage(new Path(\"/type0\"));\r\n    QuotaUsage u1 = routerFs.getQuotaUsage(new Path(\"/type0/type1\"));\r\n    assertEquals(HdfsConstants.QUOTA_RESET, u1.getQuota());\r\n    assertEquals(2, u1.getFileAndDirectoryCount());\r\n    assertEquals(HdfsConstants.QUOTA_RESET, u1.getSpaceQuota());\r\n    assertEquals(fileSize * 3, u1.getSpaceConsumed());\r\n    verifyTypeQuotaAndConsume(new long[] { -1, -1, ssQuota, -1, -1, -1 }, new long[] { 0, 0, fileSize * 3, 0, 0, 0 }, u1);\r\n    assertEquals(HdfsConstants.QUOTA_RESET, u0.getQuota());\r\n    assertEquals(4, u0.getFileAndDirectoryCount());\r\n    assertEquals(HdfsConstants.QUOTA_RESET, u0.getSpaceQuota());\r\n    assertEquals(fileSize * 3 * 2, u0.getSpaceConsumed());\r\n    verifyTypeQuotaAndConsume(new long[] { -1, -1, ssQuota * 2, -1, -1, -1 }, new long[] { 0, 0, fileSize * 3 * 2, 0, 0, 0 }, u0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetQuota",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testGetQuota() throws Exception\n{\r\n    long nsQuota = 10;\r\n    long ssQuota = 100;\r\n    final FileSystem nnFs1 = nnContext1.getFileSystem();\r\n    final FileSystem nnFs2 = nnContext2.getFileSystem();\r\n    nnFs1.mkdirs(new Path(\"/testdir7\"));\r\n    nnFs1.mkdirs(new Path(\"/testdir7/subdir\"));\r\n    nnFs2.mkdirs(new Path(\"/testdir8\"));\r\n    nnFs2.mkdirs(new Path(\"/testdir8-ext\"));\r\n    MountTable mountTable1 = MountTable.newInstance(\"/getquota\", Collections.singletonMap(\"ns0\", \"/testdir7\"));\r\n    mountTable1.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota).spaceQuota(ssQuota).build());\r\n    addMountTable(mountTable1);\r\n    MountTable mountTable2 = MountTable.newInstance(\"/getquota/subdir1\", Collections.singletonMap(\"ns0\", \"/testdir7/subdir\"));\r\n    addMountTable(mountTable2);\r\n    MountTable mountTable3 = MountTable.newInstance(\"/getquota/subdir2\", Collections.singletonMap(\"ns1\", \"/testdir8\"));\r\n    addMountTable(mountTable3);\r\n    MountTable mountTable4 = MountTable.newInstance(\"/getquota/subdir3\", Collections.singletonMap(\"ns1\", \"/testdir8-ext\"));\r\n    addMountTable(mountTable4);\r\n    DFSClient routerClient = routerContext.getClient();\r\n    routerClient.create(\"/getquota/file\", true).close();\r\n    routerClient.create(\"/getquota/subdir1/file\", true).close();\r\n    routerClient.create(\"/getquota/subdir2/file\", true).close();\r\n    routerClient.create(\"/getquota/subdir3/file\", true).close();\r\n    ClientProtocol clientProtocol = routerContext.getClient().getNamenode();\r\n    RouterQuotaUpdateService updateService = routerContext.getRouter().getQuotaCacheUpdateService();\r\n    updateService.periodicInvoke();\r\n    final QuotaUsage quota = clientProtocol.getQuotaUsage(\"/getquota\");\r\n    assertEquals(8, quota.getFileAndDirectoryCount());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testStaleQuotaRemoving",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testStaleQuotaRemoving() throws Exception\n{\r\n    long nsQuota = 20;\r\n    long ssQuota = 200;\r\n    String stalePath = \"/stalequota\";\r\n    final FileSystem nnFs1 = nnContext1.getFileSystem();\r\n    nnFs1.mkdirs(new Path(\"/testdir9\"));\r\n    MountTable mountTable = MountTable.newInstance(stalePath, Collections.singletonMap(\"ns0\", \"/testdir9\"));\r\n    mountTable.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota).spaceQuota(ssQuota).build());\r\n    addMountTable(mountTable);\r\n    RouterQuotaUpdateService updateService = routerContext.getRouter().getQuotaCacheUpdateService();\r\n    updateService.periodicInvoke();\r\n    RouterQuotaManager quotaManager = routerContext.getRouter().getQuotaManager();\r\n    RouterQuotaUsage quota = quotaManager.getQuotaUsage(stalePath);\r\n    assertEquals(nsQuota, quota.getQuota());\r\n    assertEquals(ssQuota, quota.getSpaceQuota());\r\n    removeMountTable(stalePath);\r\n    updateService.periodicInvoke();\r\n    quota = quotaManager.getQuotaUsage(stalePath);\r\n    assertNull(quota);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeMountTable",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean removeMountTable(String path) throws IOException\n{\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTableManager = client.getMountTableManager();\r\n    RemoveMountTableEntryRequest removeRequest = RemoveMountTableEntryRequest.newInstance(path);\r\n    RemoveMountTableEntryResponse removeResponse = mountTableManager.removeMountTableEntry(removeRequest);\r\n    resolver.loadCache(true);\r\n    return removeResponse.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testQuotaUpdating",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testQuotaUpdating() throws Exception\n{\r\n    long nsQuota = 30;\r\n    long ssQuota = 1024;\r\n    String path = \"/updatequota\";\r\n    final FileSystem nnFs1 = nnContext1.getFileSystem();\r\n    nnFs1.mkdirs(new Path(\"/testdir10\"));\r\n    MountTable mountTable = MountTable.newInstance(path, Collections.singletonMap(\"ns0\", \"/testdir10\"));\r\n    mountTable.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota).spaceQuota(ssQuota).build());\r\n    addMountTable(mountTable);\r\n    RouterQuotaUpdateService updateService = routerContext.getRouter().getQuotaCacheUpdateService();\r\n    updateService.periodicInvoke();\r\n    MountTable updatedMountTable = getMountTable(path);\r\n    RouterQuotaUsage quota = updatedMountTable.getQuota();\r\n    assertEquals(nsQuota, quota.getQuota());\r\n    assertEquals(ssQuota, quota.getSpaceQuota());\r\n    assertEquals(1, quota.getFileAndDirectoryCount());\r\n    assertEquals(0, quota.getSpaceConsumed());\r\n    final FileSystem routerFs = routerContext.getFileSystem();\r\n    routerFs.mkdirs(new Path(path + \"/\" + UUID.randomUUID()));\r\n    DFSClient routerClient = routerContext.getClient();\r\n    routerClient.create(path + \"/file\", true).close();\r\n    appendData(path + \"/file\", routerClient, BLOCK_SIZE);\r\n    updateService.periodicInvoke();\r\n    updatedMountTable = getMountTable(path);\r\n    quota = updatedMountTable.getQuota();\r\n    assertEquals(nsQuota, quota.getQuota());\r\n    assertEquals(ssQuota, quota.getSpaceQuota());\r\n    assertEquals(3, quota.getFileAndDirectoryCount());\r\n    assertEquals(BLOCK_SIZE, quota.getSpaceConsumed());\r\n    updatedMountTable = getMountTable(path);\r\n    nnFs1.mkdirs(new Path(\"/newPath\"));\r\n    updatedMountTable.setDestinations(Collections.singletonList(new RemoteLocation(\"ns0\", \"/newPath\", path)));\r\n    updateMountTable(updatedMountTable);\r\n    assertEquals(nsQuota, nnFs1.getQuotaUsage(new Path(\"/newPath\")).getQuota());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTable",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "MountTable getMountTable(String path) throws IOException\n{\r\n    resolver.loadCache(true);\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTableManager = client.getMountTableManager();\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(path);\r\n    GetMountTableEntriesResponse response = mountTableManager.getMountTableEntries(getRequest);\r\n    List<MountTable> results = response.getEntries();\r\n    return !results.isEmpty() ? results.get(0) : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testQuotaSynchronization",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testQuotaSynchronization() throws IOException\n{\r\n    long updateNsQuota = 3;\r\n    long updateSsQuota = 4;\r\n    FileSystem nnFs = nnContext1.getFileSystem();\r\n    nnFs.mkdirs(new Path(\"/testsync\"));\r\n    MountTable mountTable = MountTable.newInstance(\"/quotaSync\", Collections.singletonMap(\"ns0\", \"/testsync\"), Time.now(), Time.now());\r\n    mountTable.setQuota(new RouterQuotaUsage.Builder().quota(1).spaceQuota(2).build());\r\n    addMountTable(mountTable);\r\n    QuotaUsage realQuota = nnContext1.getFileSystem().getQuotaUsage(new Path(\"/testsync\"));\r\n    assertNotEquals(updateNsQuota, realQuota.getQuota());\r\n    assertNotEquals(updateSsQuota, realQuota.getSpaceQuota());\r\n    RouterQuotaUpdateService updateService = routerContext.getRouter().getQuotaCacheUpdateService();\r\n    updateService.periodicInvoke();\r\n    mountTable.setQuota(new RouterQuotaUsage.Builder().quota(updateNsQuota).spaceQuota(updateSsQuota).build());\r\n    updateMountTable(mountTable);\r\n    realQuota = nnContext1.getFileSystem().getQuotaUsage(new Path(\"/testsync\"));\r\n    assertEquals(updateNsQuota, realQuota.getQuota());\r\n    assertEquals(updateSsQuota, realQuota.getSpaceQuota());\r\n    mountTable.setQuota(new RouterQuotaUsage.Builder().quota(HdfsConstants.QUOTA_RESET).spaceQuota(HdfsConstants.QUOTA_RESET).build());\r\n    updateMountTable(mountTable);\r\n    realQuota = nnContext1.getFileSystem().getQuotaUsage(new Path(\"/testsync\"));\r\n    assertEquals(HdfsConstants.QUOTA_RESET, realQuota.getQuota());\r\n    assertEquals(HdfsConstants.QUOTA_RESET, realQuota.getSpaceQuota());\r\n    mountTable = MountTable.newInstance(\"/testupdate\", Collections.singletonMap(\"ns0\", \"/testupdate\"), Time.now(), Time.now());\r\n    addMountTable(mountTable);\r\n    mountTable.setQuota(new RouterQuotaUsage.Builder().quota(1).spaceQuota(2).build());\r\n    assertTrue(updateMountTable(mountTable));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testQuotaRefreshAfterQuotaExceed",
  "errType" : [ "NSQuotaExceededException" ],
  "containingMethodsNum" : 41,
  "sourceCodeText" : "void testQuotaRefreshAfterQuotaExceed() throws Exception\n{\r\n    long nsQuota = 3;\r\n    long ssQuota = 100;\r\n    final FileSystem nnFs1 = nnContext1.getFileSystem();\r\n    final FileSystem nnFs2 = nnContext2.getFileSystem();\r\n    nnFs1.mkdirs(new Path(\"/testdir11\"));\r\n    nnFs2.mkdirs(new Path(\"/testdir12\"));\r\n    MountTable mountTable1 = MountTable.newInstance(\"/setquota1\", Collections.singletonMap(\"ns0\", \"/testdir11\"));\r\n    mountTable1.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota).spaceQuota(ssQuota).build());\r\n    addMountTable(mountTable1);\r\n    MountTable mountTable2 = MountTable.newInstance(\"/setquota2\", Collections.singletonMap(\"ns1\", \"/testdir12\"));\r\n    mountTable2.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota).spaceQuota(ssQuota).build());\r\n    addMountTable(mountTable2);\r\n    final FileSystem routerFs = routerContext.getFileSystem();\r\n    routerFs.mkdirs(new Path(\"/setquota1/\" + UUID.randomUUID()));\r\n    routerFs.mkdirs(new Path(\"/setquota1/\" + UUID.randomUUID()));\r\n    routerFs.mkdirs(new Path(\"/setquota1/\" + UUID.randomUUID()));\r\n    RouterQuotaUpdateService updateService = routerContext.getRouter().getQuotaCacheUpdateService();\r\n    updateService.periodicInvoke();\r\n    resolver.loadCache(true);\r\n    RouterQuotaManager quotaManager = routerContext.getRouter().getQuotaManager();\r\n    ClientProtocol client1 = nnContext1.getClient().getNamenode();\r\n    ClientProtocol client2 = nnContext2.getClient().getNamenode();\r\n    QuotaUsage quota1 = client1.getQuotaUsage(\"/testdir11\");\r\n    QuotaUsage quota2 = client2.getQuotaUsage(\"/testdir12\");\r\n    QuotaUsage cacheQuota1 = quotaManager.getQuotaUsage(\"/setquota1\");\r\n    QuotaUsage cacheQuota2 = quotaManager.getQuotaUsage(\"/setquota2\");\r\n    assertEquals(4, quota1.getFileAndDirectoryCount());\r\n    assertEquals(4, cacheQuota1.getFileAndDirectoryCount());\r\n    assertEquals(1, quota2.getFileAndDirectoryCount());\r\n    assertEquals(1, cacheQuota2.getFileAndDirectoryCount());\r\n    try {\r\n        routerFs.mkdirs(new Path(\"/testdir11/\" + UUID.randomUUID()));\r\n        fail(\"Mkdir should be failed under dir /testdir11.\");\r\n    } catch (NSQuotaExceededException ignored) {\r\n    }\r\n    routerFs.mkdirs(new Path(\"/setquota2/\" + UUID.randomUUID()));\r\n    routerFs.mkdirs(new Path(\"/setquota2/\" + UUID.randomUUID()));\r\n    updateService.periodicInvoke();\r\n    quota1 = client1.getQuotaUsage(\"/testdir11\");\r\n    cacheQuota1 = quotaManager.getQuotaUsage(\"/setquota1\");\r\n    quota2 = client2.getQuotaUsage(\"/testdir12\");\r\n    cacheQuota2 = quotaManager.getQuotaUsage(\"/setquota2\");\r\n    assertEquals(4, quota1.getFileAndDirectoryCount());\r\n    assertEquals(4, cacheQuota1.getFileAndDirectoryCount());\r\n    assertEquals(3, quota2.getFileAndDirectoryCount());\r\n    assertEquals(3, cacheQuota2.getFileAndDirectoryCount());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testQuotaRefreshWhenDestinationNotPresent",
  "errType" : null,
  "containingMethodsNum" : 61,
  "sourceCodeText" : "void testQuotaRefreshWhenDestinationNotPresent() throws Exception\n{\r\n    long nsQuota = 5;\r\n    long ssQuota = 3 * BLOCK_SIZE;\r\n    final FileSystem nnFs = nnContext1.getFileSystem();\r\n    nnFs.mkdirs(new Path(\"/testdir13\"));\r\n    nnFs.mkdirs(new Path(\"/testdir14\"));\r\n    MountTable mountTable = MountTable.newInstance(\"/setdir1\", Collections.singletonMap(\"ns0\", \"/testdir13\"));\r\n    mountTable.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota).spaceQuota(ssQuota).build());\r\n    addMountTable(mountTable);\r\n    mountTable = MountTable.newInstance(\"/setdir2\", Collections.singletonMap(\"ns0\", \"/testdir14\"));\r\n    mountTable.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota).spaceQuota(ssQuota).build());\r\n    addMountTable(mountTable);\r\n    final DFSClient routerClient = routerContext.getClient();\r\n    routerClient.create(\"/setdir1/file1\", true).close();\r\n    routerClient.create(\"/setdir2/file2\", true).close();\r\n    appendData(\"/setdir1/file1\", routerClient, BLOCK_SIZE);\r\n    appendData(\"/setdir2/file2\", routerClient, BLOCK_SIZE);\r\n    RouterQuotaUpdateService updateService = routerContext.getRouter().getQuotaCacheUpdateService();\r\n    updateService.periodicInvoke();\r\n    resolver.loadCache(true);\r\n    ClientProtocol client1 = nnContext1.getClient().getNamenode();\r\n    RouterQuotaManager quotaManager = routerContext.getRouter().getQuotaManager();\r\n    QuotaUsage quota1 = client1.getQuotaUsage(\"/testdir13\");\r\n    QuotaUsage quota2 = client1.getQuotaUsage(\"/testdir14\");\r\n    QuotaUsage cacheQuota1 = quotaManager.getQuotaUsage(\"/setdir1\");\r\n    QuotaUsage cacheQuota2 = quotaManager.getQuotaUsage(\"/setdir2\");\r\n    MountTable updatedMountTable = getMountTable(\"/setdir1\");\r\n    RouterQuotaUsage mountQuota1 = updatedMountTable.getQuota();\r\n    updatedMountTable = getMountTable(\"/setdir2\");\r\n    RouterQuotaUsage mountQuota2 = updatedMountTable.getQuota();\r\n    assertEquals(2, quota1.getFileAndDirectoryCount());\r\n    assertEquals(2, cacheQuota1.getFileAndDirectoryCount());\r\n    assertEquals(2, mountQuota1.getFileAndDirectoryCount());\r\n    assertEquals(2, quota2.getFileAndDirectoryCount());\r\n    assertEquals(2, cacheQuota2.getFileAndDirectoryCount());\r\n    assertEquals(2, mountQuota2.getFileAndDirectoryCount());\r\n    assertEquals(BLOCK_SIZE, quota1.getSpaceConsumed());\r\n    assertEquals(BLOCK_SIZE, cacheQuota1.getSpaceConsumed());\r\n    assertEquals(BLOCK_SIZE, mountQuota1.getSpaceConsumed());\r\n    assertEquals(BLOCK_SIZE, quota2.getSpaceConsumed());\r\n    assertEquals(BLOCK_SIZE, cacheQuota2.getSpaceConsumed());\r\n    assertEquals(BLOCK_SIZE, mountQuota2.getSpaceConsumed());\r\n    FileSystem routerFs = routerContext.getFileSystem();\r\n    routerFs.delete(new Path(\"/setdir1/file1\"), true);\r\n    routerClient.create(\"/setdir2/file3\", true).close();\r\n    appendData(\"/setdir2/file3\", routerClient, BLOCK_SIZE);\r\n    int updatedSpace = BLOCK_SIZE + BLOCK_SIZE;\r\n    updateService.periodicInvoke();\r\n    quota2 = client1.getQuotaUsage(\"/testdir14\");\r\n    cacheQuota1 = quotaManager.getQuotaUsage(\"/setdir1\");\r\n    cacheQuota2 = quotaManager.getQuotaUsage(\"/setdir2\");\r\n    updatedMountTable = getMountTable(\"/setdir1\");\r\n    mountQuota1 = updatedMountTable.getQuota();\r\n    updatedMountTable = getMountTable(\"/setdir2\");\r\n    mountQuota2 = updatedMountTable.getQuota();\r\n    assertEquals(1, cacheQuota1.getFileAndDirectoryCount());\r\n    assertEquals(1, mountQuota1.getFileAndDirectoryCount());\r\n    assertEquals(0, cacheQuota1.getSpaceConsumed());\r\n    assertEquals(0, mountQuota1.getSpaceConsumed());\r\n    assertEquals(3, quota2.getFileAndDirectoryCount());\r\n    assertEquals(3, cacheQuota2.getFileAndDirectoryCount());\r\n    assertEquals(3, mountQuota2.getFileAndDirectoryCount());\r\n    assertEquals(updatedSpace, quota2.getSpaceConsumed());\r\n    assertEquals(updatedSpace, cacheQuota2.getSpaceConsumed());\r\n    assertEquals(updatedSpace, mountQuota2.getSpaceConsumed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testClearQuotaDefAfterRemovingMountTable",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testClearQuotaDefAfterRemovingMountTable() throws Exception\n{\r\n    long nsQuota = 5;\r\n    long ssQuota = 3 * BLOCK_SIZE;\r\n    final FileSystem nnFs = nnContext1.getFileSystem();\r\n    nnFs.mkdirs(new Path(\"/testdir15\"));\r\n    MountTable mountTable = MountTable.newInstance(\"/setdir\", Collections.singletonMap(\"ns0\", \"/testdir15\"));\r\n    mountTable.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota).spaceQuota(ssQuota).build());\r\n    addMountTable(mountTable);\r\n    RouterQuotaUpdateService updateService = routerContext.getRouter().getQuotaCacheUpdateService();\r\n    updateService.periodicInvoke();\r\n    RouterQuotaManager quotaManager = routerContext.getRouter().getQuotaManager();\r\n    ClientProtocol client = nnContext1.getClient().getNamenode();\r\n    QuotaUsage routerQuota = quotaManager.getQuotaUsage(\"/setdir\");\r\n    QuotaUsage subClusterQuota = client.getQuotaUsage(\"/testdir15\");\r\n    assertEquals(nsQuota, routerQuota.getQuota());\r\n    assertEquals(ssQuota, routerQuota.getSpaceQuota());\r\n    assertEquals(nsQuota, subClusterQuota.getQuota());\r\n    assertEquals(ssQuota, subClusterQuota.getSpaceQuota());\r\n    removeMountTable(\"/setdir\");\r\n    updateService.periodicInvoke();\r\n    routerQuota = quotaManager.getQuotaUsage(\"/setdir\");\r\n    subClusterQuota = client.getQuotaUsage(\"/testdir15\");\r\n    assertNull(routerQuota);\r\n    assertEquals(HdfsConstants.QUOTA_RESET, subClusterQuota.getQuota());\r\n    assertEquals(HdfsConstants.QUOTA_RESET, subClusterQuota.getSpaceQuota());\r\n    mountTable = MountTable.newInstance(\"/mount\", Collections.singletonMap(\"ns0\", \"/testdir16\"));\r\n    addMountTable(mountTable);\r\n    assertTrue(removeMountTable(\"/mount\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSetQuotaNotMountTable",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSetQuotaNotMountTable() throws Exception\n{\r\n    long nsQuota = 5;\r\n    long ssQuota = 100;\r\n    final FileSystem nnFs1 = nnContext1.getFileSystem();\r\n    MountTable mountTable1 = MountTable.newInstance(\"/setquotanmt\", Collections.singletonMap(\"ns0\", \"/testdir16\"));\r\n    addMountTable(mountTable1);\r\n    nnFs1.mkdirs(new Path(\"/testdir16/testdir17\"));\r\n    routerContext.getRouter().getRpcServer().setQuota(\"/setquotanmt/testdir17\", nsQuota, ssQuota, null);\r\n    RouterQuotaUpdateService updateService = routerContext.getRouter().getQuotaCacheUpdateService();\r\n    updateService.periodicInvoke();\r\n    ClientProtocol client1 = nnContext1.getClient().getNamenode();\r\n    final QuotaUsage quota1 = client1.getQuotaUsage(\"/testdir16/testdir17\");\r\n    assertEquals(nsQuota, quota1.getQuota());\r\n    assertEquals(ssQuota, quota1.getSpaceQuota());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNoQuotaaExceptionForUnrelatedOperations",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testNoQuotaaExceptionForUnrelatedOperations() throws Exception\n{\r\n    FileSystem nnFs = nnContext1.getFileSystem();\r\n    DistributedFileSystem routerFs = (DistributedFileSystem) routerContext.getFileSystem();\r\n    Path path = new Path(\"/quota\");\r\n    nnFs.mkdirs(new Path(\"/dir\"));\r\n    MountTable mountTable1 = MountTable.newInstance(\"/quota\", Collections.singletonMap(\"ns0\", \"/dir\"));\r\n    mountTable1.setQuota(new RouterQuotaUsage.Builder().quota(0).build());\r\n    addMountTable(mountTable1);\r\n    routerFs.mkdirs(new Path(\"/quota/1\"));\r\n    routerContext.getRouter().getQuotaCacheUpdateService().periodicInvoke();\r\n    intercept(NSQuotaExceededException.class, \"The NameSpace quota (directories and files) is exceeded\", () -> routerFs.mkdirs(new Path(\"/quota/2\")));\r\n    routerFs.setStoragePolicy(path, \"COLD\");\r\n    routerFs.setErasureCodingPolicy(path, \"RS-6-3-1024k\");\r\n    routerFs.unsetErasureCodingPolicy(path);\r\n    routerFs.setPermission(path, new FsPermission((short) 01777));\r\n    routerFs.setOwner(path, \"user\", \"group\");\r\n    routerFs.setTimes(path, 1L, 1L);\r\n    routerFs.listStatus(path);\r\n    routerFs.getContentSummary(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetGlobalQuota",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testGetGlobalQuota() throws Exception\n{\r\n    long nsQuota = 5;\r\n    long ssQuota = 3 * BLOCK_SIZE;\r\n    prepareGlobalQuotaTestMountTable(nsQuota, ssQuota);\r\n    Quota qModule = routerContext.getRouter().getRpcServer().getQuotaModule();\r\n    QuotaUsage qu = qModule.getGlobalQuota(\"/dir-1\");\r\n    assertEquals(nsQuota, qu.getQuota());\r\n    assertEquals(ssQuota, qu.getSpaceQuota());\r\n    qu = qModule.getGlobalQuota(\"/dir-1/dir-2\");\r\n    assertEquals(nsQuota, qu.getQuota());\r\n    assertEquals(ssQuota * 2, qu.getSpaceQuota());\r\n    qu = qModule.getGlobalQuota(\"/dir-1/dir-2/dir-3\");\r\n    assertEquals(nsQuota, qu.getQuota());\r\n    assertEquals(ssQuota * 2, qu.getSpaceQuota());\r\n    qu = qModule.getGlobalQuota(\"/dir-4\");\r\n    assertEquals(-1, qu.getQuota());\r\n    assertEquals(-1, qu.getSpaceQuota());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testFixGlobalQuota",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testFixGlobalQuota() throws Exception\n{\r\n    long nsQuota = 5;\r\n    long ssQuota = 3 * BLOCK_SIZE;\r\n    final FileSystem nnFs = nnContext1.getFileSystem();\r\n    prepareGlobalQuotaTestMountTable(nsQuota, ssQuota);\r\n    QuotaUsage qu = nnFs.getQuotaUsage(new Path(\"/dir-1\"));\r\n    assertEquals(nsQuota, qu.getQuota());\r\n    assertEquals(ssQuota, qu.getSpaceQuota());\r\n    qu = nnFs.getQuotaUsage(new Path(\"/dir-2\"));\r\n    assertEquals(nsQuota, qu.getQuota());\r\n    assertEquals(ssQuota * 2, qu.getSpaceQuota());\r\n    qu = nnFs.getQuotaUsage(new Path(\"/dir-3\"));\r\n    assertEquals(nsQuota, qu.getQuota());\r\n    assertEquals(ssQuota * 2, qu.getSpaceQuota());\r\n    qu = nnFs.getQuotaUsage(new Path(\"/dir-4\"));\r\n    assertEquals(-1, qu.getQuota());\r\n    assertEquals(-1, qu.getSpaceQuota());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetQuotaUsageOnMountPoint",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testGetQuotaUsageOnMountPoint() throws Exception\n{\r\n    long nsQuota = 5;\r\n    long ssQuota = 3 * BLOCK_SIZE;\r\n    prepareGlobalQuotaTestMountTable(nsQuota, ssQuota);\r\n    final FileSystem routerFs = routerContext.getFileSystem();\r\n    QuotaUsage quotaUsage = routerFs.getQuotaUsage(new Path(\"/dir-1\"));\r\n    assertEquals(nsQuota, quotaUsage.getQuota());\r\n    assertEquals(ssQuota, quotaUsage.getSpaceQuota());\r\n    quotaUsage = routerFs.getQuotaUsage(new Path(\"/dir-1/dir-2\"));\r\n    assertEquals(nsQuota, quotaUsage.getQuota());\r\n    assertEquals(ssQuota * 2, quotaUsage.getSpaceQuota());\r\n    quotaUsage = routerFs.getQuotaUsage(new Path(\"/dir-1/dir-2/dir-3\"));\r\n    assertEquals(nsQuota, quotaUsage.getQuota());\r\n    assertEquals(ssQuota * 2, quotaUsage.getSpaceQuota());\r\n    quotaUsage = routerFs.getQuotaUsage(new Path(\"/dir-4\"));\r\n    assertEquals(-1, quotaUsage.getQuota());\r\n    assertEquals(-1, quotaUsage.getSpaceQuota());\r\n    routerFs.mkdirs(new Path(\"/dir-1/dir-normal\"));\r\n    try {\r\n        quotaUsage = routerFs.getQuotaUsage(new Path(\"/dir-1/dir-normal\"));\r\n        assertEquals(-1, quotaUsage.getQuota());\r\n        assertEquals(-1, quotaUsage.getSpaceQuota());\r\n        routerFs.setQuota(new Path(\"/dir-1/dir-normal\"), 100, 200);\r\n        quotaUsage = routerFs.getQuotaUsage(new Path(\"/dir-1/dir-normal\"));\r\n        assertEquals(100, quotaUsage.getQuota());\r\n        assertEquals(200, quotaUsage.getSpaceQuota());\r\n    } finally {\r\n        routerFs.delete(new Path(\"/dir-1/dir-normal\"), true);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRouterQuotaUpdateService",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testRouterQuotaUpdateService() throws Exception\n{\r\n    Router router = routerContext.getRouter();\r\n    StateStoreDriver driver = router.getStateStore().getDriver();\r\n    RouterQuotaUpdateService updateService = router.getQuotaCacheUpdateService();\r\n    RouterQuotaManager quotaManager = router.getQuotaManager();\r\n    long nsQuota = 5;\r\n    long ssQuota = 3 * BLOCK_SIZE;\r\n    final FileSystem nnFs = nnContext1.getFileSystem();\r\n    nnFs.mkdirs(new Path(\"/dir-1\"));\r\n    MountTable mountTable = MountTable.newInstance(\"/dir-1\", Collections.singletonMap(\"ns0\", \"/dir-1\"));\r\n    mountTable.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota).spaceQuota(ssQuota).build());\r\n    addMountTable(mountTable);\r\n    QueryResult<MountTable> result = driver.get(MountTable.class);\r\n    RouterQuotaUsage quotaOnStorage = result.getRecords().get(0).getQuota();\r\n    assertEquals(nsQuota, quotaOnStorage.getQuota());\r\n    assertEquals(ssQuota, quotaOnStorage.getSpaceQuota());\r\n    assertEquals(0, quotaOnStorage.getFileAndDirectoryCount());\r\n    assertEquals(0, quotaOnStorage.getSpaceConsumed());\r\n    updateService.periodicInvoke();\r\n    RouterQuotaUsage quotaUsage = quotaManager.getQuotaUsage(\"/dir-1\");\r\n    assertEquals(nsQuota, quotaUsage.getQuota());\r\n    assertEquals(ssQuota, quotaUsage.getSpaceQuota());\r\n    assertEquals(1, quotaUsage.getFileAndDirectoryCount());\r\n    assertEquals(0, quotaUsage.getSpaceConsumed());\r\n    result = driver.get(MountTable.class);\r\n    quotaOnStorage = result.getRecords().get(0).getQuota();\r\n    assertEquals(nsQuota, quotaOnStorage.getQuota());\r\n    assertEquals(ssQuota, quotaOnStorage.getSpaceQuota());\r\n    assertEquals(0, quotaOnStorage.getFileAndDirectoryCount());\r\n    assertEquals(0, quotaOnStorage.getSpaceConsumed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testQuotaUpdateWhenDestinationNotPresent",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testQuotaUpdateWhenDestinationNotPresent() throws Exception\n{\r\n    long nsQuota = 5;\r\n    long ssQuota = 3 * BLOCK_SIZE;\r\n    String path = \"/dst-not-present\";\r\n    final FileSystem nnFs = nnContext1.getFileSystem();\r\n    nnFs.mkdirs(new Path(path));\r\n    MountTable mountTable = MountTable.newInstance(path, Collections.singletonMap(\"ns0\", path));\r\n    mountTable.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota).spaceQuota(ssQuota).build());\r\n    addMountTable(mountTable);\r\n    Router router = routerContext.getRouter();\r\n    RouterQuotaManager quotaManager = router.getQuotaManager();\r\n    RouterQuotaUpdateService updateService = router.getQuotaCacheUpdateService();\r\n    updateService.periodicInvoke();\r\n    RouterQuotaUsage quotaUsage = quotaManager.getQuotaUsage(path);\r\n    assertEquals(nsQuota, quotaUsage.getQuota());\r\n    assertEquals(ssQuota, quotaUsage.getSpaceQuota());\r\n    assertEquals(1, quotaUsage.getFileAndDirectoryCount());\r\n    assertEquals(0, quotaUsage.getSpaceConsumed());\r\n    mountTable.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota * 2).spaceQuota(ssQuota * 2).build());\r\n    updateMountTable(mountTable);\r\n    nnFs.delete(new Path(path), true);\r\n    updateService.periodicInvoke();\r\n    quotaUsage = quotaManager.getQuotaUsage(path);\r\n    assertEquals(nsQuota * 2, quotaUsage.getQuota());\r\n    assertEquals(ssQuota * 2, quotaUsage.getSpaceQuota());\r\n    assertEquals(0, quotaUsage.getFileAndDirectoryCount());\r\n    assertEquals(0, quotaUsage.getSpaceConsumed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "prepareGlobalQuotaTestMountTable",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void prepareGlobalQuotaTestMountTable(long nsQuota, long ssQuota) throws IOException\n{\r\n    final FileSystem nnFs = nnContext1.getFileSystem();\r\n    nnFs.mkdirs(new Path(\"/dir-1\"));\r\n    nnFs.mkdirs(new Path(\"/dir-2\"));\r\n    nnFs.mkdirs(new Path(\"/dir-3\"));\r\n    nnFs.mkdirs(new Path(\"/dir-4\"));\r\n    MountTable mountTable = MountTable.newInstance(\"/dir-1\", Collections.singletonMap(\"ns0\", \"/dir-1\"));\r\n    mountTable.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota).spaceQuota(ssQuota).build());\r\n    addMountTable(mountTable);\r\n    mountTable = MountTable.newInstance(\"/dir-1/dir-2\", Collections.singletonMap(\"ns0\", \"/dir-2\"));\r\n    mountTable.setQuota(new RouterQuotaUsage.Builder().spaceQuota(ssQuota * 2).build());\r\n    addMountTable(mountTable);\r\n    mountTable = MountTable.newInstance(\"/dir-1/dir-2/dir-3\", Collections.singletonMap(\"ns0\", \"/dir-3\"));\r\n    addMountTable(mountTable);\r\n    mountTable = MountTable.newInstance(\"/dir-4\", Collections.singletonMap(\"ns0\", \"/dir-4\"));\r\n    addMountTable(mountTable);\r\n    RouterQuotaUpdateService updateService = routerContext.getRouter().getQuotaCacheUpdateService();\r\n    updateService.periodicInvoke();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "prepareStorageTypeQuotaTestMountTable",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void prepareStorageTypeQuotaTestMountTable(StorageType type, long blkSize, long quota0, long quota1, int len0, int len1) throws Exception\n{\r\n    final FileSystem nnFs1 = nnContext1.getFileSystem();\r\n    nnFs1.mkdirs(new Path(\"/type0\"));\r\n    nnFs1.mkdirs(new Path(\"/type1\"));\r\n    ((DistributedFileSystem) nnContext1.getFileSystem()).createFile(new Path(\"/type0/file\")).storagePolicyName(\"HOT\").blockSize(blkSize).build().close();\r\n    ((DistributedFileSystem) nnContext1.getFileSystem()).createFile(new Path(\"/type1/file\")).storagePolicyName(\"HOT\").blockSize(blkSize).build().close();\r\n    DFSClient client = nnContext1.getClient();\r\n    appendData(\"/type0/file\", client, len0);\r\n    appendData(\"/type1/file\", client, len1);\r\n    MountTable mountTable = MountTable.newInstance(\"/type0\", Collections.singletonMap(\"ns0\", \"/type0\"));\r\n    mountTable.setQuota(new RouterQuotaUsage.Builder().typeQuota(type, quota0).build());\r\n    addMountTable(mountTable);\r\n    mountTable = MountTable.newInstance(\"/type0/type1\", Collections.singletonMap(\"ns0\", \"/type1\"));\r\n    mountTable.setQuota(new RouterQuotaUsage.Builder().typeQuota(type, quota1).build());\r\n    addMountTable(mountTable);\r\n    RouterQuotaUpdateService updateService = routerContext.getRouter().getQuotaCacheUpdateService();\r\n    updateService.periodicInvoke();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifyTypeQuotaAndConsume",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyTypeQuotaAndConsume(long[] quota, long[] consume, QuotaUsage usage)\n{\r\n    for (StorageType t : StorageType.values()) {\r\n        if (quota != null) {\r\n            assertEquals(quota[t.ordinal()], usage.getTypeQuota(t));\r\n        }\r\n        if (consume != null) {\r\n            assertEquals(consume[t.ordinal()], usage.getTypeConsumed(t));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getStateStore",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StateStoreService getStateStore()\n{\r\n    return stateStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "createBase",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createBase() throws IOException, InterruptedException\n{\r\n    conf = getStateStoreConfiguration();\r\n    conf.setLong(RBFConfigKeys.FEDERATION_STORE_CONNECTION_TEST_MS, TimeUnit.HOURS.toMillis(1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "destroyBase",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void destroyBase() throws Exception\n{\r\n    if (stateStore != null) {\r\n        stateStore.stop();\r\n        stateStore.close();\r\n        stateStore = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "setupBase",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setupBase() throws IOException, InterruptedException, InstantiationException, IllegalAccessException\n{\r\n    if (stateStore == null) {\r\n        stateStore = newStateStore(conf);\r\n        assertNotNull(stateStore);\r\n    }\r\n    stateStore.loadDriver();\r\n    waitStateStore(stateStore, TimeUnit.SECONDS.toMillis(10));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "globalSetUp",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void globalSetUp() throws Exception\n{\r\n    Configuration namenodeConf = new Configuration();\r\n    namenodeConf.setBoolean(DFSConfigKeys.HADOOP_CALLER_CONTEXT_ENABLED_KEY, true);\r\n    namenodeConf.setBoolean(DFS_NAMENODE_REDUNDANCY_CONSIDERLOAD_KEY, false);\r\n    cluster = new MiniRouterDFSCluster(false, NUM_SUBCLUSTERS);\r\n    cluster.setNumDatanodesPerNameservice(NUM_DNS);\r\n    cluster.addNamenodeOverrides(namenodeConf);\r\n    cluster.setIndependentDNs();\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(DFSConfigKeys.DFS_LIST_LIMIT, 5);\r\n    cluster.addNamenodeOverrides(conf);\r\n    cluster.startCluster();\r\n    Configuration routerConf = new RouterConfigBuilder().metrics().rpc().build();\r\n    routerConf.setTimeDuration(RBFConfigKeys.DN_REPORT_CACHE_EXPIRE, 1, TimeUnit.SECONDS);\r\n    cluster.addRouterOverrides(routerConf);\r\n    cluster.startRouters();\r\n    cluster.registerNamenodes();\r\n    cluster.waitNamenodeRegistration();\r\n    cluster.getCluster().getNamesystem(0).getBlockManager().getDatanodeManager().setHeartbeatInterval(1);\r\n    cluster.getCluster().getNamesystem(1).getBlockManager().getDatanodeManager().setHeartbeatInterval(1);\r\n    cluster.getCluster().getNamesystem(0).getBlockManager().getDatanodeManager().setHeartbeatExpireInterval(3000);\r\n    cluster.getCluster().getNamesystem(1).getBlockManager().getDatanodeManager().setHeartbeatExpireInterval(3000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    cluster.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSetup",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testSetup() throws Exception\n{\r\n    cluster.installMockLocations();\r\n    cluster.deleteAllFiles();\r\n    cluster.createTestDirectoriesNamenode();\r\n    Thread.sleep(100);\r\n    RouterContext rndRouter = cluster.getRandomRouter();\r\n    this.setRouter(rndRouter);\r\n    String ns0 = cluster.getNameservices().get(0);\r\n    this.setNs(ns0);\r\n    this.setNamenode(cluster.getNamenode(ns0, null));\r\n    Random rnd = new Random();\r\n    String randomFile = \"testfile-\" + rnd.nextInt();\r\n    this.nnFile = cluster.getNamenodeTestDirectoryForNS(ns) + \"/\" + randomFile;\r\n    this.routerFile = cluster.getFederatedTestDirectoryForNS(ns) + \"/\" + randomFile;\r\n    createFile(nnFS, nnFile, 32);\r\n    verifyFileExists(nnFS, nnFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRpcService",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testRpcService() throws IOException\n{\r\n    Router testRouter = new Router();\r\n    List<String> nss = cluster.getNameservices();\r\n    String ns0 = nss.get(0);\r\n    Configuration routerConfig = cluster.generateRouterConfiguration(ns0, null);\r\n    RouterRpcServer server = new RouterRpcServer(routerConfig, testRouter, testRouter.getNamenodeResolver(), testRouter.getSubclusterResolver());\r\n    server.init(routerConfig);\r\n    assertEquals(STATE.INITED, server.getServiceState());\r\n    server.start();\r\n    assertEquals(STATE.STARTED, server.getServiceState());\r\n    server.stop();\r\n    assertEquals(STATE.STOPPED, server.getServiceState());\r\n    server.close();\r\n    testRouter.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getCluster",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MiniRouterDFSCluster getCluster()\n{\r\n    return TestRouterRpc.cluster;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterContext getRouterContext()\n{\r\n    return this.router;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setRouter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setRouter(RouterContext r) throws IOException, URISyntaxException\n{\r\n    this.router = r;\r\n    this.routerProtocol = r.getClient().getNamenode();\r\n    this.routerFS = r.getFileSystem();\r\n    this.routerNamenodeProtocol = NameNodeProxies.createProxy(router.getConf(), router.getFileSystem().getUri(), NamenodeProtocol.class).getProxy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterFileSystem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSystem getRouterFileSystem()\n{\r\n    return this.routerFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodeFileSystem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSystem getNamenodeFileSystem()\n{\r\n    return this.nnFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterProtocol",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientProtocol getRouterProtocol()\n{\r\n    return this.routerProtocol;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodeProtocol",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientProtocol getNamenodeProtocol()\n{\r\n    return this.nnProtocol;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NamenodeContext getNamenode()\n{\r\n    return this.namenode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setNamenodeFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNamenodeFile(String filename)\n{\r\n    this.nnFile = filename;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodeFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNamenodeFile()\n{\r\n    return this.nnFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setRouterFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRouterFile(String filename)\n{\r\n    this.routerFile = filename;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRouterFile()\n{\r\n    return this.routerFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setNamenode",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setNamenode(NamenodeContext nn) throws IOException, URISyntaxException\n{\r\n    this.namenode = nn;\r\n    this.nnProtocol = nn.getClient().getNamenode();\r\n    this.nnFS = nn.getFileSystem();\r\n    String ns0 = cluster.getNameservices().get(0);\r\n    NamenodeContext nn0 = cluster.getNamenode(ns0, null);\r\n    this.nnNamenodeProtocol = NameNodeProxies.createProxy(nn0.getConf(), nn0.getFileSystem().getUri(), NamenodeProtocol.class).getProxy();\r\n    String ns1 = cluster.getNameservices().get(1);\r\n    NamenodeContext nn1 = cluster.getNamenode(ns1, null);\r\n    this.nnNamenodeProtocol1 = NameNodeProxies.createProxy(nn1.getConf(), nn1.getFileSystem().getUri(), NamenodeProtocol.class).getProxy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNs()\n{\r\n    return this.ns;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setNs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNs(String nameservice)\n{\r\n    this.ns = nameservice;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "compareResponses",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void compareResponses(ClientProtocol protocol1, ClientProtocol protocol2, Method m, Object[] paramList)\n{\r\n    Object return1 = null;\r\n    Exception exception1 = null;\r\n    try {\r\n        return1 = m.invoke(protocol1, paramList);\r\n    } catch (Exception ex) {\r\n        exception1 = ex;\r\n    }\r\n    Object return2 = null;\r\n    Exception exception2 = null;\r\n    try {\r\n        return2 = m.invoke(protocol2, paramList);\r\n    } catch (Exception ex) {\r\n        exception2 = ex;\r\n    }\r\n    assertEquals(return1, return2);\r\n    if (exception1 == null && exception2 == null) {\r\n        return;\r\n    }\r\n    assertEquals(exception1.getCause().getClass(), exception2.getCause().getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyListFiles",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testProxyListFiles() throws IOException, InterruptedException, URISyntaxException, NoSuchMethodException, SecurityException\n{\r\n    Set<String> requiredPaths = new TreeSet<>();\r\n    FileSubclusterResolver fileResolver = router.getRouter().getSubclusterResolver();\r\n    for (String mount : fileResolver.getMountPoints(\"/\")) {\r\n        requiredPaths.add(mount);\r\n    }\r\n    String defaultNs = cluster.getNameservices().get(0);\r\n    NamenodeContext nn = cluster.getNamenode(defaultNs, null);\r\n    FileStatus[] iterator = nn.getFileSystem().listStatus(new Path(\"/\"));\r\n    for (FileStatus file : iterator) {\r\n        requiredPaths.add(file.getPath().getName());\r\n    }\r\n    DirectoryListing listing = routerProtocol.getListing(\"/\", HdfsFileStatus.EMPTY_NAME, false);\r\n    Iterator<String> requiredPathsIterator = requiredPaths.iterator();\r\n    for (HdfsFileStatus f : listing.getPartialListing()) {\r\n        String fileName = requiredPathsIterator.next();\r\n        String currentFile = f.getFullPath(new Path(\"/\")).getName();\r\n        assertEquals(currentFile, fileName);\r\n    }\r\n    assertEquals(requiredPaths.size(), listing.getPartialListing().length);\r\n    Method m = ClientProtocol.class.getMethod(\"getListing\", String.class, byte[].class, boolean.class);\r\n    String badPath = \"/unknownlocation/unknowndir\";\r\n    compareResponses(routerProtocol, nnProtocol, m, new Object[] { badPath, HdfsFileStatus.EMPTY_NAME, false });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyListFilesLargeDir",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testProxyListFilesLargeDir() throws IOException\n{\r\n    for (RouterContext rc : cluster.getRouters()) {\r\n        MockResolver resolver = (MockResolver) rc.getRouter().getSubclusterResolver();\r\n        resolver.addLocation(\"/parent\", ns, \"/parent\");\r\n        resolver.addLocation(\"/parent/file-0\", ns, \"/parent/file-0\");\r\n        resolver.addLocation(\"/parent/file-7\", ns, \"/parent/file-7\");\r\n    }\r\n    FileStatus[] result = routerFS.listStatus(new Path(\"/parent\"));\r\n    assertEquals(2, result.length);\r\n    assertEquals(\"file-0\", result[0].getPath().getName());\r\n    assertEquals(\"file-7\", result[1].getPath().getName());\r\n    NamenodeContext nn = cluster.getNamenode(ns, null);\r\n    FileSystem nnFileSystem = nn.getFileSystem();\r\n    for (int i = 1; i < 9; i++) {\r\n        createFile(nnFileSystem, \"/parent/file-\" + i, 32);\r\n    }\r\n    result = routerFS.listStatus(new Path(\"/parent\"));\r\n    assertEquals(9, result.length);\r\n    for (int i = 0; i < 9; i++) {\r\n        assertEquals(\"file-\" + i, result[i].getPath().getName());\r\n    }\r\n    for (RouterContext rc : cluster.getRouters()) {\r\n        MockResolver resolver = (MockResolver) rc.getRouter().getSubclusterResolver();\r\n        resolver.addLocation(\"/parent/file-9\", ns, \"/parent/file-9\");\r\n    }\r\n    assertFalse(verifyFileExists(nnFileSystem, \"/parent/file-9\"));\r\n    result = routerFS.listStatus(new Path(\"/parent\"));\r\n    assertEquals(10, result.length);\r\n    for (int i = 0; i < 10; i++) {\r\n        assertEquals(\"file-\" + i, result[i].getPath().getName());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyListFilesWithConflict",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testProxyListFilesWithConflict() throws IOException, InterruptedException\n{\r\n    NamenodeContext nn = cluster.getNamenode(ns, null);\r\n    FileSystem nnFs = nn.getFileSystem();\r\n    addDirectory(nnFs, cluster.getFederatedTestDirectoryForNS(ns));\r\n    FileSystem routerFs = router.getFileSystem();\r\n    int initialCount = countContents(routerFs, \"/\");\r\n    int newCount = countContents(routerFs, \"/\");\r\n    assertEquals(initialCount, newCount);\r\n    assertEquals(1, countContents(routerFs, cluster.getFederatedPathForNS(ns)));\r\n    assertEquals(1, countContents(nnFs, cluster.getNamenodePathForNS(ns)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRename",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRename(RouterContext testRouter, String filename, String renamedFile, boolean exceptionExpected) throws IOException\n{\r\n    createFile(testRouter.getFileSystem(), filename, 32);\r\n    verifyFileExists(testRouter.getFileSystem(), filename);\r\n    boolean exceptionThrown = false;\r\n    try {\r\n        DFSClient client = testRouter.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename(filename, renamedFile);\r\n    } catch (Exception ex) {\r\n        exceptionThrown = true;\r\n    }\r\n    if (exceptionExpected) {\r\n        assertTrue(exceptionThrown);\r\n        FileContext fileContext = testRouter.getFileContext();\r\n        assertTrue(fileContext.delete(new Path(filename), true));\r\n    } else {\r\n        assertFalse(exceptionThrown);\r\n        assertTrue(verifyFileExists(testRouter.getFileSystem(), renamedFile));\r\n        FileContext fileContext = testRouter.getFileContext();\r\n        assertTrue(fileContext.delete(new Path(renamedFile), true));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRename2",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRename2(RouterContext testRouter, String filename, String renamedFile, boolean exceptionExpected) throws IOException\n{\r\n    createFile(testRouter.getFileSystem(), filename, 32);\r\n    verifyFileExists(testRouter.getFileSystem(), filename);\r\n    boolean exceptionThrown = false;\r\n    try {\r\n        DFSClient client = testRouter.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename2(filename, renamedFile, new Options.Rename[] {});\r\n    } catch (Exception ex) {\r\n        exceptionThrown = true;\r\n    }\r\n    assertEquals(exceptionExpected, exceptionThrown);\r\n    if (exceptionExpected) {\r\n        FileContext fileContext = testRouter.getFileContext();\r\n        assertTrue(fileContext.delete(new Path(filename), true));\r\n    } else {\r\n        assertTrue(verifyFileExists(testRouter.getFileSystem(), renamedFile));\r\n        FileContext fileContext = testRouter.getFileContext();\r\n        assertTrue(fileContext.delete(new Path(renamedFile), true));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyRenameFiles",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testProxyRenameFiles() throws IOException, InterruptedException\n{\r\n    Thread.sleep(5000);\r\n    List<String> nss = cluster.getNameservices();\r\n    String ns0 = nss.get(0);\r\n    String ns1 = nss.get(1);\r\n    String filename = cluster.getFederatedTestDirectoryForNS(ns0) + \"/testrename\";\r\n    String renamedFile = filename + \"-append\";\r\n    testRename(router, filename, renamedFile, false);\r\n    testRename2(router, filename, renamedFile, false);\r\n    filename = cluster.getFederatedTestDirectoryForNS(ns0) + \"/testrename\";\r\n    renamedFile = cluster.getFederatedTestDirectoryForNS(ns1) + \"/testrename\";\r\n    testRename(router, filename, renamedFile, true);\r\n    testRename2(router, filename, renamedFile, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyChownFiles",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testProxyChownFiles() throws Exception\n{\r\n    String newUsername = \"TestUser\";\r\n    String newGroup = \"TestGroup\";\r\n    routerProtocol.setOwner(routerFile, newUsername, newGroup);\r\n    FileStatus file = getFileStatus(namenode.getFileSystem(), nnFile);\r\n    assertEquals(file.getOwner(), newUsername);\r\n    assertEquals(file.getGroup(), newGroup);\r\n    Method m = ClientProtocol.class.getMethod(\"setOwner\", String.class, String.class, String.class);\r\n    String badPath = \"/unknownlocation/unknowndir\";\r\n    compareResponses(routerProtocol, nnProtocol, m, new Object[] { badPath, newUsername, newGroup });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyGetStats",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testProxyGetStats() throws Exception\n{\r\n    Supplier<Boolean> check = new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            try {\r\n                long[] combinedData = routerProtocol.getStats();\r\n                long[] individualData = getAggregateStats();\r\n                int len = Math.min(combinedData.length, individualData.length);\r\n                for (int i = 0; i < len; i++) {\r\n                    if (combinedData[i] != individualData[i]) {\r\n                        LOG.error(\"Stats for {} don't match: {} != {}\", i, combinedData[i], individualData[i]);\r\n                        return false;\r\n                    }\r\n                }\r\n                return true;\r\n            } catch (Exception e) {\r\n                LOG.error(\"Cannot get stats: {}\", e.getMessage());\r\n                return false;\r\n            }\r\n        }\r\n    };\r\n    GenericTestUtils.waitFor(check, 500, 5 * 1000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getAggregateStats",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long[] getAggregateStats() throws Exception\n{\r\n    long[] individualData = new long[10];\r\n    for (String nameservice : cluster.getNameservices()) {\r\n        NamenodeContext n = cluster.getNamenode(nameservice, null);\r\n        DFSClient client = n.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        long[] data = clientProtocol.getStats();\r\n        for (int i = 0; i < data.length; i++) {\r\n            individualData[i] += data[i];\r\n        }\r\n    }\r\n    return individualData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyGetDatanodeReport",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testProxyGetDatanodeReport() throws Exception\n{\r\n    DatanodeInfo[] combinedData = routerProtocol.getDatanodeReport(DatanodeReportType.ALL);\r\n    final Map<Integer, String> routerDNMap = new TreeMap<>();\r\n    for (DatanodeInfo dn : combinedData) {\r\n        String subcluster = dn.getNetworkLocation().split(\"/\")[1];\r\n        routerDNMap.put(dn.getXferPort(), subcluster);\r\n    }\r\n    final Map<Integer, String> nnDNMap = new TreeMap<>();\r\n    for (String nameservice : cluster.getNameservices()) {\r\n        NamenodeContext n = cluster.getNamenode(nameservice, null);\r\n        DFSClient client = n.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        DatanodeInfo[] data = clientProtocol.getDatanodeReport(DatanodeReportType.ALL);\r\n        for (int i = 0; i < data.length; i++) {\r\n            DatanodeInfo info = data[i];\r\n            nnDNMap.put(info.getXferPort(), nameservice);\r\n        }\r\n    }\r\n    assertEquals(nnDNMap, routerDNMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyGetDatanodeStorageReport",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testProxyGetDatanodeStorageReport() throws IOException, InterruptedException, URISyntaxException\n{\r\n    DatanodeStorageReport[] combinedData = routerProtocol.getDatanodeStorageReport(DatanodeReportType.ALL);\r\n    Set<String> individualData = new HashSet<>();\r\n    for (String nameservice : cluster.getNameservices()) {\r\n        NamenodeContext n = cluster.getNamenode(nameservice, null);\r\n        DFSClient client = n.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        DatanodeStorageReport[] data = clientProtocol.getDatanodeStorageReport(DatanodeReportType.ALL);\r\n        for (DatanodeStorageReport report : data) {\r\n            DatanodeInfo dn = report.getDatanodeInfo();\r\n            individualData.add(dn.toString());\r\n        }\r\n    }\r\n    assertEquals(combinedData.length, individualData.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyMkdir",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testProxyMkdir() throws Exception\n{\r\n    FileStatus[] filesInitial = routerFS.listStatus(new Path(\"/\"));\r\n    String dirPath = \"/testdir\";\r\n    FsPermission permission = new FsPermission(\"705\");\r\n    routerProtocol.mkdirs(dirPath, permission, false);\r\n    FileStatus[] files = routerFS.listStatus(new Path(\"/\"));\r\n    assertEquals(Arrays.toString(files) + \" should be \" + Arrays.toString(filesInitial) + \" + \" + dirPath, filesInitial.length + 1, files.length);\r\n    assertTrue(verifyFileExists(routerFS, dirPath));\r\n    int foundCount = 0;\r\n    for (NamenodeContext n : cluster.getNamenodes()) {\r\n        if (verifyFileExists(n.getFileSystem(), dirPath)) {\r\n            foundCount++;\r\n        }\r\n    }\r\n    assertEquals(1, foundCount);\r\n    assertTrue(deleteFile(routerFS, dirPath));\r\n    Method m = ClientProtocol.class.getMethod(\"mkdirs\", String.class, FsPermission.class, boolean.class);\r\n    String badPath = \"/unknownlocation/unknowndir\";\r\n    compareResponses(routerProtocol, nnProtocol, m, new Object[] { badPath, permission, false });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyChmodFiles",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testProxyChmodFiles() throws Exception\n{\r\n    FsPermission permission = new FsPermission(\"444\");\r\n    routerProtocol.setPermission(routerFile, permission);\r\n    FileStatus file = getFileStatus(namenode.getFileSystem(), nnFile);\r\n    assertEquals(permission, file.getPermission());\r\n    Method m = ClientProtocol.class.getMethod(\"setPermission\", String.class, FsPermission.class);\r\n    String badPath = \"/unknownlocation/unknowndir\";\r\n    compareResponses(routerProtocol, nnProtocol, m, new Object[] { badPath, permission });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxySetReplication",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testProxySetReplication() throws Exception\n{\r\n    FileStatus file = getFileStatus(nnFS, nnFile);\r\n    assertEquals(1, file.getReplication());\r\n    routerProtocol.setReplication(routerFile, (short) 2);\r\n    file = getFileStatus(nnFS, nnFile);\r\n    assertEquals(2, file.getReplication());\r\n    Method m = ClientProtocol.class.getMethod(\"setReplication\", String.class, short.class);\r\n    String badPath = \"/unknownlocation/unknowndir\";\r\n    compareResponses(routerProtocol, nnProtocol, m, new Object[] { badPath, (short) 2 });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyTruncateFile",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testProxyTruncateFile() throws Exception\n{\r\n    FileStatus file = getFileStatus(nnFS, nnFile);\r\n    assertTrue(file.getLen() > 0);\r\n    routerProtocol.truncate(routerFile, 0, \"testclient\");\r\n    file = getFileStatus(nnFS, nnFile);\r\n    assertEquals(0, file.getLen());\r\n    Method m = ClientProtocol.class.getMethod(\"truncate\", String.class, long.class, String.class);\r\n    String badPath = \"/unknownlocation/unknowndir\";\r\n    compareResponses(routerProtocol, nnProtocol, m, new Object[] { badPath, (long) 0, \"testclient\" });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testAllowDisallowSnapshots",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testAllowDisallowSnapshots() throws Exception\n{\r\n    String dirPath = \"/testdir\";\r\n    String filePath1 = \"/sample\";\r\n    FsPermission permission = new FsPermission(\"705\");\r\n    routerProtocol.mkdirs(dirPath, permission, false);\r\n    createFile(routerFS, filePath1, 32);\r\n    NamenodeContext nnContext = cluster.getNamenodes().get(0);\r\n    NameNode nn = nnContext.getNamenode();\r\n    FSNamesystem fsn = NameNodeAdapter.getNamesystem(nn);\r\n    FSDirectory fsdir = fsn.getFSDirectory();\r\n    INodeDirectory dirNode = fsdir.getINode4Write(dirPath).asDirectory();\r\n    assertFalse(dirNode.isSnapshottable());\r\n    routerProtocol.allowSnapshot(\"/testdir\");\r\n    dirNode = fsdir.getINode4Write(dirPath).asDirectory();\r\n    assertTrue(dirNode.isSnapshottable());\r\n    routerProtocol.disallowSnapshot(\"/testdir\");\r\n    dirNode = fsdir.getINode4Write(dirPath).asDirectory();\r\n    assertFalse(dirNode.isSnapshottable());\r\n    routerProtocol.delete(dirPath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testManageSnapshot",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testManageSnapshot() throws Exception\n{\r\n    final String mountPoint = \"/mntsnapshot\";\r\n    final String snapshotFolder = mountPoint + \"/folder\";\r\n    LOG.info(\"Setup a mount point for snapshots: {}\", mountPoint);\r\n    Router r = router.getRouter();\r\n    MockResolver resolver = (MockResolver) r.getSubclusterResolver();\r\n    String ns0 = cluster.getNameservices().get(0);\r\n    resolver.addLocation(mountPoint, ns0, \"/\");\r\n    FsPermission permission = new FsPermission(\"777\");\r\n    routerProtocol.mkdirs(snapshotFolder, permission, false);\r\n    try {\r\n        for (int i = 1; i <= 9; i++) {\r\n            String folderPath = snapshotFolder + \"/subfolder\" + i;\r\n            routerProtocol.mkdirs(folderPath, permission, false);\r\n        }\r\n        LOG.info(\"Create the snapshot: {}\", snapshotFolder);\r\n        routerProtocol.allowSnapshot(snapshotFolder);\r\n        String snapshotName = routerProtocol.createSnapshot(snapshotFolder, \"snap\");\r\n        assertEquals(snapshotFolder + \"/.snapshot/snap\", snapshotName);\r\n        assertTrue(verifyFileExists(routerFS, snapshotFolder + \"/.snapshot/snap\"));\r\n        LOG.info(\"Rename the snapshot and check it changed\");\r\n        routerProtocol.renameSnapshot(snapshotFolder, \"snap\", \"newsnap\");\r\n        assertFalse(verifyFileExists(routerFS, snapshotFolder + \"/.snapshot/snap\"));\r\n        assertTrue(verifyFileExists(routerFS, snapshotFolder + \"/.snapshot/newsnap\"));\r\n        LambdaTestUtils.intercept(SnapshotException.class, \"Cannot delete snapshot snap from path \" + snapshotFolder + \":\", () -> routerFS.deleteSnapshot(new Path(snapshotFolder), \"snap\"));\r\n        LOG.info(\"Delete the snapshot and check it is not there\");\r\n        routerProtocol.deleteSnapshot(snapshotFolder, \"newsnap\");\r\n        assertFalse(verifyFileExists(routerFS, snapshotFolder + \"/.snapshot/newsnap\"));\r\n    } finally {\r\n        assertTrue(routerProtocol.delete(snapshotFolder, true));\r\n        assertTrue(resolver.removeLocation(mountPoint, ns0, \"/\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetSnapshotListing",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testGetSnapshotListing() throws IOException\n{\r\n    final String snapshotPath = \"/testGetSnapshotListing\";\r\n    final String childDir = snapshotPath + \"/subdir\";\r\n    FsPermission permission = new FsPermission(\"705\");\r\n    routerProtocol.mkdirs(snapshotPath, permission, false);\r\n    routerProtocol.allowSnapshot(snapshotPath);\r\n    final String snapshot1 = \"snap1\";\r\n    final String snapshot2 = \"snap2\";\r\n    routerProtocol.createSnapshot(snapshotPath, snapshot1);\r\n    routerProtocol.mkdirs(childDir, permission, false);\r\n    routerProtocol.createSnapshot(snapshotPath, snapshot2);\r\n    SnapshottableDirectoryStatus[] dirList = routerProtocol.getSnapshottableDirListing();\r\n    assertEquals(1, dirList.length);\r\n    SnapshottableDirectoryStatus snapshotDir0 = dirList[0];\r\n    assertEquals(snapshotPath, snapshotDir0.getFullPath().toString());\r\n    SnapshotStatus[] snapshots = routerProtocol.getSnapshotListing(snapshotPath);\r\n    assertEquals(2, snapshots.length);\r\n    assertEquals(SnapshotTestHelper.getSnapshotRoot(new Path(snapshotPath), snapshot1), snapshots[0].getFullPath());\r\n    assertEquals(SnapshotTestHelper.getSnapshotRoot(new Path(snapshotPath), snapshot2), snapshots[1].getFullPath());\r\n    SnapshotDiffReport diffReport = routerProtocol.getSnapshotDiffReport(snapshotPath, snapshot1, snapshot2);\r\n    assertEquals(2, diffReport.getDiffList().size());\r\n    byte[] startPath = {};\r\n    SnapshotDiffReportListing diffReportListing = routerProtocol.getSnapshotDiffReportListing(snapshotPath, snapshot1, snapshot2, startPath, -1);\r\n    assertEquals(1, diffReportListing.getModifyList().size());\r\n    assertEquals(1, diffReportListing.getCreateList().size());\r\n    routerProtocol.deleteSnapshot(snapshotPath, snapshot1);\r\n    routerProtocol.deleteSnapshot(snapshotPath, snapshot2);\r\n    routerProtocol.disallowSnapshot(snapshotPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyGetBlockLocations",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testProxyGetBlockLocations() throws Exception\n{\r\n    LocatedBlocks locations = routerProtocol.getBlockLocations(routerFile, 0, 1024);\r\n    assertEquals(1, locations.getLocatedBlocks().size());\r\n    Method m = ClientProtocol.class.getMethod(\"getBlockLocations\", String.class, long.class, long.class);\r\n    String badPath = \"/unknownlocation/unknowndir\";\r\n    compareResponses(routerProtocol, nnProtocol, m, new Object[] { badPath, (long) 0, (long) 0 });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testProxyStoragePolicy() throws Exception\n{\r\n    HdfsFileStatus status = namenode.getClient().getFileInfo(nnFile);\r\n    BlockStoragePolicy[] policies = namenode.getClient().getStoragePolicies();\r\n    BlockStoragePolicy policy = policies[0];\r\n    while (policy.isCopyOnCreateFile()) {\r\n        Random rand = new Random();\r\n        int randIndex = rand.nextInt(policies.length);\r\n        policy = policies[randIndex];\r\n    }\r\n    routerProtocol.setStoragePolicy(routerFile, policy.getName());\r\n    HdfsFileStatus newStatus = namenode.getClient().getFileInfo(nnFile);\r\n    assertTrue(newStatus.getStoragePolicy() == policy.getId());\r\n    assertTrue(newStatus.getStoragePolicy() != status.getStoragePolicy());\r\n    Method m = ClientProtocol.class.getMethod(\"setStoragePolicy\", String.class, String.class);\r\n    String badPath = \"/unknownlocation/unknowndir\";\r\n    compareResponses(routerProtocol, nnProtocol, m, new Object[] { badPath, \"badpolicy\" });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyGetAndUnsetStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testProxyGetAndUnsetStoragePolicy() throws Exception\n{\r\n    String file = \"/testGetStoragePolicy\";\r\n    String nnFilePath = cluster.getNamenodeTestDirectoryForNS(ns) + file;\r\n    String routerFilePath = cluster.getFederatedTestDirectoryForNS(ns) + file;\r\n    createFile(routerFS, routerFilePath, 32);\r\n    BlockStoragePolicy policy = routerProtocol.getStoragePolicy(routerFilePath);\r\n    assertEquals(HdfsConstants.HOT_STORAGE_POLICY_NAME, policy.getName());\r\n    assertEquals(HdfsConstants.HOT_STORAGE_POLICY_ID, policy.getId());\r\n    BlockStoragePolicy[] policies = routerProtocol.getStoragePolicies();\r\n    BlockStoragePolicy[] nnPolicies = namenode.getClient().getStoragePolicies();\r\n    assertArrayEquals(nnPolicies, policies);\r\n    BlockStoragePolicy newPolicy = policies[0];\r\n    while (newPolicy.isCopyOnCreateFile()) {\r\n        Random rand = new Random();\r\n        int randIndex = rand.nextInt(policies.length);\r\n        newPolicy = policies[randIndex];\r\n    }\r\n    routerProtocol.setStoragePolicy(routerFilePath, newPolicy.getName());\r\n    policy = routerProtocol.getStoragePolicy(routerFilePath);\r\n    assertEquals(newPolicy.getName(), policy.getName());\r\n    assertEquals(newPolicy.getId(), policy.getId());\r\n    BlockStoragePolicy nnPolicy = namenode.getClient().getStoragePolicy(nnFilePath);\r\n    assertEquals(nnPolicy.getName(), policy.getName());\r\n    assertEquals(nnPolicy.getId(), policy.getId());\r\n    routerProtocol.unsetStoragePolicy(routerFilePath);\r\n    policy = routerProtocol.getStoragePolicy(routerFilePath);\r\n    assertEquals(HdfsConstants.HOT_STORAGE_POLICY_NAME, policy.getName());\r\n    assertEquals(HdfsConstants.HOT_STORAGE_POLICY_ID, policy.getId());\r\n    nnPolicy = namenode.getClient().getStoragePolicy(nnFilePath);\r\n    assertEquals(nnPolicy.getName(), policy.getName());\r\n    assertEquals(nnPolicy.getId(), policy.getId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testListStoragePolicies",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testListStoragePolicies() throws IOException, URISyntaxException\n{\r\n    MockResolver resolver = (MockResolver) router.getRouter().getSubclusterResolver();\r\n    try {\r\n        BlockStoragePolicy[] policies = namenode.getClient().getStoragePolicies();\r\n        assertArrayEquals(policies, routerProtocol.getStoragePolicies());\r\n        resolver.setDisableNamespace(true);\r\n        assertArrayEquals(policies, routerProtocol.getStoragePolicies());\r\n    } finally {\r\n        resolver.setDisableNamespace(false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testGetServerDefaults() throws IOException, URISyntaxException\n{\r\n    MockResolver resolver = (MockResolver) router.getRouter().getSubclusterResolver();\r\n    try {\r\n        FsServerDefaults defaults = namenode.getClient().getServerDefaults();\r\n        assertEquals(defaults.getBlockSize(), routerProtocol.getServerDefaults().getBlockSize());\r\n        resolver.setDisableNamespace(true);\r\n        assertEquals(defaults.getBlockSize(), routerProtocol.getServerDefaults().getBlockSize());\r\n    } finally {\r\n        resolver.setDisableNamespace(false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyGetPreferedBlockSize",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testProxyGetPreferedBlockSize() throws Exception\n{\r\n    long namenodeSize = nnProtocol.getPreferredBlockSize(nnFile);\r\n    long routerSize = routerProtocol.getPreferredBlockSize(routerFile);\r\n    assertEquals(routerSize, namenodeSize);\r\n    Method m = ClientProtocol.class.getMethod(\"getPreferredBlockSize\", String.class);\r\n    String badPath = \"/unknownlocation/unknowndir\";\r\n    compareResponses(routerProtocol, nnProtocol, m, new Object[] { badPath });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testConcat",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testConcat(String source, String target, boolean failureExpected)\n{\r\n    boolean failure = false;\r\n    try {\r\n        routerProtocol.concat(target, new String[] { source });\r\n    } catch (IOException ex) {\r\n        failure = true;\r\n    }\r\n    assertEquals(failureExpected, failure);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyConcatFile",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testProxyConcatFile() throws Exception\n{\r\n    String sameNameservice = ns;\r\n    String existingFile = cluster.getFederatedTestDirectoryForNS(sameNameservice) + \"_concatfile\";\r\n    int existingFileSize = 32;\r\n    createFile(routerFS, existingFile, existingFileSize);\r\n    String alternateNameservice = null;\r\n    for (String n : cluster.getNameservices()) {\r\n        if (!n.equals(sameNameservice)) {\r\n            alternateNameservice = n;\r\n            break;\r\n        }\r\n    }\r\n    String altRouterFile = cluster.getFederatedTestDirectoryForNS(alternateNameservice) + \"_newfile\";\r\n    String sameRouterFile = cluster.getFederatedTestDirectoryForNS(sameNameservice) + \"_newfile\";\r\n    createFile(routerFS, altRouterFile, DFSConfigKeys.DFS_BLOCK_SIZE_DEFAULT);\r\n    createFile(routerFS, sameRouterFile, DFSConfigKeys.DFS_BLOCK_SIZE_DEFAULT);\r\n    testConcat(existingFile, altRouterFile, true);\r\n    testConcat(existingFile, sameRouterFile, false);\r\n    FileStatus status = getFileStatus(routerFS, sameRouterFile);\r\n    assertEquals(existingFileSize + DFSConfigKeys.DFS_BLOCK_SIZE_DEFAULT, status.getLen());\r\n    Method m = ClientProtocol.class.getMethod(\"concat\", String.class, String[].class);\r\n    String badPath = \"/unknownlocation/unknowndir\";\r\n    compareResponses(routerProtocol, nnProtocol, m, new Object[] { badPath, new String[] { routerFile } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyAppend",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testProxyAppend() throws Exception\n{\r\n    EnumSet<CreateFlag> createFlag = EnumSet.of(CreateFlag.APPEND);\r\n    DFSClient routerClient = getRouterContext().getClient();\r\n    HdfsDataOutputStream stream = routerClient.append(routerFile, 1024, createFlag, null, null);\r\n    stream.writeBytes(TEST_STRING);\r\n    stream.close();\r\n    FileStatus status = getFileStatus(nnFS, nnFile);\r\n    assertTrue(status.getLen() > TEST_STRING.length());\r\n    Method m = ClientProtocol.class.getMethod(\"append\", String.class, String.class, EnumSetWritable.class);\r\n    String badPath = \"/unknownlocation/unknowndir\";\r\n    EnumSetWritable<CreateFlag> createFlagWritable = new EnumSetWritable<CreateFlag>(createFlag);\r\n    compareResponses(routerProtocol, nnProtocol, m, new Object[] { badPath, \"testClient\", createFlagWritable });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyGetAdditionalDatanode",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testProxyGetAdditionalDatanode() throws IOException, InterruptedException, URISyntaxException\n{\r\n    EnumSet<CreateFlag> createFlag = EnumSet.of(CreateFlag.CREATE);\r\n    String clientName = getRouterContext().getClient().getClientName();\r\n    String newRouterFile = routerFile + \"_additionalDatanode\";\r\n    HdfsFileStatus status = routerProtocol.create(newRouterFile, new FsPermission(\"777\"), clientName, new EnumSetWritable<CreateFlag>(createFlag), true, (short) 1, (long) 1024, CryptoProtocolVersion.supported(), null, null);\r\n    LocatedBlock block = routerProtocol.addBlock(newRouterFile, clientName, null, null, status.getFileId(), null, null);\r\n    DatanodeInfo[] exclusions = DatanodeInfo.EMPTY_ARRAY;\r\n    LocatedBlock newBlock = routerProtocol.getAdditionalDatanode(newRouterFile, status.getFileId(), block.getBlock(), block.getLocations(), block.getStorageIDs(), exclusions, 1, clientName);\r\n    assertNotNull(newBlock);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyCreateFileAlternateUser",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testProxyCreateFileAlternateUser() throws IOException, URISyntaxException, InterruptedException\n{\r\n    String routerDir = cluster.getFederatedTestDirectoryForNS(ns);\r\n    String namenodeDir = cluster.getNamenodeTestDirectoryForNS(ns);\r\n    String newRouterFile = routerDir + \"/unknownuser\";\r\n    String newNamenodeFile = namenodeDir + \"/unknownuser\";\r\n    String username = \"unknownuser\";\r\n    namenode.getFileContext().setPermission(new Path(namenodeDir), new FsPermission(\"777\"));\r\n    UserGroupInformation ugi = UserGroupInformation.createRemoteUser(username);\r\n    DFSClient client = getRouterContext().getClient(ugi);\r\n    client.create(newRouterFile, true);\r\n    FileStatus status = getFileStatus(nnFS, newNamenodeFile);\r\n    assertEquals(status.getOwner(), username);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyGetFileInfoAcessException",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testProxyGetFileInfoAcessException() throws IOException\n{\r\n    UserGroupInformation ugi = UserGroupInformation.createRemoteUser(\"unknownuser\");\r\n    Exception nnFailure = null;\r\n    try {\r\n        String testFile = cluster.getNamenodeTestFileForNS(ns);\r\n        namenode.getClient(ugi).getLocatedBlocks(testFile, 0);\r\n    } catch (Exception e) {\r\n        nnFailure = e;\r\n    }\r\n    assertNotNull(nnFailure);\r\n    Exception routerFailure = null;\r\n    try {\r\n        String testFile = cluster.getFederatedTestFileForNS(ns);\r\n        getRouterContext().getClient(ugi).getLocatedBlocks(testFile, 0);\r\n    } catch (Exception e) {\r\n        routerFailure = e;\r\n    }\r\n    assertNotNull(routerFailure);\r\n    assertEquals(routerFailure.getClass(), nnFailure.getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyVersionRequest",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testProxyVersionRequest() throws Exception\n{\r\n    MockResolver resolver = (MockResolver) router.getRouter().getSubclusterResolver();\r\n    try {\r\n        NamespaceInfo rVersion = routerNamenodeProtocol.versionRequest();\r\n        NamespaceInfo nnVersion = nnNamenodeProtocol.versionRequest();\r\n        NamespaceInfo nnVersion1 = nnNamenodeProtocol1.versionRequest();\r\n        compareVersion(rVersion, nnVersion);\r\n        resolver.setDisableNamespace(true);\r\n        boolean isNN0 = rVersion.getBlockPoolID().equals(nnVersion.getBlockPoolID());\r\n        compareVersion(rVersion, isNN0 ? nnVersion : nnVersion1);\r\n    } finally {\r\n        resolver.setDisableNamespace(false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "compareVersion",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void compareVersion(NamespaceInfo rVersion, NamespaceInfo nnVersion)\n{\r\n    assertEquals(nnVersion.getBlockPoolID(), rVersion.getBlockPoolID());\r\n    assertEquals(nnVersion.getNamespaceID(), rVersion.getNamespaceID());\r\n    assertEquals(nnVersion.getClusterID(), rVersion.getClusterID());\r\n    assertEquals(nnVersion.getLayoutVersion(), rVersion.getLayoutVersion());\r\n    assertEquals(nnVersion.getCTime(), rVersion.getCTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyGetBlockKeys",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testProxyGetBlockKeys() throws Exception\n{\r\n    MockResolver resolver = (MockResolver) router.getRouter().getSubclusterResolver();\r\n    try {\r\n        ExportedBlockKeys rKeys = routerNamenodeProtocol.getBlockKeys();\r\n        ExportedBlockKeys nnKeys = nnNamenodeProtocol.getBlockKeys();\r\n        compareBlockKeys(rKeys, nnKeys);\r\n        resolver.setDisableNamespace(true);\r\n        rKeys = routerNamenodeProtocol.getBlockKeys();\r\n        compareBlockKeys(rKeys, nnKeys);\r\n    } finally {\r\n        resolver.setDisableNamespace(false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "compareBlockKeys",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void compareBlockKeys(ExportedBlockKeys rKeys, ExportedBlockKeys nnKeys)\n{\r\n    assertEquals(nnKeys.getCurrentKey(), rKeys.getCurrentKey());\r\n    assertEquals(nnKeys.getKeyUpdateInterval(), rKeys.getKeyUpdateInterval());\r\n    assertEquals(nnKeys.getTokenLifetime(), rKeys.getTokenLifetime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyGetBlocks",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testProxyGetBlocks() throws Exception\n{\r\n    DatanodeInfo[] dns = routerProtocol.getDatanodeReport(DatanodeReportType.ALL);\r\n    DatanodeInfo dn0 = dns[0];\r\n    BlocksWithLocations routerBlockLocations = routerNamenodeProtocol.getBlocks(dn0, 1024, 0, 0);\r\n    BlocksWithLocations nnBlockLocations = nnNamenodeProtocol.getBlocks(dn0, 1024, 0, 0);\r\n    BlockWithLocations[] routerBlocks = routerBlockLocations.getBlocks();\r\n    BlockWithLocations[] nnBlocks = nnBlockLocations.getBlocks();\r\n    assertEquals(nnBlocks.length, routerBlocks.length);\r\n    for (int i = 0; i < routerBlocks.length; i++) {\r\n        assertEquals(nnBlocks[i].getBlock().getBlockId(), routerBlocks[i].getBlock().getBlockId());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyGetTransactionID",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testProxyGetTransactionID() throws IOException\n{\r\n    MockResolver resolver = (MockResolver) router.getRouter().getSubclusterResolver();\r\n    try {\r\n        long routerTransactionID = routerNamenodeProtocol.getTransactionID();\r\n        long nnTransactionID = nnNamenodeProtocol.getTransactionID();\r\n        long nnTransactionID1 = nnNamenodeProtocol1.getTransactionID();\r\n        assertEquals(nnTransactionID, routerTransactionID);\r\n        resolver.setDisableNamespace(true);\r\n        routerTransactionID = routerNamenodeProtocol.getTransactionID();\r\n        assertThat(routerTransactionID).isIn(nnTransactionID, nnTransactionID1);\r\n    } finally {\r\n        resolver.setDisableNamespace(false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyGetMostRecentCheckpointTxId",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testProxyGetMostRecentCheckpointTxId() throws IOException\n{\r\n    MockResolver resolver = (MockResolver) router.getRouter().getSubclusterResolver();\r\n    try {\r\n        long routerCheckPointId = routerNamenodeProtocol.getMostRecentCheckpointTxId();\r\n        long nnCheckPointId = nnNamenodeProtocol.getMostRecentCheckpointTxId();\r\n        assertEquals(nnCheckPointId, routerCheckPointId);\r\n        resolver.setDisableNamespace(true);\r\n        routerCheckPointId = routerNamenodeProtocol.getMostRecentCheckpointTxId();\r\n    } finally {\r\n        resolver.setDisableNamespace(false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxySetSafemode",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testProxySetSafemode() throws Exception\n{\r\n    boolean routerSafemode = routerProtocol.setSafeMode(SafeModeAction.SAFEMODE_GET, false);\r\n    boolean nnSafemode = nnProtocol.setSafeMode(SafeModeAction.SAFEMODE_GET, false);\r\n    assertEquals(nnSafemode, routerSafemode);\r\n    routerSafemode = routerProtocol.setSafeMode(SafeModeAction.SAFEMODE_GET, true);\r\n    nnSafemode = nnProtocol.setSafeMode(SafeModeAction.SAFEMODE_GET, true);\r\n    assertEquals(nnSafemode, routerSafemode);\r\n    assertFalse(routerProtocol.setSafeMode(SafeModeAction.SAFEMODE_GET, false));\r\n    assertTrue(routerProtocol.setSafeMode(SafeModeAction.SAFEMODE_ENTER, false));\r\n    assertTrue(routerProtocol.setSafeMode(SafeModeAction.SAFEMODE_GET, false));\r\n    assertFalse(routerProtocol.setSafeMode(SafeModeAction.SAFEMODE_LEAVE, false));\r\n    assertFalse(routerProtocol.setSafeMode(SafeModeAction.SAFEMODE_GET, false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyRestoreFailedStorage",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testProxyRestoreFailedStorage() throws Exception\n{\r\n    boolean routerSuccess = routerProtocol.restoreFailedStorage(\"check\");\r\n    boolean nnSuccess = nnProtocol.restoreFailedStorage(\"check\");\r\n    assertEquals(nnSuccess, routerSuccess);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyExceptionMessages",
  "errType" : [ "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testProxyExceptionMessages() throws IOException\n{\r\n    MockResolver resolver = (MockResolver) router.getRouter().getSubclusterResolver();\r\n    String ns0 = cluster.getNameservices().get(0);\r\n    resolver.addLocation(\"/mnt\", ns0, \"/\");\r\n    try {\r\n        FsPermission permission = new FsPermission(\"777\");\r\n        routerProtocol.mkdirs(\"/mnt/folder0/folder1\", permission, false);\r\n        fail(\"mkdirs for non-existing parent folder should have failed\");\r\n    } catch (IOException ioe) {\r\n        assertExceptionContains(\"/mnt/folder0\", ioe, \"Wrong path in exception for mkdirs\");\r\n    }\r\n    try {\r\n        FsPermission permission = new FsPermission(\"777\");\r\n        routerProtocol.setPermission(\"/mnt/testfile.txt\", permission);\r\n        fail(\"setPermission for non-existing file should have failed\");\r\n    } catch (IOException ioe) {\r\n        assertExceptionContains(\"/mnt/testfile.txt\", ioe, \"Wrong path in exception for setPermission\");\r\n    }\r\n    try {\r\n        FsPermission permission = new FsPermission(\"777\");\r\n        routerProtocol.mkdirs(\"/mnt/folder0/folder1\", permission, false);\r\n        routerProtocol.delete(\"/mnt/folder0\", false);\r\n        fail(\"delete for non-existing file should have failed\");\r\n    } catch (IOException ioe) {\r\n        assertExceptionContains(\"/mnt/folder0\", ioe, \"Wrong path in exception for delete\");\r\n    }\r\n    resolver.cleanRegistrations();\r\n    assertEquals(\"Parent directory doesn't exist: /ns1/a/a/b\", RouterRpcClient.processExceptionMsg(\"Parent directory doesn't exist: /a/a/b\", \"/a\", \"/ns1/a\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetReplicatedBlockStats",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testGetReplicatedBlockStats() throws Exception\n{\r\n    String testFile = \"/test-file\";\r\n    for (String nsid : cluster.getNameservices()) {\r\n        NamenodeContext context = cluster.getNamenode(nsid, null);\r\n        NameNode nameNode = context.getNamenode();\r\n        FSNamesystem namesystem = nameNode.getNamesystem();\r\n        BlockManager bm = namesystem.getBlockManager();\r\n        FileSystem fileSystem = context.getFileSystem();\r\n        createFile(fileSystem, testFile, 1024);\r\n        LocatedBlock block = NameNodeAdapter.getBlockLocations(nameNode, testFile, 0, 1024).get(0);\r\n        namesystem.writeLock();\r\n        bm.findAndMarkBlockAsCorrupt(block.getBlock(), block.getLocations()[0], \"STORAGE_ID\", \"TEST\");\r\n        namesystem.writeUnlock();\r\n        BlockManagerTestUtil.updateState(bm);\r\n        DFSTestUtil.waitCorruptReplicas(fileSystem, namesystem, new Path(testFile), block.getBlock(), 1);\r\n        ReplicatedBlockStats stats = context.getClient().getNamenode().getReplicatedBlockStats();\r\n        assertEquals(1, stats.getCorruptBlocks());\r\n    }\r\n    ReplicatedBlockStats routerStat = routerProtocol.getReplicatedBlockStats();\r\n    assertEquals(\"There should be 1 corrupt blocks for each NN\", cluster.getNameservices().size(), routerStat.getCorruptBlocks());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testErasureCoding",
  "errType" : null,
  "containingMethodsNum" : 61,
  "sourceCodeText" : "void testErasureCoding() throws Exception\n{\r\n    LOG.info(\"List the available erasurce coding policies\");\r\n    ErasureCodingPolicyInfo[] policies = checkErasureCodingPolicies();\r\n    for (ErasureCodingPolicyInfo policy : policies) {\r\n        LOG.info(\"  {}\", policy);\r\n    }\r\n    LOG.info(\"List the erasure coding codecs\");\r\n    Map<String, String> codecsRouter = routerProtocol.getErasureCodingCodecs();\r\n    Map<String, String> codecsNamenode = nnProtocol.getErasureCodingCodecs();\r\n    assertTrue(Maps.difference(codecsRouter, codecsNamenode).areEqual());\r\n    for (Entry<String, String> entry : codecsRouter.entrySet()) {\r\n        LOG.info(\"  {}: {}\", entry.getKey(), entry.getValue());\r\n    }\r\n    LOG.info(\"Create a testing directory via the router at the root level\");\r\n    String dirPath = \"/testec\";\r\n    String filePath1 = dirPath + \"/testfile1\";\r\n    FsPermission permission = new FsPermission(\"755\");\r\n    routerProtocol.mkdirs(dirPath, permission, false);\r\n    createFile(routerFS, filePath1, 32);\r\n    assertTrue(verifyFileExists(routerFS, filePath1));\r\n    DFSClient file1Protocol = getFileDFSClient(filePath1);\r\n    LOG.info(\"The policy for the new file should not be set\");\r\n    assertNull(routerProtocol.getErasureCodingPolicy(filePath1));\r\n    assertNull(file1Protocol.getErasureCodingPolicy(filePath1));\r\n    String policyName = \"RS-6-3-1024k\";\r\n    LOG.info(\"Set policy \\\"{}\\\" for \\\"{}\\\"\", policyName, dirPath);\r\n    routerProtocol.setErasureCodingPolicy(dirPath, policyName);\r\n    String filePath2 = dirPath + \"/testfile2\";\r\n    LOG.info(\"Create {} in the path with the new EC policy\", filePath2);\r\n    createFile(routerFS, filePath2, 32);\r\n    assertTrue(verifyFileExists(routerFS, filePath2));\r\n    DFSClient file2Protocol = getFileDFSClient(filePath2);\r\n    LOG.info(\"Check that the policy is set for {}\", filePath2);\r\n    ErasureCodingPolicy policyRouter1 = routerProtocol.getErasureCodingPolicy(filePath2);\r\n    ErasureCodingPolicy policyNamenode1 = file2Protocol.getErasureCodingPolicy(filePath2);\r\n    assertNotNull(policyRouter1);\r\n    assertEquals(policyName, policyRouter1.getName());\r\n    assertEquals(policyName, policyNamenode1.getName());\r\n    LOG.info(\"Create a new erasure coding policy\");\r\n    String newPolicyName = \"RS-6-3-128k\";\r\n    ECSchema ecSchema = new ECSchema(ErasureCodeConstants.RS_CODEC_NAME, 6, 3);\r\n    ErasureCodingPolicy ecPolicy = new ErasureCodingPolicy(newPolicyName, ecSchema, 128 * 1024, (byte) -1);\r\n    ErasureCodingPolicy[] newPolicies = new ErasureCodingPolicy[] { ecPolicy };\r\n    AddErasureCodingPolicyResponse[] responses = routerProtocol.addErasureCodingPolicies(newPolicies);\r\n    assertEquals(1, responses.length);\r\n    assertTrue(responses[0].isSucceed());\r\n    routerProtocol.disableErasureCodingPolicy(newPolicyName);\r\n    LOG.info(\"The new policy should be there and disabled\");\r\n    policies = checkErasureCodingPolicies();\r\n    boolean found = false;\r\n    for (ErasureCodingPolicyInfo policy : policies) {\r\n        LOG.info(\"  {}\" + policy);\r\n        if (policy.getPolicy().getName().equals(newPolicyName)) {\r\n            found = true;\r\n            assertEquals(ErasureCodingPolicyState.DISABLED, policy.getState());\r\n            break;\r\n        }\r\n    }\r\n    assertTrue(found);\r\n    LOG.info(\"Set the test folder to use the new policy\");\r\n    routerProtocol.enableErasureCodingPolicy(newPolicyName);\r\n    routerProtocol.setErasureCodingPolicy(dirPath, newPolicyName);\r\n    LOG.info(\"Create a file in the path with the new EC policy\");\r\n    String filePath3 = dirPath + \"/testfile3\";\r\n    createFile(routerFS, filePath3, 32);\r\n    assertTrue(verifyFileExists(routerFS, filePath3));\r\n    DFSClient file3Protocol = getFileDFSClient(filePath3);\r\n    ErasureCodingPolicy policyRouterFile3 = routerProtocol.getErasureCodingPolicy(filePath3);\r\n    assertEquals(newPolicyName, policyRouterFile3.getName());\r\n    ErasureCodingPolicy policyNamenodeFile3 = file3Protocol.getErasureCodingPolicy(filePath3);\r\n    assertEquals(newPolicyName, policyNamenodeFile3.getName());\r\n    LOG.info(\"Remove the policy and check the one for the test folder\");\r\n    routerProtocol.removeErasureCodingPolicy(newPolicyName);\r\n    ErasureCodingPolicy policyRouter3 = routerProtocol.getErasureCodingPolicy(filePath3);\r\n    assertEquals(newPolicyName, policyRouter3.getName());\r\n    ErasureCodingPolicy policyNamenode3 = file3Protocol.getErasureCodingPolicy(filePath3);\r\n    assertEquals(newPolicyName, policyNamenode3.getName());\r\n    LOG.info(\"Check the stats\");\r\n    ECBlockGroupStats statsRouter = routerProtocol.getECBlockGroupStats();\r\n    ECBlockGroupStats statsNamenode = getNamenodeECBlockGroupStats();\r\n    assertEquals(statsNamenode, statsRouter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodeECBlockGroupStats",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ECBlockGroupStats getNamenodeECBlockGroupStats() throws Exception\n{\r\n    List<ECBlockGroupStats> nnStats = new ArrayList<>();\r\n    for (NamenodeContext nnContext : cluster.getNamenodes()) {\r\n        ClientProtocol cp = nnContext.getClient().getNamenode();\r\n        nnStats.add(cp.getECBlockGroupStats());\r\n    }\r\n    return ECBlockGroupStats.merge(nnStats);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetCurrentTXIDandRollEdits",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGetCurrentTXIDandRollEdits() throws IOException\n{\r\n    Long rollEdits = routerProtocol.rollEdits();\r\n    Long currentTXID = routerProtocol.getCurrentEditLogTxid();\r\n    assertEquals(rollEdits, currentTXID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSaveNamespace",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSaveNamespace() throws IOException\n{\r\n    cluster.getCluster().getFileSystem(0).setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER);\r\n    cluster.getCluster().getFileSystem(1).setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER);\r\n    Boolean saveNamespace = routerProtocol.saveNamespace(0, 0);\r\n    assertTrue(saveNamespace);\r\n    cluster.getCluster().getFileSystem(0).setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE);\r\n    cluster.getCluster().getFileSystem(1).setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNamenodeMetrics",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testNamenodeMetrics() throws Exception\n{\r\n    final NamenodeBeanMetrics metrics = router.getRouter().getNamenodeMetrics();\r\n    final String jsonString0 = metrics.getLiveNodes();\r\n    JSONObject jsonObject = new JSONObject(jsonString0);\r\n    assertEquals(NUM_SUBCLUSTERS * NUM_DNS, jsonObject.names().length());\r\n    String jsonString1 = metrics.getLiveNodes();\r\n    assertEquals(jsonString0, jsonString1);\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            return !jsonString0.equals(metrics.getLiveNodes());\r\n        }\r\n    }, 500, 5 * 1000);\r\n    final String jsonString2 = metrics.getLiveNodes();\r\n    assertNotEquals(jsonString0, jsonString2);\r\n    MockResolver resolver = (MockResolver) router.getRouter().getNamenodeResolver();\r\n    resolver.cleanRegistrations();\r\n    resolver.setDisableRegistration(true);\r\n    try {\r\n        GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n            @Override\r\n            public Boolean get() {\r\n                return !jsonString2.equals(metrics.getLiveNodes());\r\n            }\r\n        }, 500, 5 * 1000);\r\n        assertEquals(\"{}\", metrics.getLiveNodes());\r\n    } finally {\r\n        resolver.setDisableRegistration(false);\r\n        cluster.registerNamenodes();\r\n        cluster.waitNamenodeRegistration();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRBFMetricsMethodsRelayOnStateStore",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRBFMetricsMethodsRelayOnStateStore()\n{\r\n    assertNull(router.getRouter().getStateStore());\r\n    RBFMetrics metrics = router.getRouter().getMetrics();\r\n    assertEquals(\"{}\", metrics.getNamenodes());\r\n    assertEquals(\"[]\", metrics.getMountTable());\r\n    assertEquals(\"{}\", metrics.getRouters());\r\n    assertEquals(0, metrics.getNumNamenodes());\r\n    assertEquals(0, metrics.getNumExpiredNamenodes());\r\n    assertEquals(\"[]\", metrics.getClusterId());\r\n    assertEquals(\"[]\", metrics.getBlockPoolId());\r\n    assertEquals(\"{}\", metrics.getNameservices());\r\n    assertEquals(0, metrics.getNumLiveNodes());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNamenodeMetricsEnteringMaintenanceNodes",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testNamenodeMetricsEnteringMaintenanceNodes() throws IOException\n{\r\n    final NamenodeBeanMetrics metrics = router.getRouter().getNamenodeMetrics();\r\n    assertEquals(\"{}\", metrics.getEnteringMaintenanceNodes());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testCacheAdmin",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testCacheAdmin() throws Exception\n{\r\n    DistributedFileSystem routerDFS = (DistributedFileSystem) routerFS;\r\n    CachePoolInfo cpInfo = new CachePoolInfo(\"Check\");\r\n    cpInfo.setOwnerName(\"Owner\");\r\n    routerProtocol.addCachePool(cpInfo);\r\n    RemoteIterator<CachePoolEntry> iter = routerDFS.listCachePools();\r\n    assertTrue(iter.hasNext());\r\n    CachePoolInfo info = iter.next().getInfo();\r\n    assertEquals(\"Owner\", info.getOwnerName());\r\n    cpInfo.setOwnerName(\"new Owner\");\r\n    routerProtocol.modifyCachePool(cpInfo);\r\n    iter = routerDFS.listCachePools();\r\n    assertTrue(iter.hasNext());\r\n    info = iter.next().getInfo();\r\n    assertEquals(\"new Owner\", info.getOwnerName());\r\n    routerProtocol.removeCachePool(\"Check\");\r\n    iter = routerDFS.listCachePools();\r\n    assertFalse(iter.hasNext());\r\n    cpInfo.setOwnerName(\"Owner\");\r\n    routerProtocol.addCachePool(cpInfo);\r\n    routerDFS.mkdirs(new Path(\"/ns1/dir\"));\r\n    CacheDirectiveInfo cacheDir = new CacheDirectiveInfo.Builder().setPath(new Path(\"/ns1/dir\")).setReplication((short) 1).setPool(\"Check\").build();\r\n    long id = routerDFS.addCacheDirective(cacheDir);\r\n    CacheDirectiveInfo filter = new CacheDirectiveInfo.Builder().setPath(new Path(\"/ns1/dir\")).build();\r\n    assertTrue(routerDFS.listCacheDirectives(filter).hasNext());\r\n    assertEquals(\"Check\", routerDFS.listCacheDirectives(filter).next().getInfo().getPool());\r\n    cacheDir = new CacheDirectiveInfo.Builder().setReplication((short) 2).setId(id).setPath(new Path(\"/ns1/dir\")).build();\r\n    routerDFS.modifyCacheDirective(cacheDir);\r\n    assertEquals((short) 2, (short) routerDFS.listCacheDirectives(filter).next().getInfo().getReplication());\r\n    routerDFS.removeCacheDirective(id);\r\n    assertFalse(routerDFS.listCacheDirectives(filter).hasNext());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testgetGroupsForUser",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testgetGroupsForUser() throws IOException\n{\r\n    String[] group = new String[] { \"bar\", \"group2\" };\r\n    UserGroupInformation.createUserForTesting(\"user\", new String[] { \"bar\", \"group2\" });\r\n    String[] result = router.getRouter().getRpcServer().getGroupsForUser(\"user\");\r\n    assertArrayEquals(group, result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetCachedDatanodeReport",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testGetCachedDatanodeReport() throws Exception\n{\r\n    RouterRpcServer rpcServer = router.getRouter().getRpcServer();\r\n    final DatanodeInfo[] datanodeReport = rpcServer.getCachedDatanodeReport(DatanodeReportType.LIVE);\r\n    assertEquals(12, datanodeReport.length);\r\n    DatanodeInfo[] datanodeReport1 = rpcServer.getCachedDatanodeReport(DatanodeReportType.LIVE);\r\n    assertArrayEquals(datanodeReport1, datanodeReport);\r\n    MiniDFSCluster miniDFSCluster = getCluster().getCluster();\r\n    DataNodeProperties dnprop = miniDFSCluster.stopDataNode(0);\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            DatanodeInfo[] dn = null;\r\n            try {\r\n                dn = rpcServer.getCachedDatanodeReport(DatanodeReportType.LIVE);\r\n            } catch (IOException ex) {\r\n                LOG.error(\"Error on getCachedDatanodeReport\");\r\n            }\r\n            return !Arrays.equals(datanodeReport, dn);\r\n        }\r\n    }, 500, 5 * 1000);\r\n    final DatanodeInfo[] datanodeReport2 = rpcServer.getCachedDatanodeReport(DatanodeReportType.LIVE);\r\n    assertEquals(datanodeReport.length - 1, datanodeReport2.length);\r\n    miniDFSCluster.restartDataNode(dnprop);\r\n    miniDFSCluster.waitActive();\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            DatanodeInfo[] dn = null;\r\n            try {\r\n                dn = rpcServer.getCachedDatanodeReport(DatanodeReportType.LIVE);\r\n            } catch (IOException ex) {\r\n                LOG.error(\"Error on getCachedDatanodeReport\");\r\n            }\r\n            return datanodeReport.length == dn.length;\r\n        }\r\n    }, 100, 10 * 1000);\r\n    final DatanodeInfo[] datanodeReport3 = rpcServer.getCachedDatanodeReport(DatanodeReportType.LIVE);\r\n    assertEquals(datanodeReport.length, datanodeReport3.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkErasureCodingPolicies",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ErasureCodingPolicyInfo[] checkErasureCodingPolicies() throws IOException\n{\r\n    ErasureCodingPolicyInfo[] policiesRouter = routerProtocol.getErasureCodingPolicies();\r\n    assertNotNull(policiesRouter);\r\n    ErasureCodingPolicyInfo[] policiesNamenode = nnProtocol.getErasureCodingPolicies();\r\n    Arrays.sort(policiesRouter, EC_POLICY_CMP);\r\n    Arrays.sort(policiesNamenode, EC_POLICY_CMP);\r\n    assertArrayEquals(policiesRouter, policiesNamenode);\r\n    return policiesRouter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getFileDFSClient",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "DFSClient getFileDFSClient(final String path)\n{\r\n    for (String nsId : cluster.getNameservices()) {\r\n        LOG.info(\"Checking {} for {}\", nsId, path);\r\n        NamenodeContext nn = cluster.getNamenode(nsId, null);\r\n        try {\r\n            DFSClient nnClientProtocol = nn.getClient();\r\n            if (nnClientProtocol.getFileInfo(path) != null) {\r\n                return nnClientProtocol;\r\n            }\r\n        } catch (Exception ignore) {\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMkdirsWithCallerContext",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testMkdirsWithCallerContext() throws IOException\n{\r\n    GenericTestUtils.LogCapturer auditlog = GenericTestUtils.LogCapturer.captureLogs(FSNamesystem.auditLog);\r\n    assertNull(CallerContext.getCurrent());\r\n    CallerContext.setCurrent(new CallerContext.Builder(\"clientContext\").build());\r\n    String dirPath = \"/test_dir_with_callercontext\";\r\n    FsPermission permission = new FsPermission(\"755\");\r\n    routerProtocol.mkdirs(dirPath, permission, false);\r\n    final String logOutput = auditlog.getOutput();\r\n    assertTrue(logOutput.contains(\"callerContext=clientIp:\"));\r\n    assertTrue(logOutput.contains(\",clientContext\"));\r\n    assertTrue(verifyFileExists(routerFS, dirPath));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSetBalancerBandwidth",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSetBalancerBandwidth() throws Exception\n{\r\n    long defaultBandwidth = DFSConfigKeys.DFS_DATANODE_BALANCE_BANDWIDTHPERSEC_DEFAULT;\r\n    long newBandwidth = defaultBandwidth * 2;\r\n    routerProtocol.setBalancerBandwidth(newBandwidth);\r\n    ArrayList<DataNode> datanodes = cluster.getCluster().getDataNodes();\r\n    GenericTestUtils.waitFor(() -> {\r\n        return datanodes.get(0).getBalancerBandwidth() == newBandwidth;\r\n    }, 100, 60 * 1000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testAddClientIpPortToCallerContext",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testAddClientIpPortToCallerContext() throws IOException\n{\r\n    GenericTestUtils.LogCapturer auditLog = GenericTestUtils.LogCapturer.captureLogs(FSNamesystem.auditLog);\r\n    CallerContext.setCurrent(new CallerContext.Builder(\"clientContext\").build());\r\n    String dirPath = \"/test\";\r\n    routerProtocol.mkdirs(dirPath, new FsPermission(\"755\"), false);\r\n    assertTrue(auditLog.getOutput().contains(\"clientIp:\"));\r\n    assertTrue(auditLog.getOutput().contains(\"clientPort:\"));\r\n    assertTrue(verifyFileExists(routerFS, dirPath));\r\n    auditLog.clearOutput();\r\n    CallerContext.setCurrent(new CallerContext.Builder(\"clientContext,clientIp:1.1.1.1,clientPort:1234\").build());\r\n    routerProtocol.getFileInfo(dirPath);\r\n    assertFalse(auditLog.getOutput().contains(\"clientIp:1.1.1.1\"));\r\n    assertFalse(auditLog.getOutput().contains(\"clientPort:1234\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    LOG.info(\"Start the Namenodes\");\r\n    Configuration nnConf = new HdfsConfiguration();\r\n    nnConf.setInt(DFSConfigKeys.DFS_NAMENODE_HANDLER_COUNT_KEY, 10);\r\n    for (final String nsId : asList(\"ns0\", \"ns1\")) {\r\n        MockNamenode nn = new MockNamenode(nsId, nnConf);\r\n        nn.transitionToActive();\r\n        nn.addFileSystemMock();\r\n        namenodes.put(nsId, nn);\r\n    }\r\n    LOG.info(\"Start the Routers\");\r\n    Configuration routerConf = new RouterConfigBuilder().stateStore().admin().rpc().build();\r\n    routerConf.set(RBFConfigKeys.DFS_ROUTER_RPC_ADDRESS_KEY, \"0.0.0.0:0\");\r\n    routerConf.set(RBFConfigKeys.DFS_ROUTER_HTTP_ADDRESS_KEY, \"0.0.0.0:0\");\r\n    routerConf.set(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, \"0.0.0.0:0\");\r\n    routerConf.setTimeDuration(RBFConfigKeys.DFS_ROUTER_CLIENT_CONNECT_TIMEOUT, 500, TimeUnit.MILLISECONDS);\r\n    Configuration stateStoreConf = getStateStoreConfiguration();\r\n    stateStoreConf.setClass(RBFConfigKeys.FEDERATION_NAMENODE_RESOLVER_CLIENT_CLASS, MembershipNamenodeResolver.class, ActiveNamenodeResolver.class);\r\n    stateStoreConf.setClass(RBFConfigKeys.FEDERATION_FILE_RESOLVER_CLIENT_CLASS, MultipleDestinationMountTableResolver.class, FileSubclusterResolver.class);\r\n    routerConf.addResource(stateStoreConf);\r\n    for (int i = 0; i < NUM_ROUTERS; i++) {\r\n        routerConf.setBoolean(RBFConfigKeys.DFS_ROUTER_ALLOW_PARTIAL_LIST, i != 0);\r\n        final Router router = new Router();\r\n        router.init(routerConf);\r\n        router.start();\r\n        routers.add(router);\r\n    }\r\n    LOG.info(\"Registering the subclusters in the Routers\");\r\n    registerSubclusters(routers, namenodes.values(), Collections.singleton(\"ns1\"));\r\n    service = Executors.newFixedThreadPool(10);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void cleanup() throws Exception\n{\r\n    LOG.info(\"Stopping the cluster\");\r\n    for (final MockNamenode nn : namenodes.values()) {\r\n        nn.stop();\r\n    }\r\n    namenodes.clear();\r\n    routers.forEach(router -> router.stop());\r\n    routers.clear();\r\n    if (service != null) {\r\n        service.shutdown();\r\n        service = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "updateMountPointFaultTolerant",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void updateMountPointFaultTolerant(final String mountPoint) throws IOException\n{\r\n    Router router = getRandomRouter();\r\n    RouterClient admin = getAdminClient(router);\r\n    MountTableManager mountTable = admin.getMountTableManager();\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(mountPoint);\r\n    GetMountTableEntriesResponse entries = mountTable.getMountTableEntries(getRequest);\r\n    MountTable updateEntry = entries.getEntries().get(0);\r\n    updateEntry.setFaultTolerant(true);\r\n    UpdateMountTableEntryRequest updateRequest = UpdateMountTableEntryRequest.newInstance(updateEntry);\r\n    UpdateMountTableEntryResponse updateResponse = mountTable.updateMountTableEntry(updateRequest);\r\n    assertTrue(updateResponse.getStatus());\r\n    refreshRoutersCaches(routers);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testWriteWithFailedSubcluster",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testWriteWithFailedSubcluster() throws Exception\n{\r\n    LOG.info(\"Stop ns1 to simulate an unavailable subcluster\");\r\n    namenodes.get(\"ns1\").stop();\r\n    final List<Callable<Boolean>> tasks = new ArrayList<>();\r\n    final List<DestinationOrder> orders = asList(DestinationOrder.HASH_ALL, DestinationOrder.SPACE, DestinationOrder.RANDOM, DestinationOrder.HASH);\r\n    for (DestinationOrder order : orders) {\r\n        tasks.add(() -> {\r\n            testWriteWithFailedSubcluster(order);\r\n            return true;\r\n        });\r\n    }\r\n    TaskResults results = collectResults(\"Full tests\", tasks);\r\n    assertEquals(orders.size(), results.getSuccess());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testWriteWithFailedSubcluster",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testWriteWithFailedSubcluster(final DestinationOrder order) throws Exception\n{\r\n    final FileSystem router0Fs = getFileSystem(routers.get(0));\r\n    final FileSystem router1Fs = getFileSystem(routers.get(1));\r\n    final FileSystem ns0Fs = getFileSystem(namenodes.get(\"ns0\").getRPCPort());\r\n    final String mountPoint = \"/\" + order + \"-failsubcluster\";\r\n    final Path mountPath = new Path(mountPoint);\r\n    LOG.info(\"Setup {} with order {}\", mountPoint, order);\r\n    createMountTableEntry(getRandomRouter(), mountPoint, order, namenodes.keySet());\r\n    refreshRoutersCaches(routers);\r\n    LOG.info(\"Write in {} should succeed writing in ns0 and fail for ns1\", mountPath);\r\n    checkDirectoriesFaultTolerant(mountPath, order, router0Fs, router1Fs, ns0Fs, false);\r\n    checkFilesFaultTolerant(mountPath, order, router0Fs, router1Fs, ns0Fs, false);\r\n    LOG.info(\"Make {} fault tolerant and everything succeeds\", mountPath);\r\n    IOException ioe = null;\r\n    try {\r\n        updateMountPointFaultTolerant(mountPoint);\r\n    } catch (IOException e) {\r\n        ioe = e;\r\n    }\r\n    if (DestinationOrder.FOLDER_ALL.contains(order)) {\r\n        assertNull(ioe);\r\n        checkDirectoriesFaultTolerant(mountPath, order, router0Fs, router1Fs, ns0Fs, true);\r\n        checkFilesFaultTolerant(mountPath, order, router0Fs, router1Fs, ns0Fs, true);\r\n    } else {\r\n        assertTrue(ioe.getMessage().startsWith(\"Invalid entry, fault tolerance only supported for ALL order\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkDirectoriesFaultTolerant",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void checkDirectoriesFaultTolerant(Path mountPoint, DestinationOrder order, FileSystem router0Fs, FileSystem router1Fs, FileSystem ns0Fs, boolean faultTolerant) throws Exception\n{\r\n    final FileStatus[] dirs0 = listStatus(router1Fs, mountPoint);\r\n    LOG.info(\"Create directories in {}\", mountPoint);\r\n    final List<Callable<Boolean>> tasks = new ArrayList<>();\r\n    for (int i = 0; i < NUM_FILES; i++) {\r\n        final Path dir = new Path(mountPoint, String.format(\"dir-%s-%03d\", faultTolerant, i));\r\n        FileSystem fs = getRandomRouterFileSystem();\r\n        tasks.add(getDirCreateTask(fs, dir));\r\n    }\r\n    TaskResults results = collectResults(\"Create dir \" + mountPoint, tasks);\r\n    LOG.info(\"Check directories results for {}: {}\", mountPoint, results);\r\n    if (faultTolerant || DestinationOrder.FOLDER_ALL.contains(order)) {\r\n        assertEquals(NUM_FILES, results.getSuccess());\r\n        assertEquals(0, results.getFailure());\r\n    } else {\r\n        assertBothResults(\"check dir \" + mountPoint, NUM_FILES, results);\r\n    }\r\n    LOG.info(\"Check directories listing for {}\", mountPoint);\r\n    tasks.add(getListFailTask(router0Fs, mountPoint));\r\n    int filesExpected = dirs0.length + results.getSuccess();\r\n    tasks.add(getListSuccessTask(router1Fs, mountPoint, filesExpected));\r\n    results = collectResults(\"List \" + mountPoint, tasks);\r\n    assertEquals(\"Failed listing\", 2, results.getSuccess());\r\n    tasks.add(getContentSummaryFailTask(router0Fs, mountPoint));\r\n    tasks.add(getContentSummarySuccessTask(router1Fs, mountPoint, filesExpected));\r\n    results = collectResults(\"Content summary \" + mountPoint, tasks);\r\n    assertEquals(\"Failed content summary\", 2, results.getSuccess());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkFilesFaultTolerant",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void checkFilesFaultTolerant(Path mountPoint, DestinationOrder order, FileSystem router0Fs, FileSystem router1Fs, FileSystem ns0Fs, boolean faultTolerant) throws Exception\n{\r\n    final FileStatus[] dirs0 = listStatus(router1Fs, mountPoint);\r\n    final Path dir0 = Path.getPathWithoutSchemeAndAuthority(dirs0[0].getPath());\r\n    LOG.info(\"Create files in {}\", dir0);\r\n    final List<Callable<Boolean>> tasks = new ArrayList<>();\r\n    for (int i = 0; i < NUM_FILES; i++) {\r\n        final String newFile = String.format(\"%s/file-%03d.txt\", dir0, i);\r\n        FileSystem fs = getRandomRouterFileSystem();\r\n        tasks.add(getFileCreateTask(fs, newFile, ns0Fs));\r\n    }\r\n    TaskResults results = collectResults(\"Create file \" + dir0, tasks);\r\n    LOG.info(\"Check files results for {}: {}\", dir0, results);\r\n    if (faultTolerant) {\r\n        assertEquals(\"Not enough success in \" + mountPoint, NUM_FILES, results.getSuccess());\r\n        assertEquals(\"Nothing should fail in \" + mountPoint, 0, results.getFailure());\r\n    } else {\r\n        assertEquals(\"Nothing should succeed in \" + mountPoint, 0, results.getSuccess());\r\n        assertEquals(\"Everything should fail in \" + mountPoint, NUM_FILES, results.getFailure());\r\n    }\r\n    LOG.info(\"Check files listing for {}\", dir0);\r\n    tasks.add(getListFailTask(router0Fs, dir0));\r\n    tasks.add(getListSuccessTask(router1Fs, dir0, results.getSuccess()));\r\n    assertEquals(2, collectResults(\"List \" + dir0, tasks).getSuccess());\r\n    tasks.add(getContentSummaryFailTask(router0Fs, dir0));\r\n    tasks.add(getContentSummarySuccessTask(router1Fs, dir0, results.getSuccess()));\r\n    results = collectResults(\"Content summary \" + dir0, tasks);\r\n    assertEquals(2, results.getSuccess());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String toString(final FileStatus[] files)\n{\r\n    final StringBuilder sb = new StringBuilder();\r\n    sb.append(\"[\");\r\n    for (final FileStatus file : files) {\r\n        if (sb.length() > 1) {\r\n            sb.append(\", \");\r\n        }\r\n        sb.append(Path.getPathWithoutSchemeAndAuthority(file.getPath()));\r\n    }\r\n    sb.append(\"]\");\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listStatus",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileStatus[] listStatus(final FileSystem fs, final Path path) throws IOException\n{\r\n    FileStatus[] files = new FileStatus[] {};\r\n    try {\r\n        files = fs.listStatus(path);\r\n    } catch (FileNotFoundException fnfe) {\r\n        LOG.debug(\"File not found: {}\", fnfe.getMessage());\r\n    }\r\n    return files;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getFileCreateTask",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Callable<Boolean> getFileCreateTask(final FileSystem fs, final String file, FileSystem checkFs)\n{\r\n    return () -> {\r\n        try {\r\n            Path path = new Path(file);\r\n            FSDataOutputStream os = fs.create(path);\r\n            os.close();\r\n            FileStatus fileStatus = checkFs.getFileStatus(path);\r\n            assertTrue(\"File not created properly: \" + fileStatus, fileStatus.getLen() > 0);\r\n            return true;\r\n        } catch (RemoteException re) {\r\n            return false;\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDirCreateTask",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Callable<Boolean> getDirCreateTask(final FileSystem fs, final Path dir)\n{\r\n    return () -> {\r\n        try {\r\n            fs.mkdirs(dir);\r\n            return true;\r\n        } catch (RemoteException re) {\r\n            return false;\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getListFailTask",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Callable<Boolean> getListFailTask(FileSystem fs, Path path)\n{\r\n    return () -> {\r\n        try {\r\n            fs.listStatus(path);\r\n            return false;\r\n        } catch (RemoteException re) {\r\n            return true;\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getListSuccessTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Callable<Boolean> getListSuccessTask(FileSystem fs, Path path, int expected)\n{\r\n    return () -> {\r\n        final FileStatus[] dirs = fs.listStatus(path);\r\n        assertEquals(toString(dirs), expected, dirs.length);\r\n        return true;\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getContentSummaryFailTask",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Callable<Boolean> getContentSummaryFailTask(FileSystem fs, Path path)\n{\r\n    return () -> {\r\n        try {\r\n            fs.getContentSummary(path);\r\n            return false;\r\n        } catch (RemoteException re) {\r\n            return true;\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getContentSummarySuccessTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Callable<Boolean> getContentSummarySuccessTask(FileSystem fs, Path path, int expected)\n{\r\n    return () -> {\r\n        ContentSummary summary = fs.getContentSummary(path);\r\n        assertEquals(\"Wrong summary for \" + path, expected, summary.getFileAndDirectoryCount());\r\n        return true;\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "collectResults",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskResults collectResults(final String tag, final Collection<Callable<Boolean>> tasks) throws Exception\n{\r\n    final TaskResults results = new TaskResults();\r\n    service.invokeAll(tasks).forEach(task -> {\r\n        try {\r\n            boolean succeeded = task.get();\r\n            if (succeeded) {\r\n                LOG.info(\"Got success for {}\", tag);\r\n                results.incrSuccess();\r\n            } else {\r\n                LOG.info(\"Got failure for {}\", tag);\r\n                results.incrFailure();\r\n            }\r\n        } catch (Exception e) {\r\n            StringWriter stackTrace = new StringWriter();\r\n            PrintWriter writer = new PrintWriter(stackTrace);\r\n            if (e instanceof ExecutionException) {\r\n                e.getCause().printStackTrace(writer);\r\n            } else {\r\n                e.printStackTrace(writer);\r\n            }\r\n            fail(\"Failed to run \\\"\" + tag + \"\\\": \" + stackTrace);\r\n        }\r\n    });\r\n    tasks.clear();\r\n    return results;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "assertBothResults",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertBothResults(String msg, int expected, TaskResults actual)\n{\r\n    assertEquals(msg, expected, actual.getTotal());\r\n    assertTrue(\"Expected some success for \" + msg, actual.getSuccess() > 0);\r\n    assertTrue(\"Expected some failure for \" + msg, actual.getFailure() > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRandomRouter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Router getRandomRouter()\n{\r\n    Random rnd = new Random();\r\n    int index = rnd.nextInt(routers.size());\r\n    return routers.get(index);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRandomRouterFileSystem",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FileSystem getRandomRouterFileSystem() throws Exception\n{\r\n    final UserGroupInformation userUgi = UserGroupInformation.createUserForTesting(\"user-\" + UUID.randomUUID(), new String[] { \"group\" });\r\n    Router router = getRandomRouter();\r\n    return userUgi.doAs((PrivilegedExceptionAction<FileSystem>) () -> getFileSystem(router));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testReadWithFailedSubcluster",
  "errType" : [ "FileNotFoundException", "RemoteException" ],
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testReadWithFailedSubcluster() throws Exception\n{\r\n    DestinationOrder order = DestinationOrder.HASH_ALL;\r\n    final String mountPoint = \"/\" + order + \"-testread\";\r\n    final Path mountPath = new Path(mountPoint);\r\n    LOG.info(\"Setup {} with order {}\", mountPoint, order);\r\n    createMountTableEntry(routers, mountPoint, order, namenodes.keySet());\r\n    FileSystem fs = getRandomRouterFileSystem();\r\n    final Path fileexisting = new Path(mountPath, \"fileexisting\");\r\n    final Path filenotexisting = new Path(mountPath, \"filenotexisting\");\r\n    FSDataOutputStream os = fs.create(fileexisting);\r\n    assertNotNull(os);\r\n    os.close();\r\n    FSDataInputStream fsdis = fs.open(fileexisting);\r\n    assertNotNull(\"We should be able to read the file\", fsdis);\r\n    LambdaTestUtils.intercept(FileNotFoundException.class, () -> fs.open(filenotexisting));\r\n    String nsIdWithFile = null;\r\n    for (Entry<String, MockNamenode> entry : namenodes.entrySet()) {\r\n        String nsId = entry.getKey();\r\n        MockNamenode nn = entry.getValue();\r\n        int rpc = nn.getRPCPort();\r\n        FileSystem nnfs = getFileSystem(rpc);\r\n        try {\r\n            FileStatus fileStatus = nnfs.getFileStatus(fileexisting);\r\n            assertNotNull(fileStatus);\r\n            assertNull(\"The file cannot be in two subclusters\", nsIdWithFile);\r\n            nsIdWithFile = nsId;\r\n        } catch (FileNotFoundException fnfe) {\r\n            LOG.debug(\"File not found in {}\", nsId);\r\n        }\r\n    }\r\n    assertNotNull(\"The file has to be in one subcluster\", nsIdWithFile);\r\n    LOG.info(\"Stop {} to simulate an unavailable subcluster\", nsIdWithFile);\r\n    namenodes.get(nsIdWithFile).stop();\r\n    try {\r\n        fs.open(fileexisting);\r\n        fail(\"It should throw an unavailable cluster exception\");\r\n    } catch (RemoteException re) {\r\n        IOException ioe = re.unwrapRemoteException();\r\n        assertTrue(\"Expected an unavailable exception for:\" + ioe.getClass(), RouterRpcClient.isUnavailableException(ioe));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    cluster = new StateStoreDFSCluster(false, 3, MultipleDestinationMountTableResolver.class);\r\n    Configuration routerConf = new RouterConfigBuilder().stateStore().admin().quota().rpc().build();\r\n    Configuration hdfsConf = new Configuration(false);\r\n    hdfsConf.setBoolean(DFSConfigKeys.DFS_NAMENODE_ACLS_ENABLED_KEY, true);\r\n    cluster.addRouterOverrides(routerConf);\r\n    cluster.addNamenodeOverrides(hdfsConf);\r\n    cluster.startCluster();\r\n    cluster.startRouters();\r\n    cluster.waitClusterUp();\r\n    routerContext = cluster.getRandomRouter();\r\n    resolver = (MountTableResolver) routerContext.getRouter().getSubclusterResolver();\r\n    nnFs0 = (DistributedFileSystem) cluster.getNamenode(cluster.getNameservices().get(0), null).getFileSystem();\r\n    nnFs1 = (DistributedFileSystem) cluster.getNamenode(cluster.getNameservices().get(1), null).getFileSystem();\r\n    nnFs2 = (DistributedFileSystem) cluster.getNamenode(cluster.getNameservices().get(2), null).getFileSystem();\r\n    routerFs = (DistributedFileSystem) routerContext.getFileSystem();\r\n    rpcServer = routerContext.getRouter().getRpcServer();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (cluster != null) {\r\n        cluster.stopRouter(routerContext);\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setupOrderMountPath",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void setupOrderMountPath(DestinationOrder order) throws Exception\n{\r\n    Map<String, String> destMap = new HashMap<>();\r\n    destMap.put(\"ns0\", \"/tmp\");\r\n    destMap.put(\"ns1\", \"/tmp\");\r\n    nnFs0.mkdirs(new Path(\"/tmp\"));\r\n    nnFs1.mkdirs(new Path(\"/tmp\"));\r\n    MountTable addEntry = MountTable.newInstance(\"/mount\", destMap);\r\n    addEntry.setDestOrder(order);\r\n    assertTrue(addMountTable(addEntry));\r\n    routerFs.mkdirs(new Path(\"/mount/dir/dir\"));\r\n    DFSTestUtil.createFile(routerFs, new Path(\"/mount/dir/file\"), 100L, (short) 1, 1024L);\r\n    DFSTestUtil.createFile(routerFs, new Path(\"/mount/file\"), 100L, (short) 1, 1024L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "resetTestEnvironment",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void resetTestEnvironment() throws IOException\n{\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTableManager = client.getMountTableManager();\r\n    RemoveMountTableEntryRequest req2 = RemoveMountTableEntryRequest.newInstance(\"/mount\");\r\n    mountTableManager.removeMountTableEntry(req2);\r\n    nnFs0.delete(new Path(\"/tmp\"), true);\r\n    nnFs1.delete(new Path(\"/tmp\"), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testInvocationSpaceOrder",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testInvocationSpaceOrder() throws Exception\n{\r\n    setupOrderMountPath(DestinationOrder.SPACE);\r\n    boolean isDirAll = rpcServer.isPathAll(\"/mount/dir\");\r\n    assertTrue(isDirAll);\r\n    testInvocation(isDirAll);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testInvocationHashAllOrder",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testInvocationHashAllOrder() throws Exception\n{\r\n    setupOrderMountPath(DestinationOrder.HASH_ALL);\r\n    boolean isDirAll = rpcServer.isPathAll(\"/mount/dir\");\r\n    assertTrue(isDirAll);\r\n    testInvocation(isDirAll);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testInvocationRandomOrder",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testInvocationRandomOrder() throws Exception\n{\r\n    setupOrderMountPath(DestinationOrder.RANDOM);\r\n    boolean isDirAll = rpcServer.isPathAll(\"/mount/dir\");\r\n    assertTrue(isDirAll);\r\n    testInvocation(isDirAll);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testInvocationHashOrder",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testInvocationHashOrder() throws Exception\n{\r\n    setupOrderMountPath(DestinationOrder.HASH);\r\n    boolean isDirAll = rpcServer.isPathAll(\"/mount/dir\");\r\n    assertFalse(isDirAll);\r\n    testInvocation(isDirAll);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testInvocationLocalOrder",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testInvocationLocalOrder() throws Exception\n{\r\n    setupOrderMountPath(DestinationOrder.LOCAL);\r\n    boolean isDirAll = rpcServer.isPathAll(\"/mount/dir\");\r\n    assertFalse(isDirAll);\r\n    testInvocation(isDirAll);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testInvocation",
  "errType" : null,
  "containingMethodsNum" : 34,
  "sourceCodeText" : "void testInvocation(boolean dirAll) throws IOException\n{\r\n    Path mountDir = new Path(\"/mount/dir/dir\");\r\n    Path nameSpaceFile = new Path(\"/tmp/dir/file\");\r\n    Path mountFile = new Path(\"/mount/dir/file\");\r\n    Path mountEntry = new Path(\"/mount\");\r\n    Path mountDest = new Path(\"/tmp\");\r\n    Path nameSpaceDir = new Path(\"/tmp/dir/dir\");\r\n    final String name = \"user.a1\";\r\n    final byte[] value = { 0x31, 0x32, 0x33 };\r\n    testDirectoryAndFileLevelInvocation(dirAll, mountDir, nameSpaceFile, mountFile, nameSpaceDir, name, value);\r\n    mountDir = new Path(\"/mount/dir\");\r\n    nameSpaceFile = new Path(\"/tmp/file\");\r\n    mountFile = new Path(\"/mount/file\");\r\n    nameSpaceDir = new Path(\"/tmp/dir\");\r\n    testDirectoryAndFileLevelInvocation(dirAll, mountDir, nameSpaceFile, mountFile, nameSpaceDir, name, value);\r\n    routerFs.setOwner(mountEntry, \"testuser\", \"testgroup\");\r\n    routerFs.setPermission(mountEntry, FsPermission.createImmutable((short) 777));\r\n    assertEquals(\"testuser\", routerFs.getFileStatus(mountEntry).getOwner());\r\n    assertEquals(\"testuser\", nnFs0.getFileStatus(mountDest).getOwner());\r\n    assertEquals(\"testuser\", nnFs1.getFileStatus(mountDest).getOwner());\r\n    assertEquals((short) 777, routerFs.getFileStatus(mountEntry).getPermission().toShort());\r\n    assertEquals((short) 777, nnFs0.getFileStatus(mountDest).getPermission().toShort());\r\n    assertEquals((short) 777, nnFs1.getFileStatus(mountDest).getPermission().toShort());\r\n    routerFs.setStoragePolicy(mountEntry, \"COLD\");\r\n    assertEquals(\"COLD\", routerFs.getStoragePolicy(mountEntry).getName());\r\n    assertEquals(\"COLD\", nnFs0.getStoragePolicy(mountDest).getName());\r\n    assertEquals(\"COLD\", nnFs1.getStoragePolicy(mountDest).getName());\r\n    routerFs.unsetStoragePolicy(mountEntry);\r\n    assertEquals(\"HOT\", routerFs.getStoragePolicy(mountDest).getName());\r\n    assertEquals(\"HOT\", nnFs0.getStoragePolicy(mountDest).getName());\r\n    assertEquals(\"HOT\", nnFs1.getStoragePolicy(mountDest).getName());\r\n    routerFs.setErasureCodingPolicy(mountEntry, \"RS-6-3-1024k\");\r\n    assertEquals(\"RS-6-3-1024k\", routerFs.getErasureCodingPolicy(mountEntry).getName());\r\n    assertEquals(\"RS-6-3-1024k\", nnFs0.getErasureCodingPolicy(mountDest).getName());\r\n    assertEquals(\"RS-6-3-1024k\", nnFs1.getErasureCodingPolicy(mountDest).getName());\r\n    routerFs.unsetErasureCodingPolicy(mountEntry);\r\n    assertNull(routerFs.getErasureCodingPolicy(mountDest));\r\n    assertNull(nnFs0.getErasureCodingPolicy(mountDest));\r\n    assertNull(nnFs1.getErasureCodingPolicy(mountDest));\r\n    routerFs.setXAttr(mountEntry, name, value);\r\n    assertArrayEquals(value, routerFs.getXAttr(mountEntry, name));\r\n    assertArrayEquals(value, nnFs0.getXAttr(mountDest, name));\r\n    assertArrayEquals(value, nnFs1.getXAttr(mountDest, name));\r\n    routerFs.removeXAttr(mountEntry, name);\r\n    assertEquals(0, routerFs.getXAttrs(mountEntry).size());\r\n    assertEquals(0, nnFs0.getXAttrs(mountDest).size());\r\n    assertEquals(0, nnFs1.getXAttrs(mountDest).size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testDirectoryAndFileLevelInvocation",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testDirectoryAndFileLevelInvocation(boolean dirAll, Path mountDir, Path nameSpaceFile, Path mountFile, Path nameSpaceDir, final String name, final byte[] value) throws IOException\n{\r\n    routerFs.setOwner(mountDir, \"testuser\", \"testgroup\");\r\n    routerFs.setPermission(mountDir, FsPermission.createImmutable((short) 777));\r\n    routerFs.setStoragePolicy(mountDir, \"COLD\");\r\n    routerFs.setErasureCodingPolicy(mountDir, \"RS-6-3-1024k\");\r\n    routerFs.setXAttr(mountDir, name, value);\r\n    boolean checkedDir1 = verifyDirectoryLevelInvocations(dirAll, nameSpaceDir, nnFs0, name, value);\r\n    boolean checkedDir2 = verifyDirectoryLevelInvocations(dirAll, nameSpaceDir, nnFs1, name, value);\r\n    assertTrue(\"The file didn't existed in either of the subclusters.\", checkedDir1 || checkedDir2);\r\n    routerFs.unsetStoragePolicy(mountDir);\r\n    routerFs.removeXAttr(mountDir, name);\r\n    routerFs.unsetErasureCodingPolicy(mountDir);\r\n    checkedDir1 = verifyDirectoryLevelUnsetInvocations(dirAll, nnFs0, nameSpaceDir);\r\n    checkedDir2 = verifyDirectoryLevelUnsetInvocations(dirAll, nnFs1, nameSpaceDir);\r\n    assertTrue(\"The file didn't existed in either of the subclusters.\", checkedDir1 || checkedDir2);\r\n    routerFs.setOwner(mountFile, \"testuser\", \"testgroup\");\r\n    routerFs.setPermission(mountFile, FsPermission.createImmutable((short) 777));\r\n    routerFs.setStoragePolicy(mountFile, \"COLD\");\r\n    routerFs.setReplication(mountFile, (short) 2);\r\n    routerFs.setXAttr(mountFile, name, value);\r\n    verifyFileLevelInvocations(nameSpaceFile, nnFs0, mountFile, name, value);\r\n    verifyFileLevelInvocations(nameSpaceFile, nnFs1, mountFile, name, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifyDirectoryLevelUnsetInvocations",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean verifyDirectoryLevelUnsetInvocations(boolean dirAll, DistributedFileSystem nnFs, Path nameSpaceDir) throws IOException\n{\r\n    boolean checked = false;\r\n    if (dirAll || nnFs.exists(nameSpaceDir)) {\r\n        checked = true;\r\n        assertEquals(\"HOT\", nnFs.getStoragePolicy(nameSpaceDir).getName());\r\n        assertNull(nnFs.getErasureCodingPolicy(nameSpaceDir));\r\n        assertEquals(0, nnFs.getXAttrs(nameSpaceDir).size());\r\n    }\r\n    return checked;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifyFileLevelInvocations",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void verifyFileLevelInvocations(Path nameSpaceFile, DistributedFileSystem nnFs, Path mountFile, final String name, final byte[] value) throws IOException\n{\r\n    if (nnFs.exists(nameSpaceFile)) {\r\n        assertEquals(\"testuser\", nnFs.getFileStatus(nameSpaceFile).getOwner());\r\n        assertEquals((short) 777, nnFs.getFileStatus(nameSpaceFile).getPermission().toShort());\r\n        assertEquals(\"COLD\", nnFs.getStoragePolicy(nameSpaceFile).getName());\r\n        assertEquals((short) 2, nnFs.getFileStatus(nameSpaceFile).getReplication());\r\n        assertArrayEquals(value, nnFs.getXAttr(nameSpaceFile, name));\r\n        routerFs.unsetStoragePolicy(mountFile);\r\n        routerFs.removeXAttr(mountFile, name);\r\n        assertEquals(0, nnFs.getXAttrs(nameSpaceFile).size());\r\n        assertEquals(\"HOT\", nnFs.getStoragePolicy(nameSpaceFile).getName());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifyDirectoryLevelInvocations",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean verifyDirectoryLevelInvocations(boolean dirAll, Path nameSpaceDir, DistributedFileSystem nnFs, final String name, final byte[] value) throws IOException\n{\r\n    boolean checked = false;\r\n    if (dirAll || nnFs.exists(nameSpaceDir)) {\r\n        checked = true;\r\n        assertEquals(\"testuser\", nnFs.getFileStatus(nameSpaceDir).getOwner());\r\n        assertEquals(\"COLD\", nnFs.getStoragePolicy(nameSpaceDir).getName());\r\n        assertEquals(\"RS-6-3-1024k\", nnFs.getErasureCodingPolicy(nameSpaceDir).getName());\r\n        assertArrayEquals(value, nnFs.getXAttr(nameSpaceDir, name));\r\n        assertEquals((short) 777, nnFs.getFileStatus(nameSpaceDir).getPermission().toShort());\r\n    }\r\n    return checked;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addMountTable",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean addMountTable(final MountTable entry) throws IOException\n{\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTableManager = client.getMountTableManager();\r\n    AddMountTableEntryRequest addRequest = AddMountTableEntryRequest.newInstance(entry);\r\n    AddMountTableEntryResponse addResponse = mountTableManager.addMountTableEntry(addRequest);\r\n    resolver.loadCache(true);\r\n    return addResponse.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testECMultipleDestinations",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testECMultipleDestinations() throws Exception\n{\r\n    setupOrderMountPath(DestinationOrder.HASH_ALL);\r\n    Path mountPath = new Path(\"/mount/dir\");\r\n    routerFs.setErasureCodingPolicy(mountPath, \"RS-6-3-1024k\");\r\n    assertTrue(routerFs.getFileStatus(mountPath).isErasureCoded());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testACLMultipleDestinations",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testACLMultipleDestinations() throws Exception\n{\r\n    setupOrderMountPath(DestinationOrder.HASH_ALL);\r\n    Path mountPath = new Path(\"/mount/dir/dir\");\r\n    Path nsPath = new Path(\"/tmp/dir/dir\");\r\n    List<AclEntry> aclSpec = Collections.singletonList(AclEntry.parseAclEntry(\"default:USER:TestUser:rwx\", true));\r\n    routerFs.setAcl(mountPath, aclSpec);\r\n    assertEquals(5, nnFs0.getAclStatus(nsPath).getEntries().size());\r\n    assertEquals(5, nnFs1.getAclStatus(nsPath).getEntries().size());\r\n    aclSpec = Collections.singletonList(AclEntry.parseAclEntry(\"USER:User:rwx::\", true));\r\n    routerFs.modifyAclEntries(mountPath, aclSpec);\r\n    assertEquals(7, nnFs0.getAclStatus(nsPath).getEntries().size());\r\n    assertEquals(7, nnFs1.getAclStatus(nsPath).getEntries().size());\r\n    routerFs.removeAclEntries(mountPath, aclSpec);\r\n    assertEquals(6, nnFs0.getAclStatus(nsPath).getEntries().size());\r\n    assertEquals(6, nnFs1.getAclStatus(nsPath).getEntries().size());\r\n    routerFs.modifyAclEntries(mountPath, aclSpec);\r\n    routerFs.removeDefaultAcl(mountPath);\r\n    assertEquals(2, nnFs0.getAclStatus(nsPath).getEntries().size());\r\n    assertEquals(2, nnFs1.getAclStatus(nsPath).getEntries().size());\r\n    routerFs.removeAcl(mountPath);\r\n    assertEquals(0, nnFs0.getAclStatus(nsPath).getEntries().size());\r\n    assertEquals(0, nnFs1.getAclStatus(nsPath).getEntries().size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetDestinationHashAll",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetDestinationHashAll() throws Exception\n{\r\n    testGetDestination(DestinationOrder.HASH_ALL, Arrays.asList(\"ns1\"), Arrays.asList(\"ns1\"), Arrays.asList(\"ns1\", \"ns0\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetDestinationHash",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetDestinationHash() throws Exception\n{\r\n    testGetDestination(DestinationOrder.HASH, Arrays.asList(\"ns1\"), Arrays.asList(\"ns1\"), Arrays.asList(\"ns1\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetDestinationRandom",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetDestinationRandom() throws Exception\n{\r\n    testGetDestination(DestinationOrder.RANDOM, null, null, Arrays.asList(\"ns0\", \"ns1\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testIsMultiDestDir",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testIsMultiDestDir() throws Exception\n{\r\n    RouterClientProtocol client = routerContext.getRouter().getRpcServer().getClientProtocolModule();\r\n    setupOrderMountPath(DestinationOrder.HASH_ALL);\r\n    assertTrue(client.isMultiDestDirectory(\"/mount/dir\"));\r\n    assertFalse(client.isMultiDestDirectory(\"/mount/nodir\"));\r\n    assertFalse(client.isMultiDestDirectory(\"/mount/dir/file\"));\r\n    routerFs.createSymlink(new Path(\"/mount/dir/file\"), new Path(\"/mount/dir/link\"), true);\r\n    assertFalse(client.isMultiDestDirectory(\"/mount/dir/link\"));\r\n    routerFs.createSymlink(new Path(\"/mount/dir/dir\"), new Path(\"/mount/dir/linkDir\"), true);\r\n    assertFalse(client.isMultiDestDirectory(\"/mount/dir/linkDir\"));\r\n    resetTestEnvironment();\r\n    setupOrderMountPath(DestinationOrder.HASH);\r\n    assertFalse(client.isMultiDestDirectory(\"/mount/dir\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSnapshotPathResolution",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testSnapshotPathResolution() throws Exception\n{\r\n    Map<String, String> destMap = new HashMap<>();\r\n    destMap.put(\"ns0\", \"/tmp_ns0\");\r\n    destMap.put(\"ns1\", \"/tmp_ns1\");\r\n    nnFs0.mkdirs(new Path(\"/tmp_ns0\"));\r\n    nnFs1.mkdirs(new Path(\"/tmp_ns1\"));\r\n    MountTable addEntry = MountTable.newInstance(\"/mountSnap\", destMap);\r\n    addEntry.setDestOrder(DestinationOrder.HASH);\r\n    assertTrue(addMountTable(addEntry));\r\n    nnFs0.mkdirs(new Path(\"/tmp_ns0/snapDir\"));\r\n    Path snapDir = new Path(\"/mountSnap/snapDir\");\r\n    Path snapshotPath = new Path(\"/mountSnap/snapDir/.snapshot/snap\");\r\n    routerFs.allowSnapshot(snapDir);\r\n    Path snapshot = routerFs.createSnapshot(snapDir, \"snap\");\r\n    assertEquals(snapshotPath, snapshot);\r\n    SnapshotStatus[] snapshots = routerFs.getSnapshotListing(snapDir);\r\n    assertEquals(snapshotPath, snapshots[0].getFullPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRenameMultipleDestDirectories",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRenameMultipleDestDirectories() throws Exception\n{\r\n    verifyRenameOnMultiDestDirectories(DestinationOrder.HASH_ALL, false);\r\n    resetTestEnvironment();\r\n    verifyRenameOnMultiDestDirectories(DestinationOrder.RANDOM, false);\r\n    resetTestEnvironment();\r\n    verifyRenameOnMultiDestDirectories(DestinationOrder.SPACE, false);\r\n    resetTestEnvironment();\r\n    verifyRenameOnMultiDestDirectories(DestinationOrder.HASH_ALL, true);\r\n    resetTestEnvironment();\r\n    verifyRenameOnMultiDestDirectories(DestinationOrder.RANDOM, true);\r\n    resetTestEnvironment();\r\n    verifyRenameOnMultiDestDirectories(DestinationOrder.SPACE, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testClearQuota",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testClearQuota() throws Exception\n{\r\n    long nsQuota = 5;\r\n    long ssQuota = 100;\r\n    Path path = new Path(\"/router_test\");\r\n    nnFs0.mkdirs(path);\r\n    nnFs1.mkdirs(path);\r\n    MountTable addEntry = MountTable.newInstance(\"/router_test\", Collections.singletonMap(\"ns0\", \"/router_test\"));\r\n    addEntry.setQuota(new RouterQuotaUsage.Builder().build());\r\n    assertTrue(addMountTable(addEntry));\r\n    RouterQuotaUpdateService updateService = routerContext.getRouter().getQuotaCacheUpdateService();\r\n    updateService.periodicInvoke();\r\n    RouterAdmin admin = getRouterAdmin();\r\n    String[] argv = new String[] { \"-setQuota\", path.toString(), \"-nsQuota\", String.valueOf(nsQuota), \"-ssQuota\", String.valueOf(ssQuota) };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    updateService.periodicInvoke();\r\n    resolver.loadCache(true);\r\n    ContentSummary cs = routerFs.getContentSummary(path);\r\n    assertEquals(nsQuota, cs.getQuota());\r\n    assertEquals(ssQuota, cs.getSpaceQuota());\r\n    argv = new String[] { \"-clrQuota\", path.toString() };\r\n    assertEquals(0, ToolRunner.run(admin, argv));\r\n    updateService.periodicInvoke();\r\n    resolver.loadCache(true);\r\n    ContentSummary cs1 = routerFs.getContentSummary(path);\r\n    assertEquals(-1, cs1.getQuota());\r\n    assertEquals(-1, cs1.getSpaceQuota());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testContentSummaryWithMultipleDest",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testContentSummaryWithMultipleDest() throws Exception\n{\r\n    MountTable addEntry;\r\n    long nsQuota = 5;\r\n    long ssQuota = 100;\r\n    Path path = new Path(\"/testContentSummaryWithMultipleDest\");\r\n    Map<String, String> destMap = new HashMap<>();\r\n    destMap.put(\"ns0\", \"/testContentSummaryWithMultipleDest\");\r\n    destMap.put(\"ns1\", \"/testContentSummaryWithMultipleDest\");\r\n    nnFs0.mkdirs(path);\r\n    nnFs1.mkdirs(path);\r\n    addEntry = MountTable.newInstance(\"/testContentSummaryWithMultipleDest\", destMap);\r\n    addEntry.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota).spaceQuota(ssQuota).build());\r\n    assertTrue(addMountTable(addEntry));\r\n    RouterQuotaUpdateService updateService = routerContext.getRouter().getQuotaCacheUpdateService();\r\n    updateService.periodicInvoke();\r\n    ContentSummary cs = routerFs.getContentSummary(path);\r\n    assertEquals(nsQuota, cs.getQuota());\r\n    assertEquals(ssQuota, cs.getSpaceQuota());\r\n    ContentSummary ns0Cs = nnFs0.getContentSummary(path);\r\n    assertEquals(nsQuota, ns0Cs.getQuota());\r\n    assertEquals(ssQuota, ns0Cs.getSpaceQuota());\r\n    ContentSummary ns1Cs = nnFs1.getContentSummary(path);\r\n    assertEquals(nsQuota, ns1Cs.getQuota());\r\n    assertEquals(ssQuota, ns1Cs.getSpaceQuota());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testContentSummaryMultipleDestWithMaxValue",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testContentSummaryMultipleDestWithMaxValue() throws Exception\n{\r\n    MountTable addEntry;\r\n    long nsQuota = Long.MAX_VALUE - 2;\r\n    long ssQuota = Long.MAX_VALUE - 2;\r\n    Path path = new Path(\"/testContentSummaryMultipleDestWithMaxValue\");\r\n    Map<String, String> destMap = new HashMap<>();\r\n    destMap.put(\"ns0\", \"/testContentSummaryMultipleDestWithMaxValue\");\r\n    destMap.put(\"ns1\", \"/testContentSummaryMultipleDestWithMaxValue\");\r\n    nnFs0.mkdirs(path);\r\n    nnFs1.mkdirs(path);\r\n    addEntry = MountTable.newInstance(\"/testContentSummaryMultipleDestWithMaxValue\", destMap);\r\n    addEntry.setQuota(new RouterQuotaUsage.Builder().quota(nsQuota).spaceQuota(ssQuota).build());\r\n    assertTrue(addMountTable(addEntry));\r\n    RouterQuotaUpdateService updateService = routerContext.getRouter().getQuotaCacheUpdateService();\r\n    updateService.periodicInvoke();\r\n    ContentSummary cs = routerFs.getContentSummary(path);\r\n    assertEquals(nsQuota, cs.getQuota());\r\n    assertEquals(ssQuota, cs.getSpaceQuota());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testInvokeAtAvailableNs",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testInvokeAtAvailableNs() throws IOException\n{\r\n    Path path = new Path(\"/testInvokeAtAvailableNs\");\r\n    Map<String, String> destMap = new HashMap<>();\r\n    destMap.put(\"ns0\", \"/testInvokeAtAvailableNs\");\r\n    destMap.put(\"ns1\", \"/testInvokeAtAvailableNs\");\r\n    nnFs0.mkdirs(path);\r\n    nnFs1.mkdirs(path);\r\n    MountTable addEntry = MountTable.newInstance(\"/testInvokeAtAvailableNs\", destMap);\r\n    addEntry.setQuota(new RouterQuotaUsage.Builder().build());\r\n    addEntry.setDestOrder(DestinationOrder.RANDOM);\r\n    addEntry.setFaultTolerant(true);\r\n    assertTrue(addMountTable(addEntry));\r\n    MiniDFSCluster dfsCluster = cluster.getCluster();\r\n    dfsCluster.shutdownNameNode(0);\r\n    dfsCluster.shutdownNameNode(1);\r\n    try {\r\n        RemoteMethod method = new RemoteMethod(\"getServerDefaults\");\r\n        FsServerDefaults serverDefaults = rpcServer.invokeAtAvailableNs(method, FsServerDefaults.class);\r\n        assertNotNull(serverDefaults);\r\n    } finally {\r\n        dfsCluster.restartNameNode(0);\r\n        dfsCluster.restartNameNode(1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testWriteWithUnavailableSubCluster",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testWriteWithUnavailableSubCluster() throws IOException\n{\r\n    Path path = new Path(\"/testWriteWithUnavailableSubCluster\");\r\n    Map<String, String> destMap = new HashMap<>();\r\n    destMap.put(\"ns0\", \"/testWriteWithUnavailableSubCluster\");\r\n    destMap.put(\"ns1\", \"/testWriteWithUnavailableSubCluster\");\r\n    nnFs0.mkdirs(path);\r\n    nnFs1.mkdirs(path);\r\n    MountTable addEntry = MountTable.newInstance(\"/testWriteWithUnavailableSubCluster\", destMap);\r\n    addEntry.setQuota(new RouterQuotaUsage.Builder().build());\r\n    addEntry.setDestOrder(DestinationOrder.RANDOM);\r\n    addEntry.setFaultTolerant(true);\r\n    assertTrue(addMountTable(addEntry));\r\n    MiniDFSCluster dfsCluster = cluster.getCluster();\r\n    dfsCluster.shutdownNameNode(0);\r\n    FSDataOutputStream out = null;\r\n    Path filePath = new Path(path, \"aa\");\r\n    try {\r\n        out = routerFs.create(filePath);\r\n        out.write(\"hello\".getBytes());\r\n        out.hflush();\r\n        assertTrue(routerFs.exists(filePath));\r\n    } finally {\r\n        IOUtils.closeStream(out);\r\n        dfsCluster.restartNameNode(0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifyRenameOnMultiDestDirectories",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void verifyRenameOnMultiDestDirectories(DestinationOrder order, boolean isRename2) throws Exception\n{\r\n    setupOrderMountPath(order);\r\n    Path src = new Path(\"/mount/dir/dir\");\r\n    Path nnSrc = new Path(\"/tmp/dir/dir\");\r\n    Path dst = new Path(\"/mount/dir/subdir\");\r\n    Path nnDst = new Path(\"/tmp/dir/subdir\");\r\n    Path fileSrc = new Path(\"/mount/dir/dir/file\");\r\n    Path nnFileSrc = new Path(\"/tmp/dir/dir/file\");\r\n    Path fileDst = new Path(\"/mount/dir/subdir/file\");\r\n    Path nnFileDst = new Path(\"/tmp/dir/subdir/file\");\r\n    DFSTestUtil.createFile(routerFs, fileSrc, 100L, (short) 1, 1024L);\r\n    if (isRename2) {\r\n        routerFs.rename(src, dst, Rename.NONE);\r\n    } else {\r\n        assertTrue(routerFs.rename(src, dst));\r\n    }\r\n    assertTrue(nnFs0.exists(nnDst));\r\n    assertTrue(nnFs1.exists(nnDst));\r\n    assertFalse(nnFs0.exists(nnSrc));\r\n    assertFalse(nnFs1.exists(nnSrc));\r\n    assertFalse(routerFs.exists(fileSrc));\r\n    assertTrue(routerFs.exists(fileDst));\r\n    assertTrue(nnFs0.exists(nnFileDst) || nnFs1.exists(nnFileDst));\r\n    assertFalse(nnFs0.exists(nnFileSrc) || nnFs1.exists(nnFileSrc));\r\n    Path fileRenamed = new Path(\"/mount/dir/subdir/renamedFile\");\r\n    Path nnFileRenamed = new Path(\"/tmp/dir/subdir/renamedFile\");\r\n    if (isRename2) {\r\n        routerFs.rename(fileDst, fileRenamed, Rename.NONE);\r\n    } else {\r\n        assertTrue(routerFs.rename(fileDst, fileRenamed));\r\n    }\r\n    assertTrue(routerFs.exists(fileRenamed));\r\n    assertFalse(routerFs.exists(fileDst));\r\n    assertTrue(nnFs0.exists(nnFileRenamed) || nnFs1.exists(nnFileRenamed));\r\n    assertFalse(nnFs0.exists(nnFileDst) || nnFs1.exists(nnFileDst));\r\n    Path dst1 = new Path(\"/mount/dir/renameddir\");\r\n    Path nnDst1 = new Path(\"/tmp/dir/renameddir\");\r\n    nnFs1.delete(nnDst, true);\r\n    if (isRename2) {\r\n        routerFs.rename(dst, dst1, Rename.NONE);\r\n    } else {\r\n        assertTrue(routerFs.rename(dst, dst1));\r\n    }\r\n    assertTrue(nnFs0.exists(nnDst1));\r\n    assertFalse(nnFs0.exists(nnDst));\r\n    Path src1 = new Path(\"/mount/dir\");\r\n    Path dst2 = new Path(\"/mount/OneDest\");\r\n    Path nnDst2 = new Path(\"/tmp/OneDest\");\r\n    nnFs0.mkdirs(nnDst2);\r\n    if (isRename2) {\r\n        routerFs.rename(src1, dst2, Rename.NONE);\r\n    } else {\r\n        assertTrue(routerFs.rename(src1, dst2));\r\n    }\r\n    assertTrue(nnFs0.exists(nnDst2));\r\n    assertTrue(nnFs1.exists(nnDst2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetDestination",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testGetDestination(DestinationOrder order, List<String> expectFileLocation, List<String> expectNoFileLocation, List<String> expectDirLocation) throws Exception\n{\r\n    setupOrderMountPath(order);\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTableManager = client.getMountTableManager();\r\n    final String pathFile = \"dir/file\";\r\n    final Path pathRouterFile = new Path(\"/mount\", pathFile);\r\n    final Path pathLocalFile = new Path(\"/tmp\", pathFile);\r\n    FileStatus fileStatus = routerFs.getFileStatus(pathRouterFile);\r\n    assertTrue(fileStatus + \" should be a file\", fileStatus.isFile());\r\n    GetDestinationResponse respFile = mountTableManager.getDestination(GetDestinationRequest.newInstance(pathRouterFile));\r\n    if (expectFileLocation != null) {\r\n        assertEquals(expectFileLocation, respFile.getDestinations());\r\n        assertPathStatus(expectFileLocation, pathLocalFile, false);\r\n    } else {\r\n        Collection<String> dests = respFile.getDestinations();\r\n        assertPathStatus(dests, pathLocalFile, false);\r\n    }\r\n    final String pathNoFile = \"dir/no-file\";\r\n    final Path pathRouterNoFile = new Path(\"/mount\", pathNoFile);\r\n    final Path pathLocalNoFile = new Path(\"/tmp\", pathNoFile);\r\n    LambdaTestUtils.intercept(FileNotFoundException.class, () -> routerFs.getFileStatus(pathRouterNoFile));\r\n    GetDestinationResponse respNoFile = mountTableManager.getDestination(GetDestinationRequest.newInstance(pathRouterNoFile));\r\n    if (expectNoFileLocation != null) {\r\n        assertEquals(expectNoFileLocation, respNoFile.getDestinations());\r\n    }\r\n    assertPathStatus(Collections.emptyList(), pathLocalNoFile, false);\r\n    final String pathNestedDir = \"dir/dir\";\r\n    final Path pathRouterNestedDir = new Path(\"/mount\", pathNestedDir);\r\n    final Path pathLocalNestedDir = new Path(\"/tmp\", pathNestedDir);\r\n    FileStatus dirStatus = routerFs.getFileStatus(pathRouterNestedDir);\r\n    assertTrue(dirStatus + \" should be a directory\", dirStatus.isDirectory());\r\n    GetDestinationResponse respDir = mountTableManager.getDestination(GetDestinationRequest.newInstance(pathRouterNestedDir));\r\n    assertEqualsCollection(expectDirLocation, respDir.getDestinations());\r\n    assertPathStatus(expectDirLocation, pathLocalNestedDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "assertPathStatus",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void assertPathStatus(Collection<String> expectedLocations, Path path, boolean isDir) throws Exception\n{\r\n    for (String nsId : NS_IDS) {\r\n        final FileSystem fs = getFileSystem(nsId);\r\n        if (expectedLocations.contains(nsId)) {\r\n            assertTrue(path + \" should exist in \" + nsId, fs.exists(path));\r\n            final FileStatus status = fs.getFileStatus(path);\r\n            if (isDir) {\r\n                assertTrue(path + \" should be a directory\", status.isDirectory());\r\n            } else {\r\n                assertTrue(path + \" should be a file\", status.isFile());\r\n            }\r\n        } else {\r\n            assertFalse(path + \" should not exist in \" + nsId, fs.exists(path));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "assertEqualsCollection",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertEqualsCollection(Collection<String> col1, Collection<String> col2)\n{\r\n    assertEquals(new TreeSet<>(col1), new TreeSet<>(col2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FileSystem getFileSystem(final String nsId)\n{\r\n    if (nsId.equals(\"ns0\")) {\r\n        return nnFs0;\r\n    }\r\n    if (nsId.equals(\"ns1\")) {\r\n        return nnFs1;\r\n    }\r\n    if (nsId.equals(\"ns2\")) {\r\n        return nnFs2;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterAdmin",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "RouterAdmin getRouterAdmin()\n{\r\n    Router router = routerContext.getRouter();\r\n    Configuration configuration = routerContext.getConf();\r\n    InetSocketAddress routerSocket = router.getAdminServerAddress();\r\n    configuration.setSocketAddr(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, routerSocket);\r\n    return new RouterAdmin(configuration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void create() throws IOException\n{\r\n    deleteStateStore();\r\n    conf = getStateStoreConfiguration();\r\n    conf.setTimeDuration(DFS_ROUTER_SAFEMODE_EXTENSION, TimeUnit.SECONDS.toMillis(2), TimeUnit.MILLISECONDS);\r\n    conf.setTimeDuration(DFS_ROUTER_CACHE_TIME_TO_LIVE_MS, 200, TimeUnit.MILLISECONDS);\r\n    conf.setTimeDuration(DFS_ROUTER_SAFEMODE_EXPIRATION, TimeUnit.SECONDS.toMillis(1), TimeUnit.MILLISECONDS);\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_RPC_BIND_HOST_KEY, \"0.0.0.0\");\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_RPC_ADDRESS_KEY, \"127.0.0.1:0\");\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, \"127.0.0.1:0\");\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_ADMIN_BIND_HOST_KEY, \"0.0.0.0\");\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_HTTP_ADDRESS_KEY, \"127.0.0.1:0\");\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_HTTPS_ADDRESS_KEY, \"127.0.0.1:0\");\r\n    conf = new RouterConfigBuilder(conf).rpc().admin().safemode().stateStore().metrics().build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void destroy()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws IOException, URISyntaxException\n{\r\n    router = new Router();\r\n    router.init(conf);\r\n    router.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup() throws IOException\n{\r\n    if (router != null) {\r\n        router.stop();\r\n        router = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSafemodeService",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSafemodeService() throws IOException\n{\r\n    RouterSafemodeService server = new RouterSafemodeService(router);\r\n    server.init(conf);\r\n    assertEquals(STATE.INITED, server.getServiceState());\r\n    server.start();\r\n    assertEquals(STATE.STARTED, server.getServiceState());\r\n    server.stop();\r\n    assertEquals(STATE.STOPPED, server.getServiceState());\r\n    server.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRouterExitSafemode",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRouterExitSafemode() throws InterruptedException, IllegalStateException, IOException\n{\r\n    assertTrue(router.getSafemodeService().isInSafeMode());\r\n    verifyRouter(RouterServiceState.SAFEMODE);\r\n    long interval = conf.getTimeDuration(DFS_ROUTER_SAFEMODE_EXTENSION, TimeUnit.SECONDS.toMillis(2), TimeUnit.MILLISECONDS) + conf.getTimeDuration(DFS_ROUTER_CACHE_TIME_TO_LIVE_MS, TimeUnit.SECONDS.toMillis(1), TimeUnit.MILLISECONDS);\r\n    Thread.sleep(interval);\r\n    assertFalse(router.getSafemodeService().isInSafeMode());\r\n    verifyRouter(RouterServiceState.RUNNING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRouterEnterSafemode",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testRouterEnterSafemode() throws IllegalStateException, IOException, InterruptedException\n{\r\n    assertTrue(router.getSafemodeService().isInSafeMode());\r\n    verifyRouter(RouterServiceState.SAFEMODE);\r\n    long interval0 = conf.getTimeDuration(DFS_ROUTER_SAFEMODE_EXTENSION, TimeUnit.SECONDS.toMillis(2), TimeUnit.MILLISECONDS) - 1000;\r\n    long t0 = Time.now();\r\n    while (Time.now() - t0 < interval0) {\r\n        verifyRouter(RouterServiceState.SAFEMODE);\r\n        Thread.sleep(100);\r\n    }\r\n    long interval1 = 1000 + 2 * conf.getTimeDuration(DFS_ROUTER_CACHE_TIME_TO_LIVE_MS, TimeUnit.SECONDS.toMillis(1), TimeUnit.MILLISECONDS);\r\n    Thread.sleep(interval1);\r\n    assertFalse(router.getSafemodeService().isInSafeMode());\r\n    verifyRouter(RouterServiceState.RUNNING);\r\n    router.getStateStore().stopCacheUpdateService();\r\n    long interval2 = conf.getTimeDuration(DFS_ROUTER_SAFEMODE_EXPIRATION, TimeUnit.SECONDS.toMillis(2), TimeUnit.MILLISECONDS) + 2 * conf.getTimeDuration(DFS_ROUTER_CACHE_TIME_TO_LIVE_MS, TimeUnit.SECONDS.toMillis(1), TimeUnit.MILLISECONDS);\r\n    Thread.sleep(interval2);\r\n    assertTrue(router.getSafemodeService().isInSafeMode());\r\n    verifyRouter(RouterServiceState.SAFEMODE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRouterRpcSafeMode",
  "errType" : [ "StandbyException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRouterRpcSafeMode() throws IllegalStateException, IOException\n{\r\n    assertTrue(router.getSafemodeService().isInSafeMode());\r\n    verifyRouter(RouterServiceState.SAFEMODE);\r\n    boolean exception = false;\r\n    try {\r\n        router.getRpcServer().delete(\"/testfile.txt\", true);\r\n        fail(\"We should have thrown a safe mode exception\");\r\n    } catch (StandbyException sme) {\r\n        exception = true;\r\n    }\r\n    assertTrue(\"We should have thrown a safe mode exception\", exception);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRouterManualSafeMode",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testRouterManualSafeMode() throws Exception\n{\r\n    InetSocketAddress adminAddr = router.getAdminServerAddress();\r\n    conf.setSocketAddr(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, adminAddr);\r\n    RouterAdmin admin = new RouterAdmin(conf);\r\n    assertTrue(router.getSafemodeService().isInSafeMode());\r\n    verifyRouter(RouterServiceState.SAFEMODE);\r\n    long interval = conf.getTimeDuration(DFS_ROUTER_SAFEMODE_EXTENSION, TimeUnit.SECONDS.toMillis(2), TimeUnit.MILLISECONDS) + 300;\r\n    Thread.sleep(interval);\r\n    verifyRouter(RouterServiceState.RUNNING);\r\n    assertEquals(0, ToolRunner.run(admin, new String[] { \"-safemode\", \"enter\" }));\r\n    verifyRouter(RouterServiceState.SAFEMODE);\r\n    interval = 2 * conf.getTimeDuration(DFS_ROUTER_CACHE_TIME_TO_LIVE_MS, TimeUnit.SECONDS.toMillis(1), TimeUnit.MILLISECONDS);\r\n    Thread.sleep(interval);\r\n    verifyRouter(RouterServiceState.SAFEMODE);\r\n    assertEquals(0, ToolRunner.run(admin, new String[] { \"-safemode\", \"leave\" }));\r\n    verifyRouter(RouterServiceState.RUNNING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifyRouter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyRouter(RouterServiceState status) throws IllegalStateException, IOException\n{\r\n    assertEquals(status, router.getRouterState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRouterNotInitMountTable",
  "errType" : [ "StateStoreUnavailableException", "StandbyException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testRouterNotInitMountTable() throws Exception\n{\r\n    MountTableResolver mountTable = (MountTableResolver) router.getSubclusterResolver();\r\n    mountTable.setDisabled(true);\r\n    int interval = 2 * (int) conf.getTimeDuration(DFS_ROUTER_SAFEMODE_EXTENSION, TimeUnit.SECONDS.toMillis(2), TimeUnit.MILLISECONDS);\r\n    GenericTestUtils.waitFor(() -> router.getRouterState() == RouterServiceState.RUNNING, 100, interval);\r\n    try {\r\n        router.getRpcServer().getFileInfo(\"/mnt/file.txt\");\r\n        fail(\"We should have thrown StateStoreUnavailableException\");\r\n    } catch (StateStoreUnavailableException e) {\r\n        assertEquals(\"Mount Table not initialized\", e.getMessage());\r\n    }\r\n    RouterAdminServer admin = router.getAdminServer();\r\n    EnterSafeModeRequest request = EnterSafeModeRequest.newInstance();\r\n    admin.enterSafeMode(request);\r\n    verifyRouter(RouterServiceState.SAFEMODE);\r\n    try {\r\n        router.getRpcServer().getFileInfo(\"/mnt/file.txt\");\r\n        fail(\"We should have thrown a safe mode exception\");\r\n    } catch (StandbyException e) {\r\n        String msg = e.getMessage();\r\n        assertTrue(\"Wrong message: \" + msg, msg.endsWith(\"is in safe mode and cannot handle READ requests\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "getStateStoreDriver",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StateStoreDriver getStateStoreDriver()\n{\r\n    return stateStore.getDriver();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "cleanMetrics",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cleanMetrics()\n{\r\n    if (stateStore != null) {\r\n        StateStoreMetrics metrics = stateStore.getMetrics();\r\n        metrics.reset();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "tearDownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDownCluster()\n{\r\n    if (stateStore != null) {\r\n        stateStore.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "getStateStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void getStateStore(Configuration config) throws Exception\n{\r\n    conf = config;\r\n    stateStore = FederationStateStoreTestUtils.newStateStore(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "generateRandomString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String generateRandomString()\n{\r\n    String randomString = \"randomString-\" + RANDOM.nextInt();\r\n    return randomString;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "generateRandomLong",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long generateRandomLong()\n{\r\n    return RANDOM.nextLong();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "generateRandomEnum",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T generateRandomEnum(Class<T> enumClass)\n{\r\n    int x = RANDOM.nextInt(enumClass.getEnumConstants().length);\r\n    T data = enumClass.getEnumConstants()[x];\r\n    return data;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "generateFakeRecord",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "T generateFakeRecord(Class<T> recordClass) throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    if (recordClass == MembershipState.class) {\r\n        return (T) MembershipState.newInstance(generateRandomString(), generateRandomString(), generateRandomString(), generateRandomString(), generateRandomString(), generateRandomString(), generateRandomString(), generateRandomString(), \"http\", generateRandomString(), generateRandomEnum(FederationNamenodeServiceState.class), false);\r\n    } else if (recordClass == MountTable.class) {\r\n        String src = \"/\" + generateRandomString();\r\n        Map<String, String> destMap = Collections.singletonMap(generateRandomString(), \"/\" + generateRandomString());\r\n        return (T) MountTable.newInstance(src, destMap);\r\n    } else if (recordClass == RouterState.class) {\r\n        RouterState routerState = RouterState.newInstance(generateRandomString(), generateRandomLong(), generateRandomEnum(RouterServiceState.class));\r\n        StateStoreVersion version = generateFakeRecord(StateStoreVersion.class);\r\n        routerState.setStateStoreVersion(version);\r\n        return (T) routerState;\r\n    } else if (recordClass == DisabledNameservice.class) {\r\n        return (T) DisabledNameservice.newInstance(generateRandomString());\r\n    } else if (recordClass == StateStoreVersion.class) {\r\n        return (T) StateStoreVersion.newInstance(generateRandomLong(), generateRandomLong());\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "validateRecord",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "boolean validateRecord(BaseRecord original, BaseRecord committed, boolean assertEquals)\n{\r\n    boolean ret = true;\r\n    Map<String, Class<?>> fields = getFields(original);\r\n    for (String key : fields.keySet()) {\r\n        if (key.equals(\"dateModified\") || key.equals(\"dateCreated\") || key.equals(\"proto\")) {\r\n            continue;\r\n        }\r\n        Object data1 = getField(original, key);\r\n        Object data2 = getField(committed, key);\r\n        if (assertEquals) {\r\n            assertEquals(\"Field \" + key + \" does not match\", data1, data2);\r\n        } else if (!data1.equals(data2)) {\r\n            ret = false;\r\n        }\r\n    }\r\n    long now = stateStore.getDriver().getTime();\r\n    assertTrue(committed.getDateCreated() <= now && committed.getDateCreated() > 0);\r\n    if (!committed.isExpired()) {\r\n        assertTrue(committed.getDateModified() >= committed.getDateCreated());\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "removeAll",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeAll(StateStoreDriver driver) throws IOException\n{\r\n    driver.removeAll(MembershipState.class);\r\n    driver.removeAll(MountTable.class);\r\n    driver.removeAll(RouterState.class);\r\n    driver.removeAll(DisabledNameservice.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testInsert",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testInsert(StateStoreDriver driver, Class<T> recordClass) throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    assertTrue(driver.removeAll(recordClass));\r\n    QueryResult<T> queryResult0 = driver.get(recordClass);\r\n    List<T> records0 = queryResult0.getRecords();\r\n    assertTrue(records0.isEmpty());\r\n    BaseRecord record = generateFakeRecord(recordClass);\r\n    driver.put(record, true, false);\r\n    QueryResult<T> queryResult1 = driver.get(recordClass);\r\n    List<T> records1 = queryResult1.getRecords();\r\n    assertEquals(1, records1.size());\r\n    T record0 = records1.get(0);\r\n    validateRecord(record, record0, true);\r\n    List<T> insertList = new ArrayList<>();\r\n    for (int i = 0; i < 10; i++) {\r\n        T newRecord = generateFakeRecord(recordClass);\r\n        insertList.add(newRecord);\r\n    }\r\n    driver.putAll(insertList, true, false);\r\n    QueryResult<T> queryResult2 = driver.get(recordClass);\r\n    List<T> records2 = queryResult2.getRecords();\r\n    assertEquals(11, records2.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testFetchErrors",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testFetchErrors(StateStoreDriver driver, Class<T> clazz) throws IllegalAccessException, IOException\n{\r\n    driver.removeAll(clazz);\r\n    QueryResult<T> result0 = driver.get(clazz);\r\n    assertNotNull(result0);\r\n    List<T> records0 = result0.getRecords();\r\n    assertEquals(records0.size(), 0);\r\n    BaseRecord record = generateFakeRecord(clazz);\r\n    assertTrue(driver.put(record, true, false));\r\n    QueryResult<T> result1 = driver.get(clazz);\r\n    List<T> records1 = result1.getRecords();\r\n    assertEquals(1, records1.size());\r\n    validateRecord(record, records1.get(0), true);\r\n    final T fakeRecord = generateFakeRecord(clazz);\r\n    final Query<T> query = new Query<T>(fakeRecord);\r\n    T getRecord = driver.get(clazz, query);\r\n    assertNull(getRecord);\r\n    assertEquals(driver.getMultiple(clazz, query).size(), 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testPut",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testPut(StateStoreDriver driver, Class<T> clazz) throws IllegalArgumentException, ReflectiveOperationException, IOException, SecurityException\n{\r\n    driver.removeAll(clazz);\r\n    QueryResult<T> records = driver.get(clazz);\r\n    assertTrue(records.getRecords().isEmpty());\r\n    List<T> insertList = new ArrayList<>();\r\n    for (int i = 0; i < 10; i++) {\r\n        T newRecord = generateFakeRecord(clazz);\r\n        insertList.add(newRecord);\r\n    }\r\n    assertTrue(driver.putAll(insertList, false, true));\r\n    records = driver.get(clazz);\r\n    assertEquals(records.getRecords().size(), 10);\r\n    BaseRecord updatedRecord = generateFakeRecord(clazz);\r\n    BaseRecord existingRecord = records.getRecords().get(0);\r\n    Map<String, String> primaryKeys = existingRecord.getPrimaryKeys();\r\n    for (Entry<String, String> entry : primaryKeys.entrySet()) {\r\n        String key = entry.getKey();\r\n        String value = entry.getValue();\r\n        Class<?> fieldType = getFieldType(existingRecord, key);\r\n        Object field = fromString(value, fieldType);\r\n        assertTrue(setField(updatedRecord, key, field));\r\n    }\r\n    assertFalse(driver.put(updatedRecord, false, true));\r\n    QueryResult<T> newRecords = driver.get(clazz);\r\n    assertEquals(10, newRecords.getRecords().size());\r\n    assertEquals(\"A single entry was improperly updated in the store\", 10, countMatchingEntries(records.getRecords(), newRecords.getRecords()));\r\n    assertTrue(driver.put(updatedRecord, true, false));\r\n    newRecords = driver.get(clazz);\r\n    assertEquals(10, newRecords.getRecords().size());\r\n    T record = records.getRecords().get(0);\r\n    if (record.hasOtherFields()) {\r\n        assertEquals(\"Record of type \" + clazz + \" not updated in the store\", 9, countMatchingEntries(records.getRecords(), newRecords.getRecords()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "countMatchingEntries",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int countMatchingEntries(Collection<? extends BaseRecord> committedList, Collection<? extends BaseRecord> matchList)\n{\r\n    int matchingCount = 0;\r\n    for (BaseRecord committed : committedList) {\r\n        for (BaseRecord match : matchList) {\r\n            try {\r\n                if (match.getPrimaryKey().equals(committed.getPrimaryKey())) {\r\n                    if (validateRecord(match, committed, false)) {\r\n                        matchingCount++;\r\n                    }\r\n                    break;\r\n                }\r\n            } catch (Exception ex) {\r\n            }\r\n        }\r\n    }\r\n    return matchingCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testRemove",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testRemove(StateStoreDriver driver, Class<T> clazz) throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    assertTrue(driver.removeAll(clazz));\r\n    QueryResult<T> records = driver.get(clazz);\r\n    assertTrue(records.getRecords().isEmpty());\r\n    List<T> insertList = new ArrayList<>();\r\n    for (int i = 0; i < 10; i++) {\r\n        T newRecord = generateFakeRecord(clazz);\r\n        insertList.add(newRecord);\r\n    }\r\n    assertTrue(driver.putAll(insertList, false, true));\r\n    records = driver.get(clazz);\r\n    assertEquals(records.getRecords().size(), 10);\r\n    assertTrue(driver.remove(records.getRecords().get(0)));\r\n    records = driver.get(clazz);\r\n    assertEquals(records.getRecords().size(), 9);\r\n    final T firstRecord = records.getRecords().get(0);\r\n    final Query<T> query0 = new Query<T>(firstRecord);\r\n    assertTrue(driver.remove(clazz, query0) > 0);\r\n    final T secondRecord = records.getRecords().get(1);\r\n    final Query<T> query1 = new Query<T>(secondRecord);\r\n    assertTrue(driver.remove(clazz, query1) > 0);\r\n    records = driver.get(clazz);\r\n    assertEquals(records.getRecords().size(), 7);\r\n    assertTrue(driver.removeAll(clazz));\r\n    records = driver.get(clazz);\r\n    assertTrue(records.getRecords().isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testInsert",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testInsert(StateStoreDriver driver) throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    testInsert(driver, MembershipState.class);\r\n    testInsert(driver, MountTable.class);\r\n    testInsert(driver, RouterState.class);\r\n    testInsert(driver, DisabledNameservice.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testPut",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testPut(StateStoreDriver driver) throws IllegalArgumentException, ReflectiveOperationException, IOException, SecurityException\n{\r\n    testPut(driver, MembershipState.class);\r\n    testPut(driver, MountTable.class);\r\n    testPut(driver, RouterState.class);\r\n    testPut(driver, DisabledNameservice.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testRemove",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testRemove(StateStoreDriver driver) throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    testRemove(driver, MembershipState.class);\r\n    testRemove(driver, MountTable.class);\r\n    testRemove(driver, RouterState.class);\r\n    testRemove(driver, DisabledNameservice.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testFetchErrors",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testFetchErrors(StateStoreDriver driver) throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    testFetchErrors(driver, MembershipState.class);\r\n    testFetchErrors(driver, MountTable.class);\r\n    testFetchErrors(driver, RouterState.class);\r\n    testFetchErrors(driver, DisabledNameservice.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testMetrics",
  "errType" : null,
  "containingMethodsNum" : 42,
  "sourceCodeText" : "void testMetrics(StateStoreDriver driver) throws IOException, IllegalArgumentException, IllegalAccessException\n{\r\n    MountTable insertRecord = this.generateFakeRecord(MountTable.class);\r\n    StateStoreMetrics metrics = stateStore.getMetrics();\r\n    assertEquals(0, metrics.getWriteOps());\r\n    driver.put(insertRecord, true, false);\r\n    assertEquals(1, metrics.getWriteOps());\r\n    metrics.reset();\r\n    assertEquals(0, metrics.getWriteOps());\r\n    driver.put(insertRecord, true, false);\r\n    assertEquals(1, metrics.getWriteOps());\r\n    metrics.reset();\r\n    assertEquals(0, metrics.getReadOps());\r\n    final String querySourcePath = insertRecord.getSourcePath();\r\n    MountTable partial = MountTable.newInstance();\r\n    partial.setSourcePath(querySourcePath);\r\n    final Query<MountTable> query = new Query<>(partial);\r\n    driver.get(MountTable.class, query);\r\n    assertEquals(1, metrics.getReadOps());\r\n    metrics.reset();\r\n    assertEquals(0, metrics.getReadOps());\r\n    driver.get(MountTable.class);\r\n    assertEquals(1, metrics.getReadOps());\r\n    metrics.reset();\r\n    assertEquals(0, metrics.getReadOps());\r\n    driver.getMultiple(MountTable.class, query);\r\n    assertEquals(1, metrics.getReadOps());\r\n    metrics.reset();\r\n    assertEquals(0, metrics.getFailureOps());\r\n    driver.put(insertRecord, false, true);\r\n    assertEquals(1, metrics.getFailureOps());\r\n    metrics.reset();\r\n    assertEquals(0, metrics.getRemoveOps());\r\n    driver.remove(insertRecord);\r\n    assertEquals(1, metrics.getRemoveOps());\r\n    metrics.reset();\r\n    driver.put(insertRecord, true, false);\r\n    assertEquals(0, metrics.getRemoveOps());\r\n    driver.remove(MountTable.class, query);\r\n    assertEquals(1, metrics.getRemoveOps());\r\n    metrics.reset();\r\n    driver.put(insertRecord, true, false);\r\n    assertEquals(0, metrics.getRemoveOps());\r\n    driver.removeAll(MountTable.class);\r\n    assertEquals(1, metrics.getRemoveOps());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "setField",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean setField(BaseRecord record, String fieldName, Object data)\n{\r\n    Method m = locateSetter(record, fieldName);\r\n    if (m != null) {\r\n        try {\r\n            m.invoke(record, data);\r\n        } catch (Exception e) {\r\n            LOG.error(\"Cannot set field \" + fieldName + \" on object \" + record.getClass().getName() + \" to data \" + data + \" of type \" + data.getClass(), e);\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "locateSetter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Method locateSetter(BaseRecord record, String fieldName)\n{\r\n    for (Method m : record.getClass().getMethods()) {\r\n        if (m.getName().equalsIgnoreCase(\"set\" + fieldName)) {\r\n            return m;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "getFields",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Map<String, Class<?>> getFields(BaseRecord record)\n{\r\n    Map<String, Class<?>> getters = new HashMap<>();\r\n    for (Method m : record.getClass().getDeclaredMethods()) {\r\n        if (m.getName().startsWith(\"get\")) {\r\n            try {\r\n                Class<?> type = m.getReturnType();\r\n                char[] c = m.getName().substring(3).toCharArray();\r\n                c[0] = Character.toLowerCase(c[0]);\r\n                String key = new String(c);\r\n                getters.put(key, type);\r\n            } catch (Exception e) {\r\n                LOG.error(\"Cannot execute getter \" + m.getName() + \" on object \" + record);\r\n            }\r\n        }\r\n    }\r\n    return getters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "getFieldType",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Class<?> getFieldType(BaseRecord record, String fieldName)\n{\r\n    Method m = locateGetter(record, fieldName);\r\n    return m.getReturnType();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "getField",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Object getField(BaseRecord record, String fieldName)\n{\r\n    Object result = null;\r\n    Method m = locateGetter(record, fieldName);\r\n    if (m != null) {\r\n        try {\r\n            result = m.invoke(record);\r\n        } catch (Exception e) {\r\n            LOG.error(\"Cannot get field \" + fieldName + \" on object \" + record);\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "locateGetter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Method locateGetter(BaseRecord record, String fieldName)\n{\r\n    for (Method m : record.getClass().getMethods()) {\r\n        if (m.getName().equalsIgnoreCase(\"get\" + fieldName)) {\r\n            return m;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "fromString",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "T fromString(String data, Class<T> clazz)\n{\r\n    if (data.equals(\"null\")) {\r\n        return null;\r\n    } else if (clazz == String.class) {\r\n        return (T) data;\r\n    } else if (clazz == Long.class || clazz == long.class) {\r\n        return (T) Long.valueOf(data);\r\n    } else if (clazz == Integer.class || clazz == int.class) {\r\n        return (T) Integer.valueOf(data);\r\n    } else if (clazz == Double.class || clazz == double.class) {\r\n        return (T) Double.valueOf(data);\r\n    } else if (clazz == Float.class || clazz == float.class) {\r\n        return (T) Float.valueOf(data);\r\n    } else if (clazz == Boolean.class || clazz == boolean.class) {\r\n        return (T) Boolean.valueOf(data);\r\n    } else if (clazz.isEnum()) {\r\n        return (T) Enum.valueOf((Class<Enum>) clazz, data);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    LOG.info(\"Start the Namenodes\");\r\n    Configuration nnConf = new HdfsConfiguration();\r\n    nnConf.setInt(DFSConfigKeys.DFS_NAMENODE_HANDLER_COUNT_KEY, 10);\r\n    for (final String nsId : asList(\"ns0\", \"ns1\")) {\r\n        MockNamenode nn = new MockNamenode(nsId, nnConf);\r\n        nn.transitionToActive();\r\n        nn.addFileSystemMock();\r\n        namenodes.put(nsId, nn);\r\n    }\r\n    LOG.info(\"Start the Routers\");\r\n    Configuration routerConf = new RouterConfigBuilder().stateStore().admin().rpc().build();\r\n    routerConf.set(RBFConfigKeys.DFS_ROUTER_RPC_ADDRESS_KEY, \"0.0.0.0:0\");\r\n    routerConf.set(RBFConfigKeys.DFS_ROUTER_HTTP_ADDRESS_KEY, \"0.0.0.0:0\");\r\n    routerConf.set(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, \"0.0.0.0:0\");\r\n    Configuration stateStoreConf = getStateStoreConfiguration();\r\n    stateStoreConf.setClass(RBFConfigKeys.FEDERATION_NAMENODE_RESOLVER_CLIENT_CLASS, MembershipNamenodeResolver.class, ActiveNamenodeResolver.class);\r\n    stateStoreConf.setClass(RBFConfigKeys.FEDERATION_FILE_RESOLVER_CLIENT_CLASS, MultipleDestinationMountTableResolver.class, FileSubclusterResolver.class);\r\n    routerConf.addResource(stateStoreConf);\r\n    routerConf.setBoolean(RBFConfigKeys.DFS_ROUTER_ALLOW_PARTIAL_LIST, false);\r\n    router = new Router();\r\n    router.init(routerConf);\r\n    router.start();\r\n    LOG.info(\"Registering the subclusters in the Routers\");\r\n    registerSubclusters(router, namenodes.values());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void cleanup() throws Exception\n{\r\n    LOG.info(\"Stopping the cluster\");\r\n    for (final MockNamenode nn : namenodes.values()) {\r\n        nn.stop();\r\n    }\r\n    namenodes.clear();\r\n    if (router != null) {\r\n        router.stop();\r\n        router = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSuccess",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSuccess() throws Exception\n{\r\n    FileSystem fs = getFileSystem(router);\r\n    String mountPoint = \"/test-success\";\r\n    createMountTableEntry(router, mountPoint, DestinationOrder.HASH_ALL, namenodes.keySet());\r\n    Path folder = new Path(mountPoint, \"folder-all\");\r\n    for (int i = 0; i < NUM_FILES; i++) {\r\n        Path file = new Path(folder, \"file-\" + i + \".txt\");\r\n        FSDataOutputStream os = fs.create(file);\r\n        os.close();\r\n    }\r\n    FileStatus[] files = fs.listStatus(folder);\r\n    assertEquals(NUM_FILES, files.length);\r\n    ContentSummary contentSummary = fs.getContentSummary(folder);\r\n    assertEquals(NUM_FILES, contentSummary.getFileCount());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testFileNotFound",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testFileNotFound() throws Exception\n{\r\n    FileSystem fs = getFileSystem(router);\r\n    String mountPoint = \"/test-non-existing\";\r\n    createMountTableEntry(router, mountPoint, DestinationOrder.HASH_ALL, namenodes.keySet());\r\n    Path path = new Path(mountPoint, \"folder-all\");\r\n    LambdaTestUtils.intercept(FileNotFoundException.class, () -> fs.listStatus(path));\r\n    LambdaTestUtils.intercept(FileNotFoundException.class, () -> fs.getContentSummary(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testOneMissing",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testOneMissing() throws Exception\n{\r\n    FileSystem fs = getFileSystem(router);\r\n    String mountPoint = \"/test-one-missing\";\r\n    createMountTableEntry(router, mountPoint, DestinationOrder.HASH_ALL, namenodes.keySet());\r\n    MockNamenode nn = namenodes.get(\"ns0\");\r\n    int nnRpcPort = nn.getRPCPort();\r\n    FileSystem nnFs = getFileSystem(nnRpcPort);\r\n    Path folder = new Path(mountPoint, \"folder-all\");\r\n    for (int i = 0; i < NUM_FILES; i++) {\r\n        Path file = new Path(folder, \"file-\" + i + \".txt\");\r\n        FSDataOutputStream os = nnFs.create(file);\r\n        os.close();\r\n    }\r\n    FileStatus[] files = fs.listStatus(folder);\r\n    assertEquals(NUM_FILES, files.length);\r\n    ContentSummary summary = fs.getContentSummary(folder);\r\n    assertEquals(NUM_FILES, summary.getFileAndDirectoryCount());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setUp()\n{\r\n    conf = new Configuration(false);\r\n    conf.setClass(\"hadoop.security.group.mapping\", TestRouterUserMappings.MockUnixGroupsMapping.class, GroupMappingServiceProvider.class);\r\n    conf.setLong(\"hadoop.security.groups.cache.secs\", GROUP_REFRESH_TIMEOUT_SEC);\r\n    conf = new RouterConfigBuilder(conf).rpc().admin().build();\r\n    Groups.getUserToGroupsMappingService(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setUpMultiRoutersAndReturnDefaultFs",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "String setUpMultiRoutersAndReturnDefaultFs() throws Exception\n{\r\n    cluster = new MiniRouterDFSCluster(true, 2);\r\n    cluster.addRouterOverrides(conf);\r\n    cluster.startRouters();\r\n    conf.set(DFSConfigKeys.DFS_INTERNAL_NAMESERVICES_KEY, \"ns0,ns1\");\r\n    conf.set(DFSConfigKeys.DFS_NAMESERVICES, \"ns0,ns1,\" + ROUTER_NS);\r\n    conf.set(HdfsClientConfigKeys.Failover.PROXY_PROVIDER_KEY_PREFIX + \".\" + ROUTER_NS, ConfiguredFailoverProxyProvider.class.getCanonicalName());\r\n    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, HDFS_SCHEMA + ROUTER_NS);\r\n    conf.set(DFSConfigKeys.DFS_HA_NAMENODES_KEY_PREFIX + \".\" + ROUTER_NS, \"r1,r2\");\r\n    List<MiniRouterDFSCluster.RouterContext> routers = cluster.getRouters();\r\n    for (int i = 0; i < routers.size(); i++) {\r\n        MiniRouterDFSCluster.RouterContext context = routers.get(i);\r\n        conf.set(DFSConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY + \".\" + ROUTER_NS + \".r\" + (i + 1), LOOPBACK_ADDRESS + \":\" + context.getRouter().getRpcServerAddress().getPort());\r\n    }\r\n    return HDFS_SCHEMA + ROUTER_NS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRefreshSuperUserGroupsConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRefreshSuperUserGroupsConfiguration() throws Exception\n{\r\n    testRefreshSuperUserGroupsConfigurationInternal(setUpMultiRoutersAndReturnDefaultFs());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGroupMappingRefresh",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGroupMappingRefresh() throws Exception\n{\r\n    testGroupMappingRefreshInternal(setUpMultiRoutersAndReturnDefaultFs());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRefreshSuperUserGroupsConfigurationInternal",
  "errType" : [ "AuthorizationException", "AuthorizationException" ],
  "containingMethodsNum" : 39,
  "sourceCodeText" : "void testRefreshSuperUserGroupsConfigurationInternal(String defaultFs) throws Exception\n{\r\n    final String superUser = \"super_user\";\r\n    final List<String> groupNames1 = new ArrayList<>();\r\n    groupNames1.add(\"gr1\");\r\n    groupNames1.add(\"gr2\");\r\n    final List<String> groupNames2 = new ArrayList<>();\r\n    groupNames2.add(\"gr3\");\r\n    groupNames2.add(\"gr4\");\r\n    final Set<String> groupNamesSet1 = new LinkedHashSet<>();\r\n    groupNamesSet1.addAll(groupNames1);\r\n    final Set<String> groupNamesSet2 = new LinkedHashSet<>();\r\n    groupNamesSet2.addAll(groupNames2);\r\n    String userKeyGroups = DefaultImpersonationProvider.getTestProvider().getProxySuperuserGroupConfKey(superUser);\r\n    String userKeyHosts = DefaultImpersonationProvider.getTestProvider().getProxySuperuserIpConfKey(superUser);\r\n    conf.set(userKeyGroups, \"gr3,gr4,gr5\");\r\n    conf.set(userKeyHosts, LOOPBACK_ADDRESS);\r\n    ProxyUsers.refreshSuperUserGroupsConfiguration(conf);\r\n    UserGroupInformation ugi1 = mock(UserGroupInformation.class);\r\n    UserGroupInformation ugi2 = mock(UserGroupInformation.class);\r\n    UserGroupInformation suUgi = mock(UserGroupInformation.class);\r\n    when(ugi1.getRealUser()).thenReturn(suUgi);\r\n    when(ugi2.getRealUser()).thenReturn(suUgi);\r\n    when(suUgi.getShortUserName()).thenReturn(superUser);\r\n    when(suUgi.getUserName()).thenReturn(superUser + \"L\");\r\n    when(ugi1.getShortUserName()).thenReturn(\"user1\");\r\n    when(ugi2.getShortUserName()).thenReturn(\"user2\");\r\n    when(ugi1.getUserName()).thenReturn(\"userL1\");\r\n    when(ugi2.getUserName()).thenReturn(\"userL2\");\r\n    when(ugi1.getGroups()).thenReturn(groupNames1);\r\n    when(ugi2.getGroups()).thenReturn(groupNames2);\r\n    when(ugi1.getGroupsSet()).thenReturn(groupNamesSet1);\r\n    when(ugi2.getGroupsSet()).thenReturn(groupNamesSet2);\r\n    LambdaTestUtils.intercept(AuthorizationException.class, () -> ProxyUsers.authorize(ugi1, LOOPBACK_ADDRESS));\r\n    try {\r\n        ProxyUsers.authorize(ugi2, LOOPBACK_ADDRESS);\r\n        LOG.info(\"auth for {} succeeded\", ugi2.getUserName());\r\n    } catch (AuthorizationException e) {\r\n        fail(\"first auth for \" + ugi2.getShortUserName() + \" should've succeeded: \" + e.getLocalizedMessage());\r\n    }\r\n    String rsrc = \"testGroupMappingRefresh_rsrc.xml\";\r\n    tempResource = addNewConfigResource(rsrc, userKeyGroups, \"gr2\", userKeyHosts, LOOPBACK_ADDRESS);\r\n    conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY, defaultFs);\r\n    DFSAdmin admin = new DFSAdmin(conf);\r\n    String[] args = new String[] { \"-refreshSuperUserGroupsConfiguration\" };\r\n    admin.run(args);\r\n    LambdaTestUtils.intercept(AuthorizationException.class, () -> ProxyUsers.authorize(ugi2, LOOPBACK_ADDRESS));\r\n    try {\r\n        ProxyUsers.authorize(ugi1, LOOPBACK_ADDRESS);\r\n        LOG.info(\"auth for {} succeeded\", ugi1.getUserName());\r\n    } catch (AuthorizationException e) {\r\n        fail(\"second auth for \" + ugi1.getShortUserName() + \" should've succeeded: \" + e.getLocalizedMessage());\r\n    }\r\n    testGroupsForUserCLI(conf, \"user\");\r\n    testGroupsForUserProtocol(conf, \"user\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGroupsForUserCLI",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testGroupsForUserCLI(Configuration config, String username) throws Exception\n{\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    PrintStream oldOut = System.out;\r\n    System.setOut(new PrintStream(out));\r\n    new GetGroups(config).run(new String[] { username });\r\n    assertTrue(\"Wrong output: \" + out, out.toString().startsWith(username + \" : \" + username));\r\n    out.reset();\r\n    System.setOut(oldOut);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGroupsForUserProtocol",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGroupsForUserProtocol(Configuration config, String username) throws IOException\n{\r\n    GetUserMappingsProtocol proto = NameNodeProxies.createProxy(config, FileSystem.getDefaultUri(config), GetUserMappingsProtocol.class).getProxy();\r\n    String[] groups = proto.getGroupsForUser(username);\r\n    assertArrayEquals(new String[] { \"user1\", \"user2\" }, groups);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGroupMappingRefreshInternal",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testGroupMappingRefreshInternal(String defaultFs) throws Exception\n{\r\n    Groups groups = Groups.getUserToGroupsMappingService(conf);\r\n    String user = \"test_user123\";\r\n    LOG.info(\"First attempt:\");\r\n    List<String> g1 = groups.getGroups(user);\r\n    LOG.info(\"Group 1 :{}\", g1);\r\n    LOG.info(\"Second attempt, should be the same:\");\r\n    List<String> g2 = groups.getGroups(user);\r\n    LOG.info(\"Group 2 :{}\", g2);\r\n    for (int i = 0; i < g2.size(); i++) {\r\n        assertEquals(\"Should be same group \", g1.get(i), g2.get(i));\r\n    }\r\n    conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY, defaultFs);\r\n    DFSAdmin admin = new DFSAdmin(conf);\r\n    String[] args = new String[] { \"-refreshUserToGroupsMappings\" };\r\n    admin.run(args);\r\n    LOG.info(\"Third attempt(after refresh command), should be different:\");\r\n    List<String> g3 = groups.getGroups(user);\r\n    LOG.info(\"Group 3:{}\", g3);\r\n    for (int i = 0; i < g3.size(); i++) {\r\n        assertNotEquals(\"Should be different group: \" + g1.get(i) + \" and \" + g3.get(i), g1.get(i), g3.get(i));\r\n    }\r\n    LOG.info(\"Fourth attempt(after timeout), should be different:\");\r\n    GenericTestUtils.waitFor(() -> {\r\n        List<String> g4;\r\n        try {\r\n            g4 = groups.getGroups(user);\r\n        } catch (IOException e) {\r\n            LOG.debug(\"Failed to get groups for user:{}\", user);\r\n            return false;\r\n        }\r\n        LOG.info(\"Group 4 : {}\", g4);\r\n        return !g3.equals(g4);\r\n    }, 50, Math.toIntExact(TimeUnit.SECONDS.toMillis(GROUP_REFRESH_TIMEOUT_SEC * 30)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addNewConfigResource",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String addNewConfigResource(String rsrcName, String keyGroup, String groups, String keyHosts, String hosts) throws FileNotFoundException, UnsupportedEncodingException\n{\r\n    Configuration conf = new Configuration();\r\n    URL url = conf.getResource(\"hdfs-site.xml\");\r\n    String urlPath = URLDecoder.decode(url.getPath(), \"UTF-8\");\r\n    Path p = new Path(urlPath);\r\n    Path dir = p.getParent();\r\n    String tmp = dir.toString() + \"/\" + rsrcName;\r\n    StringBuilder newResource = new StringBuilder().append(\"<configuration>\").append(\"<property>\").append(\"<name>\").append(keyGroup).append(\"</name>\").append(\"<value>\").append(groups).append(\"</value>\").append(\"</property>\").append(\"<property>\").append(\"<name>\").append(keyHosts).append(\"</name>\").append(\"<value>\").append(hosts).append(\"</value>\").append(\"</property>\").append(\"</configuration>\");\r\n    PrintWriter writer = new PrintWriter(new FileOutputStream(tmp));\r\n    writer.println(newResource.toString());\r\n    writer.close();\r\n    Configuration.addDefaultResource(rsrcName);\r\n    return tmp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (router != null) {\r\n        router.shutDown();\r\n        router = null;\r\n    }\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n    if (tempResource != null) {\r\n        File f = new File(tempResource);\r\n        f.delete();\r\n        tempResource = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security\\token",
  "methodName" : "testMultiNodeOperationWithoutWatch",
  "errType" : [ "SecretManager.InvalidToken", "SecretManager.InvalidToken" ],
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testMultiNodeOperationWithoutWatch() throws Exception\n{\r\n    String connectString = zkServer.getConnectString();\r\n    Configuration conf = getSecretConf(connectString);\r\n    conf.setBoolean(ZK_DTSM_TOKEN_WATCHER_ENABLED, false);\r\n    conf.setInt(ZK_DTSM_ROUTER_TOKEN_SYNC_INTERVAL, 3);\r\n    for (int i = 0; i < TEST_RETRIES; i++) {\r\n        ZKDelegationTokenSecretManagerImpl dtsm1 = new ZKDelegationTokenSecretManagerImpl(conf);\r\n        ZKDelegationTokenSecretManagerImpl dtsm2 = new ZKDelegationTokenSecretManagerImpl(conf);\r\n        DelegationTokenManager tm1, tm2;\r\n        tm1 = new DelegationTokenManager(conf, new Text(\"bla\"));\r\n        tm1.setExternalDelegationTokenSecretManager(dtsm1);\r\n        tm2 = new DelegationTokenManager(conf, new Text(\"bla\"));\r\n        tm2.setExternalDelegationTokenSecretManager(dtsm2);\r\n        Token<DelegationTokenIdentifier> token = (Token<DelegationTokenIdentifier>) tm1.createToken(UserGroupInformation.getCurrentUser(), \"foo\");\r\n        Assert.assertNotNull(token);\r\n        tm2.verifyToken(token);\r\n        tm2.renewToken(token, \"foo\");\r\n        tm1.verifyToken(token);\r\n        tm1.cancelToken(token, \"foo\");\r\n        try {\r\n            verifyTokenFail(tm2, token);\r\n            fail(\"Expected InvalidToken\");\r\n        } catch (SecretManager.InvalidToken it) {\r\n        }\r\n        token = (Token<DelegationTokenIdentifier>) tm2.createToken(UserGroupInformation.getCurrentUser(), \"bar\");\r\n        Assert.assertNotNull(token);\r\n        tm1.verifyToken(token);\r\n        tm1.renewToken(token, \"bar\");\r\n        tm2.verifyToken(token);\r\n        tm2.cancelToken(token, \"bar\");\r\n        try {\r\n            verifyTokenFail(tm1, token);\r\n            fail(\"Expected InvalidToken\");\r\n        } catch (SecretManager.InvalidToken it) {\r\n        }\r\n        dtsm1.stopThreads();\r\n        dtsm2.stopThreads();\r\n        verifyDestroy(tm1, conf);\r\n        verifyDestroy(tm2, conf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security\\token",
  "methodName" : "testMultiNodeTokenRemovalShortSyncWithoutWatch",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testMultiNodeTokenRemovalShortSyncWithoutWatch() throws Exception\n{\r\n    String connectString = zkServer.getConnectString();\r\n    Configuration conf = getSecretConf(connectString);\r\n    conf.setBoolean(ZK_DTSM_TOKEN_WATCHER_ENABLED, false);\r\n    conf.setInt(ZK_DTSM_ROUTER_TOKEN_SYNC_INTERVAL, 3);\r\n    conf.setInt(RENEW_INTERVAL, 10);\r\n    conf.setInt(REMOVAL_SCAN_INTERVAL, 10);\r\n    for (int i = 0; i < TEST_RETRIES; i++) {\r\n        ZKDelegationTokenSecretManagerImpl dtsm1 = new ZKDelegationTokenSecretManagerImpl(conf);\r\n        ZKDelegationTokenSecretManagerImpl dtsm2 = new ZKDelegationTokenSecretManagerImpl(conf);\r\n        DelegationTokenManager tm1, tm2;\r\n        tm1 = new DelegationTokenManager(conf, new Text(\"bla\"));\r\n        tm1.setExternalDelegationTokenSecretManager(dtsm1);\r\n        tm2 = new DelegationTokenManager(conf, new Text(\"bla\"));\r\n        tm2.setExternalDelegationTokenSecretManager(dtsm2);\r\n        Token<DelegationTokenIdentifier> token = (Token<DelegationTokenIdentifier>) tm1.createToken(UserGroupInformation.getCurrentUser(), \"foo\");\r\n        Assert.assertNotNull(token);\r\n        tm2.verifyToken(token);\r\n        Thread.sleep(9 * 1000);\r\n        tm2.renewToken(token, \"foo\");\r\n        tm1.verifyToken(token);\r\n        Thread.sleep(4 * 1000);\r\n        tm1.verifyToken(token);\r\n        tm2.verifyToken(token);\r\n        dtsm1.stopThreads();\r\n        dtsm2.stopThreads();\r\n        verifyDestroy(tm1, conf);\r\n        verifyDestroy(tm2, conf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security\\token",
  "methodName" : "testMultiNodeTokenRemovalLongSyncWithoutWatch",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testMultiNodeTokenRemovalLongSyncWithoutWatch() throws Exception\n{\r\n    String connectString = zkServer.getConnectString();\r\n    Configuration conf = getSecretConf(connectString);\r\n    conf.setBoolean(ZK_DTSM_TOKEN_WATCHER_ENABLED, false);\r\n    conf.setInt(ZK_DTSM_ROUTER_TOKEN_SYNC_INTERVAL, 20);\r\n    conf.setInt(RENEW_INTERVAL, 10);\r\n    conf.setInt(REMOVAL_SCAN_INTERVAL, 10);\r\n    for (int i = 0; i < TEST_RETRIES; i++) {\r\n        ZKDelegationTokenSecretManagerImpl dtsm1 = new ZKDelegationTokenSecretManagerImpl(conf);\r\n        ZKDelegationTokenSecretManagerImpl dtsm2 = new ZKDelegationTokenSecretManagerImpl(conf);\r\n        ZKDelegationTokenSecretManagerImpl dtsm3 = new ZKDelegationTokenSecretManagerImpl(conf);\r\n        DelegationTokenManager tm1, tm2, tm3;\r\n        tm1 = new DelegationTokenManager(conf, new Text(\"bla\"));\r\n        tm1.setExternalDelegationTokenSecretManager(dtsm1);\r\n        tm2 = new DelegationTokenManager(conf, new Text(\"bla\"));\r\n        tm2.setExternalDelegationTokenSecretManager(dtsm2);\r\n        tm3 = new DelegationTokenManager(conf, new Text(\"bla\"));\r\n        tm3.setExternalDelegationTokenSecretManager(dtsm3);\r\n        Token<DelegationTokenIdentifier> token = (Token<DelegationTokenIdentifier>) tm1.createToken(UserGroupInformation.getCurrentUser(), \"foo\");\r\n        Assert.assertNotNull(token);\r\n        tm2.verifyToken(token);\r\n        Thread.sleep(9 * 1000);\r\n        long renewalTime = tm2.renewToken(token, \"foo\");\r\n        LOG.info(\"Renew for token {} at current time {} renewal time {}\", token.getIdentifier(), Time.formatTime(Time.now()), Time.formatTime(renewalTime));\r\n        tm1.verifyToken(token);\r\n        Thread.sleep(4 * 1000);\r\n        tm2.verifyToken(token);\r\n        tm3.verifyToken(token);\r\n        dtsm1.stopThreads();\r\n        dtsm2.stopThreads();\r\n        dtsm3.stopThreads();\r\n        verifyDestroy(tm1, conf);\r\n        verifyDestroy(tm2, conf);\r\n        verifyDestroy(tm3, conf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    curatorTestingServer = new TestingServer();\r\n    curatorTestingServer.start();\r\n    final String connectString = curatorTestingServer.getConnectString();\r\n    int numNameservices = 2;\r\n    cluster = new MiniRouterDFSCluster(false, numNameservices);\r\n    Configuration conf = new RouterConfigBuilder().refreshCache().admin().rpc().heartbeat().build();\r\n    conf.setClass(RBFConfigKeys.FEDERATION_FILE_RESOLVER_CLIENT_CLASS, RBFConfigKeys.FEDERATION_FILE_RESOLVER_CLIENT_CLASS_DEFAULT, FileSubclusterResolver.class);\r\n    conf.set(CommonConfigurationKeys.ZK_ADDRESS, connectString);\r\n    conf.setBoolean(RBFConfigKeys.DFS_ROUTER_STORE_ENABLE, true);\r\n    cluster.addRouterOverrides(conf);\r\n    cluster.startCluster();\r\n    cluster.startRouters();\r\n    cluster.waitClusterUp();\r\n    routerContext = cluster.getRandomRouter();\r\n    RouterStore routerStateManager = routerContext.getRouter().getRouterStateManager();\r\n    mountTableManager = routerContext.getAdminClient().getMountTableManager();\r\n    FederationTestUtils.waitRouterRegistered(routerStateManager, numNameservices, 60000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "destory",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void destory()\n{\r\n    try {\r\n        curatorTestingServer.close();\r\n        cluster.shutdown();\r\n    } catch (IOException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws IOException\n{\r\n    clearEntries();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "clearEntries",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void clearEntries() throws IOException\n{\r\n    List<MountTable> result = getMountTableEntries();\r\n    for (MountTable mountTable : result) {\r\n        RemoveMountTableEntryResponse removeMountTableEntry = mountTableManager.removeMountTableEntry(RemoveMountTableEntryRequest.newInstance(mountTable.getSourcePath()));\r\n        assertTrue(removeMountTableEntry.getStatus());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountTableEntriesCacheUpdatedAfterAddAPICall",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testMountTableEntriesCacheUpdatedAfterAddAPICall() throws IOException\n{\r\n    int existingEntriesCount = getNumMountTableEntries();\r\n    String srcPath = \"/addPath\";\r\n    MountTable newEntry = MountTable.newInstance(srcPath, Collections.singletonMap(\"ns0\", \"/addPathDest\"), Time.now(), Time.now());\r\n    addMountTableEntry(mountTableManager, newEntry);\r\n    List<RouterContext> routers = getRouters();\r\n    for (RouterContext rc : routers) {\r\n        List<MountTable> result = getMountTableEntries(rc.getAdminClient().getMountTableManager());\r\n        assertEquals(1 + existingEntriesCount, result.size());\r\n        MountTable mountTableResult = result.get(0);\r\n        assertEquals(srcPath, mountTableResult.getSourcePath());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountTableEntriesCacheUpdatedAfterRemoveAPICall",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testMountTableEntriesCacheUpdatedAfterRemoveAPICall() throws IOException\n{\r\n    String srcPath = \"/removePathSrc\";\r\n    MountTable newEntry = MountTable.newInstance(srcPath, Collections.singletonMap(\"ns0\", \"/removePathDest\"), Time.now(), Time.now());\r\n    addMountTableEntry(mountTableManager, newEntry);\r\n    int addCount = getNumMountTableEntries();\r\n    assertEquals(1, addCount);\r\n    RemoveMountTableEntryResponse removeMountTableEntry = mountTableManager.removeMountTableEntry(RemoveMountTableEntryRequest.newInstance(srcPath));\r\n    assertTrue(removeMountTableEntry.getStatus());\r\n    int removeCount = getNumMountTableEntries();\r\n    assertEquals(addCount - 1, removeCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountTableEntriesCacheUpdatedAfterUpdateAPICall",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testMountTableEntriesCacheUpdatedAfterUpdateAPICall() throws IOException\n{\r\n    String srcPath = \"/updatePathSrc\";\r\n    MountTable newEntry = MountTable.newInstance(srcPath, Collections.singletonMap(\"ns0\", \"/updatePathDest\"), Time.now(), Time.now());\r\n    addMountTableEntry(mountTableManager, newEntry);\r\n    int addCount = getNumMountTableEntries();\r\n    assertEquals(1, addCount);\r\n    String key = \"ns1\";\r\n    String value = \"/updatePathDest2\";\r\n    MountTable upateEntry = MountTable.newInstance(srcPath, Collections.singletonMap(key, value), Time.now(), Time.now());\r\n    UpdateMountTableEntryResponse updateMountTableEntry = mountTableManager.updateMountTableEntry(UpdateMountTableEntryRequest.newInstance(upateEntry));\r\n    assertTrue(updateMountTableEntry.getStatus());\r\n    MountTable updatedMountTable = getMountTableEntry(srcPath);\r\n    assertNotNull(\"Updated mount table entrty cannot be null\", updatedMountTable);\r\n    assertEquals(1, updatedMountTable.getDestinations().size());\r\n    assertEquals(key, updatedMountTable.getDestinations().get(0).getNameserviceId());\r\n    assertEquals(value, updatedMountTable.getDestinations().get(0).getDest());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testCachedRouterClientBehaviourAfterRouterStoped",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCachedRouterClientBehaviourAfterRouterStoped() throws IOException\n{\r\n    String srcPath = \"/addPathClientCache\";\r\n    MountTable newEntry = MountTable.newInstance(srcPath, Collections.singletonMap(\"ns0\", \"/addPathClientCacheDest\"), Time.now(), Time.now());\r\n    addMountTableEntry(mountTableManager, newEntry);\r\n    List<RouterContext> routers = getRouters();\r\n    for (RouterContext rc : routers) {\r\n        List<MountTable> result = getMountTableEntries(rc.getAdminClient().getMountTableManager());\r\n        assertEquals(1, result.size());\r\n        MountTable mountTableResult = result.get(0);\r\n        assertEquals(srcPath, mountTableResult.getSourcePath());\r\n    }\r\n    for (RouterContext rc : routers) {\r\n        InetSocketAddress adminServerAddress = rc.getRouter().getAdminServerAddress();\r\n        if (!routerContext.getRouter().getAdminServerAddress().equals(adminServerAddress)) {\r\n            cluster.stopRouter(rc);\r\n            break;\r\n        }\r\n    }\r\n    srcPath = \"/addPathClientCache2\";\r\n    newEntry = MountTable.newInstance(srcPath, Collections.singletonMap(\"ns0\", \"/addPathClientCacheDest2\"), Time.now(), Time.now());\r\n    addMountTableEntry(mountTableManager, newEntry);\r\n    for (RouterContext rc : getRouters()) {\r\n        List<MountTable> result = getMountTableEntries(rc.getAdminClient().getMountTableManager());\r\n        assertEquals(2, result.size());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouters",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<RouterContext> getRouters()\n{\r\n    List<RouterContext> result = new ArrayList<>();\r\n    for (RouterContext rc : cluster.getRouters()) {\r\n        if (rc.getRouter().getServiceState() == STATE.STARTED) {\r\n            result.add(rc);\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRefreshMountTableEntriesAPI",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testRefreshMountTableEntriesAPI() throws IOException\n{\r\n    RefreshMountTableEntriesRequest request = RefreshMountTableEntriesRequest.newInstance();\r\n    RefreshMountTableEntriesResponse refreshMountTableEntriesRes = mountTableManager.refreshMountTableEntries(request);\r\n    assertTrue(refreshMountTableEntriesRes.getResult());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountTableEntriesCacheUpdateTimeout",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMountTableEntriesCacheUpdateTimeout() throws IOException\n{\r\n    @SuppressWarnings(\"resource\")\r\n    MountTableRefresherService mountTableRefresherService = new MountTableRefresherService(routerContext.getRouter()) {\r\n\r\n        @Override\r\n        protected MountTableRefresherThread getLocalRefresher(String adminAddress) {\r\n            return new MountTableRefresherThread(null, adminAddress) {\r\n\r\n                @Override\r\n                public void run() {\r\n                    try {\r\n                        Thread.sleep(60000);\r\n                    } catch (InterruptedException e) {\r\n                    }\r\n                }\r\n            };\r\n        }\r\n    };\r\n    Configuration config = routerContext.getRouter().getConfig();\r\n    config.setTimeDuration(RBFConfigKeys.MOUNT_TABLE_CACHE_UPDATE_TIMEOUT, 5, TimeUnit.SECONDS);\r\n    mountTableRefresherService.init(config);\r\n    mountTableRefresherService.refresh();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRouterClientConnectionExpiration",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRouterClientConnectionExpiration() throws Exception\n{\r\n    final AtomicInteger createCounter = new AtomicInteger();\r\n    final AtomicInteger removeCounter = new AtomicInteger();\r\n    @SuppressWarnings(\"resource\")\r\n    MountTableRefresherService mountTableRefresherService = new MountTableRefresherService(routerContext.getRouter()) {\r\n\r\n        @Override\r\n        protected void closeRouterClient(RouterClient client) {\r\n            super.closeRouterClient(client);\r\n            removeCounter.incrementAndGet();\r\n        }\r\n\r\n        @Override\r\n        protected RouterClient createRouterClient(InetSocketAddress routerSocket, Configuration config) throws IOException {\r\n            createCounter.incrementAndGet();\r\n            return super.createRouterClient(routerSocket, config);\r\n        }\r\n    };\r\n    int clientCacheTime = 2000;\r\n    Configuration config = routerContext.getRouter().getConfig();\r\n    config.setTimeDuration(RBFConfigKeys.MOUNT_TABLE_CACHE_UPDATE_CLIENT_MAX_TIME, clientCacheTime, TimeUnit.MILLISECONDS);\r\n    mountTableRefresherService.init(config);\r\n    mountTableRefresherService.refresh();\r\n    assertNotEquals(\"No RouterClient is created.\", 0, createCounter.get());\r\n    GenericTestUtils.waitFor(() -> createCounter.get() == removeCounter.get(), 100, 3 * clientCacheTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumMountTableEntries",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getNumMountTableEntries() throws IOException\n{\r\n    List<MountTable> records = getMountTableEntries();\r\n    int oldEntriesCount = records.size();\r\n    return oldEntriesCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "MountTable getMountTableEntry(String srcPath) throws IOException\n{\r\n    List<MountTable> mountTableEntries = getMountTableEntries();\r\n    for (MountTable mountTable : mountTableEntries) {\r\n        String sourcePath = mountTable.getSourcePath();\r\n        if (srcPath.equals(sourcePath)) {\r\n            return mountTable;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addMountTableEntry(MountTableManager mountTableMgr, MountTable newEntry) throws IOException\n{\r\n    AddMountTableEntryRequest addRequest = AddMountTableEntryRequest.newInstance(newEntry);\r\n    AddMountTableEntryResponse addResponse = mountTableMgr.addMountTableEntry(addRequest);\r\n    assertTrue(addResponse.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTableEntries",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<MountTable> getMountTableEntries() throws IOException\n{\r\n    return getMountTableEntries(mountTableManager);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTableEntries",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<MountTable> getMountTableEntries(MountTableManager mountTableManagerParam) throws IOException\n{\r\n    GetMountTableEntriesRequest request = GetMountTableEntriesRequest.newInstance(\"/\");\r\n    return mountTableManagerParam.getMountTableEntries(request).getEntries();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    router = new Router();\r\n    router.setRouterId(routerId);\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(RBFConfigKeys.DFS_ROUTER_CACHE_TIME_TO_LIVE_MS, 1);\r\n    Configuration routerConfig = new RouterConfigBuilder(conf).stateStore().build();\r\n    routerConfig.setLong(RBFConfigKeys.FEDERATION_STORE_CONNECTION_TEST_MS, TimeUnit.HOURS.toMillis(1));\r\n    routerConfig.setClass(RBFConfigKeys.FEDERATION_STORE_DRIVER_CLASS, StateStoreZooKeeperImpl.class, StateStoreDriver.class);\r\n    testingServer = new TestingServer();\r\n    String connectStr = testingServer.getConnectString();\r\n    curatorFramework = CuratorFrameworkFactory.builder().connectString(connectStr).retryPolicy(new RetryNTimes(100, 100)).build();\r\n    curatorFramework.start();\r\n    routerConfig.set(CommonConfigurationKeys.ZK_ADDRESS, connectStr);\r\n    router.init(routerConfig);\r\n    router.start();\r\n    waitStateStore(router.getStateStore(), TimeUnit.SECONDS.toMicros(10));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testStateStoreUnavailable",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testStateStoreUnavailable() throws IOException\n{\r\n    curatorFramework.close();\r\n    testingServer.stop();\r\n    router.getStateStore().stop();\r\n    assertFalse(router.getStateStore().isDriverReady());\r\n    RouterHeartbeatService heartbeatService = new RouterHeartbeatService(router);\r\n    heartbeatService.updateStateStore();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testStateStoreAvailable",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testStateStoreAvailable() throws Exception\n{\r\n    StateStoreService stateStore = router.getStateStore();\r\n    assertTrue(router.getStateStore().isDriverReady());\r\n    RouterStore routerStore = router.getRouterStateManager();\r\n    stateStore.refreshCaches(true);\r\n    GetRouterRegistrationRequest request = GetRouterRegistrationRequest.newInstance(routerId);\r\n    GetRouterRegistrationResponse response = router.getRouterStateManager().getRouterRegistration(request);\r\n    RouterState routerState = response.getRouter();\r\n    String id = routerState.getRouterId();\r\n    StateStoreVersion version = routerState.getStateStoreVersion();\r\n    assertNull(id);\r\n    assertNull(version);\r\n    RouterHeartbeatService heartbeatService = new RouterHeartbeatService(router);\r\n    heartbeatService.updateStateStore();\r\n    stateStore.refreshCaches(true);\r\n    request = GetRouterRegistrationRequest.newInstance(routerId);\r\n    response = routerStore.getRouterRegistration(request);\r\n    routerState = response.getRouter();\r\n    id = routerState.getRouterId();\r\n    version = routerState.getStateStoreVersion();\r\n    assertNotNull(id);\r\n    assertNotNull(version);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void tearDown() throws IOException\n{\r\n    if (curatorFramework != null) {\r\n        curatorFramework.close();\r\n    }\r\n    if (testingServer != null) {\r\n        testingServer.stop();\r\n    }\r\n    if (router != null) {\r\n        router.shutDown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    curatorTestingServer = new TestingServer();\r\n    curatorTestingServer.start();\r\n    final String connectString = curatorTestingServer.getConnectString();\r\n    int numNameservices = 2;\r\n    Configuration conf = new RouterConfigBuilder().refreshCache().admin().rpc().heartbeat().build();\r\n    conf.addResource(initSecurity());\r\n    conf.setClass(RBFConfigKeys.FEDERATION_STORE_DRIVER_CLASS, StateStoreZooKeeperImpl.class, StateStoreDriver.class);\r\n    conf.setClass(RBFConfigKeys.FEDERATION_FILE_RESOLVER_CLIENT_CLASS, RBFConfigKeys.FEDERATION_FILE_RESOLVER_CLIENT_CLASS_DEFAULT, FileSubclusterResolver.class);\r\n    conf.set(CommonConfigurationKeys.ZK_ADDRESS, connectString);\r\n    conf.setBoolean(RBFConfigKeys.DFS_ROUTER_STORE_ENABLE, true);\r\n    cluster = new MiniRouterDFSCluster(false, numNameservices, conf);\r\n    cluster.addRouterOverrides(conf);\r\n    cluster.startCluster(conf);\r\n    cluster.startRouters();\r\n    cluster.waitClusterUp();\r\n    routerContext = cluster.getRandomRouter();\r\n    RouterStore routerStateManager = routerContext.getRouter().getRouterStateManager();\r\n    mountTableManager = routerContext.getAdminClient().getMountTableManager();\r\n    FederationTestUtils.waitRouterRegistered(routerStateManager, numNameservices, 60000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "destory",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void destory()\n{\r\n    try {\r\n        curatorTestingServer.close();\r\n        cluster.shutdown();\r\n    } catch (IOException e) {\r\n        LOG.error(\"Found error when destroy, caused by: {}\", e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws IOException\n{\r\n    clearEntries();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "clearEntries",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void clearEntries() throws IOException\n{\r\n    List<MountTable> result = getMountTableEntries();\r\n    for (MountTable mountTable : result) {\r\n        RemoveMountTableEntryResponse removeMountTableEntry = mountTableManager.removeMountTableEntry(RemoveMountTableEntryRequest.newInstance(mountTable.getSourcePath()));\r\n        assertTrue(removeMountTableEntry.getStatus());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountTableEntriesCacheUpdatedAfterAddAPICall",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testMountTableEntriesCacheUpdatedAfterAddAPICall() throws IOException\n{\r\n    String srcPath = \"/addPath\";\r\n    MountTable newEntry = MountTable.newInstance(srcPath, Collections.singletonMap(\"ns0\", \"/addPathDest\"), Time.now(), Time.now());\r\n    addMountTableEntry(mountTableManager, newEntry);\r\n    List<RouterContext> routers = getRouters();\r\n    for (RouterContext rc : routers) {\r\n        List<MountTable> result = getMountTableEntries(rc.getAdminClient().getMountTableManager());\r\n        assertEquals(1, result.size());\r\n        MountTable mountTableResult = result.get(0);\r\n        assertEquals(srcPath, mountTableResult.getSourcePath());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountTableEntriesCacheUpdatedAfterRemoveAPICall",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testMountTableEntriesCacheUpdatedAfterRemoveAPICall() throws IOException\n{\r\n    String srcPath = \"/removePathSrc\";\r\n    MountTable newEntry = MountTable.newInstance(srcPath, Collections.singletonMap(\"ns0\", \"/removePathDest\"), Time.now(), Time.now());\r\n    addMountTableEntry(mountTableManager, newEntry);\r\n    List<RouterContext> routers = getRouters();\r\n    for (RouterContext rc : routers) {\r\n        List<MountTable> result = getMountTableEntries(rc.getAdminClient().getMountTableManager());\r\n        assertEquals(1, result.size());\r\n        MountTable mountTableResult = result.get(0);\r\n        assertEquals(srcPath, mountTableResult.getSourcePath());\r\n    }\r\n    RemoveMountTableEntryResponse removeMountTableEntry = mountTableManager.removeMountTableEntry(RemoveMountTableEntryRequest.newInstance(srcPath));\r\n    assertTrue(removeMountTableEntry.getStatus());\r\n    routers = getRouters();\r\n    for (RouterContext rc : routers) {\r\n        List<MountTable> result = getMountTableEntries(rc.getAdminClient().getMountTableManager());\r\n        assertEquals(0, result.size());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountTableEntriesCacheUpdatedAfterUpdateAPICall",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testMountTableEntriesCacheUpdatedAfterUpdateAPICall() throws IOException\n{\r\n    String srcPath = \"/updatePathSrc\";\r\n    String dstPath = \"/updatePathDest\";\r\n    String nameServiceId = \"ns0\";\r\n    MountTable newEntry = MountTable.newInstance(srcPath, Collections.singletonMap(\"ns0\", \"/updatePathDest\"), Time.now(), Time.now());\r\n    addMountTableEntry(mountTableManager, newEntry);\r\n    List<RouterContext> routers = getRouters();\r\n    for (RouterContext rc : routers) {\r\n        List<MountTable> result = getMountTableEntries(rc.getAdminClient().getMountTableManager());\r\n        assertEquals(1, result.size());\r\n        MountTable mountTableResult = result.get(0);\r\n        assertEquals(srcPath, mountTableResult.getSourcePath());\r\n        assertEquals(nameServiceId, mountTableResult.getDestinations().get(0).getNameserviceId());\r\n        assertEquals(dstPath, mountTableResult.getDestinations().get(0).getDest());\r\n    }\r\n    String key = \"ns1\";\r\n    String value = \"/updatePathDest2\";\r\n    MountTable upateEntry = MountTable.newInstance(srcPath, Collections.singletonMap(key, value), Time.now(), Time.now());\r\n    UpdateMountTableEntryResponse updateMountTableEntry = mountTableManager.updateMountTableEntry(UpdateMountTableEntryRequest.newInstance(upateEntry));\r\n    assertTrue(updateMountTableEntry.getStatus());\r\n    MountTable updatedMountTable = getMountTableEntry(srcPath);\r\n    assertNotNull(\"Updated mount table entrty cannot be null\", updatedMountTable);\r\n    routers = getRouters();\r\n    for (RouterContext rc : routers) {\r\n        List<MountTable> result = getMountTableEntries(rc.getAdminClient().getMountTableManager());\r\n        assertEquals(1, result.size());\r\n        MountTable mountTableResult = result.get(0);\r\n        assertEquals(srcPath, mountTableResult.getSourcePath());\r\n        assertEquals(key, updatedMountTable.getDestinations().get(0).getNameserviceId());\r\n        assertEquals(value, updatedMountTable.getDestinations().get(0).getDest());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testCachedRouterClientBehaviourAfterRouterStoped",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCachedRouterClientBehaviourAfterRouterStoped() throws IOException\n{\r\n    String srcPath = \"/addPathClientCache\";\r\n    MountTable newEntry = MountTable.newInstance(srcPath, Collections.singletonMap(\"ns0\", \"/addPathClientCacheDest\"), Time.now(), Time.now());\r\n    addMountTableEntry(mountTableManager, newEntry);\r\n    List<RouterContext> routers = getRouters();\r\n    for (RouterContext rc : routers) {\r\n        List<MountTable> result = getMountTableEntries(rc.getAdminClient().getMountTableManager());\r\n        assertEquals(1, result.size());\r\n        MountTable mountTableResult = result.get(0);\r\n        assertEquals(srcPath, mountTableResult.getSourcePath());\r\n    }\r\n    for (RouterContext rc : routers) {\r\n        InetSocketAddress adminServerAddress = rc.getRouter().getAdminServerAddress();\r\n        if (!routerContext.getRouter().getAdminServerAddress().equals(adminServerAddress)) {\r\n            cluster.stopRouter(rc);\r\n            break;\r\n        }\r\n    }\r\n    srcPath = \"/addPathClientCache2\";\r\n    newEntry = MountTable.newInstance(srcPath, Collections.singletonMap(\"ns0\", \"/addPathClientCacheDest2\"), Time.now(), Time.now());\r\n    addMountTableEntry(mountTableManager, newEntry);\r\n    for (RouterContext rc : getRouters()) {\r\n        List<MountTable> result = getMountTableEntries(rc.getAdminClient().getMountTableManager());\r\n        assertEquals(2, result.size());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouters",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<RouterContext> getRouters()\n{\r\n    List<RouterContext> result = new ArrayList<>();\r\n    for (RouterContext rc : cluster.getRouters()) {\r\n        if (rc.getRouter().getServiceState() == STATE.STARTED) {\r\n            result.add(rc);\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "MountTable getMountTableEntry(String srcPath) throws IOException\n{\r\n    List<MountTable> mountTableEntries = getMountTableEntries();\r\n    for (MountTable mountTable : mountTableEntries) {\r\n        String sourcePath = mountTable.getSourcePath();\r\n        if (srcPath.equals(sourcePath)) {\r\n            return mountTable;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addMountTableEntry(MountTableManager mountTableMgr, MountTable newEntry) throws IOException\n{\r\n    AddMountTableEntryRequest addRequest = AddMountTableEntryRequest.newInstance(newEntry);\r\n    AddMountTableEntryResponse addResponse = mountTableMgr.addMountTableEntry(addRequest);\r\n    assertTrue(addResponse.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTableEntries",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<MountTable> getMountTableEntries() throws IOException\n{\r\n    return getMountTableEntries(mountTableManager);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTableEntries",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<MountTable> getMountTableEntries(MountTableManager mountTableManagerParam) throws IOException\n{\r\n    GetMountTableEntriesRequest request = GetMountTableEntriesRequest.newInstance(\"/\");\r\n    return mountTableManagerParam.getMountTableEntries(request).getEntries();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "globalSetUp",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void globalSetUp() throws Exception\n{\r\n    startTime = Time.now();\r\n    cluster = new StateStoreDFSCluster(false, 2);\r\n    Configuration conf = new RouterConfigBuilder().stateStore().admin().rpc().build();\r\n    conf.setInt(RBFConfigKeys.DFS_ROUTER_ADMIN_MAX_COMPONENT_LENGTH_KEY, 20);\r\n    cluster.addRouterOverrides(conf);\r\n    cluster.startCluster();\r\n    cluster.startRouters();\r\n    cluster.waitClusterUp();\r\n    nnContext0 = cluster.getNamenode(\"ns0\", null);\r\n    nnContext1 = cluster.getNamenode(\"ns1\", null);\r\n    nnFs0 = nnContext0.getFileSystem();\r\n    nnFs1 = nnContext1.getFileSystem();\r\n    routerContext = cluster.getRandomRouter();\r\n    routerFs = routerContext.getFileSystem();\r\n    Router router = routerContext.getRouter();\r\n    routerProtocol = routerContext.getClient().getNamenode();\r\n    mountTable = (MountTableResolver) router.getSubclusterResolver();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (cluster != null) {\r\n        cluster.stopRouter(routerContext);\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "clearMountTable",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void clearMountTable() throws IOException\n{\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTableManager = client.getMountTableManager();\r\n    GetMountTableEntriesRequest req1 = GetMountTableEntriesRequest.newInstance(\"/\");\r\n    GetMountTableEntriesResponse response = mountTableManager.getMountTableEntries(req1);\r\n    for (MountTable entry : response.getEntries()) {\r\n        RemoveMountTableEntryRequest req2 = RemoveMountTableEntryRequest.newInstance(entry.getSourcePath());\r\n        mountTableManager.removeMountTableEntry(req2);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testReadOnly",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testReadOnly() throws Exception\n{\r\n    MountTable readOnlyEntry = MountTable.newInstance(\"/readonly\", Collections.singletonMap(\"ns0\", \"/testdir\"));\r\n    readOnlyEntry.setReadOnly(true);\r\n    assertTrue(addMountTable(readOnlyEntry));\r\n    MountTable regularEntry = MountTable.newInstance(\"/regular\", Collections.singletonMap(\"ns0\", \"/testdir\"));\r\n    assertTrue(addMountTable(regularEntry));\r\n    assertTrue(routerFs.mkdirs(new Path(\"/regular/newdir\")));\r\n    FileStatus dirStatusNn = nnFs0.getFileStatus(new Path(\"/testdir/newdir\"));\r\n    assertTrue(dirStatusNn.isDirectory());\r\n    FileStatus dirStatusRegular = routerFs.getFileStatus(new Path(\"/regular/newdir\"));\r\n    assertTrue(dirStatusRegular.isDirectory());\r\n    FileStatus dirStatusReadOnly = routerFs.getFileStatus(new Path(\"/readonly/newdir\"));\r\n    assertTrue(dirStatusReadOnly.isDirectory());\r\n    try {\r\n        routerFs.mkdirs(new Path(\"/readonly/newdirfail\"));\r\n        fail(\"We should not be able to write into a read only mount point\");\r\n    } catch (IOException ioe) {\r\n        String msg = ioe.getMessage();\r\n        assertTrue(msg.startsWith(\"/readonly/newdirfail is in a read only mount point\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addMountTable",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean addMountTable(final MountTable entry) throws IOException\n{\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTableManager = client.getMountTableManager();\r\n    AddMountTableEntryRequest addRequest = AddMountTableEntryRequest.newInstance(entry);\r\n    AddMountTableEntryResponse addResponse = mountTableManager.addMountTableEntry(addRequest);\r\n    mountTable.loadCache(true);\r\n    return addResponse.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "updateMountTable",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean updateMountTable(final MountTable entry) throws IOException\n{\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTableManager = client.getMountTableManager();\r\n    UpdateMountTableEntryRequest updateRequest = UpdateMountTableEntryRequest.newInstance(entry);\r\n    UpdateMountTableEntryResponse updateResponse = mountTableManager.updateMountTableEntry(updateRequest);\r\n    mountTable.loadCache(true);\r\n    return updateResponse.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountPointLimit",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMountPointLimit() throws Exception\n{\r\n    MountTable addEntry = MountTable.newInstance(\"/testdir-shortlength\", Collections.singletonMap(\"ns0\", \"/testdir-shortlength\"));\r\n    assertTrue(addMountTable(addEntry));\r\n    final MountTable longAddEntry = MountTable.newInstance(\"/testdir-verylonglength\", Collections.singletonMap(\"ns0\", \"/testdir-verylonglength\"));\r\n    LambdaTestUtils.intercept(IOException.class, \"The maximum path component name limit of testdir-verylonglength in \" + \"directory /testdir-verylonglength is exceeded\", () -> addMountTable(longAddEntry));\r\n    final MountTable updateEntry = MountTable.newInstance(\"/testdir-shortlength\", Collections.singletonMap(\"ns0\", \"/testdir-shortlength-change-to-long\"));\r\n    LambdaTestUtils.intercept(IOException.class, \"The maximum path component name limit of \" + \"testdir-shortlength-change-to-long in directory \" + \"/testdir-shortlength-change-to-long is exceeded\", () -> updateMountTable(updateEntry));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testListFilesTime",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void testListFilesTime() throws Exception\n{\r\n    try {\r\n        MountTable addEntry = MountTable.newInstance(\"/testdir\", Collections.singletonMap(\"ns0\", \"/testdir\"));\r\n        assertTrue(addMountTable(addEntry));\r\n        addEntry = MountTable.newInstance(\"/testdir2\", Collections.singletonMap(\"ns0\", \"/testdir2\"));\r\n        assertTrue(addMountTable(addEntry));\r\n        addEntry = MountTable.newInstance(\"/testdir/subdir\", Collections.singletonMap(\"ns0\", \"/testdir/subdir\"));\r\n        assertTrue(addMountTable(addEntry));\r\n        addEntry = MountTable.newInstance(\"/testdir3/subdir1\", Collections.singletonMap(\"ns0\", \"/testdir3\"));\r\n        assertTrue(addMountTable(addEntry));\r\n        addEntry = MountTable.newInstance(\"/testA/testB/testC/testD\", Collections.singletonMap(\"ns0\", \"/test\"));\r\n        assertTrue(addMountTable(addEntry));\r\n        assertTrue(nnFs0.mkdirs(new Path(\"/newdir\")));\r\n        Map<String, Long> pathModTime = new TreeMap<>();\r\n        for (String mount : mountTable.getMountPoints(\"/\")) {\r\n            if (mountTable.getMountPoint(\"/\" + mount) != null) {\r\n                pathModTime.put(mount, mountTable.getMountPoint(\"/\" + mount).getDateModified());\r\n            } else {\r\n                List<MountTable> entries = mountTable.getMounts(\"/\" + mount);\r\n                for (MountTable entry : entries) {\r\n                    if (pathModTime.get(mount) == null || pathModTime.get(mount) < entry.getDateModified()) {\r\n                        pathModTime.put(mount, entry.getDateModified());\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        FileStatus[] iterator = nnFs0.listStatus(new Path(\"/\"));\r\n        for (FileStatus file : iterator) {\r\n            pathModTime.put(file.getPath().getName(), file.getModificationTime());\r\n        }\r\n        DirectoryListing listing = routerProtocol.getListing(\"/\", HdfsFileStatus.EMPTY_NAME, false);\r\n        Iterator<String> pathModTimeIterator = pathModTime.keySet().iterator();\r\n        for (HdfsFileStatus f : listing.getPartialListing()) {\r\n            String fileName = pathModTimeIterator.next();\r\n            String currentFile = f.getFullPath(new Path(\"/\")).getName();\r\n            Long currentTime = f.getModificationTime();\r\n            Long expectedTime = pathModTime.get(currentFile);\r\n            assertEquals(currentFile, fileName);\r\n            assertTrue(currentTime > startTime);\r\n            assertEquals(currentTime, expectedTime);\r\n        }\r\n        assertEquals(pathModTime.size(), listing.getPartialListing().length);\r\n    } finally {\r\n        nnFs0.delete(new Path(\"/newdir\"), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetMountPointStatusWithIOException",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testGetMountPointStatusWithIOException() throws IOException, InterruptedException\n{\r\n    try {\r\n        MountTable addEntry = MountTable.newInstance(\"/testA\", Collections.singletonMap(\"ns0\", \"/testA\"));\r\n        assertTrue(addMountTable(addEntry));\r\n        addEntry = MountTable.newInstance(\"/testA/testB\", Collections.singletonMap(\"ns0\", \"/testA/testB\"));\r\n        addEntry.setOwnerName(\"userB\");\r\n        addEntry.setGroupName(\"groupB\");\r\n        assertTrue(addMountTable(addEntry));\r\n        addEntry = MountTable.newInstance(\"/testB\", Collections.singletonMap(\"ns0\", \"/test1/testB\"));\r\n        assertTrue(addMountTable(addEntry));\r\n        assertTrue(nnFs0.mkdirs(new Path(\"/test1\")));\r\n        nnFs0.setPermission(new Path(\"/test1\"), FsPermission.createImmutable((short) 0700));\r\n        UserGroupInformation user = UserGroupInformation.createUserForTesting(\"mock_user\", new String[] { \"mock_group\" });\r\n        LambdaTestUtils.doAs(user, () -> getListing(\"/testA\"));\r\n    } finally {\r\n        nnFs0.delete(new Path(\"/test1\"), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetMountPointStatus",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testGetMountPointStatus() throws IOException\n{\r\n    MountTable addEntry = MountTable.newInstance(\"/testA/testB/testC/testD\", Collections.singletonMap(\"ns0\", \"/testA/testB/testC/testD\"));\r\n    assertTrue(addMountTable(addEntry));\r\n    RouterClientProtocol clientProtocol = new RouterClientProtocol(nnFs0.getConf(), routerContext.getRouter().getRpcServer());\r\n    String src = \"/\";\r\n    String child = \"testA\";\r\n    Path childPath = new Path(src, child);\r\n    HdfsFileStatus dirStatus = clientProtocol.getMountPointStatus(childPath.toString(), 0, 0);\r\n    assertEquals(child, dirStatus.getLocalName());\r\n    String src1 = \"/testA\";\r\n    String child1 = \"testB\";\r\n    Path childPath1 = new Path(src1, child1);\r\n    HdfsFileStatus dirStatus1 = clientProtocol.getMountPointStatus(childPath1.toString(), 0, 0);\r\n    assertEquals(child1, dirStatus1.getLocalName());\r\n    String src2 = \"/testA/testB\";\r\n    String child2 = \"testC\";\r\n    Path childPath2 = new Path(src2, child2);\r\n    HdfsFileStatus dirStatus2 = clientProtocol.getMountPointStatus(childPath2.toString(), 0, 0);\r\n    assertEquals(child2, dirStatus2.getLocalName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getListing",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void getListing(String testPath) throws IOException, URISyntaxException\n{\r\n    ClientProtocol clientProtocol1 = routerContext.getClient().getNamenode();\r\n    DirectoryListing listing = clientProtocol1.getListing(testPath, HdfsFileStatus.EMPTY_NAME, false);\r\n    assertEquals(1, listing.getPartialListing().length);\r\n    HdfsFileStatus fileStatus = listing.getPartialListing()[0];\r\n    String currentOwner = fileStatus.getOwner();\r\n    String currentGroup = fileStatus.getGroup();\r\n    String currentFileName = fileStatus.getFullPath(new Path(\"/\")).getName();\r\n    assertEquals(\"testB\", currentFileName);\r\n    assertEquals(\"userB\", currentOwner);\r\n    assertEquals(\"groupB\", currentGroup);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testListNonExistPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testListNonExistPath() throws Exception\n{\r\n    mountTable.setDefaultNSEnable(false);\r\n    LambdaTestUtils.intercept(FileNotFoundException.class, \"File /base does not exist.\", \"Expect FileNotFoundException.\", () -> routerFs.listStatus(new Path(\"/base\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testListWhenDisableDefaultMountTable",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testListWhenDisableDefaultMountTable() throws IOException\n{\r\n    mountTable.setDefaultNSEnable(false);\r\n    assertTrue(addMountTable(createEntry(\"/base/dir1\", \"ns0\", \"/base/dir1\", \"group2\", \"owner2\", (short) 0750)));\r\n    assertTrue(addMountTable(createEntry(\"/base/dir2\", \"ns0\", \"/base/dir2\", \"group3\", \"owner3\", (short) 0755)));\r\n    FileStatus[] list = routerFs.listStatus(new Path(\"/base\"));\r\n    assertEquals(2, list.length);\r\n    for (FileStatus status : list) {\r\n        if (status.getPath().toUri().getPath().equals(\"/base/dir1\")) {\r\n            assertEquals(\"group2\", status.getGroup());\r\n            assertEquals(\"owner2\", status.getOwner());\r\n            assertEquals((short) 0750, status.getPermission().toShort());\r\n        } else if (status.getPath().toUri().getPath().equals(\"/base/dir2\")) {\r\n            assertEquals(\"group3\", status.getGroup());\r\n            assertEquals(\"owner3\", status.getOwner());\r\n            assertEquals((short) 0755, status.getPermission().toShort());\r\n        } else {\r\n            fail(\"list result should be either /base/dir1 or /base/dir2.\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountTablePermissionsNoDest",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testMountTablePermissionsNoDest() throws IOException\n{\r\n    MountTable addEntry;\r\n    addEntry = MountTable.newInstance(\"/testdir1\", Collections.singletonMap(\"ns0\", \"/tmp/testdir1\"));\r\n    addEntry.setGroupName(\"group1\");\r\n    addEntry.setOwnerName(\"owner1\");\r\n    addEntry.setMode(FsPermission.createImmutable((short) 0775));\r\n    assertTrue(addMountTable(addEntry));\r\n    FileStatus[] list = routerFs.listStatus(new Path(\"/\"));\r\n    assertEquals(\"group1\", list[0].getGroup());\r\n    assertEquals(\"owner1\", list[0].getOwner());\r\n    assertEquals((short) 0775, list[0].getPermission().toShort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createEntry",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "MountTable createEntry(String mountPath, String ns, String remotePath, String group, String owner, short permission) throws IOException\n{\r\n    MountTable entry = MountTable.newInstance(mountPath, Collections.singletonMap(ns, remotePath));\r\n    entry.setGroupName(group);\r\n    entry.setOwnerName(owner);\r\n    entry.setMode(FsPermission.createImmutable(permission));\r\n    return entry;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountTablePermissionsWithDest",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testMountTablePermissionsWithDest() throws IOException\n{\r\n    try {\r\n        MountTable addEntry = MountTable.newInstance(\"/testdir\", Collections.singletonMap(\"ns0\", \"/tmp/testdir\"));\r\n        assertTrue(addMountTable(addEntry));\r\n        nnFs0.mkdirs(new Path(\"/tmp/testdir\"));\r\n        nnFs0.setOwner(new Path(\"/tmp/testdir\"), \"Aowner\", \"Agroup\");\r\n        nnFs0.setPermission(new Path(\"/tmp/testdir\"), FsPermission.createImmutable((short) 775));\r\n        FileStatus[] list = routerFs.listStatus(new Path(\"/\"));\r\n        assertEquals(\"Agroup\", list[0].getGroup());\r\n        assertEquals(\"Aowner\", list[0].getOwner());\r\n        assertEquals((short) 775, list[0].getPermission().toShort());\r\n    } finally {\r\n        nnFs0.delete(new Path(\"/tmp\"), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountTablePermissionsMultiDest",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testMountTablePermissionsMultiDest() throws IOException\n{\r\n    try {\r\n        Map<String, String> destMap = new HashMap<>();\r\n        destMap.put(\"ns0\", \"/tmp/testdir\");\r\n        destMap.put(\"ns1\", \"/tmp/testdir01\");\r\n        MountTable addEntry = MountTable.newInstance(\"/testdir\", destMap);\r\n        assertTrue(addMountTable(addEntry));\r\n        nnFs0.mkdirs(new Path(\"/tmp/testdir\"));\r\n        nnFs0.setOwner(new Path(\"/tmp/testdir\"), \"Aowner\", \"Agroup\");\r\n        nnFs0.setPermission(new Path(\"/tmp/testdir\"), FsPermission.createImmutable((short) 775));\r\n        nnFs1.mkdirs(new Path(\"/tmp/testdir01\"));\r\n        nnFs1.setOwner(new Path(\"/tmp/testdir01\"), \"Aowner\", \"Agroup\");\r\n        nnFs1.setPermission(new Path(\"/tmp/testdir01\"), FsPermission.createImmutable((short) 775));\r\n        FileStatus[] list = routerFs.listStatus(new Path(\"/\"));\r\n        assertEquals(\"Agroup\", list[0].getGroup());\r\n        assertEquals(\"Aowner\", list[0].getOwner());\r\n        assertEquals((short) 775, list[0].getPermission().toShort());\r\n    } finally {\r\n        nnFs0.delete(new Path(\"/tmp\"), true);\r\n        nnFs1.delete(new Path(\"/tmp\"), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountTablePermissionsMultiDestDifferentPerm",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testMountTablePermissionsMultiDestDifferentPerm() throws IOException\n{\r\n    try {\r\n        Map<String, String> destMap = new HashMap<>();\r\n        destMap.put(\"ns0\", \"/tmp/testdir\");\r\n        destMap.put(\"ns1\", \"/tmp/testdir01\");\r\n        MountTable addEntry = MountTable.newInstance(\"/testdir\", destMap);\r\n        assertTrue(addMountTable(addEntry));\r\n        nnFs0.mkdirs(new Path(\"/tmp/testdir\"));\r\n        nnFs0.setOwner(new Path(\"/tmp/testdir\"), \"Aowner\", \"Agroup\");\r\n        nnFs0.setPermission(new Path(\"/tmp/testdir\"), FsPermission.createImmutable((short) 775));\r\n        nnFs1.mkdirs(new Path(\"/tmp/testdir01\"));\r\n        nnFs1.setOwner(new Path(\"/tmp/testdir01\"), \"Aowner01\", \"Agroup01\");\r\n        nnFs1.setPermission(new Path(\"/tmp/testdir01\"), FsPermission.createImmutable((short) 755));\r\n        FileStatus[] list = routerFs.listStatus(new Path(\"/\"));\r\n        assertTrue(\"Agroup\".equals(list[0].getGroup()) || \"Agroup01\".equals(list[0].getGroup()));\r\n        assertTrue(\"Aowner\".equals(list[0].getOwner()) || \"Aowner01\".equals(list[0].getOwner()));\r\n        assertTrue(((short) 775) == list[0].getPermission().toShort() || ((short) 755) == list[0].getPermission().toShort());\r\n    } finally {\r\n        nnFs0.delete(new Path(\"/tmp\"), true);\r\n        nnFs1.delete(new Path(\"/tmp\"), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountPointResolved",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testMountPointResolved() throws IOException\n{\r\n    MountTable addEntry = MountTable.newInstance(\"/testdir\", Collections.singletonMap(\"ns0\", \"/tmp/testdir\"));\r\n    addEntry.setGroupName(\"group1\");\r\n    addEntry.setOwnerName(\"owner1\");\r\n    assertTrue(addMountTable(addEntry));\r\n    HdfsFileStatus finfo = routerProtocol.getFileInfo(\"/testdir\");\r\n    FileStatus[] finfo1 = routerFs.listStatus(new Path(\"/\"));\r\n    assertEquals(\"owner1\", finfo.getOwner());\r\n    assertEquals(\"owner1\", finfo1[0].getOwner());\r\n    assertEquals(\"group1\", finfo.getGroup());\r\n    assertEquals(\"group1\", finfo1[0].getGroup());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountPointChildren",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testMountPointChildren() throws IOException\n{\r\n    try {\r\n        MountTable addEntry = MountTable.newInstance(\"/testdir\", Collections.singletonMap(\"ns0\", \"/tmp/testdir\"));\r\n        assertTrue(addMountTable(addEntry));\r\n        nnFs0.mkdirs(new Path(\"/tmp/testdir\"));\r\n        nnFs0.mkdirs(new Path(\"/tmp/testdir/1\"));\r\n        nnFs0.mkdirs(new Path(\"/tmp/testdir/2\"));\r\n        FileStatus[] finfo1 = routerFs.listStatus(new Path(\"/\"));\r\n        assertEquals(2, ((HdfsFileStatus) finfo1[0]).getChildrenNum());\r\n    } finally {\r\n        nnFs0.delete(new Path(\"/tmp\"), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMountPointChildrenMultiDest",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testMountPointChildrenMultiDest() throws IOException\n{\r\n    try {\r\n        Map<String, String> destMap = new HashMap<>();\r\n        destMap.put(\"ns0\", \"/tmp/testdir\");\r\n        destMap.put(\"ns1\", \"/tmp/testdir01\");\r\n        MountTable addEntry = MountTable.newInstance(\"/testdir\", destMap);\r\n        assertTrue(addMountTable(addEntry));\r\n        nnFs0.mkdirs(new Path(\"/tmp/testdir\"));\r\n        nnFs0.mkdirs(new Path(\"/tmp/testdir\"));\r\n        nnFs1.mkdirs(new Path(\"/tmp/testdir01\"));\r\n        nnFs0.mkdirs(new Path(\"/tmp/testdir/1\"));\r\n        nnFs1.mkdirs(new Path(\"/tmp/testdir01/1\"));\r\n        FileStatus[] finfo1 = routerFs.listStatus(new Path(\"/\"));\r\n        assertEquals(2, ((HdfsFileStatus) finfo1[0]).getChildrenNum());\r\n    } finally {\r\n        nnFs0.delete(new Path(\"/tmp\"), true);\r\n        nnFs0.delete(new Path(\"/tmp\"), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testPathInException",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testPathInException() throws Exception\n{\r\n    MountTable addEntry = MountTable.newInstance(\"/mount\", Collections.singletonMap(\"ns0\", \"/tmp/testdir\"));\r\n    addEntry.setDestOrder(DestinationOrder.HASH_ALL);\r\n    assertTrue(addMountTable(addEntry));\r\n    LambdaTestUtils.intercept(FileNotFoundException.class, \"Directory/File does not exist /mount/file\", () -> routerFs.setOwner(new Path(\"/mount/file\"), \"user\", \"group\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetListingWithTrailingSlash",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testGetListingWithTrailingSlash() throws IOException\n{\r\n    try {\r\n        MountTable addEntry = MountTable.newInstance(\"/testlist\", Collections.singletonMap(\"ns0\", \"/testlist\"));\r\n        assertTrue(addMountTable(addEntry));\r\n        addEntry = MountTable.newInstance(\"/testlist/tmp0\", Collections.singletonMap(\"ns0\", \"/testlist/tmp0\"));\r\n        assertTrue(addMountTable(addEntry));\r\n        addEntry = MountTable.newInstance(\"/testlist/tmp1\", Collections.singletonMap(\"ns1\", \"/testlist/tmp1\"));\r\n        assertTrue(addMountTable(addEntry));\r\n        nnFs0.mkdirs(new Path(\"/testlist/tmp0\"));\r\n        nnFs1.mkdirs(new Path(\"/testlist/tmp1\"));\r\n        DirectoryListing list = routerProtocol.getListing(\"/testlist/\", HdfsFileStatus.EMPTY_NAME, false);\r\n        HdfsFileStatus[] statuses = list.getPartialListing();\r\n        assertEquals(2, statuses.length);\r\n    } finally {\r\n        nnFs0.delete(new Path(\"/testlist/tmp0\"), true);\r\n        nnFs1.delete(new Path(\"/testlist/tmp1\"), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testDeleteMountPoint",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testDeleteMountPoint() throws Exception\n{\r\n    try {\r\n        MountTable addEntry = MountTable.newInstance(\"/testdelete/subdir\", Collections.singletonMap(\"ns0\", \"/testdelete/subdir\"));\r\n        assertTrue(addMountTable(addEntry));\r\n        nnFs0.mkdirs(new Path(\"/testdelete/subdir\"));\r\n        LambdaTestUtils.intercept(AccessControlException.class, \"The operation is not allowed because there are mount points: \" + \"subdir under the path: /testdelete\", () -> routerFs.delete(new Path(\"/testdelete\"), true));\r\n        LambdaTestUtils.intercept(AccessControlException.class, \"The operation is not allowed because there are mount points: \" + \"subdir under the path: /testdelete\", () -> routerFs.delete(new Path(\"/testdelete\"), false));\r\n        LambdaTestUtils.intercept(AccessControlException.class, \"The operation is not allowed because the path: \" + \"/testdelete/subdir is a mount point\", () -> routerFs.delete(new Path(\"/testdelete/subdir\"), true));\r\n        LambdaTestUtils.intercept(AccessControlException.class, \"The operation is not allowed because the path: \" + \"/testdelete/subdir is a mount point\", () -> routerFs.delete(new Path(\"/testdelete/subdir\"), false));\r\n    } finally {\r\n        nnFs0.delete(new Path(\"/testdelete\"), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRenameMountPoint",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testRenameMountPoint() throws Exception\n{\r\n    try {\r\n        MountTable addEntry = MountTable.newInstance(\"/testrename1/sub\", Collections.singletonMap(\"ns0\", \"/testrename1/sub\"));\r\n        assertTrue(addMountTable(addEntry));\r\n        addEntry = MountTable.newInstance(\"/testrename2/sub\", Collections.singletonMap(\"ns0\", \"/testrename2/sub\"));\r\n        assertTrue(addMountTable(addEntry));\r\n        nnFs0.mkdirs(new Path(\"/testrename1/sub/sub\"));\r\n        nnFs0.mkdirs(new Path(\"/testrename2\"));\r\n        assertTrue(nnFs0.exists(new Path(\"/testrename1/sub/sub\")));\r\n        assertFalse(nnFs0.exists(new Path(\"/testrename2/sub\")));\r\n        assertTrue(routerFs.rename(new Path(\"/testrename1/sub/sub\"), new Path(\"/testrename2\")));\r\n        assertFalse(nnFs0.exists(new Path(\"/testrename1/sub/sub\")));\r\n        assertTrue(nnFs0.exists(new Path(\"/testrename2/sub\")));\r\n        nnFs0.mkdirs(new Path(\"/testrename1/sub/sub\"));\r\n        assertFalse(routerFs.rename(new Path(\"/testrename1/sub/sub\"), new Path(\"/testrename2\")));\r\n        LambdaTestUtils.intercept(AccessControlException.class, \"The operation is not allowed because the path: \" + \"/testrename1/sub is a mount point\", () -> routerFs.rename(new Path(\"/testrename1/sub\"), new Path(\"/testrename2/sub\")));\r\n        LambdaTestUtils.intercept(AccessControlException.class, \"The operation is not allowed because there are mount points: \" + \"sub under the path: /testrename1\", () -> routerFs.rename(new Path(\"/testrename1\"), new Path(\"/testrename2/sub\")));\r\n    } finally {\r\n        nnFs0.delete(new Path(\"/testrename1\"), true);\r\n        nnFs0.delete(new Path(\"/testrename2\"), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testListStatusMountPoint",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testListStatusMountPoint() throws Exception\n{\r\n    try {\r\n        MountTable addEntry = MountTable.newInstance(\"/mount/testLsMountEntry\", Collections.singletonMap(\"ns0\", \"/testLsMountEntryDest\"));\r\n        assertTrue(addMountTable(addEntry));\r\n        nnFs0.mkdirs(new Path(\"/testLsMountEntryDest\"));\r\n        DistributedFileSystem routerDfs = (DistributedFileSystem) routerFs;\r\n        Path mountPath = new Path(\"/mount/testLsMountEntry\");\r\n        routerDfs.setErasureCodingPolicy(mountPath, \"RS-6-3-1024k\");\r\n        assertTrue(routerDfs.listStatus(new Path(\"/mount\"))[0].isErasureCoded());\r\n    } finally {\r\n        nnFs0.delete(new Path(\"/testLsMountEntryDest\"), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "globalSetUp",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void globalSetUp() throws Exception\n{\r\n    File workDir = new File(System.getProperty(\"test.dir\", \"target\"));\r\n    kdc = new MiniKdc(MiniKdc.createConf(), workDir);\r\n    kdc.start();\r\n    kdc.createPrincipal(new File(keytab), clientPrincipal, serverPrincipal);\r\n    baseConf.setBoolean(DFSConfigKeys.HADOOP_CALLER_CONTEXT_ENABLED_KEY, true);\r\n    SecurityUtil.setAuthenticationMethod(KERBEROS, baseConf);\r\n    baseConf.set(RBFConfigKeys.DFS_ROUTER_KERBEROS_PRINCIPAL_KEY, serverPrincipal);\r\n    baseConf.set(RBFConfigKeys.DFS_ROUTER_KEYTAB_FILE_KEY, keytab);\r\n    baseConf.set(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, serverPrincipal);\r\n    baseConf.set(DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY, keytab);\r\n    baseConf.set(DFSConfigKeys.DFS_DATANODE_KERBEROS_PRINCIPAL_KEY, serverPrincipal);\r\n    baseConf.set(DFSConfigKeys.DFS_DATANODE_KEYTAB_FILE_KEY, keytab);\r\n    baseConf.setBoolean(DFSConfigKeys.DFS_BLOCK_ACCESS_TOKEN_ENABLE_KEY, true);\r\n    baseConf.set(DFS_DATA_TRANSFER_PROTECTION_KEY, DFS_DATA_TRANSFER_PROTECTION_DEFAULT);\r\n    baseConf.setBoolean(IGNORE_SECURE_PORTS_FOR_TESTING_KEY, true);\r\n    baseConf.setClass(HADOOP_SECURITY_IMPERSONATION_PROVIDER_CLASS, AllowUserImpersonationProvider.class, ImpersonationProvider.class);\r\n    DistCpProcedure.enableForTest();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "globalTearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void globalTearDown()\n{\r\n    kdc.stop();\r\n    DistCpProcedure.disableForTest();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    super.tearDown();\r\n    cluster.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    cluster = new MiniRouterDFSCluster(false, NUM_SUBCLUSTERS);\r\n    cluster.setNumDatanodesPerNameservice(NUM_DNS);\r\n    cluster.addNamenodeOverrides(baseConf);\r\n    cluster.setIndependentDNs();\r\n    cluster.startCluster();\r\n    String journal = \"hdfs://\" + cluster.getCluster().getNameNode(1).getClientNamenodeAddress() + \"/journal\";\r\n    Configuration routerConf = new RouterConfigBuilder().metrics().rpc().routerRenameOption().set(SCHEDULER_JOURNAL_URI, journal).set(DFS_ROUTER_FEDERATION_RENAME_MAP, \"1\").set(DFS_ROUTER_FEDERATION_RENAME_BANDWIDTH, \"1\").set(ZK_DTSM_ZK_CONNECTION_STRING, hostPort).set(ZK_DTSM_ZK_AUTH_TYPE, \"none\").set(RM_PRINCIPAL, serverPrincipal).build();\r\n    routerConf.setTimeDuration(RBFConfigKeys.DN_REPORT_CACHE_EXPIRE, 1, TimeUnit.SECONDS);\r\n    cluster.addRouterOverrides(baseConf);\r\n    cluster.addRouterOverrides(routerConf);\r\n    cluster.startRouters();\r\n    cluster.registerNamenodes();\r\n    cluster.waitNamenodeRegistration();\r\n    cluster.getCluster().getNamesystem(0).getBlockManager().getDatanodeManager().setHeartbeatInterval(1);\r\n    cluster.getCluster().getNamesystem(1).getBlockManager().getDatanodeManager().setHeartbeatInterval(1);\r\n    cluster.getCluster().getNamesystem(0).getBlockManager().getDatanodeManager().setHeartbeatExpireInterval(3000);\r\n    cluster.getCluster().getNamesystem(1).getBlockManager().getDatanodeManager().setHeartbeatExpireInterval(3000);\r\n    cluster.installMockLocations();\r\n    cluster.createTestDirectoriesNamenode();\r\n    RouterContext rndRouter = cluster.getRandomRouter();\r\n    setRouter(rndRouter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "prepareEnv",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void prepareEnv(FileSystem fs, Path path, Path renamedPath) throws IOException\n{\r\n    fs.setPermission(path.getParent(), FsPermission.createImmutable((short) 511));\r\n    fs.setPermission(renamedPath.getParent(), FsPermission.createImmutable((short) 511));\r\n    fs.mkdirs(path);\r\n    String file = path.toString() + \"/file\";\r\n    createFile(fs, file, 32);\r\n    verifyFileExists(fs, path.toString());\r\n    verifyFileExists(fs, file);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRenameDir",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRenameDir(RouterContext testRouter, String path, String renamedPath, boolean exceptionExpected, Callable<Object> call) throws IOException\n{\r\n    prepareEnv(testRouter.getFileSystem(), new Path(path), new Path(renamedPath));\r\n    boolean exceptionThrown = false;\r\n    try {\r\n        call.call();\r\n        assertFalse(verifyFileExists(testRouter.getFileSystem(), path));\r\n        assertTrue(verifyFileExists(testRouter.getFileSystem(), renamedPath + \"/file\"));\r\n    } catch (Exception ex) {\r\n        exceptionThrown = true;\r\n        assertTrue(verifyFileExists(testRouter.getFileSystem(), path + \"/file\"));\r\n        assertFalse(verifyFileExists(testRouter.getFileSystem(), renamedPath));\r\n    } finally {\r\n        FileContext fileContext = testRouter.getFileContext();\r\n        fileContext.delete(new Path(path), true);\r\n        fileContext.delete(new Path(renamedPath), true);\r\n    }\r\n    if (exceptionExpected) {\r\n        assertTrue(exceptionThrown);\r\n    } else {\r\n        assertFalse(exceptionThrown);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setRouter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRouter(RouterContext r) throws IOException\n{\r\n    this.router = r;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testClientRename",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testClientRename() throws IOException\n{\r\n    String ns0 = cluster.getNameservices().get(0);\r\n    String ns1 = cluster.getNameservices().get(1);\r\n    String dir = cluster.getFederatedTestDirectoryForNS(ns0) + \"/\" + getMethodName();\r\n    String renamedDir = cluster.getFederatedTestDirectoryForNS(ns1) + \"/\" + getMethodName();\r\n    testRenameDir(router, dir, renamedDir, false, () -> {\r\n        UserGroupInformation ugi = UserGroupInformation.loginUserFromKeytabAndReturnUGI(clientPrincipal, keytab);\r\n        ugi.doAs((PrivilegedExceptionAction<Boolean>) () -> {\r\n            DFSClient client = router.getClient();\r\n            ClientProtocol clientProtocol = client.getNamenode();\r\n            clientProtocol.rename(dir, renamedDir);\r\n            return null;\r\n        });\r\n        return null;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "globalSetUp",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void globalSetUp() throws Exception\n{\r\n    cluster = new StateStoreDFSCluster(false, 1);\r\n    Configuration conf = new RouterConfigBuilder().stateStore().admin().rpc().build();\r\n    cluster.addRouterOverrides(conf);\r\n    cluster.startRouters();\r\n    routerContext = cluster.getRandomRouter();\r\n    mockMountTable = cluster.generateMockMountTable();\r\n    Router router = routerContext.getRouter();\r\n    stateStore = router.getStateStore();\r\n    ActiveNamenodeResolver membership = router.getNamenodeResolver();\r\n    membership.registerNamenode(createNamenodeReport(\"ns0\", \"nn1\", HAServiceProtocol.HAServiceState.ACTIVE));\r\n    membership.registerNamenode(createNamenodeReport(\"ns1\", \"nn1\", HAServiceProtocol.HAServiceState.ACTIVE));\r\n    stateStore.refreshCaches(true);\r\n    routerConf = new Configuration();\r\n    InetSocketAddress routerSocket = router.getAdminServerAddress();\r\n    routerConf.setSocketAddr(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, routerSocket);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    cluster.stopRouter(routerContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "testSetup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSetup() throws Exception\n{\r\n    assertTrue(synchronizeRecords(stateStore, mockMountTable, MountTable.class));\r\n    routerContext.resetAdminClient();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "testUpdateMountpoint",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testUpdateMountpoint() throws Exception\n{\r\n    String mount = \"/test-path\";\r\n    String dst = \"/test-dst\";\r\n    MountTable newEntry = MountTable.newInstance(mount, Collections.singletonMap(\"ns0\", mount), Time.now(), Time.now());\r\n    MountTableManager mountTable = routerContext.getAdminClient().getMountTableManager();\r\n    AddMountTableEntryRequest addRequest = AddMountTableEntryRequest.newInstance(newEntry);\r\n    AddMountTableEntryResponse addResponse = mountTable.addMountTableEntry(addRequest);\r\n    assertTrue(addResponse.getStatus());\r\n    GetMountTableEntriesRequest request = GetMountTableEntriesRequest.newInstance(\"/\");\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesResponse response = mountTable.getMountTableEntries(request);\r\n    assertEquals(3, response.getEntries().size());\r\n    MountTableProcedure.disableWrite(mount, routerConf);\r\n    String dstNs = \"ns1\";\r\n    MountTableProcedure smtp = new MountTableProcedure(\"single-mount-table-procedure\", null, 1000, mount, dst, dstNs, routerConf);\r\n    assertTrue(smtp.execute());\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    MountTable entry = MountTableProcedure.getMountEntry(mount, mountTable);\r\n    assertNotNull(entry);\r\n    assertEquals(1, entry.getDestinations().size());\r\n    String nsId = entry.getDestinations().get(0).getNameserviceId();\r\n    String dstPath = entry.getDestinations().get(0).getDest();\r\n    assertEquals(dstNs, nsId);\r\n    assertEquals(dst, dstPath);\r\n    URI address = routerContext.getFileSystemURI();\r\n    DFSClient routerClient = new DFSClient(address, routerConf);\r\n    MountTableProcedure.enableWrite(mount, routerConf);\r\n    intercept(RemoteException.class, \"No namenode available to invoke mkdirs\", \"Expect no namenode exception.\", () -> routerClient.mkdirs(mount + \"/file\", new FsPermission(020), false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "testDisableAndEnableWrite",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testDisableAndEnableWrite() throws Exception\n{\r\n    String mount = \"/test-write\";\r\n    MountTable newEntry = MountTable.newInstance(mount, Collections.singletonMap(\"ns0\", mount), Time.now(), Time.now());\r\n    MountTableManager mountTable = routerContext.getAdminClient().getMountTableManager();\r\n    AddMountTableEntryRequest addRequest = AddMountTableEntryRequest.newInstance(newEntry);\r\n    AddMountTableEntryResponse addResponse = mountTable.addMountTableEntry(addRequest);\r\n    assertTrue(addResponse.getStatus());\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    URI address = routerContext.getFileSystemURI();\r\n    DFSClient routerClient = new DFSClient(address, routerConf);\r\n    intercept(RemoteException.class, \"No namenode available to invoke mkdirs\", \"Expect no namenode exception.\", () -> routerClient.mkdirs(mount + \"/file\", new FsPermission(020), false));\r\n    MountTableProcedure.disableWrite(mount, routerConf);\r\n    intercept(RemoteException.class, \"is in a read only mount point\", \"Expect readonly exception.\", () -> routerClient.mkdirs(mount + \"/dir\", new FsPermission(020), false));\r\n    MountTableProcedure.enableWrite(mount, routerConf);\r\n    intercept(RemoteException.class, \"No namenode available to invoke mkdirs\", \"Expect no namenode exception.\", () -> routerClient.mkdirs(mount + \"/file\", new FsPermission(020), false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "testSeDeserialize",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSeDeserialize() throws Exception\n{\r\n    String fedPath = \"/test-path\";\r\n    String dst = \"/test-dst\";\r\n    String dstNs = \"ns1\";\r\n    MountTableProcedure smtp = new MountTableProcedure(\"single-mount-table-procedure\", null, 1000, fedPath, dst, dstNs, routerConf);\r\n    ByteArrayOutputStream bao = new ByteArrayOutputStream();\r\n    DataOutput dataOut = new DataOutputStream(bao);\r\n    smtp.write(dataOut);\r\n    smtp = new MountTableProcedure();\r\n    smtp.readFields(new DataInputStream(new ByteArrayInputStream(bao.toByteArray())));\r\n    assertEquals(fedPath, smtp.getMount());\r\n    assertEquals(dst, smtp.getDstPath());\r\n    assertEquals(dstNs, smtp.getDstNs());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws IOException, InterruptedException\n{\r\n    disabledStore = getStateStore().getRegisteredRecordStore(DisabledNameserviceStore.class);\r\n    assertTrue(clearRecords(getStateStore(), DisabledNameservice.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testDisableNameservice",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testDisableNameservice() throws IOException\n{\r\n    Set<String> disabledNameservices = disabledStore.getDisabledNameservices();\r\n    assertEquals(0, disabledNameservices.size());\r\n    disabledStore.disableNameservice(\"ns0\");\r\n    disabledStore.disableNameservice(\"ns1\");\r\n    disabledStore.loadCache(true);\r\n    disabledNameservices = disabledStore.getDisabledNameservices();\r\n    assertEquals(2, disabledNameservices.size());\r\n    assertTrue(disabledNameservices.contains(\"ns0\") && disabledNameservices.contains(\"ns1\"));\r\n    disabledStore.enableNameservice(\"ns0\");\r\n    disabledStore.loadCache(true);\r\n    disabledNameservices = disabledStore.getDisabledNameservices();\r\n    assertEquals(1, disabledNameservices.size());\r\n    assertTrue(disabledNameservices.contains(\"ns1\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws Exception\n{\r\n    RouterHDFSContract.createCluster(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "before",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void before() throws Exception\n{\r\n    globalSetUp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "after",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void after()\n{\r\n    tearDown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSetup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSetup() throws Exception\n{\r\n    setup();\r\n    router = getRouterContext();\r\n    routerFS = getRouterFileSystem();\r\n    cluster = getCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRenameDir",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRenameDir(RouterContext testRouter, String path, String renamedPath, boolean exceptionExpected, Callable<Object> call) throws IOException\n{\r\n    createDir(testRouter.getFileSystem(), path);\r\n    boolean exceptionThrown = false;\r\n    try {\r\n        call.call();\r\n        assertFalse(verifyFileExists(testRouter.getFileSystem(), path));\r\n        assertTrue(verifyFileExists(testRouter.getFileSystem(), renamedPath + \"/file\"));\r\n    } catch (Exception ex) {\r\n        exceptionThrown = true;\r\n        assertTrue(verifyFileExists(testRouter.getFileSystem(), path + \"/file\"));\r\n        assertFalse(verifyFileExists(testRouter.getFileSystem(), renamedPath));\r\n    } finally {\r\n        FileContext fileContext = testRouter.getFileContext();\r\n        fileContext.delete(new Path(path), true);\r\n        fileContext.delete(new Path(renamedPath), true);\r\n    }\r\n    if (exceptionExpected) {\r\n        assertTrue(exceptionThrown);\r\n    } else {\r\n        assertFalse(exceptionThrown);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSuccessfulRbfRename",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSuccessfulRbfRename() throws Exception\n{\r\n    List<String> nss = cluster.getNameservices();\r\n    String ns0 = nss.get(0);\r\n    String ns1 = nss.get(1);\r\n    String dir = cluster.getFederatedTestDirectoryForNS(ns0) + \"/\" + getMethodName();\r\n    String renamedDir = cluster.getFederatedTestDirectoryForNS(ns1) + \"/\" + getMethodName();\r\n    testRenameDir(router, dir, renamedDir, false, () -> {\r\n        DFSClient client = router.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename(dir, renamedDir);\r\n        return null;\r\n    });\r\n    testRenameDir(router, dir, renamedDir, false, () -> {\r\n        DFSClient client = router.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename2(dir, renamedDir);\r\n        return null;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRbfRenameFile",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testRbfRenameFile() throws Exception\n{\r\n    List<String> nss = cluster.getNameservices();\r\n    String ns0 = nss.get(0);\r\n    String ns1 = nss.get(1);\r\n    String file = cluster.getFederatedTestDirectoryForNS(ns0) + \"/\" + getMethodName();\r\n    String renamedFile = cluster.getFederatedTestDirectoryForNS(ns1) + \"/\" + getMethodName();\r\n    createFile(routerFS, file, 32);\r\n    getRouterFileSystem().mkdirs(new Path(renamedFile));\r\n    LambdaTestUtils.intercept(RemoteException.class, \"should be a directory\", \"Expect RemoteException.\", () -> {\r\n        DFSClient client = router.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename(file, renamedFile);\r\n        return null;\r\n    });\r\n    LambdaTestUtils.intercept(RemoteException.class, \"should be a directory\", \"Expect RemoteException.\", () -> {\r\n        DFSClient client = router.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename2(file, renamedFile);\r\n        return null;\r\n    });\r\n    getRouterFileSystem().delete(new Path(file), true);\r\n    getRouterFileSystem().delete(new Path(renamedFile), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRbfRenameWhenDstAlreadyExists",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testRbfRenameWhenDstAlreadyExists() throws Exception\n{\r\n    List<String> nss = cluster.getNameservices();\r\n    String ns0 = nss.get(0);\r\n    String ns1 = nss.get(1);\r\n    String dir = cluster.getFederatedTestDirectoryForNS(ns0) + \"/\" + getMethodName();\r\n    String renamedDir = cluster.getFederatedTestDirectoryForNS(ns1) + \"/\" + getMethodName();\r\n    createDir(routerFS, dir);\r\n    getRouterFileSystem().mkdirs(new Path(renamedDir));\r\n    LambdaTestUtils.intercept(RemoteException.class, \"already exists\", \"Expect RemoteException.\", () -> {\r\n        DFSClient client = router.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename(dir, renamedDir);\r\n        return null;\r\n    });\r\n    LambdaTestUtils.intercept(RemoteException.class, \"already exists\", \"Expect RemoteException.\", () -> {\r\n        DFSClient client = router.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename2(dir, renamedDir);\r\n        return null;\r\n    });\r\n    getRouterFileSystem().delete(new Path(dir), true);\r\n    getRouterFileSystem().delete(new Path(renamedDir), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRbfRenameWhenSrcNotExists",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRbfRenameWhenSrcNotExists() throws Exception\n{\r\n    List<String> nss = cluster.getNameservices();\r\n    String ns0 = nss.get(0);\r\n    String ns1 = nss.get(1);\r\n    String dir = cluster.getFederatedTestDirectoryForNS(ns0) + \"/\" + getMethodName();\r\n    String renamedDir = cluster.getFederatedTestDirectoryForNS(ns1) + \"/\" + getMethodName();\r\n    LambdaTestUtils.intercept(RemoteException.class, \"File does not exist\", \"Expect RemoteException.\", () -> {\r\n        DFSClient client = router.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename(dir, renamedDir);\r\n        return null;\r\n    });\r\n    LambdaTestUtils.intercept(RemoteException.class, \"File does not exist\", \"Expect RemoteException.\", () -> {\r\n        DFSClient client = router.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename2(dir, renamedDir);\r\n        return null;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRbfRenameOfMountPoint",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRbfRenameOfMountPoint() throws Exception\n{\r\n    List<String> nss = cluster.getNameservices();\r\n    String ns0 = nss.get(0);\r\n    String ns1 = nss.get(1);\r\n    String dir = cluster.getFederatedPathForNS(ns0);\r\n    String renamedDir = cluster.getFederatedPathForNS(ns1);\r\n    LambdaTestUtils.intercept(RemoteException.class, \"is a mount point\", \"Expect RemoteException.\", () -> {\r\n        DFSClient client = router.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename(dir, renamedDir);\r\n        return null;\r\n    });\r\n    LambdaTestUtils.intercept(RemoteException.class, \"is a mount point\", \"Expect RemoteException.\", () -> {\r\n        DFSClient client = router.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename2(dir, renamedDir);\r\n        return null;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRbfRenameWithMultiDestination",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRbfRenameWithMultiDestination() throws Exception\n{\r\n    List<String> nss = cluster.getNameservices();\r\n    String ns1 = nss.get(1);\r\n    FileSystem rfs = getRouterFileSystem();\r\n    String dir = \"/same/\" + getMethodName();\r\n    String renamedDir = cluster.getFederatedTestDirectoryForNS(ns1) + \"/\" + getMethodName();\r\n    createDir(rfs, dir);\r\n    getRouterFileSystem().mkdirs(new Path(renamedDir));\r\n    LambdaTestUtils.intercept(RemoteException.class, \"The remote location should be exactly one\", \"Expect RemoteException.\", () -> {\r\n        DFSClient client = router.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename(dir, renamedDir);\r\n        return null;\r\n    });\r\n    LambdaTestUtils.intercept(RemoteException.class, \"The remote location should be exactly one\", \"Expect RemoteException.\", () -> {\r\n        DFSClient client = router.getClient();\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename2(dir, renamedDir);\r\n        return null;\r\n    });\r\n    getRouterFileSystem().delete(new Path(dir), true);\r\n    getRouterFileSystem().delete(new Path(renamedDir), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testCounter",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testCounter() throws Exception\n{\r\n    final RouterRpcServer rpcServer = router.getRouter().getRpcServer();\r\n    List<String> nss = cluster.getNameservices();\r\n    String ns0 = nss.get(0);\r\n    String ns1 = nss.get(1);\r\n    RouterFederationRename rbfRename = Mockito.spy(new RouterFederationRename(rpcServer, router.getConf()));\r\n    String path = \"/src\";\r\n    createDir(cluster.getCluster().getFileSystem(0), path);\r\n    int expectedSchedulerCount = rpcServer.getSchedulerJobCount() + 1;\r\n    AtomicInteger maxSchedulerCount = new AtomicInteger();\r\n    AtomicBoolean watch = new AtomicBoolean(true);\r\n    Thread watcher = new Thread(() -> {\r\n        while (watch.get()) {\r\n            int schedulerCount = rpcServer.getSchedulerJobCount();\r\n            if (schedulerCount > maxSchedulerCount.get()) {\r\n                maxSchedulerCount.set(schedulerCount);\r\n            }\r\n            try {\r\n                Thread.sleep(1);\r\n            } catch (InterruptedException e) {\r\n            }\r\n        }\r\n    });\r\n    watcher.start();\r\n    rbfRename.routerFedRename(\"/src\", \"/dst\", Arrays.asList(new RemoteLocation(ns0, path, null)), Arrays.asList(new RemoteLocation(ns1, path, null)));\r\n    verify(rbfRename).countIncrement();\r\n    verify(rbfRename).countDecrement();\r\n    watch.set(false);\r\n    watcher.interrupt();\r\n    watcher.join();\r\n    assertEquals(expectedSchedulerCount, maxSchedulerCount.get());\r\n    assertFalse(cluster.getCluster().getFileSystem(0).exists(new Path(path)));\r\n    assertTrue(cluster.getCluster().getFileSystem(1).delete(new Path(path), true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setUpBeforeClass",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUpBeforeClass() throws Exception\n{\r\n    router = new Router();\r\n    Configuration config = new RouterConfigBuilder().admin().rpc().build();\r\n    router.init(config);\r\n    router.start();\r\n    admin = new RouterAdmin(config);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDownBeforeClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDownBeforeClass() throws IOException\n{\r\n    if (router != null) {\r\n        router.stop();\r\n        router.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    firstHandler = Mockito.mock(RefreshHandler.class);\r\n    Mockito.when(firstHandler.handleRefresh(Mockito.anyString(), Mockito.any(String[].class))).thenReturn(RefreshResponse.successResponse());\r\n    RefreshRegistry.defaultRegistry().register(\"firstHandler\", firstHandler);\r\n    secondHandler = Mockito.mock(RefreshHandler.class);\r\n    Mockito.when(secondHandler.handleRefresh(\"secondHandler\", new String[] { \"one\", \"two\" })).thenReturn(new RefreshResponse(3, \"three\"));\r\n    Mockito.when(secondHandler.handleRefresh(\"secondHandler\", new String[] { \"one\" })).thenReturn(new RefreshResponse(2, \"two\"));\r\n    RefreshRegistry.defaultRegistry().register(\"secondHandler\", secondHandler);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    RefreshRegistry.defaultRegistry().unregisterAll(\"firstHandler\");\r\n    RefreshRegistry.defaultRegistry().unregisterAll(\"secondHandler\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testInvalidCommand",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testInvalidCommand() throws Exception\n{\r\n    String[] args = new String[] { \"-refreshRouterArgs\", \"nn\" };\r\n    int exitCode = admin.run(args);\r\n    assertEquals(\"RouterAdmin should fail due to bad args\", -1, exitCode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testInvalidIdentifier",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testInvalidIdentifier() throws Exception\n{\r\n    String[] argv = new String[] { \"-refreshRouterArgs\", \"localhost:\" + router.getAdminServerAddress().getPort(), \"unregisteredIdentity\" };\r\n    int exitCode = admin.run(argv);\r\n    assertEquals(\"RouterAdmin should fail due to no handler registered\", -1, exitCode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testValidIdentifier",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testValidIdentifier() throws Exception\n{\r\n    String[] args = new String[] { \"-refreshRouterArgs\", \"localhost:\" + router.getAdminServerAddress().getPort(), \"firstHandler\" };\r\n    int exitCode = admin.run(args);\r\n    assertEquals(\"RouterAdmin should succeed\", 0, exitCode);\r\n    Mockito.verify(firstHandler).handleRefresh(\"firstHandler\", new String[] {});\r\n    Mockito.verify(secondHandler, Mockito.never()).handleRefresh(Mockito.anyString(), Mockito.any(String[].class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testVariableArgs",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testVariableArgs() throws Exception\n{\r\n    String[] args = new String[] { \"-refreshRouterArgs\", \"localhost:\" + router.getAdminServerAddress().getPort(), \"secondHandler\", \"one\" };\r\n    int exitCode = admin.run(args);\r\n    assertEquals(\"RouterAdmin should return 2\", 2, exitCode);\r\n    exitCode = admin.run(new String[] { \"-refreshRouterArgs\", \"localhost:\" + router.getAdminServerAddress().getPort(), \"secondHandler\", \"one\", \"two\" });\r\n    assertEquals(\"RouterAdmin should now return 3\", 3, exitCode);\r\n    Mockito.verify(secondHandler).handleRefresh(\"secondHandler\", new String[] { \"one\" });\r\n    Mockito.verify(secondHandler).handleRefresh(\"secondHandler\", new String[] { \"one\", \"two\" });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testUnregistration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testUnregistration() throws Exception\n{\r\n    RefreshRegistry.defaultRegistry().unregisterAll(\"firstHandler\");\r\n    String[] args = new String[] { \"-refreshRouterArgs\", \"localhost:\" + router.getAdminServerAddress().getPort(), \"firstHandler\" };\r\n    int exitCode = admin.run(args);\r\n    assertEquals(\"RouterAdmin should return -1\", -1, exitCode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testUnregistrationReturnValue",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testUnregistrationReturnValue()\n{\r\n    RefreshHandler mockHandler = Mockito.mock(RefreshHandler.class);\r\n    RefreshRegistry.defaultRegistry().register(\"test\", mockHandler);\r\n    boolean ret = RefreshRegistry.defaultRegistry().unregister(\"test\", mockHandler);\r\n    assertTrue(ret);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMultipleRegistration",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testMultipleRegistration() throws Exception\n{\r\n    RefreshRegistry.defaultRegistry().register(\"sharedId\", firstHandler);\r\n    RefreshRegistry.defaultRegistry().register(\"sharedId\", secondHandler);\r\n    String[] args = new String[] { \"-refreshRouterArgs\", \"localhost:\" + router.getAdminServerAddress().getPort(), \"sharedId\", \"one\" };\r\n    int exitCode = admin.run(args);\r\n    assertEquals(-1, exitCode);\r\n    Mockito.verify(firstHandler).handleRefresh(\"sharedId\", new String[] { \"one\" });\r\n    Mockito.verify(secondHandler).handleRefresh(\"sharedId\", new String[] { \"one\" });\r\n    RefreshRegistry.defaultRegistry().unregisterAll(\"sharedId\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMultipleReturnCodeMerging",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testMultipleReturnCodeMerging() throws Exception\n{\r\n    RefreshHandler handlerOne = Mockito.mock(RefreshHandler.class);\r\n    Mockito.when(handlerOne.handleRefresh(Mockito.anyString(), Mockito.any(String[].class))).thenReturn(new RefreshResponse(23, \"Twenty Three\"));\r\n    RefreshHandler handlerTwo = Mockito.mock(RefreshHandler.class);\r\n    Mockito.when(handlerTwo.handleRefresh(Mockito.anyString(), Mockito.any(String[].class))).thenReturn(new RefreshResponse(10, \"Ten\"));\r\n    RefreshRegistry.defaultRegistry().register(\"shared\", handlerOne);\r\n    RefreshRegistry.defaultRegistry().register(\"shared\", handlerTwo);\r\n    String[] args = new String[] { \"-refreshRouterArgs\", \"localhost:\" + router.getAdminServerAddress().getPort(), \"shared\" };\r\n    int exitCode = admin.run(args);\r\n    assertEquals(-1, exitCode);\r\n    Mockito.verify(handlerOne).handleRefresh(\"shared\", new String[] {});\r\n    Mockito.verify(handlerTwo).handleRefresh(\"shared\", new String[] {});\r\n    RefreshRegistry.defaultRegistry().unregisterAll(\"shared\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testExceptionResultsInNormalError",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testExceptionResultsInNormalError() throws Exception\n{\r\n    RefreshHandler exceptionalHandler = Mockito.mock(RefreshHandler.class);\r\n    Mockito.when(exceptionalHandler.handleRefresh(Mockito.anyString(), Mockito.any(String[].class))).thenThrow(new RuntimeException(\"Exceptional Handler Throws Exception\"));\r\n    RefreshHandler otherExceptionalHandler = Mockito.mock(RefreshHandler.class);\r\n    Mockito.when(otherExceptionalHandler.handleRefresh(Mockito.anyString(), Mockito.any(String[].class))).thenThrow(new RuntimeException(\"More Exceptions\"));\r\n    RefreshRegistry.defaultRegistry().register(\"exceptional\", exceptionalHandler);\r\n    RefreshRegistry.defaultRegistry().register(\"exceptional\", otherExceptionalHandler);\r\n    String[] args = new String[] { \"-refreshRouterArgs\", \"localhost:\" + router.getAdminServerAddress().getPort(), \"exceptional\" };\r\n    int exitCode = admin.run(args);\r\n    assertEquals(-1, exitCode);\r\n    Mockito.verify(exceptionalHandler).handleRefresh(\"exceptional\", new String[] {});\r\n    Mockito.verify(otherExceptionalHandler).handleRefresh(\"exceptional\", new String[] {});\r\n    RefreshRegistry.defaultRegistry().unregisterAll(\"exceptional\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "testListEmptyRootDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testListEmptyRootDirectory() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "testRmEmptyRootDirNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRmEmptyRootDirNonRecursive() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "testRecursiveRootListing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRecursiveRootListing() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "testRmRootRecursive",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRmRootRecursive()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "testRmEmptyRootDirRecursive",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRmEmptyRootDirRecursive()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterHDFSContract.createCluster();\r\n    RouterHDFSContract.getFileSystem().getDefaultBlockSize(new Path(\"/\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "globalSetUp",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void globalSetUp() throws Exception\n{\r\n    cluster = new StateStoreDFSCluster(false, 2);\r\n    Configuration conf = new RouterConfigBuilder().stateStore().admin().rpc().http().build();\r\n    cluster.addRouterOverrides(conf);\r\n    cluster.startCluster();\r\n    cluster.startRouters();\r\n    cluster.waitClusterUp();\r\n    routerContext = cluster.getRandomRouter();\r\n    routerFs = routerContext.getFileSystem();\r\n    Router router = routerContext.getRouter();\r\n    mountTable = (MountTableResolver) router.getSubclusterResolver();\r\n    webAddress = router.getHttpServerAddress();\r\n    assertNotNull(webAddress);\r\n    StateStoreService stateStore = routerContext.getRouter().getStateStore();\r\n    MembershipStore membership = stateStore.getRegisteredRecordStore(MembershipStore.class);\r\n    GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance();\r\n    GetNamenodeRegistrationsResponse response = membership.getNamenodeRegistrations(request);\r\n    memberships = response.getNamenodeMemberships();\r\n    Collections.sort(memberships);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (cluster != null) {\r\n        cluster.stopRouter(routerContext);\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "clearMountTable",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void clearMountTable() throws IOException\n{\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTableManager = client.getMountTableManager();\r\n    GetMountTableEntriesRequest req1 = GetMountTableEntriesRequest.newInstance(\"/\");\r\n    GetMountTableEntriesResponse response = mountTableManager.getMountTableEntries(req1);\r\n    for (MountTable entry : response.getEntries()) {\r\n        RemoveMountTableEntryRequest req2 = RemoveMountTableEntryRequest.newInstance(entry.getSourcePath());\r\n        mountTableManager.removeMountTableEntry(req2);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addMountTable",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean addMountTable(final MountTable entry) throws IOException\n{\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTableManager = client.getMountTableManager();\r\n    AddMountTableEntryRequest addRequest = AddMountTableEntryRequest.newInstance(entry);\r\n    AddMountTableEntryResponse addResponse = mountTableManager.addMountTableEntry(addRequest);\r\n    mountTable.loadCache(true);\r\n    return addResponse.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testFsck",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "void testFsck() throws Exception\n{\r\n    MountTable addEntry = MountTable.newInstance(\"/testdir\", Collections.singletonMap(\"ns0\", \"/testdir\"));\r\n    assertTrue(addMountTable(addEntry));\r\n    addEntry = MountTable.newInstance(\"/testdir2\", Collections.singletonMap(\"ns1\", \"/testdir2\"));\r\n    assertTrue(addMountTable(addEntry));\r\n    routerFs.createNewFile(new Path(\"/testdir/testfile\"));\r\n    routerFs.createNewFile(new Path(\"/testdir2/testfile2\"));\r\n    routerFs.createNewFile(new Path(\"/testdir2/testfile3\"));\r\n    routerFs.createNewFile(new Path(\"/testdir2/testfile4\"));\r\n    try (CloseableHttpClient httpClient = HttpClients.createDefault()) {\r\n        HttpGet httpGet = new HttpGet(\"http://\" + webAddress.getHostName() + \":\" + webAddress.getPort() + \"/fsck\");\r\n        try (CloseableHttpResponse httpResponse = httpClient.execute(httpGet)) {\r\n            assertEquals(HttpStatus.SC_OK, httpResponse.getStatusLine().getStatusCode());\r\n            String out = EntityUtils.toString(httpResponse.getEntity(), StandardCharsets.UTF_8);\r\n            LOG.info(out);\r\n            assertTrue(out.contains(\"Federated FSCK started\"));\r\n            assertTrue(out.contains(\"Total files:\\t1\"));\r\n            assertTrue(out.contains(\"Total files:\\t3\"));\r\n            assertTrue(out.contains(\"Federated FSCK ended\"));\r\n            int nnCount = 0;\r\n            for (MembershipState nn : memberships) {\r\n                if (nn.getState() == FederationNamenodeServiceState.ACTIVE) {\r\n                    assertTrue(out.contains(\"Checking \" + nn + \" at \" + nn.getWebAddress() + \"\\n\"));\r\n                    nnCount++;\r\n                }\r\n            }\r\n            assertEquals(2, nnCount);\r\n        }\r\n        httpGet = new HttpGet(\"http://\" + webAddress.getHostName() + \":\" + webAddress.getPort() + \"/fsck?path=/testdir\");\r\n        try (CloseableHttpResponse httpResponse = httpClient.execute(httpGet)) {\r\n            assertEquals(HttpStatus.SC_OK, httpResponse.getStatusLine().getStatusCode());\r\n            String out = EntityUtils.toString(httpResponse.getEntity(), StandardCharsets.UTF_8);\r\n            LOG.info(out);\r\n            assertTrue(out.contains(\"Federated FSCK started\"));\r\n            assertTrue(out.contains(\"Total files:\\t1\"));\r\n            assertFalse(out.contains(\"Total files:\\t3\"));\r\n            assertTrue(out.contains(\"Federated FSCK ended\"));\r\n            int nnCount = 0;\r\n            for (MembershipState nn : memberships) {\r\n                if (nn.getState() == FederationNamenodeServiceState.ACTIVE) {\r\n                    assertTrue(out.contains(\"Checking \" + nn + \" at \" + nn.getWebAddress() + \"\\n\"));\r\n                    nnCount++;\r\n                }\r\n            }\r\n            assertEquals(2, nnCount);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws Exception\n{\r\n    RouterHDFSContract.createCluster(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws Exception\n{\r\n    RouterHDFSContract.createCluster(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "testListEmptyRootDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testListEmptyRootDirectory() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "testRmEmptyRootDirNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRmEmptyRootDirNonRecursive() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "testRecursiveRootListing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRecursiveRootListing() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "testRmRootRecursive",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRmRootRecursive()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "testRmEmptyRootDirRecursive",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRmEmptyRootDirRecursive()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    router = new Router();\r\n    Configuration routerConf = new RouterConfigBuilder().quota(false).rpc().build();\r\n    routerConf.set(RBFConfigKeys.DFS_ROUTER_RPC_ADDRESS_KEY, \"0.0.0.0:0\");\r\n    router.init(routerConf);\r\n    router.setRouterId(\"TestRouterId\");\r\n    router.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws IOException\n{\r\n    if (router != null) {\r\n        router.stop();\r\n        router.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkDisableQuota",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkDisableQuota()\n{\r\n    assertFalse(router.isQuotaEnabled());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSetQuota",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSetQuota() throws Exception\n{\r\n    long nsQuota = 1024;\r\n    long ssQuota = 1024;\r\n    Quota quotaModule = router.getRpcServer().getQuotaModule();\r\n    LambdaTestUtils.intercept(IOException.class, \"The quota system is disabled in Router.\", \"The setQuota call should fail.\", () -> quotaModule.setQuota(\"/test\", nsQuota, ssQuota, null, false));\r\n    LambdaTestUtils.intercept(IOException.class, \"The quota system is disabled in Router.\", \"The setQuota call should fail.\", () -> quotaModule.setQuota(\"/test\", nsQuota, ssQuota, null, true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetQuotaUsage",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetQuotaUsage() throws Exception\n{\r\n    try {\r\n        Quota quotaModule = router.getRpcServer().getQuotaModule();\r\n        quotaModule.getQuotaUsage(\"/test\");\r\n        fail(\"The getQuotaUsage call should fail.\");\r\n    } catch (IOException ioe) {\r\n        GenericTestUtils.assertExceptionContains(\"The quota system is disabled in Router.\", ioe);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetGlobalQuota",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetGlobalQuota() throws Exception\n{\r\n    LambdaTestUtils.intercept(IOException.class, \"The quota system is disabled in Router.\", \"The getGlobalQuota call should fail.\", () -> {\r\n        Quota quotaModule = router.getRpcServer().getQuotaModule();\r\n        quotaModule.getGlobalQuota(\"/test\");\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws Exception\n{\r\n    RouterHDFSContract.createCluster(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "createIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DelegationTokenIdentifier createIdentifier()\n{\r\n    return new DelegationTokenIdentifier();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "testGetterSetter",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testGetterSetter() throws IOException\n{\r\n    MountTable record = MountTable.newInstance(SRC, DST_MAP);\r\n    validateDestinations(record);\r\n    assertEquals(SRC, record.getSourcePath());\r\n    assertEquals(DST, record.getDestinations());\r\n    assertTrue(DATE_CREATED > 0);\r\n    assertTrue(DATE_MOD > 0);\r\n    RouterQuotaUsage quota = record.getQuota();\r\n    assertEquals(0, quota.getFileAndDirectoryCount());\r\n    assertEquals(HdfsConstants.QUOTA_RESET, quota.getQuota());\r\n    assertEquals(0, quota.getSpaceConsumed());\r\n    assertEquals(HdfsConstants.QUOTA_RESET, quota.getSpaceQuota());\r\n    MountTable record2 = MountTable.newInstance(SRC, DST_MAP, DATE_CREATED, DATE_MOD);\r\n    validateDestinations(record2);\r\n    assertEquals(SRC, record2.getSourcePath());\r\n    assertEquals(DST, record2.getDestinations());\r\n    assertEquals(DATE_CREATED, record2.getDateCreated());\r\n    assertEquals(DATE_MOD, record2.getDateModified());\r\n    assertFalse(record.isReadOnly());\r\n    assertEquals(DestinationOrder.HASH, record.getDestOrder());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "testSerialization",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSerialization() throws IOException\n{\r\n    testSerialization(DestinationOrder.RANDOM);\r\n    testSerialization(DestinationOrder.HASH);\r\n    testSerialization(DestinationOrder.LOCAL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "testSerialization",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testSerialization(final DestinationOrder order) throws IOException\n{\r\n    MountTable record = MountTable.newInstance(SRC, DST_MAP, DATE_CREATED, DATE_MOD);\r\n    record.setReadOnly(true);\r\n    record.setDestOrder(order);\r\n    record.setQuota(QUOTA);\r\n    StateStoreSerializer serializer = StateStoreSerializer.getSerializer();\r\n    String serializedString = serializer.serializeString(record);\r\n    MountTable record2 = serializer.deserialize(serializedString, MountTable.class);\r\n    validateDestinations(record2);\r\n    assertEquals(SRC, record2.getSourcePath());\r\n    assertEquals(DST, record2.getDestinations());\r\n    assertEquals(DATE_CREATED, record2.getDateCreated());\r\n    assertEquals(DATE_MOD, record2.getDateModified());\r\n    assertTrue(record2.isReadOnly());\r\n    assertEquals(order, record2.getDestOrder());\r\n    RouterQuotaUsage quotaGet = record2.getQuota();\r\n    assertEquals(NS_COUNT, quotaGet.getFileAndDirectoryCount());\r\n    assertEquals(NS_QUOTA, quotaGet.getQuota());\r\n    assertEquals(SS_COUNT, quotaGet.getSpaceConsumed());\r\n    assertEquals(SS_QUOTA, quotaGet.getSpaceQuota());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "testReadOnly",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testReadOnly() throws IOException\n{\r\n    Map<String, String> dest = new LinkedHashMap<>();\r\n    dest.put(DST_NS_0, DST_PATH_0);\r\n    dest.put(DST_NS_1, DST_PATH_1);\r\n    MountTable record1 = MountTable.newInstance(SRC, dest);\r\n    record1.setReadOnly(true);\r\n    validateDestinations(record1);\r\n    assertEquals(SRC, record1.getSourcePath());\r\n    assertEquals(DST, record1.getDestinations());\r\n    assertTrue(DATE_CREATED > 0);\r\n    assertTrue(DATE_MOD > 0);\r\n    assertTrue(record1.isReadOnly());\r\n    MountTable record2 = MountTable.newInstance(SRC, DST_MAP, DATE_CREATED, DATE_MOD);\r\n    record2.setReadOnly(true);\r\n    validateDestinations(record2);\r\n    assertEquals(SRC, record2.getSourcePath());\r\n    assertEquals(DST, record2.getDestinations());\r\n    assertEquals(DATE_CREATED, record2.getDateCreated());\r\n    assertEquals(DATE_MOD, record2.getDateModified());\r\n    assertTrue(record2.isReadOnly());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "testFaultTolerant",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testFaultTolerant() throws IOException\n{\r\n    Map<String, String> dest = new LinkedHashMap<>();\r\n    dest.put(DST_NS_0, DST_PATH_0);\r\n    dest.put(DST_NS_1, DST_PATH_1);\r\n    MountTable record0 = MountTable.newInstance(SRC, dest);\r\n    assertFalse(record0.isFaultTolerant());\r\n    MountTable record1 = MountTable.newInstance(SRC, dest);\r\n    assertFalse(record1.isFaultTolerant());\r\n    assertEquals(record0, record1);\r\n    record1.setFaultTolerant(true);\r\n    assertTrue(record1.isFaultTolerant());\r\n    assertNotEquals(record0, record1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "testOrder",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testOrder() throws IOException\n{\r\n    testOrder(DestinationOrder.HASH);\r\n    testOrder(DestinationOrder.LOCAL);\r\n    testOrder(DestinationOrder.RANDOM);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "testOrder",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testOrder(final DestinationOrder order) throws IOException\n{\r\n    MountTable record = MountTable.newInstance(SRC, DST_MAP, DATE_CREATED, DATE_MOD);\r\n    record.setDestOrder(order);\r\n    validateDestinations(record);\r\n    assertEquals(SRC, record.getSourcePath());\r\n    assertEquals(DST, record.getDestinations());\r\n    assertEquals(DATE_CREATED, record.getDateCreated());\r\n    assertEquals(DATE_MOD, record.getDateModified());\r\n    assertEquals(order, record.getDestOrder());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "validateDestinations",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void validateDestinations(MountTable record)\n{\r\n    assertEquals(SRC, record.getSourcePath());\r\n    assertEquals(2, record.getDestinations().size());\r\n    RemoteLocation location1 = record.getDestinations().get(0);\r\n    assertEquals(DST_NS_0, location1.getNameserviceId());\r\n    assertEquals(DST_PATH_0, location1.getDest());\r\n    RemoteLocation location2 = record.getDestinations().get(1);\r\n    assertEquals(DST_NS_1, location2.getNameserviceId());\r\n    assertEquals(DST_PATH_1, location2.getDest());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "testQuota",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testQuota() throws IOException\n{\r\n    MountTable record = MountTable.newInstance(SRC, DST_MAP);\r\n    record.setQuota(QUOTA);\r\n    validateDestinations(record);\r\n    assertEquals(SRC, record.getSourcePath());\r\n    assertEquals(DST, record.getDestinations());\r\n    assertTrue(DATE_CREATED > 0);\r\n    assertTrue(DATE_MOD > 0);\r\n    RouterQuotaUsage quotaGet = record.getQuota();\r\n    assertEquals(NS_COUNT, quotaGet.getFileAndDirectoryCount());\r\n    assertEquals(NS_QUOTA, quotaGet.getQuota());\r\n    assertEquals(SS_COUNT, quotaGet.getSpaceConsumed());\r\n    assertEquals(SS_QUOTA, quotaGet.getSpaceQuota());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "testValidation",
  "errType" : [ "Exception", "Exception", "Exception" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testValidation() throws IOException\n{\r\n    Map<String, String> destinations = new HashMap<>();\r\n    destinations.put(\"ns0\", \"/testValidate-dest\");\r\n    try {\r\n        MountTable.newInstance(\"testValidate\", destinations);\r\n        fail(\"Mount table entry should be created failed.\");\r\n    } catch (Exception e) {\r\n        GenericTestUtils.assertExceptionContains(MountTable.ERROR_MSG_MUST_START_WITH_BACK_SLASH, e);\r\n    }\r\n    destinations.clear();\r\n    destinations.put(\"ns0\", \"testValidate-dest\");\r\n    try {\r\n        MountTable.newInstance(\"/testValidate\", destinations);\r\n        fail(\"Mount table entry should be created failed.\");\r\n    } catch (Exception e) {\r\n        GenericTestUtils.assertExceptionContains(MountTable.ERROR_MSG_ALL_DEST_MUST_START_WITH_BACK_SLASH, e);\r\n    }\r\n    destinations.clear();\r\n    destinations.put(\"\", \"/testValidate-dest\");\r\n    try {\r\n        MountTable.newInstance(\"/testValidate\", destinations);\r\n        fail(\"Mount table entry should be created failed.\");\r\n    } catch (Exception e) {\r\n        GenericTestUtils.assertExceptionContains(MountTable.ERROR_MSG_INVALID_DEST_NS, e);\r\n    }\r\n    destinations.clear();\r\n    destinations.put(\"ns0\", \"/testValidate-dest\");\r\n    MountTable record = MountTable.newInstance(\"/testValidate\", destinations);\r\n    assertNotNull(record);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws Exception\n{\r\n    RouterHDFSContract.createCluster(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "globalSetUp",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void globalSetUp() throws Exception\n{\r\n    cluster = new MiniRouterDFSCluster(false, NUM_SUBCLUSTERS);\r\n    cluster.setNumDatanodesPerNameservice(NUM_DNS);\r\n    cluster.startCluster();\r\n    Configuration routerConf = new RouterConfigBuilder().metrics().rpc().quota().build();\r\n    cluster.addRouterOverrides(routerConf);\r\n    cluster.startRouters();\r\n    cluster.registerNamenodes();\r\n    cluster.waitNamenodeRegistration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testSetup",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSetup() throws Exception\n{\r\n    cluster.installMockLocations();\r\n    cluster.deleteAllFiles();\r\n    cluster.createTestDirectoriesNamenode();\r\n    Thread.sleep(100);\r\n    routerContext = cluster.getRouters().get(0);\r\n    this.routerFS = routerContext.getFileSystem();\r\n    router = routerContext.getRouter();\r\n    MockResolver resolver = (MockResolver) router.getSubclusterResolver();\r\n    resolver.addLocation(\"/target-ns0\", cluster.getNameservices().get(0), \"/target-ns0\");\r\n    resolver.addLocation(\"/target-ns1\", cluster.getNameservices().get(1), \"/target-ns1\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    cluster.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "testProxyOp",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testProxyOp() throws IOException\n{\r\n    routerFS.listStatus(new Path(\"/target-ns0\"));\r\n    assertCounter(\"ProxyOp\", 1L, getMetrics(NAMESERVICE_RPC_METRICS_PREFIX + \"ns0\"));\r\n    assertCounter(\"ProxyOp\", 0L, getMetrics(NAMESERVICE_RPC_METRICS_PREFIX + \"ns1\"));\r\n    routerFS.listStatus(new Path(\"/target-ns1\"));\r\n    assertCounter(\"ProxyOp\", 1L, getMetrics(NAMESERVICE_RPC_METRICS_PREFIX + \"ns0\"));\r\n    assertCounter(\"ProxyOp\", 1L, getMetrics(NAMESERVICE_RPC_METRICS_PREFIX + \"ns1\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "globalSetUp",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void globalSetUp() throws Exception\n{\r\n    cluster = new StateStoreDFSCluster(false, 2);\r\n    Configuration conf = new RouterConfigBuilder().stateStore().rpc().http().admin().build();\r\n    cluster.addRouterOverrides(conf);\r\n    cluster.setIndependentDNs();\r\n    cluster.startCluster();\r\n    cluster.startRouters();\r\n    cluster.waitClusterUp();\r\n    router = cluster.getRandomRouter();\r\n    httpUri = \"http://\" + router.getHttpAddress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testWebHdfsCreate",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testWebHdfsCreate() throws Exception\n{\r\n    String path = \"/tmp/file\";\r\n    URL url = new URL(getUri(path));\r\n    LOG.info(\"URL: {}\", url);\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"PUT\");\r\n    assertEquals(HttpURLConnection.HTTP_CREATED, conn.getResponseCode());\r\n    verifyFile(\"ns0\", path, true);\r\n    verifyFile(\"ns1\", path, false);\r\n    conn.disconnect();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testWebHdfsCreateWithMounts",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testWebHdfsCreateWithMounts() throws Exception\n{\r\n    String mountPoint = \"/tmp-ns1\";\r\n    String path = \"/tmp-ns1/file\";\r\n    createMountTableEntry(router.getRouter(), mountPoint, DestinationOrder.RANDOM, Collections.singletonList(\"ns1\"));\r\n    URL url = new URL(getUri(path));\r\n    LOG.info(\"URL: {}\", url);\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"PUT\");\r\n    assertEquals(HttpURLConnection.HTTP_CREATED, conn.getResponseCode());\r\n    verifyFile(\"ns1\", path, true);\r\n    verifyFile(\"ns0\", path, false);\r\n    conn.disconnect();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getUri",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getUri(String path)\n{\r\n    final String user = System.getProperty(\"user.name\");\r\n    final StringBuilder uri = new StringBuilder(httpUri);\r\n    uri.append(\"/webhdfs/v1\").append(path).append(\"?op=CREATE\").append(\"&user.name=\" + user);\r\n    return uri.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifyFile",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyFile(String ns, String path, boolean shouldExist) throws Exception\n{\r\n    FileSystem fs = cluster.getNamenode(ns, null).getFileSystem();\r\n    try {\r\n        fs.getFileStatus(new Path(path));\r\n        if (!shouldExist) {\r\n            fail(path + \" should not exist in ns \" + ns);\r\n        }\r\n    } catch (FileNotFoundException e) {\r\n        if (shouldExist) {\r\n            fail(path + \" should exist in ns \" + ns);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetNsFromDataNodeNetworkLocation",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetNsFromDataNodeNetworkLocation()\n{\r\n    assertEquals(\"ns0\", RouterWebHdfsMethods.getNsFromDataNodeNetworkLocation(\"/ns0/rack-info1\"));\r\n    assertEquals(\"ns0\", RouterWebHdfsMethods.getNsFromDataNodeNetworkLocation(\"/ns0/row1/rack-info1\"));\r\n    assertEquals(\"\", RouterWebHdfsMethods.getNsFromDataNodeNetworkLocation(\"/row0\"));\r\n    assertEquals(\"\", RouterWebHdfsMethods.getNsFromDataNodeNetworkLocation(\"whatever-rack-info1\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testWebHdfsCreateWithInvalidPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testWebHdfsCreateWithInvalidPath() throws Exception\n{\r\n    String path = \"//tmp//file\";\r\n    assertResponse(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "assertResponse",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void assertResponse(String path) throws IOException\n{\r\n    URL url = new URL(getUri(path));\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"PUT\");\r\n    assertEquals(HttpURLConnection.HTTP_BAD_REQUEST, conn.getResponseCode());\r\n    Map<?, ?> response = WebHdfsFileSystem.jsonParse(conn, true);\r\n    assertEquals(\"InvalidPathException\", ((LinkedHashMap) response.get(\"RemoteException\")).get(\"exception\"));\r\n    conn.disconnect();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterWebHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterWebHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterWebHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "testListEmptyRootDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testListEmptyRootDirectory() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "testRmEmptyRootDirNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRmEmptyRootDirNonRecursive() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "testRecursiveRootListing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRecursiveRootListing() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "testRmRootRecursive",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRmRootRecursive()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "testRmEmptyRootDirRecursive",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRmEmptyRootDirRecursive()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "testSimpleRootListing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testSimpleRootListing()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void create() throws Exception\n{\r\n    Configuration conf = getStateStoreConfiguration();\r\n    conf.setLong(RBFConfigKeys.FEDERATION_STORE_MEMBERSHIP_EXPIRATION_MS, TimeUnit.SECONDS.toMillis(5));\r\n    stateStore = newStateStore(conf);\r\n    assertNotNull(stateStore);\r\n    namenodeResolver = new MembershipNamenodeResolver(conf, stateStore);\r\n    namenodeResolver.setRouterId(ROUTERS[0]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void destroy() throws Exception\n{\r\n    stateStore.stop();\r\n    stateStore.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup() throws IOException, InterruptedException\n{\r\n    stateStore.loadDriver();\r\n    waitStateStore(stateStore, 10000);\r\n    boolean cleared = clearRecords(stateStore, MembershipState.class);\r\n    assertTrue(cleared);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testStateStoreDisconnected",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testStateStoreDisconnected() throws Exception\n{\r\n    NamenodeStatusReport report = createNamenodeReport(NAMESERVICES[0], NAMENODES[0], HAServiceState.ACTIVE);\r\n    assertTrue(namenodeResolver.registerNamenode(report));\r\n    stateStore.closeDriver();\r\n    assertFalse(stateStore.isDriverReady());\r\n    stateStore.refreshCaches(true);\r\n    List<? extends FederationNamenodeContext> nns = namenodeResolver.getNamenodesForBlockPoolId(NAMESERVICES[0]);\r\n    assertNull(nns);\r\n    verifyException(namenodeResolver, \"registerNamenode\", StateStoreUnavailableException.class, new Class[] { NamenodeStatusReport.class }, new Object[] { report });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "verifyFirstRegistration",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void verifyFirstRegistration(String nsId, String nnId, int resultsCount, FederationNamenodeServiceState state) throws IOException\n{\r\n    List<? extends FederationNamenodeContext> namenodes = namenodeResolver.getNamenodesForNameserviceId(nsId);\r\n    if (resultsCount == 0) {\r\n        assertNull(namenodes);\r\n    } else {\r\n        assertEquals(resultsCount, namenodes.size());\r\n        if (namenodes.size() > 0) {\r\n            FederationNamenodeContext namenode = namenodes.get(0);\r\n            assertEquals(state, namenode.getState());\r\n            assertEquals(nnId, namenode.getNamenodeId());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testRegistrationExpired",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testRegistrationExpired() throws InterruptedException, IOException\n{\r\n    NamenodeStatusReport report = createNamenodeReport(NAMESERVICES[0], NAMENODES[0], HAServiceState.ACTIVE);\r\n    assertTrue(namenodeResolver.registerNamenode(report));\r\n    stateStore.refreshCaches(true);\r\n    verifyFirstRegistration(NAMESERVICES[0], NAMENODES[0], 1, FederationNamenodeServiceState.ACTIVE);\r\n    Thread.sleep(6000);\r\n    stateStore.refreshCaches(true);\r\n    verifyFirstRegistration(NAMESERVICES[0], NAMENODES[0], 0, FederationNamenodeServiceState.ACTIVE);\r\n    assertTrue(namenodeResolver.registerNamenode(report));\r\n    stateStore.refreshCaches(true);\r\n    verifyFirstRegistration(NAMESERVICES[0], NAMENODES[0], 1, FederationNamenodeServiceState.ACTIVE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testRegistrationNamenodeSelection",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void testRegistrationNamenodeSelection() throws InterruptedException, IOException\n{\r\n    assertTrue(namenodeResolver.registerNamenode(createNamenodeReport(NAMESERVICES[0], NAMENODES[0], HAServiceState.ACTIVE)));\r\n    Thread.sleep(100);\r\n    assertTrue(namenodeResolver.registerNamenode(createNamenodeReport(NAMESERVICES[0], NAMENODES[1], HAServiceState.STANDBY)));\r\n    stateStore.refreshCaches(true);\r\n    verifyFirstRegistration(NAMESERVICES[0], NAMENODES[0], 2, FederationNamenodeServiceState.ACTIVE);\r\n    assertTrue(namenodeResolver.registerNamenode(createNamenodeReport(NAMESERVICES[0], NAMENODES[0], HAServiceState.ACTIVE)));\r\n    Thread.sleep(6000);\r\n    assertTrue(namenodeResolver.registerNamenode(createNamenodeReport(NAMESERVICES[0], NAMENODES[1], HAServiceState.STANDBY)));\r\n    stateStore.refreshCaches(true);\r\n    verifyFirstRegistration(NAMESERVICES[0], NAMENODES[1], 1, FederationNamenodeServiceState.STANDBY);\r\n    assertTrue(namenodeResolver.registerNamenode(createNamenodeReport(NAMESERVICES[0], NAMENODES[0], HAServiceState.ACTIVE)));\r\n    Thread.sleep(100);\r\n    assertTrue(namenodeResolver.registerNamenode(createNamenodeReport(NAMESERVICES[0], NAMENODES[1], null)));\r\n    stateStore.refreshCaches(true);\r\n    verifyFirstRegistration(NAMESERVICES[0], NAMENODES[0], 2, FederationNamenodeServiceState.ACTIVE);\r\n    assertTrue(namenodeResolver.registerNamenode(createNamenodeReport(NAMESERVICES[0], NAMENODES[1], HAServiceState.STANDBY)));\r\n    Thread.sleep(1000);\r\n    assertTrue(namenodeResolver.registerNamenode(createNamenodeReport(NAMESERVICES[0], NAMENODES[0], null)));\r\n    stateStore.refreshCaches(true);\r\n    verifyFirstRegistration(NAMESERVICES[0], NAMENODES[1], 2, FederationNamenodeServiceState.STANDBY);\r\n    assertTrue(namenodeResolver.registerNamenode(createNamenodeReport(NAMESERVICES[0], NAMENODES[0], null)));\r\n    Thread.sleep(100);\r\n    assertTrue(namenodeResolver.registerNamenode(createNamenodeReport(NAMESERVICES[0], NAMENODES[1], HAServiceState.STANDBY)));\r\n    Thread.sleep(100);\r\n    assertTrue(namenodeResolver.registerNamenode(createNamenodeReport(NAMESERVICES[0], NAMENODES[2], HAServiceState.ACTIVE)));\r\n    stateStore.refreshCaches(true);\r\n    verifyFirstRegistration(NAMESERVICES[0], NAMENODES[2], 3, FederationNamenodeServiceState.ACTIVE);\r\n    assertTrue(namenodeResolver.registerNamenode(createNamenodeReport(NAMESERVICES[0], NAMENODES[0], HAServiceState.STANDBY)));\r\n    assertTrue(namenodeResolver.registerNamenode(createNamenodeReport(NAMESERVICES[0], NAMENODES[2], HAServiceState.STANDBY)));\r\n    Thread.sleep(1500);\r\n    assertTrue(namenodeResolver.registerNamenode(createNamenodeReport(NAMESERVICES[0], NAMENODES[1], HAServiceState.STANDBY)));\r\n    stateStore.refreshCaches(true);\r\n    verifyFirstRegistration(NAMESERVICES[0], NAMENODES[1], 3, FederationNamenodeServiceState.STANDBY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testCacheUpdateOnNamenodeStateUpdate",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testCacheUpdateOnNamenodeStateUpdate() throws IOException\n{\r\n    assertTrue(namenodeResolver.registerNamenode(createNamenodeReport(NAMESERVICES[0], NAMENODES[0], HAServiceState.STANDBY)));\r\n    stateStore.refreshCaches(true);\r\n    FederationNamenodeContext namenode = namenodeResolver.getNamenodesForNameserviceId(NAMESERVICES[0]).get(0);\r\n    assertEquals(FederationNamenodeServiceState.STANDBY, namenode.getState());\r\n    String rpcAddr = namenode.getRpcAddress();\r\n    InetSocketAddress inetAddr = getInetSocketAddress(rpcAddr);\r\n    namenodeResolver.updateActiveNamenode(NAMESERVICES[0], inetAddr);\r\n    FederationNamenodeContext namenode1 = namenodeResolver.getNamenodesForNameserviceId(NAMESERVICES[0]).get(0);\r\n    assertEquals(\"The namenode state should be ACTIVE post update.\", FederationNamenodeServiceState.ACTIVE, namenode1.getState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testCacheUpdateOnNamenodeStateUpdateWithIp",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCacheUpdateOnNamenodeStateUpdateWithIp() throws IOException\n{\r\n    final String rpcAddress = \"127.0.0.1:10000\";\r\n    assertTrue(namenodeResolver.registerNamenode(createNamenodeReport(NAMESERVICES[0], NAMENODES[0], rpcAddress, HAServiceState.STANDBY)));\r\n    stateStore.refreshCaches(true);\r\n    InetSocketAddress inetAddr = getInetSocketAddress(rpcAddress);\r\n    namenodeResolver.updateActiveNamenode(NAMESERVICES[0], inetAddr);\r\n    FederationNamenodeContext namenode = namenodeResolver.getNamenodesForNameserviceId(NAMESERVICES[0]).get(0);\r\n    assertEquals(\"The namenode state should be ACTIVE post update.\", FederationNamenodeServiceState.ACTIVE, namenode.getState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getInetSocketAddress",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "InetSocketAddress getInetSocketAddress(String rpcAddr)\n{\r\n    String[] rpcAddrArr = rpcAddr.split(\":\");\r\n    int port = Integer.parseInt(rpcAddrArr[1]);\r\n    String hostname = rpcAddrArr[0];\r\n    return new InetSocketAddress(hostname, port);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "globalSetUp",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void globalSetUp() throws Exception\n{\r\n    cluster = new MiniRouterDFSCluster(true, 2);\r\n    cluster.startCluster();\r\n    List<String> nss = cluster.getNameservices();\r\n    String ns = nss.get(0);\r\n    Configuration conf = cluster.generateNamenodeConfiguration(ns);\r\n    namenodeResolver = new MockResolver(conf);\r\n    namenodeResolver.setRouterId(\"testrouter\");\r\n    services = new ArrayList<>();\r\n    for (NamenodeContext nn : cluster.getNamenodes()) {\r\n        String nsId = nn.getNameserviceId();\r\n        String nnId = nn.getNamenodeId();\r\n        NamenodeHeartbeatService service = new NamenodeHeartbeatService(namenodeResolver, nsId, nnId);\r\n        service.init(conf);\r\n        service.start();\r\n        services.add(service);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void tearDown() throws IOException\n{\r\n    cluster.shutdown();\r\n    for (NamenodeHeartbeatService service : services) {\r\n        service.stop();\r\n        service.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNamenodeHeartbeatService",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testNamenodeHeartbeatService() throws IOException\n{\r\n    MiniRouterDFSCluster testCluster = new MiniRouterDFSCluster(true, 1);\r\n    Configuration heartbeatConfig = testCluster.generateNamenodeConfiguration(NAMESERVICES[0]);\r\n    NamenodeHeartbeatService server = new NamenodeHeartbeatService(namenodeResolver, NAMESERVICES[0], NAMENODES[0]);\r\n    server.init(heartbeatConfig);\r\n    assertEquals(STATE.INITED, server.getServiceState());\r\n    server.start();\r\n    assertEquals(STATE.STARTED, server.getServiceState());\r\n    server.stop();\r\n    assertEquals(STATE.STOPPED, server.getServiceState());\r\n    server.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testLocalNamenodeHeartbeatService",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testLocalNamenodeHeartbeatService() throws IOException\n{\r\n    Router router = new Router();\r\n    Configuration conf = new Configuration();\r\n    assertEquals(null, DFSUtil.getNamenodeNameServiceId(conf));\r\n    router.setConf(conf);\r\n    assertNull(router.createLocalNamenodeHeartbeatService());\r\n    conf.set(DFS_NAMESERVICES, \"ns1\");\r\n    assertEquals(\"ns1\", DFSUtil.getNamenodeNameServiceId(conf));\r\n    conf.set(DFSUtil.addKeySuffixes(DFS_HA_NAMENODES_KEY_PREFIX, \"ns1\"), \"nn1,nn2\");\r\n    conf.set(DFSUtil.addKeySuffixes(DFS_NAMENODE_RPC_ADDRESS_KEY, \"ns1\", \"nn1\"), \"localhost:8020\");\r\n    conf.set(DFSUtil.addKeySuffixes(DFS_NAMENODE_RPC_ADDRESS_KEY, \"ns1\", \"nn2\"), \"ns1-nn2.example.com:8020\");\r\n    router.setConf(conf);\r\n    NamenodeHeartbeatService heartbeatService = router.createLocalNamenodeHeartbeatService();\r\n    assertNotNull(heartbeatService);\r\n    heartbeatService.init(conf);\r\n    assertEquals(\"ns1-nn1:localhost:8020\", heartbeatService.getNamenodeDesc());\r\n    heartbeatService.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testHearbeat",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testHearbeat() throws InterruptedException, IOException\n{\r\n    if (cluster.isHighAvailability()) {\r\n        for (String ns : cluster.getNameservices()) {\r\n            cluster.switchToActive(ns, NAMENODES[0]);\r\n            cluster.switchToStandby(ns, NAMENODES[1]);\r\n        }\r\n    }\r\n    Thread.sleep(5000);\r\n    for (String ns : cluster.getNameservices()) {\r\n        List<? extends FederationNamenodeContext> nns = namenodeResolver.getNamenodesForNameserviceId(ns);\r\n        FederationNamenodeContext active = nns.get(0);\r\n        assertEquals(NAMENODES[0], active.getNamenodeId());\r\n        FederationNamenodeContext standby = nns.get(1);\r\n        assertEquals(NAMENODES[1], standby.getNamenodeId());\r\n    }\r\n    List<String> nss = cluster.getNameservices();\r\n    String failoverNS = nss.get(0);\r\n    String normalNs = nss.get(1);\r\n    cluster.switchToStandby(failoverNS, NAMENODES[0]);\r\n    cluster.switchToActive(failoverNS, NAMENODES[1]);\r\n    Thread.sleep(5000);\r\n    List<? extends FederationNamenodeContext> failoverNSs = namenodeResolver.getNamenodesForNameserviceId(failoverNS);\r\n    FederationNamenodeContext active = failoverNSs.get(0);\r\n    assertEquals(NAMENODES[1], active.getNamenodeId());\r\n    FederationNamenodeContext standby = failoverNSs.get(1);\r\n    assertEquals(NAMENODES[0], standby.getNamenodeId());\r\n    List<? extends FederationNamenodeContext> normalNss = namenodeResolver.getNamenodesForNameserviceId(normalNs);\r\n    active = normalNss.get(0);\r\n    assertEquals(NAMENODES[0], active.getNamenodeId());\r\n    standby = normalNss.get(1);\r\n    assertEquals(NAMENODES[1], standby.getNamenodeId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNamenodeHeartbeatServiceHAServiceProtocolProxy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testNamenodeHeartbeatServiceHAServiceProtocolProxy()\n{\r\n    testNamenodeHeartbeatServiceHAServiceProtocol(\"test-ns\", \"nn\", 1000, -1, -1, 1003, \"host01.test:1000\", \"host02.test:1000\");\r\n    testNamenodeHeartbeatServiceHAServiceProtocol(\"test-ns\", \"nn\", 1000, 1001, -1, 1003, \"host01.test:1001\", \"host02.test:1001\");\r\n    testNamenodeHeartbeatServiceHAServiceProtocol(\"test-ns\", \"nn\", 1000, -1, 1002, 1003, \"host01.test:1002\", \"host02.test:1002\");\r\n    testNamenodeHeartbeatServiceHAServiceProtocol(\"test-ns\", \"nn\", 1000, 1001, 1002, 1003, \"host01.test:1002\", \"host02.test:1002\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNamenodeHeartbeatServiceHAServiceProtocol",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testNamenodeHeartbeatServiceHAServiceProtocol(String nsId, String nnId, int rpcPort, int servicePort, int lifelinePort, int webAddressPort, String expected0, String expected1)\n{\r\n    Configuration conf = generateNamenodeConfiguration(nsId, nnId, rpcPort, servicePort, lifelinePort, webAddressPort);\r\n    Router testRouter = new Router();\r\n    testRouter.setConf(conf);\r\n    Collection<NamenodeHeartbeatService> heartbeatServices = testRouter.createNamenodeHeartbeatServices();\r\n    assertEquals(2, heartbeatServices.size());\r\n    Iterator<NamenodeHeartbeatService> iterator = heartbeatServices.iterator();\r\n    NamenodeHeartbeatService service0 = iterator.next();\r\n    service0.init(conf);\r\n    assertNotNull(service0.getLocalTarget());\r\n    assertEquals(expected0, service0.getLocalTarget().getHealthMonitorAddress().toString());\r\n    NamenodeHeartbeatService service1 = iterator.next();\r\n    service1.init(conf);\r\n    assertNotNull(service1.getLocalTarget());\r\n    assertEquals(expected1, service1.getLocalTarget().getHealthMonitorAddress().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNamenodeHeartbeatServiceNNResolution",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testNamenodeHeartbeatServiceNNResolution()\n{\r\n    String nsId = \"test-ns\";\r\n    String nnId = \"nn\";\r\n    int rpcPort = 1000;\r\n    int servicePort = 1001;\r\n    int lifelinePort = 1002;\r\n    int webAddressPort = 1003;\r\n    Configuration conf = generateNamenodeConfiguration(nsId, nnId, rpcPort, servicePort, lifelinePort, webAddressPort);\r\n    Router testRouter = new Router();\r\n    testRouter.setConf(conf);\r\n    Collection<NamenodeHeartbeatService> heartbeatServices = testRouter.createNamenodeHeartbeatServices();\r\n    assertEquals(2, heartbeatServices.size());\r\n    Iterator<NamenodeHeartbeatService> iterator = heartbeatServices.iterator();\r\n    NamenodeHeartbeatService service = iterator.next();\r\n    service.init(conf);\r\n    assertEquals(\"test-ns-nn-host01.test:host01.test:1001\", service.getNamenodeDesc());\r\n    service = iterator.next();\r\n    service.init(conf);\r\n    assertEquals(\"test-ns-nn-host02.test:host02.test:1001\", service.getNamenodeDesc());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "generateNamenodeConfiguration",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Configuration generateNamenodeConfiguration(String nsId, String nnId, int rpcPort, int servicePort, int lifelinePort, int webAddressPort)\n{\r\n    Configuration conf = new HdfsConfiguration();\r\n    String suffix = nsId + \".\" + nnId;\r\n    conf.setBoolean(RBFConfigKeys.DFS_ROUTER_MONITOR_LOCAL_NAMENODE, false);\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_MONITOR_NAMENODE, nsId + \".\" + nnId);\r\n    conf.setBoolean(RBFConfigKeys.DFS_ROUTER_MONITOR_NAMENODE_RESOLUTION_ENABLED + \".\" + nsId, true);\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_MONITOR_NAMENODE_RESOLVER_IMPL + \".\" + nsId, MockDomainNameResolver.class.getName());\r\n    conf.set(DFS_NAMENODE_RPC_ADDRESS_KEY + \".\" + suffix, MockDomainNameResolver.DOMAIN + \":\" + rpcPort);\r\n    if (servicePort >= 0) {\r\n        conf.set(DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY + \".\" + suffix, MockDomainNameResolver.DOMAIN + \":\" + servicePort);\r\n    }\r\n    if (lifelinePort >= 0) {\r\n        conf.set(DFS_NAMENODE_LIFELINE_RPC_ADDRESS_KEY + \".\" + suffix, MockDomainNameResolver.DOMAIN + \":\" + lifelinePort);\r\n    }\r\n    conf.set(DFS_NAMENODE_HTTP_ADDRESS_KEY + \".\" + suffix, MockDomainNameResolver.DOMAIN + \":\" + webAddressPort);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initialize()\n{\r\n    Service[] services = new RouterPolicyProvider().getServices();\r\n    policyProviderProtocols = new HashSet<>(services.length);\r\n    for (Service service : services) {\r\n        policyProviderProtocols.add(service.getProtocol());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "data",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<Class<?>[]> data()\n{\r\n    return Arrays.asList(new Class<?>[][] { { RouterRpcServer.class }, { NameNodeRpcServer.class }, { DataNode.class }, { RouterAdminServer.class } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testPolicyProviderForServer",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testPolicyProviderForServer()\n{\r\n    List<?> ifaces = ClassUtils.getAllInterfaces(rpcServerClass);\r\n    Set<Class<?>> serverProtocols = new HashSet<>(ifaces.size());\r\n    for (Object obj : ifaces) {\r\n        Class<?> iface = (Class<?>) obj;\r\n        if (iface.getSimpleName().endsWith(\"Protocol\")) {\r\n            serverProtocols.add(iface);\r\n        }\r\n    }\r\n    LOG.info(\"Running test {} for RPC server {}.  Found server protocols {} \" + \"and policy provider protocols {}.\", testName.getMethodName(), rpcServerClass.getName(), serverProtocols, policyProviderProtocols);\r\n    assertFalse(\"Expected to find at least one protocol in server.\", serverProtocols.isEmpty());\r\n    final Set<Class<?>> differenceSet = Sets.difference(serverProtocols, policyProviderProtocols);\r\n    assertTrue(String.format(\"Following protocols for server %s are not defined in \" + \"%s: %s\", rpcServerClass.getName(), RouterPolicyProvider.class.getName(), Arrays.toString(differenceSet.toArray())), differenceSet.isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "globalSetUp",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void globalSetUp() throws Exception\n{\r\n    cluster = new MiniRouterDFSCluster(false, 1);\r\n    StorageType[][] newtypes = new StorageType[][] { { StorageType.ARCHIVE, StorageType.DISK } };\r\n    cluster.setStorageTypes(newtypes);\r\n    Configuration conf = cluster.getNamenodes().get(0).getConf();\r\n    conf.set(DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_MODE_KEY, HdfsConstants.StoragePolicySatisfierMode.EXTERNAL.toString());\r\n    conf.setLong(DFSConfigKeys.DFS_SPS_DATANODE_CACHE_REFRESH_INTERVAL_MS, 1000);\r\n    cluster.addNamenodeOverrides(conf);\r\n    cluster.setNumDatanodesPerNameservice(1);\r\n    cluster.startCluster();\r\n    Configuration routerConf = new RouterConfigBuilder().metrics().rpc().build();\r\n    routerConf.setTimeDuration(RBFConfigKeys.DN_REPORT_CACHE_EXPIRE, 1, TimeUnit.SECONDS);\r\n    cluster.addRouterOverrides(routerConf);\r\n    cluster.startRouters();\r\n    cluster.registerNamenodes();\r\n    cluster.waitNamenodeRegistration();\r\n    cluster.installMockLocations();\r\n    MiniRouterDFSCluster.RouterContext rndRouter = cluster.getRandomRouter();\r\n    routerProtocol = rndRouter.getClient().getNamenode();\r\n    routerFS = rndRouter.getFileSystem();\r\n    nnFS = cluster.getNamenodes().get(0).getFileSystem();\r\n    NameNodeConnector nnc = DFSTestUtil.getNameNodeConnector(conf, HdfsServerConstants.MOVER_ID_PATH, 1, false);\r\n    StoragePolicySatisfier externalSps = new StoragePolicySatisfier(conf);\r\n    Context externalCtxt = new ExternalSPSContext(externalSps, nnc);\r\n    externalSps.init(externalCtxt);\r\n    externalSps.start(HdfsConstants.StoragePolicySatisfierMode.EXTERNAL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    cluster.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testStoragePolicySatisfier",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testStoragePolicySatisfier() throws Exception\n{\r\n    final String file = \"/testStoragePolicySatisfierCommand\";\r\n    short repl = 1;\r\n    int size = 32;\r\n    DFSTestUtil.createFile(routerFS, new Path(file), size, repl, 0);\r\n    DFSTestUtil.waitExpectedStorageType(file, StorageType.DISK, 1, 20000, (DistributedFileSystem) routerFS);\r\n    routerProtocol.setStoragePolicy(file, HdfsConstants.COLD_STORAGE_POLICY_NAME);\r\n    BlockStoragePolicy storagePolicy = routerProtocol.getStoragePolicy(file);\r\n    assertEquals(HdfsConstants.COLD_STORAGE_POLICY_NAME, storagePolicy.getName());\r\n    routerProtocol.satisfyStoragePolicy(file);\r\n    DFSTestUtil.waitExpectedStorageType(file, StorageType.ARCHIVE, 1, 20000, (DistributedFileSystem) routerFS);\r\n    DFSTestUtil.waitExpectedStorageType(file, StorageType.ARCHIVE, 1, 20000, (DistributedFileSystem) nnFS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "setupMock",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setupMock() throws IOException\n{\r\n    NamespaceInfo nsInfo = new NamespaceInfo(1, this.nsId, this.nsId, 1);\r\n    when(mockNn.versionRequest()).thenReturn(nsInfo);\r\n    when(mockNn.getServiceStatus()).thenAnswer(new Answer<HAServiceStatus>() {\r\n\r\n        @Override\r\n        public HAServiceStatus answer(InvocationOnMock invocation) throws Throwable {\r\n            HAServiceStatus haStatus = new HAServiceStatus(getHAServiceState());\r\n            haStatus.setNotReadyToBecomeActive(\"\");\r\n            return haStatus;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "setupRPCServer",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setupRPCServer(final Configuration conf) throws IOException\n{\r\n    RPC.setProtocolEngine(conf, ClientNamenodeProtocolPB.class, ProtobufRpcEngine2.class);\r\n    ClientNamenodeProtocolServerSideTranslatorPB clientNNProtoXlator = new ClientNamenodeProtocolServerSideTranslatorPB(mockNn);\r\n    BlockingService clientNNPbService = ClientNamenodeProtocol.newReflectiveBlockingService(clientNNProtoXlator);\r\n    int numHandlers = conf.getInt(DFSConfigKeys.DFS_NAMENODE_HANDLER_COUNT_KEY, DFSConfigKeys.DFS_NAMENODE_HANDLER_COUNT_DEFAULT);\r\n    rpcServer = new RPC.Builder(conf).setProtocol(ClientNamenodeProtocolPB.class).setInstance(clientNNPbService).setBindAddress(\"0.0.0.0\").setPort(0).setNumHandlers(numHandlers).build();\r\n    NamenodeProtocolServerSideTranslatorPB nnProtoXlator = new NamenodeProtocolServerSideTranslatorPB(mockNn);\r\n    BlockingService nnProtoPbService = NamenodeProtocolService.newReflectiveBlockingService(nnProtoXlator);\r\n    DFSUtil.addPBProtocol(conf, NamenodeProtocolPB.class, nnProtoPbService, rpcServer);\r\n    DatanodeProtocolServerSideTranslatorPB dnProtoPbXlator = new DatanodeProtocolServerSideTranslatorPB(mockNn, 1000);\r\n    BlockingService dnProtoPbService = DatanodeProtocolService.newReflectiveBlockingService(dnProtoPbXlator);\r\n    DFSUtil.addPBProtocol(conf, DatanodeProtocolPB.class, dnProtoPbService, rpcServer);\r\n    HAServiceProtocolServerSideTranslatorPB haServiceProtoXlator = new HAServiceProtocolServerSideTranslatorPB(mockNn);\r\n    BlockingService haProtoPbService = HAServiceProtocolService.newReflectiveBlockingService(haServiceProtoXlator);\r\n    DFSUtil.addPBProtocol(conf, HAServiceProtocolPB.class, haProtoPbService, rpcServer);\r\n    this.rpcServer.addTerseExceptions(RemoteException.class, SafeModeException.class, FileNotFoundException.class, FileAlreadyExistsException.class, AccessControlException.class, LeaseExpiredException.class, NotReplicatedYetException.class, IOException.class, ConnectException.class, StandbyException.class);\r\n    rpcServer.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "setupHTTPServer",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setupHTTPServer(Configuration conf) throws IOException\n{\r\n    HttpServer2.Builder builder = new HttpServer2.Builder().setName(\"hdfs\").setConf(conf).setACL(new AccessControlList(conf.get(DFS_ADMIN, \" \"))).addEndpoint(URI.create(\"http://0.0.0.0:0\"));\r\n    httpServer = builder.build();\r\n    httpServer.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getRPCPort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRPCPort()\n{\r\n    return rpcServer.getListenerAddress().getPort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getHTTPPort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getHTTPPort()\n{\r\n    return httpServer.getConnectorAddress(0).getPort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getMock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NamenodeProtocols getMock()\n{\r\n    return mockNn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNameserviceId()\n{\r\n    return nsId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getHAServiceState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HAServiceState getHAServiceState()\n{\r\n    return haState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "transitionToActive",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void transitionToActive()\n{\r\n    this.haState = HAServiceState.ACTIVE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "transitionToStandby",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void transitionToStandby()\n{\r\n    this.haState = HAServiceState.STANDBY;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<DatanodeInfo> getDatanodes()\n{\r\n    return this.dns;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stop() throws Exception\n{\r\n    if (rpcServer != null) {\r\n        rpcServer.stop();\r\n        rpcServer = null;\r\n    }\r\n    if (httpServer != null) {\r\n        httpServer.stop();\r\n        httpServer = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "addFileSystemMock",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void addFileSystemMock() throws IOException\n{\r\n    final SortedMap<String, String> fs = new ConcurrentSkipListMap<String, String>();\r\n    DirectoryListing l = mockNn.getListing(anyString(), any(), anyBoolean());\r\n    when(l).thenAnswer(invocation -> {\r\n        String src = getSrc(invocation);\r\n        LOG.info(\"{} getListing({})\", nsId, src);\r\n        if (fs.get(src) == null) {\r\n            throw new FileNotFoundException(\"File does not exist \" + src);\r\n        }\r\n        if (!src.endsWith(\"/\")) {\r\n            src += \"/\";\r\n        }\r\n        Map<String, String> files = fs.subMap(src, src + Character.MAX_VALUE);\r\n        List<HdfsFileStatus> list = new ArrayList<>();\r\n        for (String file : files.keySet()) {\r\n            if (file.substring(src.length()).indexOf('/') < 0) {\r\n                HdfsFileStatus fileStatus = getMockHdfsFileStatus(file, fs.get(file));\r\n                list.add(fileStatus);\r\n            }\r\n        }\r\n        HdfsFileStatus[] array = list.toArray(new HdfsFileStatus[list.size()]);\r\n        return new DirectoryListing(array, 0);\r\n    });\r\n    when(mockNn.getFileInfo(anyString())).thenAnswer(invocation -> {\r\n        String src = getSrc(invocation);\r\n        LOG.info(\"{} getFileInfo({})\", nsId, src);\r\n        return getMockHdfsFileStatus(src, fs.get(src));\r\n    });\r\n    HdfsFileStatus c = mockNn.create(anyString(), any(), anyString(), any(), anyBoolean(), anyShort(), anyLong(), any(), any(), any());\r\n    when(c).thenAnswer(invocation -> {\r\n        String src = getSrc(invocation);\r\n        LOG.info(\"{} create({})\", nsId, src);\r\n        boolean createParent = (boolean) invocation.getArgument(4);\r\n        if (createParent) {\r\n            Path path = new Path(src).getParent();\r\n            while (!path.isRoot()) {\r\n                LOG.info(\"{} create parent {}\", nsId, path);\r\n                fs.put(path.toString(), \"DIRECTORY\");\r\n                path = path.getParent();\r\n            }\r\n        }\r\n        fs.put(src, \"FILE\");\r\n        return getMockHdfsFileStatus(src, \"FILE\");\r\n    });\r\n    LocatedBlocks b = mockNn.getBlockLocations(anyString(), anyLong(), anyLong());\r\n    when(b).thenAnswer(invocation -> {\r\n        String src = getSrc(invocation);\r\n        LOG.info(\"{} getBlockLocations({})\", nsId, src);\r\n        if (!fs.containsKey(src)) {\r\n            LOG.error(\"{} cannot find {} for getBlockLocations\", nsId, src);\r\n            throw new FileNotFoundException(\"File does not exist \" + src);\r\n        }\r\n        return mock(LocatedBlocks.class);\r\n    });\r\n    boolean f = mockNn.complete(anyString(), anyString(), any(), anyLong());\r\n    when(f).thenAnswer(invocation -> {\r\n        String src = getSrc(invocation);\r\n        if (!fs.containsKey(src)) {\r\n            LOG.error(\"{} cannot find {} for complete\", nsId, src);\r\n            throw new FileNotFoundException(\"File does not exist \" + src);\r\n        }\r\n        return true;\r\n    });\r\n    LocatedBlock a = mockNn.addBlock(anyString(), anyString(), any(), any(), anyLong(), any(), any());\r\n    when(a).thenAnswer(invocation -> {\r\n        String src = getSrc(invocation);\r\n        if (!fs.containsKey(src)) {\r\n            LOG.error(\"{} cannot find {} for addBlock\", nsId, src);\r\n            throw new FileNotFoundException(\"File does not exist \" + src);\r\n        }\r\n        return getMockLocatedBlock(nsId);\r\n    });\r\n    boolean m = mockNn.mkdirs(anyString(), any(), anyBoolean());\r\n    when(m).thenAnswer(invocation -> {\r\n        String src = getSrc(invocation);\r\n        LOG.info(\"{} mkdirs({})\", nsId, src);\r\n        boolean createParent = (boolean) invocation.getArgument(2);\r\n        if (createParent) {\r\n            Path path = new Path(src).getParent();\r\n            while (!path.isRoot()) {\r\n                LOG.info(\"{} mkdir parent {}\", nsId, path);\r\n                fs.put(path.toString(), \"DIRECTORY\");\r\n                path = path.getParent();\r\n            }\r\n        }\r\n        fs.put(src, \"DIRECTORY\");\r\n        return true;\r\n    });\r\n    when(mockNn.getServerDefaults()).thenAnswer(invocation -> {\r\n        LOG.info(\"{} getServerDefaults\", nsId);\r\n        FsServerDefaults defaults = mock(FsServerDefaults.class);\r\n        when(defaults.getChecksumType()).thenReturn(Type.valueOf(DataChecksum.CHECKSUM_CRC32));\r\n        when(defaults.getKeyProviderUri()).thenReturn(nsId);\r\n        return defaults;\r\n    });\r\n    when(mockNn.getContentSummary(anyString())).thenAnswer(invocation -> {\r\n        String src = getSrc(invocation);\r\n        LOG.info(\"{} getContentSummary({})\", nsId, src);\r\n        if (fs.get(src) == null) {\r\n            throw new FileNotFoundException(\"File does not exist \" + src);\r\n        }\r\n        if (!src.endsWith(\"/\")) {\r\n            src += \"/\";\r\n        }\r\n        Map<String, String> files = fs.subMap(src, src + Character.MAX_VALUE);\r\n        int numFiles = 0;\r\n        int numDirs = 0;\r\n        int length = 0;\r\n        for (Entry<String, String> entry : files.entrySet()) {\r\n            String file = entry.getKey();\r\n            if (file.substring(src.length()).indexOf('/') < 0) {\r\n                String type = entry.getValue();\r\n                if (\"DIRECTORY\".equals(type)) {\r\n                    numDirs++;\r\n                } else if (\"FILE\".equals(type)) {\r\n                    numFiles++;\r\n                    length += 100;\r\n                }\r\n            }\r\n        }\r\n        return new ContentSummary.Builder().fileCount(numFiles).directoryCount(numDirs).length(length).erasureCodingPolicy(\"\").build();\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "addDatanodeMock",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addDatanodeMock() throws IOException\n{\r\n    when(mockNn.getDatanodeReport(any(DatanodeReportType.class))).thenAnswer(invocation -> {\r\n        LOG.info(\"{} getDatanodeReport()\", nsId, invocation.getArgument(0));\r\n        return dns.toArray();\r\n    });\r\n    when(mockNn.getDatanodeStorageReport(any(DatanodeReportType.class))).thenAnswer(invocation -> {\r\n        LOG.info(\"{} getDatanodeStorageReport()\", nsId, invocation.getArgument(0));\r\n        DatanodeStorageReport[] ret = new DatanodeStorageReport[dns.size()];\r\n        for (int i = 0; i < dns.size(); i++) {\r\n            DatanodeInfo dn = dns.get(i);\r\n            DatanodeStorage storage = new DatanodeStorage(dn.getName());\r\n            StorageReport[] storageReports = new StorageReport[] { new StorageReport(storage, false, 0L, 0L, 0L, 0L, 0L) };\r\n            ret[i] = new DatanodeStorageReport(dn, storageReports);\r\n        }\r\n        return ret;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getSrc",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getSrc(InvocationOnMock invocation)\n{\r\n    return (String) invocation.getArguments()[0];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getMockHdfsFileStatus",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "HdfsFileStatus getMockHdfsFileStatus(final String filename, final String type)\n{\r\n    if (type == null) {\r\n        return null;\r\n    }\r\n    HdfsFileStatus fileStatus = mock(HdfsFileStatus.class);\r\n    when(fileStatus.getLocalNameInBytes()).thenReturn(filename.getBytes());\r\n    when(fileStatus.getPermission()).thenReturn(mock(FsPermission.class));\r\n    when(fileStatus.getOwner()).thenReturn(\"owner\");\r\n    when(fileStatus.getGroup()).thenReturn(\"group\");\r\n    if (type.equals(\"FILE\")) {\r\n        when(fileStatus.getLen()).thenReturn(100L);\r\n        when(fileStatus.getReplication()).thenReturn((short) 1);\r\n        when(fileStatus.getBlockSize()).thenReturn(HdfsClientConfigKeys.DFS_BLOCK_SIZE_DEFAULT);\r\n    } else if (type.equals(\"DIRECTORY\")) {\r\n        when(fileStatus.isDir()).thenReturn(true);\r\n        when(fileStatus.isDirectory()).thenReturn(true);\r\n    }\r\n    return fileStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getMockLocatedBlock",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "LocatedBlock getMockLocatedBlock(final String nsId)\n{\r\n    LocatedBlock lb = mock(LocatedBlock.class);\r\n    when(lb.getCachedLocations()).thenReturn(DatanodeInfo.EMPTY_ARRAY);\r\n    DatanodeID nodeId = new DatanodeID(\"localhost\", \"localhost\", \"dn0\", 1111, 1112, 1113, 1114);\r\n    DatanodeInfo dnInfo = new DatanodeDescriptor(nodeId);\r\n    DatanodeInfoWithStorage datanodeInfoWithStorage = new DatanodeInfoWithStorage(dnInfo, \"storageID\", StorageType.DEFAULT);\r\n    when(lb.getLocations()).thenReturn(new DatanodeInfoWithStorage[] { datanodeInfoWithStorage });\r\n    ExtendedBlock eb = mock(ExtendedBlock.class);\r\n    when(eb.getBlockPoolId()).thenReturn(nsId);\r\n    when(lb.getBlock()).thenReturn(eb);\r\n    @SuppressWarnings(\"unchecked\")\r\n    Token<BlockTokenIdentifier> tok = mock(Token.class);\r\n    when(tok.getIdentifier()).thenReturn(nsId.getBytes());\r\n    when(tok.getPassword()).thenReturn(nsId.getBytes());\r\n    when(tok.getKind()).thenReturn(new Text(nsId));\r\n    when(tok.getService()).thenReturn(new Text(nsId));\r\n    when(lb.getBlockToken()).thenReturn(tok);\r\n    return lb;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "registerSubclusters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void registerSubclusters(Router router, Collection<MockNamenode> namenodes) throws IOException\n{\r\n    registerSubclusters(singletonList(router), namenodes, emptySet());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "registerSubclusters",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void registerSubclusters(List<Router> routers, Collection<MockNamenode> namenodes, Set<String> unavailableSubclusters) throws IOException\n{\r\n    for (final Router router : routers) {\r\n        MembershipNamenodeResolver resolver = (MembershipNamenodeResolver) router.getNamenodeResolver();\r\n        for (final MockNamenode nn : namenodes) {\r\n            String nsId = nn.getNameserviceId();\r\n            String rpcAddress = \"localhost:\" + nn.getRPCPort();\r\n            String httpAddress = \"localhost:\" + nn.getHTTPPort();\r\n            String scheme = \"http\";\r\n            NamenodeStatusReport report = new NamenodeStatusReport(nsId, null, rpcAddress, rpcAddress, rpcAddress, scheme, httpAddress);\r\n            if (unavailableSubclusters.contains(nsId)) {\r\n                LOG.info(\"Register {} as UNAVAILABLE\", nsId);\r\n                report.setRegistrationValid(false);\r\n            } else {\r\n                LOG.info(\"Register {} as ACTIVE\", nsId);\r\n                report.setRegistrationValid(true);\r\n            }\r\n            report.setNamespaceInfo(new NamespaceInfo(0, nsId, nsId, 0));\r\n            resolver.registerNamenode(report);\r\n        }\r\n        resolver.loadCache(true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterWebHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterWebHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterWebHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "setupCluster",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void setupCluster() throws Exception\n{\r\n    curatorTestingServer = new TestingServer();\r\n    curatorTestingServer.start();\r\n    String connectString = curatorTestingServer.getConnectString();\r\n    curatorFramework = CuratorFrameworkFactory.builder().connectString(connectString).retryPolicy(new RetryNTimes(100, 100)).build();\r\n    curatorFramework.start();\r\n    Configuration conf = getStateStoreConfiguration(StateStoreZooKeeperImpl.class);\r\n    conf.set(CommonConfigurationKeys.ZK_ADDRESS, connectString);\r\n    conf.setLong(RBFConfigKeys.FEDERATION_STORE_CONNECTION_TEST_MS, TimeUnit.HOURS.toMillis(1));\r\n    baseZNode = conf.get(FEDERATION_STORE_ZK_PARENT_PATH, FEDERATION_STORE_ZK_PARENT_PATH_DEFAULT);\r\n    getStateStore(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "tearDownCluster",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDownCluster()\n{\r\n    curatorFramework.close();\r\n    try {\r\n        curatorTestingServer.stop();\r\n    } catch (IOException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "startup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void startup() throws IOException\n{\r\n    removeAll(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "generateFakeZNode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String generateFakeZNode(Class<T> recordClass) throws IOException\n{\r\n    String nodeName = StateStoreUtils.getRecordName(recordClass);\r\n    String primaryKey = \"test\";\r\n    if (nodeName != null) {\r\n        return baseZNode + \"/\" + nodeName + \"/\" + primaryKey;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testGetNullRecord",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetNullRecord(StateStoreDriver driver) throws Exception\n{\r\n    testGetNullRecord(driver, MembershipState.class);\r\n    testGetNullRecord(driver, MountTable.class);\r\n    testGetNullRecord(driver, RouterState.class);\r\n    testGetNullRecord(driver, DisabledNameservice.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testGetNullRecord",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetNullRecord(StateStoreDriver driver, Class<T> recordClass) throws Exception\n{\r\n    driver.removeAll(recordClass);\r\n    String znode = generateFakeZNode(recordClass);\r\n    assertNull(curatorFramework.checkExists().forPath(znode));\r\n    curatorFramework.create().withMode(CreateMode.PERSISTENT).withACL(null).forPath(znode, null);\r\n    assertNotNull(curatorFramework.checkExists().forPath(znode));\r\n    driver.get(recordClass);\r\n    assertNull(curatorFramework.checkExists().forPath(znode));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testGetNullRecord",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetNullRecord() throws Exception\n{\r\n    testGetNullRecord(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testInsert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testInsert() throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    testInsert(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testUpdate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUpdate() throws IllegalArgumentException, ReflectiveOperationException, IOException, SecurityException\n{\r\n    testPut(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testDelete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDelete() throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    testRemove(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testFetchErrors",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFetchErrors() throws IllegalArgumentException, IllegalAccessException, IOException\n{\r\n    testFetchErrors(getStateStoreDriver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "testLocalResolver",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testLocalResolver() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    Router router = mock(Router.class);\r\n    StateStoreService stateStore = mock(StateStoreService.class);\r\n    MembershipStore membership = mock(MembershipStore.class);\r\n    when(router.getStateStore()).thenReturn(stateStore);\r\n    when(stateStore.getRegisteredRecordStore(any(Class.class))).thenReturn(membership);\r\n    GetNamenodeRegistrationsResponse response = GetNamenodeRegistrationsResponse.newInstance();\r\n    List<MembershipState> records = new LinkedList<>();\r\n    records.add(newMembershipState(\"client0\", \"subcluster0\"));\r\n    records.add(newMembershipState(\"client1\", \"subcluster1\"));\r\n    records.add(newMembershipState(\"client2\", \"subcluster2\"));\r\n    response.setNamenodeMemberships(records);\r\n    when(membership.getNamenodeRegistrations(any(GetNamenodeRegistrationsRequest.class))).thenReturn(response);\r\n    StringBuilder sb = new StringBuilder(\"clientX\");\r\n    LocalResolver localResolver = new LocalResolver(conf, router);\r\n    LocalResolver spyLocalResolver = spy(localResolver);\r\n    doAnswer(new Answer<String>() {\r\n\r\n        @Override\r\n        public String answer(InvocationOnMock invocation) throws Throwable {\r\n            return sb.toString();\r\n        }\r\n    }).when(spyLocalResolver).getClientAddr();\r\n    MultipleDestinationMountTableResolver resolver = new MultipleDestinationMountTableResolver(conf, router);\r\n    resolver.addResolver(DestinationOrder.LOCAL, spyLocalResolver);\r\n    Map<String, String> mapLocal = new HashMap<>();\r\n    mapLocal.put(\"subcluster0\", \"/local\");\r\n    mapLocal.put(\"subcluster1\", \"/local\");\r\n    mapLocal.put(\"subcluster2\", \"/local\");\r\n    MountTable localEntry = MountTable.newInstance(\"/local\", mapLocal);\r\n    localEntry.setDestOrder(DestinationOrder.LOCAL);\r\n    resolver.addEntry(localEntry);\r\n    PathLocation dest = resolver.getDestinationForPath(\"/local/file0.txt\");\r\n    assertDestination(\"subcluster0\", dest);\r\n    setClient(sb, \"client2\");\r\n    dest = resolver.getDestinationForPath(\"/local/file0.txt\");\r\n    assertDestination(\"subcluster2\", dest);\r\n    setClient(sb, \"client1\");\r\n    dest = resolver.getDestinationForPath(\"/local/file0.txt\");\r\n    assertDestination(\"subcluster1\", dest);\r\n    setClient(sb, \"client0\");\r\n    dest = resolver.getDestinationForPath(\"/local/file0.txt\");\r\n    assertDestination(\"subcluster0\", dest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "assertDestination",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertDestination(String expectedNsId, PathLocation loc)\n{\r\n    List<RemoteLocation> dests = loc.getDestinations();\r\n    RemoteLocation dest = dests.get(0);\r\n    assertEquals(expectedNsId, dest.getNameserviceId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "newMembershipState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MembershipState newMembershipState(String addr, String nsId)\n{\r\n    return MembershipState.newInstance(\"routerId\", nsId, \"nn0\", \"cluster0\", \"blockPool0\", addr + \":8001\", addr + \":8002\", addr + \":8003\", \"http\", addr + \":8004\", FederationNamenodeServiceState.ACTIVE, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "setClient",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setClient(StringBuilder sb, String client)\n{\r\n    sb.replace(0, sb.length(), client);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "getRouterUserName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRouterUserName()\n{\r\n    return ROUTER_USER_NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "initSecurity",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "Configuration initSecurity() throws Exception\n{\r\n    baseDir = GenericTestUtils.getTestDir(SecurityConfUtil.class.getSimpleName());\r\n    FileUtil.fullyDelete(baseDir);\r\n    assertTrue(baseDir.mkdirs());\r\n    Properties kdcConf = MiniKdc.createConf();\r\n    kdc = new MiniKdc(kdcConf, baseDir);\r\n    kdc.start();\r\n    Configuration conf = new HdfsConfiguration();\r\n    SecurityUtil.setAuthenticationMethod(UserGroupInformation.AuthenticationMethod.KERBEROS, conf);\r\n    UserGroupInformation.setConfiguration(conf);\r\n    assertTrue(\"Expected configuration to enable security\", UserGroupInformation.isSecurityEnabled());\r\n    File keytabFile = new File(baseDir, \"test.keytab\");\r\n    String keytab = keytabFile.getAbsolutePath();\r\n    String krbInstance = Path.WINDOWS ? \"127.0.0.1\" : \"localhost\";\r\n    kdc.createPrincipal(keytabFile, SPNEGO_USER_NAME + \"/\" + krbInstance, ROUTER_USER_NAME + \"/\" + krbInstance);\r\n    routerPrincipal = ROUTER_USER_NAME + \"/\" + krbInstance + \"@\" + kdc.getRealm();\r\n    spnegoPrincipal = SPNEGO_USER_NAME + \"/\" + krbInstance + \"@\" + kdc.getRealm();\r\n    conf.set(DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, routerPrincipal);\r\n    conf.set(DFS_NAMENODE_KEYTAB_FILE_KEY, keytab);\r\n    conf.set(DFS_DATANODE_KERBEROS_PRINCIPAL_KEY, routerPrincipal);\r\n    conf.set(DFS_DATANODE_KEYTAB_FILE_KEY, keytab);\r\n    conf.set(PREFIX + \"type\", \"kerberos\");\r\n    conf.set(PREFIX + \"kerberos.principal\", spnegoPrincipal);\r\n    conf.set(PREFIX + \"kerberos.keytab\", keytab);\r\n    conf.set(DFS_NAMENODE_HTTPS_ADDRESS_KEY, \"localhost:0\");\r\n    conf.set(DFS_DATANODE_HTTPS_ADDRESS_KEY, \"localhost:0\");\r\n    conf.setBoolean(DFS_BLOCK_ACCESS_TOKEN_ENABLE_KEY, true);\r\n    conf.set(DFS_DATA_TRANSFER_PROTECTION_KEY, \"authentication\");\r\n    conf.set(DFS_HTTP_POLICY_KEY, HttpConfig.Policy.HTTPS_ONLY.name());\r\n    keystoresDir = baseDir.getAbsolutePath();\r\n    sslConfDir = KeyStoreTestUtil.getClasspathDir(SecurityConfUtil.class);\r\n    KeyStoreTestUtil.setupSSLConfig(keystoresDir, sslConfDir, conf, false);\r\n    conf.set(DFS_CLIENT_HTTPS_KEYSTORE_RESOURCE_KEY, KeyStoreTestUtil.getClientSSLConfigFileName());\r\n    conf.set(DFS_SERVER_HTTPS_KEYSTORE_RESOURCE_KEY, KeyStoreTestUtil.getServerSSLConfigFileName());\r\n    conf.set(DFS_ROUTER_KEYTAB_FILE_KEY, keytab);\r\n    conf.set(DFS_ROUTER_KERBEROS_PRINCIPAL_KEY, routerPrincipal);\r\n    conf.set(DFS_ROUTER_KERBEROS_INTERNAL_SPNEGO_PRINCIPAL_KEY, spnegoPrincipal);\r\n    conf.setClass(RBFConfigKeys.FEDERATION_STORE_DRIVER_CLASS, StateStoreFileImpl.class, StateStoreDriver.class);\r\n    conf.set(DFS_ROUTER_RPC_BIND_HOST_KEY, \"localhost\");\r\n    conf.set(DFS_ROUTER_DELEGATION_TOKEN_DRIVER_CLASS, MockDelegationTokenSecretManager.class.getName());\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void destroy() throws Exception\n{\r\n    if (kdc != null) {\r\n        kdc.stop();\r\n        FileUtil.fullyDelete(baseDir);\r\n        KeyStoreTestUtil.cleanupSSLConfig(keystoresDir, sslConfDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterWebHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterWebHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterWebHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterWebHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterWebHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterWebHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup()\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "setupCluster",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setupCluster(boolean fairnessEnable, boolean ha) throws Exception\n{\r\n    cluster = new StateStoreDFSCluster(ha, 2);\r\n    Configuration routerConf = new RouterConfigBuilder().stateStore().rpc().build();\r\n    if (fairnessEnable) {\r\n        routerConf.setClass(RBFConfigKeys.DFS_ROUTER_FAIRNESS_POLICY_CONTROLLER_CLASS, StaticRouterRpcFairnessPolicyController.class, RouterRpcFairnessPolicyController.class);\r\n    }\r\n    routerConf.setInt(RBFConfigKeys.DFS_ROUTER_HANDLER_COUNT_KEY, 3);\r\n    cluster.setNumDatanodesPerNameservice(0);\r\n    cluster.addRouterOverrides(routerConf);\r\n    cluster.startCluster();\r\n    cluster.startRouters();\r\n    cluster.waitClusterUp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "testFairnessControlOff",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testFairnessControlOff() throws Exception\n{\r\n    setupCluster(false, false);\r\n    startLoadTest(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "testFairnessControlOn",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testFairnessControlOn() throws Exception\n{\r\n    setupCluster(true, false);\r\n    startLoadTest(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "startLoadTest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void startLoadTest(boolean fairness) throws Exception\n{\r\n    startLoadTest(true, fairness);\r\n    startLoadTest(false, fairness);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "startLoadTest",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void startLoadTest(final boolean isConcurrent, final boolean fairness) throws Exception\n{\r\n    RouterContext routerContext = cluster.getRandomRouter();\r\n    URI address = routerContext.getFileSystemURI();\r\n    Configuration conf = new HdfsConfiguration();\r\n    final int numOps = 10;\r\n    AtomicInteger overloadException = new AtomicInteger();\r\n    if (fairness) {\r\n        if (isConcurrent) {\r\n            LOG.info(\"Taking fanout lock first\");\r\n            assertTrue(routerContext.getRouter().getRpcServer().getRPCClient().getRouterRpcFairnessPolicyController().acquirePermit(RouterRpcFairnessConstants.CONCURRENT_NS));\r\n        } else {\r\n            for (String ns : cluster.getNameservices()) {\r\n                LOG.info(\"Taking lock first for ns: {}\", ns);\r\n                assertTrue(routerContext.getRouter().getRpcServer().getRPCClient().getRouterRpcFairnessPolicyController().acquirePermit(ns));\r\n            }\r\n        }\r\n    }\r\n    int originalRejectedPermits = getTotalRejectedPermits(routerContext);\r\n    innerCalls(address, numOps, isConcurrent, conf, overloadException);\r\n    int latestRejectedPermits = getTotalRejectedPermits(routerContext);\r\n    assertEquals(latestRejectedPermits - originalRejectedPermits, overloadException.get());\r\n    if (fairness) {\r\n        assertTrue(overloadException.get() > 0);\r\n        if (isConcurrent) {\r\n            LOG.info(\"Release fanout lock that was taken before test\");\r\n            routerContext.getRouter().getRpcServer().getRPCClient().getRouterRpcFairnessPolicyController().releasePermit(RouterRpcFairnessConstants.CONCURRENT_NS);\r\n        } else {\r\n            for (String ns : cluster.getNameservices()) {\r\n                routerContext.getRouter().getRpcServer().getRPCClient().getRouterRpcFairnessPolicyController().releasePermit(ns);\r\n            }\r\n        }\r\n    } else {\r\n        assertEquals(\"Number of failed RPCs without fairness configured\", 0, overloadException.get());\r\n    }\r\n    int originalAcceptedPermits = getTotalAcceptedPermits(routerContext);\r\n    overloadException = new AtomicInteger();\r\n    innerCalls(address, numOps, isConcurrent, conf, overloadException);\r\n    int latestAcceptedPermits = getTotalAcceptedPermits(routerContext);\r\n    assertEquals(latestAcceptedPermits - originalAcceptedPermits, numOps);\r\n    assertEquals(overloadException.get(), 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "invokeSequential",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void invokeSequential(ClientProtocol routerProto) throws IOException\n{\r\n    routerProto.getFileInfo(\"/test.txt\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "invokeConcurrent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void invokeConcurrent(ClientProtocol routerProto, String clientName) throws IOException\n{\r\n    routerProto.renewLease(clientName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "getTotalRejectedPermits",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getTotalRejectedPermits(RouterContext routerContext)\n{\r\n    int totalRejectedPermits = 0;\r\n    for (String ns : cluster.getNameservices()) {\r\n        totalRejectedPermits += routerContext.getRouterRpcClient().getRejectedPermitForNs(ns);\r\n    }\r\n    totalRejectedPermits += routerContext.getRouterRpcClient().getRejectedPermitForNs(RouterRpcFairnessConstants.CONCURRENT_NS);\r\n    return totalRejectedPermits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "getTotalAcceptedPermits",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getTotalAcceptedPermits(RouterContext routerContext)\n{\r\n    int totalAcceptedPermits = 0;\r\n    for (String ns : cluster.getNameservices()) {\r\n        totalAcceptedPermits += routerContext.getRouterRpcClient().getAcceptedPermitForNs(ns);\r\n    }\r\n    totalAcceptedPermits += routerContext.getRouterRpcClient().getAcceptedPermitForNs(RouterRpcFairnessConstants.CONCURRENT_NS);\r\n    return totalAcceptedPermits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "innerCalls",
  "errType" : [ "RemoteException", "Throwable", "IOException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void innerCalls(URI address, int numOps, boolean isConcurrent, Configuration conf, AtomicInteger overloadException) throws IOException\n{\r\n    for (int i = 0; i < numOps; i++) {\r\n        DFSClient routerClient = null;\r\n        try {\r\n            routerClient = new DFSClient(address, conf);\r\n            String clientName = routerClient.getClientName();\r\n            ClientProtocol routerProto = routerClient.getNamenode();\r\n            if (isConcurrent) {\r\n                invokeConcurrent(routerProto, clientName);\r\n            } else {\r\n                invokeSequential(routerProto);\r\n            }\r\n        } catch (RemoteException re) {\r\n            IOException ioe = re.unwrapRemoteException();\r\n            assertTrue(\"Wrong exception: \" + ioe, ioe instanceof StandbyException);\r\n            assertExceptionContains(\"is overloaded for NS\", ioe);\r\n            overloadException.incrementAndGet();\r\n        } catch (Throwable e) {\r\n            throw e;\r\n        } finally {\r\n            if (routerClient != null) {\r\n                try {\r\n                    routerClient.close();\r\n                } catch (IOException e) {\r\n                    LOG.error(\"Cannot close the client\");\r\n                }\r\n            }\r\n        }\r\n        overloadException.get();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    conf = new Configuration();\r\n    connManager = new ConnectionManager(conf);\r\n    NetUtils.addStaticResolution(\"nn1\", \"localhost\");\r\n    NetUtils.createSocketAddrForHost(\"nn1\", 8080);\r\n    connManager.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void shutdown()\n{\r\n    if (connManager != null) {\r\n        connManager.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testCleanup",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testCleanup() throws Exception\n{\r\n    Map<ConnectionPoolId, ConnectionPool> poolMap = connManager.getPools();\r\n    ConnectionPool pool1 = new ConnectionPool(conf, TEST_NN_ADDRESS, TEST_USER1, 0, 10, 0.5f, ClientProtocol.class);\r\n    addConnectionsToPool(pool1, 9, 4);\r\n    poolMap.put(new ConnectionPoolId(TEST_USER1, TEST_NN_ADDRESS, ClientProtocol.class), pool1);\r\n    ConnectionPool pool2 = new ConnectionPool(conf, TEST_NN_ADDRESS, TEST_USER2, 0, 10, 0.5f, ClientProtocol.class);\r\n    addConnectionsToPool(pool2, 10, 10);\r\n    poolMap.put(new ConnectionPoolId(TEST_USER2, TEST_NN_ADDRESS, ClientProtocol.class), pool2);\r\n    checkPoolConnections(TEST_USER1, 9, 4);\r\n    checkPoolConnections(TEST_USER2, 10, 10);\r\n    connManager.cleanup(pool1);\r\n    checkPoolConnections(TEST_USER1, 8, 4);\r\n    checkPoolConnections(TEST_USER2, 10, 10);\r\n    connManager.cleanup(pool1);\r\n    checkPoolConnections(TEST_USER1, 8, 4);\r\n    checkPoolConnections(TEST_USER2, 10, 10);\r\n    ConnectionPool pool3 = new ConnectionPool(conf, TEST_NN_ADDRESS, TEST_USER3, 2, 10, 0.5f, ClientProtocol.class);\r\n    addConnectionsToPool(pool3, 8, 0);\r\n    poolMap.put(new ConnectionPoolId(TEST_USER3, TEST_NN_ADDRESS, ClientProtocol.class), pool3);\r\n    checkPoolConnections(TEST_USER3, 10, 0);\r\n    for (int i = 0; i < 10; i++) {\r\n        connManager.cleanup(pool3);\r\n    }\r\n    checkPoolConnections(TEST_USER3, 2, 0);\r\n    addConnectionsToPool(pool3, 8, 2);\r\n    checkPoolConnections(TEST_USER3, 10, 2);\r\n    for (int i = 0; i < 10; i++) {\r\n        connManager.cleanup(pool3);\r\n    }\r\n    checkPoolConnections(TEST_USER3, 4, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testConnectionCreatorWithException",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testConnectionCreatorWithException() throws Exception\n{\r\n    ConnectionPool badPool = new ConnectionPool(conf, UNRESOLVED_TEST_NN_ADDRESS, TEST_USER1, 0, 10, 0.5f, ClientProtocol.class);\r\n    BlockingQueue<ConnectionPool> queue = new ArrayBlockingQueue<>(1);\r\n    queue.add(badPool);\r\n    ConnectionManager.ConnectionCreator connectionCreator = new ConnectionManager.ConnectionCreator(queue);\r\n    connectionCreator.setDaemon(true);\r\n    connectionCreator.start();\r\n    GenericTestUtils.waitFor(() -> queue.isEmpty(), 50, 5000);\r\n    assertTrue(queue.isEmpty());\r\n    assertTrue(connectionCreator.isAlive());\r\n    connectionCreator.interrupt();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetConnectionWithException",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetConnectionWithException() throws Exception\n{\r\n    String exceptionCause = \"java.net.UnknownHostException: unknownhost\";\r\n    exceptionRule.expect(IllegalArgumentException.class);\r\n    exceptionRule.expectMessage(exceptionCause);\r\n    ConnectionPool badPool = new ConnectionPool(conf, UNRESOLVED_TEST_NN_ADDRESS, TEST_USER1, 1, 10, 0.5f, ClientProtocol.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetConnection",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGetConnection() throws Exception\n{\r\n    Map<ConnectionPoolId, ConnectionPool> poolMap = connManager.getPools();\r\n    final int totalConns = 10;\r\n    int activeConns = 5;\r\n    ConnectionPool pool = new ConnectionPool(conf, TEST_NN_ADDRESS, TEST_USER1, 0, 10, 0.5f, ClientProtocol.class);\r\n    addConnectionsToPool(pool, totalConns, activeConns);\r\n    poolMap.put(new ConnectionPoolId(TEST_USER1, TEST_NN_ADDRESS, ClientProtocol.class), pool);\r\n    final int remainingSlots = totalConns - activeConns;\r\n    for (int i = 0; i < remainingSlots; i++) {\r\n        ConnectionContext cc = pool.getConnection();\r\n        assertTrue(cc.isUsable());\r\n        cc.getClient();\r\n        activeConns++;\r\n    }\r\n    checkPoolConnections(TEST_USER1, totalConns, activeConns);\r\n    ConnectionContext cc = pool.getConnection();\r\n    assertTrue(cc.isActive());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testValidClientIndex",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testValidClientIndex() throws Exception\n{\r\n    ConnectionPool pool = new ConnectionPool(conf, TEST_NN_ADDRESS, TEST_USER1, 2, 2, 0.5f, ClientProtocol.class);\r\n    for (int i = -3; i <= 3; i++) {\r\n        pool.getClientIndex().set(i);\r\n        ConnectionContext conn = pool.getConnection();\r\n        assertNotNull(conn);\r\n        assertTrue(conn.isUsable());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getGetConnectionNamenodeProtocol",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void getGetConnectionNamenodeProtocol() throws Exception\n{\r\n    Map<ConnectionPoolId, ConnectionPool> poolMap = connManager.getPools();\r\n    final int totalConns = 10;\r\n    int activeConns = 5;\r\n    ConnectionPool pool = new ConnectionPool(conf, TEST_NN_ADDRESS, TEST_USER1, 0, 10, 0.5f, NamenodeProtocol.class);\r\n    addConnectionsToPool(pool, totalConns, activeConns);\r\n    poolMap.put(new ConnectionPoolId(TEST_USER1, TEST_NN_ADDRESS, NamenodeProtocol.class), pool);\r\n    final int remainingSlots = totalConns - activeConns;\r\n    for (int i = 0; i < remainingSlots; i++) {\r\n        ConnectionContext cc = pool.getConnection();\r\n        assertTrue(cc.isUsable());\r\n        cc.getClient();\r\n        activeConns++;\r\n    }\r\n    checkPoolConnections(TEST_USER1, totalConns, activeConns);\r\n    ConnectionContext cc = pool.getConnection();\r\n    assertTrue(cc.isActive());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addConnectionsToPool",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addConnectionsToPool(ConnectionPool pool, int numTotalConn, int numActiveConn) throws IOException\n{\r\n    for (int i = 0; i < numTotalConn; i++) {\r\n        ConnectionContext cc = pool.newConnection();\r\n        pool.addConnection(cc);\r\n        if (i < numActiveConn) {\r\n            cc.getClient();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkPoolConnections",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkPoolConnections(UserGroupInformation ugi, int numOfConns, int numOfActiveConns)\n{\r\n    boolean connPoolFoundForUser = false;\r\n    for (Map.Entry<ConnectionPoolId, ConnectionPool> e : connManager.getPools().entrySet()) {\r\n        if (e.getKey().getUgi() == ugi) {\r\n            assertEquals(numOfConns, e.getValue().getNumConnections());\r\n            assertEquals(numOfActiveConns, e.getValue().getNumActiveConnections());\r\n            assertEquals(numOfConns - numOfActiveConns, e.getValue().getNumIdleConnections());\r\n            connPoolFoundForUser = true;\r\n        }\r\n    }\r\n    if (!connPoolFoundForUser) {\r\n        fail(\"Connection pool not found for user \" + ugi.getUserName());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testConfigureConnectionActiveRatio",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testConfigureConnectionActiveRatio() throws IOException\n{\r\n    testConnectionCleanup(0.8f, 10, 7, 9);\r\n    testConnectionCleanup(0.8f, 10, 6, 8);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testConnectionCleanup",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testConnectionCleanup(float ratio, int totalConns, int activeConns, int leftConns) throws IOException\n{\r\n    Configuration tmpConf = new Configuration();\r\n    tmpConf.setFloat(RBFConfigKeys.DFS_ROUTER_NAMENODE_CONNECTION_MIN_ACTIVE_RATIO, ratio);\r\n    ConnectionManager tmpConnManager = new ConnectionManager(tmpConf);\r\n    tmpConnManager.start();\r\n    tmpConnManager.getConnection(TEST_USER1, TEST_NN_ADDRESS, NamenodeProtocol.class);\r\n    Map<ConnectionPoolId, ConnectionPool> poolMap = tmpConnManager.getPools();\r\n    ConnectionPoolId connectionPoolId = new ConnectionPoolId(TEST_USER1, TEST_NN_ADDRESS, NamenodeProtocol.class);\r\n    ConnectionPool pool = poolMap.get(connectionPoolId);\r\n    assertEquals(ratio, pool.getMinActiveRatio(), 0.001f);\r\n    pool.getConnection().getClient();\r\n    assertEquals(1, pool.getNumActiveConnections());\r\n    addConnectionsToPool(pool, totalConns - 1, activeConns - 1);\r\n    tmpConnManager.cleanup(pool);\r\n    assertEquals(leftConns, pool.getNumConnections());\r\n    tmpConnManager.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testUnsupportedProtoExceptionMsg",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUnsupportedProtoExceptionMsg() throws Exception\n{\r\n    LambdaTestUtils.intercept(IllegalStateException.class, \"Unsupported protocol for connection to NameNode: \" + TestConnectionManager.class.getName(), () -> ConnectionPool.newConnection(conf, TEST_NN_ADDRESS, TEST_USER1, TestConnectionManager.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws Exception\n{\r\n    RouterHDFSContract.createCluster(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "globalSetUp",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void globalSetUp() throws Exception\n{\r\n    cluster = new StateStoreDFSCluster(false, 2);\r\n    Configuration conf = new RouterConfigBuilder().stateStore().admin().rpc().http().build();\r\n    conf.set(FS_TRASH_INTERVAL_KEY, \"100\");\r\n    cluster.addRouterOverrides(conf);\r\n    cluster.startCluster();\r\n    cluster.startRouters();\r\n    cluster.waitClusterUp();\r\n    ns0 = cluster.getNameservices().get(0);\r\n    ns1 = cluster.getNameservices().get(1);\r\n    routerContext = cluster.getRandomRouter();\r\n    routerFs = routerContext.getFileSystem();\r\n    nnContext = cluster.getNamenode(ns0, null);\r\n    nnFs = nnContext.getFileSystem();\r\n    Router router = routerContext.getRouter();\r\n    mountTable = (MountTableResolver) router.getSubclusterResolver();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (cluster != null) {\r\n        cluster.stopRouter(routerContext);\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "clearMountTable",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void clearMountTable() throws IOException\n{\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTableManager = client.getMountTableManager();\r\n    GetMountTableEntriesRequest req1 = GetMountTableEntriesRequest.newInstance(\"/\");\r\n    GetMountTableEntriesResponse response = mountTableManager.getMountTableEntries(req1);\r\n    for (MountTable entry : response.getEntries()) {\r\n        RemoveMountTableEntryRequest req2 = RemoveMountTableEntryRequest.newInstance(entry.getSourcePath());\r\n        mountTableManager.removeMountTableEntry(req2);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "clearFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void clearFile() throws IOException\n{\r\n    FileStatus[] fileStatuses = nnFs.listStatus(new Path(\"/\"));\r\n    for (FileStatus file : fileStatuses) {\r\n        nnFs.delete(file.getPath(), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addMountTable",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean addMountTable(final MountTable entry) throws IOException\n{\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTableManager = client.getMountTableManager();\r\n    AddMountTableEntryRequest addRequest = AddMountTableEntryRequest.newInstance(entry);\r\n    AddMountTableEntryResponse addResponse = mountTableManager.addMountTableEntry(addRequest);\r\n    mountTable.loadCache(true);\r\n    return addResponse.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMoveToTrashNoMountPoint",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testMoveToTrashNoMountPoint() throws IOException, URISyntaxException, InterruptedException\n{\r\n    MountTable addEntry = MountTable.newInstance(MOUNT_POINT, Collections.singletonMap(ns0, MOUNT_POINT));\r\n    assertTrue(addMountTable(addEntry));\r\n    DFSClient client = nnContext.getClient();\r\n    client.setOwner(\"/\", TEST_USER, TEST_USER);\r\n    UserGroupInformation ugi = UserGroupInformation.createRemoteUser(TEST_USER);\r\n    client = nnContext.getClient(ugi);\r\n    client.mkdirs(MOUNT_POINT, new FsPermission(\"777\"), true);\r\n    assertTrue(client.exists(MOUNT_POINT));\r\n    client.create(FILE, true);\r\n    Path filePath = new Path(FILE);\r\n    FileStatus[] fileStatuses = routerFs.listStatus(filePath);\r\n    assertEquals(1, fileStatuses.length);\r\n    assertEquals(TEST_USER, fileStatuses[0].getOwner());\r\n    Configuration routerConf = routerContext.getConf();\r\n    FileSystem fs = DFSTestUtil.getFileSystemAs(ugi, routerConf);\r\n    Trash trash = new Trash(fs, routerConf);\r\n    assertTrue(trash.moveToTrash(filePath));\r\n    fileStatuses = nnFs.listStatus(new Path(TRASH_ROOT + CURRENT + MOUNT_POINT));\r\n    assertEquals(1, fileStatuses.length);\r\n    assertTrue(nnFs.exists(new Path(TRASH_ROOT + CURRENT + FILE)));\r\n    assertTrue(nnFs.exists(new Path(\"/user/\" + TEST_USER + \"/.Trash/Current\" + FILE)));\r\n    client.create(FILE, true);\r\n    filePath = new Path(FILE);\r\n    fileStatuses = routerFs.listStatus(filePath);\r\n    assertEquals(1, fileStatuses.length);\r\n    assertTrue(trash.moveToTrash(filePath));\r\n    fileStatuses = routerFs.listStatus(new Path(TRASH_ROOT + CURRENT + MOUNT_POINT));\r\n    assertEquals(2, fileStatuses.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testDeleteToTrashExistMountPoint",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testDeleteToTrashExistMountPoint() throws IOException, URISyntaxException, InterruptedException\n{\r\n    MountTable addEntry = MountTable.newInstance(MOUNT_POINT, Collections.singletonMap(ns0, MOUNT_POINT));\r\n    assertTrue(addMountTable(addEntry));\r\n    addEntry = MountTable.newInstance(TRASH_ROOT, Collections.singletonMap(ns1, TRASH_ROOT));\r\n    assertTrue(addMountTable(addEntry));\r\n    DFSClient client = nnContext.getClient();\r\n    client.setOwner(\"/\", TEST_USER, TEST_USER);\r\n    UserGroupInformation ugi = UserGroupInformation.createRemoteUser(TEST_USER);\r\n    client = nnContext.getClient(ugi);\r\n    client.mkdirs(MOUNT_POINT, new FsPermission(\"777\"), true);\r\n    assertTrue(client.exists(MOUNT_POINT));\r\n    client.create(FILE, true);\r\n    Path filePath = new Path(FILE);\r\n    FileStatus[] fileStatuses = routerFs.listStatus(filePath);\r\n    assertEquals(1, fileStatuses.length);\r\n    assertEquals(TEST_USER, fileStatuses[0].getOwner());\r\n    Configuration routerConf = routerContext.getConf();\r\n    FileSystem fs = DFSTestUtil.getFileSystemAs(ugi, routerConf);\r\n    Trash trash = new Trash(fs, routerConf);\r\n    assertTrue(trash.moveToTrash(filePath));\r\n    fileStatuses = nnFs.listStatus(new Path(TRASH_ROOT + CURRENT + MOUNT_POINT));\r\n    assertEquals(1, fileStatuses.length);\r\n    assertTrue(nnFs.exists(new Path(TRASH_ROOT + CURRENT + FILE)));\r\n    client.create(FILE, true);\r\n    filePath = new Path(FILE);\r\n    fileStatuses = nnFs.listStatus(filePath);\r\n    assertEquals(1, fileStatuses.length);\r\n    assertTrue(trash.moveToTrash(filePath));\r\n    fileStatuses = nnFs.listStatus(new Path(TRASH_ROOT + CURRENT + MOUNT_POINT));\r\n    assertEquals(2, fileStatuses.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testIsTrashPath",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testIsTrashPath() throws IOException\n{\r\n    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n    assertNotNull(ugi);\r\n    assertTrue(MountTableResolver.isTrashPath(\"/user/\" + ugi.getUserName() + \"/.Trash/Current\" + MOUNT_POINT));\r\n    assertTrue(MountTableResolver.isTrashPath(\"/user/\" + ugi.getUserName() + \"/.Trash/\" + Time.now() + MOUNT_POINT));\r\n    assertFalse(MountTableResolver.isTrashPath(MOUNT_POINT));\r\n    assertFalse(MountTableResolver.isTrashPath(\"/home/user/\" + ugi.getUserName() + \"/.Trash/Current\" + MOUNT_POINT));\r\n    assertFalse(MountTableResolver.isTrashPath(\"/home/user/\" + ugi.getUserName() + \"/.Trash/\" + Time.now() + MOUNT_POINT));\r\n    assertFalse(MountTableResolver.isTrashPath(\"\"));\r\n    assertFalse(MountTableResolver.isTrashPath(\"/home/user/empty.Trash/Current\"));\r\n    assertFalse(MountTableResolver.isTrashPath(\"/home/user/.Trash\"));\r\n    assertFalse(MountTableResolver.isTrashPath(\"/.Trash/Current\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSubtractTrashCurrentPath",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSubtractTrashCurrentPath() throws IOException\n{\r\n    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n    assertNotNull(ugi);\r\n    assertEquals(MOUNT_POINT, MountTableResolver.subtractTrashCurrentPath(\"/user/\" + ugi.getUserName() + \"/.Trash/Current\" + MOUNT_POINT));\r\n    assertEquals(MOUNT_POINT, MountTableResolver.subtractTrashCurrentPath(\"/user/\" + ugi.getUserName() + \"/.Trash/\" + Time.now() + MOUNT_POINT));\r\n    assertEquals(\"/home/user/\" + ugi.getUserName() + \"/.Trash/Current\" + MOUNT_POINT, MountTableResolver.subtractTrashCurrentPath(\"/home/user/\" + ugi.getUserName() + \"/.Trash/Current\" + MOUNT_POINT));\r\n    long time = Time.now();\r\n    assertEquals(\"/home/user/\" + ugi.getUserName() + \"/.Trash/\" + time + MOUNT_POINT, MountTableResolver.subtractTrashCurrentPath(\"/home/user/\" + ugi.getUserName() + \"/.Trash/\" + time + MOUNT_POINT));\r\n    assertEquals(\"\", MountTableResolver.subtractTrashCurrentPath(\"\"));\r\n    assertEquals(\"/home/user/empty.Trash/Current\", MountTableResolver.subtractTrashCurrentPath(\"/home/user/empty.Trash/Current\"));\r\n    assertEquals(\"/home/user/.Trash\", MountTableResolver.subtractTrashCurrentPath(\"/home/user/.Trash\"));\r\n    assertEquals(\"/.Trash/Current\", MountTableResolver.subtractTrashCurrentPath(\"/.Trash/Current\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws Exception\n{\r\n    RouterHDFSContract.createCluster(false, 1, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "testRouterDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testRouterDelegationToken() throws Exception\n{\r\n    RouterMBean bean = FederationTestUtils.getBean(ROUTER_BEAN, RouterMBean.class);\r\n    assertEquals(0, bean.getCurrentTokensCount());\r\n    Token<DelegationTokenIdentifier> token = (Token<DelegationTokenIdentifier>) getFileSystem().getDelegationToken(\"router\");\r\n    assertNotNull(token);\r\n    assertEquals(\"HDFS_DELEGATION_TOKEN\", token.getKind().toString());\r\n    DelegationTokenIdentifier identifier = token.decodeIdentifier();\r\n    assertNotNull(identifier);\r\n    String owner = identifier.getOwner().toString();\r\n    String host = Path.WINDOWS ? \"127.0.0.1\" : \"localhost\";\r\n    String expectedOwner = \"router/\" + host + \"@EXAMPLE.COM\";\r\n    assertEquals(expectedOwner, owner);\r\n    assertEquals(\"router\", identifier.getRenewer().toString());\r\n    int masterKeyId = identifier.getMasterKeyId();\r\n    assertTrue(masterKeyId > 0);\r\n    int sequenceNumber = identifier.getSequenceNumber();\r\n    assertTrue(sequenceNumber > 0);\r\n    long existingMaxTime = token.decodeIdentifier().getMaxDate();\r\n    assertTrue(identifier.getMaxDate() >= identifier.getIssueDate());\r\n    assertEquals(1, bean.getCurrentTokensCount());\r\n    long expiryTime = token.renew(initSecurity());\r\n    assertNotNull(token);\r\n    assertEquals(existingMaxTime, token.decodeIdentifier().getMaxDate());\r\n    assertTrue(expiryTime <= existingMaxTime);\r\n    identifier = token.decodeIdentifier();\r\n    assertEquals(identifier.getMasterKeyId(), masterKeyId);\r\n    assertEquals(identifier.getSequenceNumber(), sequenceNumber);\r\n    assertEquals(1, bean.getCurrentTokensCount());\r\n    token.cancel(initSecurity());\r\n    assertEquals(0, bean.getCurrentTokensCount());\r\n    exceptionRule.expect(SecretManager.InvalidToken.class);\r\n    token.renew(initSecurity());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    Configuration conf = SecurityConfUtil.initSecurity();\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_HTTP_ADDRESS_KEY, \"0.0.0.0:0\");\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_HTTPS_ADDRESS_KEY, \"0.0.0.0:0\");\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_RPC_ADDRESS_KEY, \"0.0.0.0:0\");\r\n    conf.set(FILTER_INITIALIZER_PROPERTY, NoAuthFilterInitializer.class.getName());\r\n    conf.set(HADOOP_HTTP_AUTHENTICATION_TYPE, \"simple\");\r\n    Configuration routerConf = new RouterConfigBuilder().rpc().http().build();\r\n    conf.addResource(routerConf);\r\n    router = new Router();\r\n    router.init(conf);\r\n    router.start();\r\n    InetSocketAddress webAddress = router.getHttpServerAddress();\r\n    URI webURI = new URI(SWebHdfs.SCHEME, null, webAddress.getHostName(), webAddress.getPort(), null, null, null);\r\n    fs = (WebHdfsFileSystem) FileSystem.get(webURI, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanup() throws Exception\n{\r\n    if (router != null) {\r\n        router.stop();\r\n        router.close();\r\n    }\r\n    SecurityConfUtil.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "testGetDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testGetDelegationToken() throws Exception\n{\r\n    final String renewer = \"renewer0\";\r\n    Token<?> token = getDelegationToken(fs, renewer);\r\n    assertNotNull(token);\r\n    DelegationTokenIdentifier tokenId = getTokenIdentifier(token.getIdentifier());\r\n    assertEquals(\"router\", tokenId.getOwner().toString());\r\n    assertEquals(renewer, tokenId.getRenewer().toString());\r\n    assertEquals(\"\", tokenId.getRealUser().toString());\r\n    assertEquals(\"SWEBHDFS delegation\", token.getKind().toString());\r\n    assertNotNull(token.getPassword());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "testRenewDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testRenewDelegationToken() throws Exception\n{\r\n    Token<?> token = getDelegationToken(fs, \"router\");\r\n    DelegationTokenIdentifier tokenId = getTokenIdentifier(token.getIdentifier());\r\n    long t = renewDelegationToken(fs, token);\r\n    assertTrue(t + \" should not be larger than \" + tokenId.getMaxDate(), t <= tokenId.getMaxDate());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "testCancelDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCancelDelegationToken() throws Exception\n{\r\n    Token<?> token = getDelegationToken(fs, \"router\");\r\n    cancelDelegationToken(fs, token);\r\n    LambdaTestUtils.intercept(IOException.class, \"Server returned HTTP response code: 403 \", () -> renewDelegationToken(fs, token));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> getDelegationToken(WebHdfsFileSystem webHdfs, String renewer) throws IOException\n{\r\n    Map<?, ?> json = sendHttpRequest(webHdfs, GetOpParam.Op.GETDELEGATIONTOKEN, new RenewerParam(renewer));\r\n    return WebHdfsTestUtil.convertJsonToDelegationToken(json);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "renewDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long renewDelegationToken(WebHdfsFileSystem webHdfs, Token<?> token) throws IOException\n{\r\n    Map<?, ?> json = sendHttpRequest(webHdfs, PutOpParam.Op.RENEWDELEGATIONTOKEN, new TokenArgumentParam(token.encodeToUrlString()));\r\n    return ((Number) json.get(\"long\")).longValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "cancelDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cancelDelegationToken(WebHdfsFileSystem webHdfs, Token<?> token) throws IOException\n{\r\n    sendHttpRequest(webHdfs, PutOpParam.Op.CANCELDELEGATIONTOKEN, new TokenArgumentParam(token.encodeToUrlString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "sendHttpRequest",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Map<?, ?> sendHttpRequest(WebHdfsFileSystem webHdfs, final HttpOpParam.Op op, final Param<?, ?>... parameters) throws IOException\n{\r\n    String user = SecurityConfUtil.getRouterUserName();\r\n    List<Param<?, ?>> pList = new ArrayList<>();\r\n    pList.add(new UserParam(user));\r\n    pList.addAll(Arrays.asList(parameters));\r\n    final URL url = WebHdfsTestUtil.toUrl(webHdfs, op, null, pList.toArray(new Param<?, ?>[pList.size()]));\r\n    HttpURLConnection conn = WebHdfsTestUtil.openConnection(url, webHdfs.getConf());\r\n    conn.setRequestMethod(op.getType().toString());\r\n    WebHdfsTestUtil.sendRequest(conn);\r\n    final Map<?, ?> json = WebHdfsTestUtil.getAndParseResponse(conn);\r\n    conn.disconnect();\r\n    return json;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "getTokenIdentifier",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DelegationTokenIdentifier getTokenIdentifier(byte[] id) throws IOException\n{\r\n    DelegationTokenIdentifier identifier = new DelegationTokenIdentifier();\r\n    ByteArrayInputStream bais = new ByteArrayInputStream(id);\r\n    DataInputStream dais = new DataInputStream(bais);\r\n    identifier.readFields(dais);\r\n    return identifier;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSetup",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testSetup() throws Exception\n{\r\n    MiniRouterDFSCluster cluster = getCluster();\r\n    getCluster().installMockLocations();\r\n    List<RouterContext> routers = cluster.getRouters();\r\n    for (RouterContext rc : routers) {\r\n        Router router = rc.getRouter();\r\n        MockResolver resolver = (MockResolver) router.getSubclusterResolver();\r\n        resolver.addLocation(\"/\", cluster.getNameservices().get(1), \"/\");\r\n    }\r\n    for (RouterContext rc : routers) {\r\n        Router router = rc.getRouter();\r\n        MockResolver resolver = (MockResolver) router.getSubclusterResolver();\r\n        List<String> nss = cluster.getNameservices();\r\n        String ns0 = nss.get(0);\r\n        resolver.addLocation(\"/same\", ns0, \"/\");\r\n        resolver.addLocation(\"/same\", ns0, cluster.getNamenodePathForNS(ns0));\r\n    }\r\n    cluster.deleteAllFiles();\r\n    cluster.createTestDirectoriesNamenode();\r\n    Thread.sleep(100);\r\n    RouterContext router = cluster.getRandomRouter();\r\n    this.setRouter(router);\r\n    String ns = cluster.getRandomNameservice();\r\n    this.setNs(ns);\r\n    this.setNamenode(cluster.getNamenode(ns, null));\r\n    Random r = new Random();\r\n    String randomString = \"testfile-\" + r.nextInt();\r\n    setNamenodeFile(\"/\" + randomString);\r\n    setRouterFile(\"/\" + randomString);\r\n    FileSystem nnFs = getNamenodeFileSystem();\r\n    FileSystem routerFs = getRouterFileSystem();\r\n    createFile(nnFs, getNamenodeFile(), 32);\r\n    verifyFileExists(nnFs, getNamenodeFile());\r\n    verifyFileExists(routerFs, getRouterFile());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testListing",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testListing(String path) throws IOException\n{\r\n    Set<String> requiredPaths = new TreeSet<>();\r\n    RouterContext rc = getRouterContext();\r\n    Router router = rc.getRouter();\r\n    FileSubclusterResolver subclusterResolver = router.getSubclusterResolver();\r\n    List<String> mountList = subclusterResolver.getMountPoints(path);\r\n    if (mountList != null) {\r\n        requiredPaths.addAll(mountList);\r\n    }\r\n    PathLocation location = subclusterResolver.getDestinationForPath(path);\r\n    for (RemoteLocation loc : location.getDestinations()) {\r\n        String nsId = loc.getNameserviceId();\r\n        String dest = loc.getDest();\r\n        NamenodeContext nn = getCluster().getNamenode(nsId, null);\r\n        FileSystem fs = nn.getFileSystem();\r\n        FileStatus[] files = fs.listStatus(new Path(dest));\r\n        for (FileStatus file : files) {\r\n            String pathName = file.getPath().getName();\r\n            requiredPaths.add(pathName);\r\n        }\r\n    }\r\n    DirectoryListing listing = getRouterProtocol().getListing(path, HdfsFileStatus.EMPTY_NAME, false);\r\n    Iterator<String> requiredPathsIterator = requiredPaths.iterator();\r\n    HdfsFileStatus[] partialListing = listing.getPartialListing();\r\n    for (HdfsFileStatus fileStatus : listing.getPartialListing()) {\r\n        String fileName = requiredPathsIterator.next();\r\n        String currentFile = fileStatus.getFullPath(new Path(path)).getName();\r\n        assertEquals(currentFile, fileName);\r\n    }\r\n    assertEquals(requiredPaths + \" doesn't match \" + Arrays.toString(partialListing), requiredPaths.size(), partialListing.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyOpWithRemoteException",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testProxyOpWithRemoteException() throws IOException\n{\r\n    final String testPath = \"/proxy_op/remote_exception.txt\";\r\n    final FederationRPCMetrics metrics = getRouterContext().getRouter().getRpcServer().getRPCMetrics();\r\n    String ns1 = getCluster().getNameservices().get(1);\r\n    final FileSystem fileSystem1 = getCluster().getNamenode(ns1, null).getFileSystem();\r\n    try {\r\n        createFile(fileSystem1, testPath, 32);\r\n        long beforeProxyOp = metrics.getProxyOps();\r\n        getRouterProtocol().getBlockLocations(testPath, 0, 1);\r\n        assertEquals(2, metrics.getProxyOps() - beforeProxyOp);\r\n    } finally {\r\n        fileSystem1.delete(new Path(testPath), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyListFiles",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testProxyListFiles() throws IOException, InterruptedException, URISyntaxException, NoSuchMethodException, SecurityException\n{\r\n    testListing(\"/\");\r\n    testListing(\"/same\");\r\n    ClientProtocol namenodeProtocol = getCluster().getRandomNamenode().getClient().getNamenode();\r\n    Method m = ClientProtocol.class.getMethod(\"getListing\", String.class, byte[].class, boolean.class);\r\n    String badPath = \"/unknownlocation/unknowndir\";\r\n    compareResponses(getRouterProtocol(), namenodeProtocol, m, new Object[] { badPath, HdfsFileStatus.EMPTY_NAME, false });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxyRenameFiles",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testProxyRenameFiles() throws IOException, InterruptedException\n{\r\n    super.testProxyRenameFiles();\r\n    List<String> nss = getCluster().getNameservices();\r\n    String ns0 = nss.get(0);\r\n    String ns1 = nss.get(1);\r\n    String testDir0 = getCluster().getFederatedTestDirectoryForNS(ns0);\r\n    String filename0 = testDir0 + \"/testrename\";\r\n    String renamedFile = \"/testrename\";\r\n    testRename(getRouterContext(), filename0, renamedFile, false);\r\n    testRename2(getRouterContext(), filename0, renamedFile, false);\r\n    String testDir1 = getCluster().getFederatedTestDirectoryForNS(ns1);\r\n    String filename1 = testDir1 + \"/testrename\";\r\n    testRename(getRouterContext(), filename1, renamedFile, false);\r\n    testRename2(getRouterContext(), filename1, renamedFile, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testPreviousBlockNotNull",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testPreviousBlockNotNull() throws IOException, URISyntaxException\n{\r\n    final FederationRPCMetrics metrics = getRouterContext().getRouter().getRpcServer().getRPCMetrics();\r\n    final ClientProtocol clientProtocol = getRouterProtocol();\r\n    final EnumSet<CreateFlag> createFlag = EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE);\r\n    final String clientName = getRouterContext().getClient().getClientName();\r\n    final String testPath = \"/getAdditionalData/test.txt\";\r\n    final String ns1 = getCluster().getNameservices().get(1);\r\n    final FileSystem fileSystem1 = getCluster().getNamenode(ns1, null).getFileSystem();\r\n    try {\r\n        createFile(fileSystem1, testPath, 32);\r\n        HdfsFileStatus status = clientProtocol.create(testPath, new FsPermission(\"777\"), clientName, new EnumSetWritable<>(createFlag), true, (short) 1, (long) 1024, CryptoProtocolVersion.supported(), null, null);\r\n        long proxyNumCreate = metrics.getProcessingOps();\r\n        LocatedBlock blockOne = clientProtocol.addBlock(testPath, clientName, null, null, status.getFileId(), null, null);\r\n        assertNotNull(blockOne);\r\n        long proxyNumAddBlock = metrics.getProcessingOps();\r\n        assertEquals(2, proxyNumAddBlock - proxyNumCreate);\r\n        LocatedBlock blockTwo = clientProtocol.addBlock(testPath, clientName, blockOne.getBlock(), null, status.getFileId(), null, null);\r\n        assertNotNull(blockTwo);\r\n        long proxyNumAddBlock2 = metrics.getProcessingOps();\r\n        assertEquals(1, proxyNumAddBlock2 - proxyNumAddBlock);\r\n        DatanodeInfo[] exclusions = DatanodeInfo.EMPTY_ARRAY;\r\n        LocatedBlock newBlock = clientProtocol.getAdditionalDatanode(testPath, status.getFileId(), blockTwo.getBlock(), blockTwo.getLocations(), blockTwo.getStorageIDs(), exclusions, 1, clientName);\r\n        assertNotNull(newBlock);\r\n        long proxyNumAdditionalDatanode = metrics.getProcessingOps();\r\n        assertEquals(1, proxyNumAdditionalDatanode - proxyNumAddBlock2);\r\n        clientProtocol.complete(testPath, clientName, newBlock.getBlock(), status.getFileId());\r\n        long proxyNumComplete = metrics.getProcessingOps();\r\n        assertEquals(1, proxyNumComplete - proxyNumAdditionalDatanode);\r\n    } finally {\r\n        clientProtocol.delete(testPath, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRecoverLease",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRecoverLease() throws Exception\n{\r\n    Path testPath = new Path(\"/recovery/test_recovery_lease\");\r\n    DistributedFileSystem routerFs = (DistributedFileSystem) getRouterFileSystem();\r\n    FSDataOutputStream fsDataOutputStream = null;\r\n    try {\r\n        fsDataOutputStream = routerFs.create(testPath);\r\n        fsDataOutputStream.write(\"hello world\".getBytes());\r\n        fsDataOutputStream.hflush();\r\n        boolean result = routerFs.recoverLease(testPath);\r\n        assertFalse(result);\r\n    } finally {\r\n        IOUtils.closeStream(fsDataOutputStream);\r\n        routerFs.delete(testPath, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testIsFileClosed",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testIsFileClosed() throws Exception\n{\r\n    Path testPath = new Path(\"/is_file_closed.txt\");\r\n    DistributedFileSystem routerFs = (DistributedFileSystem) getRouterFileSystem();\r\n    FSDataOutputStream fsDataOutputStream = null;\r\n    try {\r\n        fsDataOutputStream = routerFs.create(testPath);\r\n        fsDataOutputStream.write(\"hello world\".getBytes());\r\n        fsDataOutputStream.hflush();\r\n        boolean result = routerFs.isFileClosed(testPath);\r\n        assertFalse(result);\r\n    } finally {\r\n        IOUtils.closeStream(fsDataOutputStream);\r\n        routerFs.delete(testPath, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetContentSummaryEc",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetContentSummaryEc() throws Exception\n{\r\n    DistributedFileSystem routerDFS = (DistributedFileSystem) getRouterFileSystem();\r\n    Path dir = new Path(\"/\");\r\n    String expectedECPolicy = \"RS-6-3-1024k\";\r\n    try {\r\n        routerDFS.setErasureCodingPolicy(dir, expectedECPolicy);\r\n        assertEquals(expectedECPolicy, routerDFS.getContentSummary(dir).getErasureCodingPolicy());\r\n    } finally {\r\n        routerDFS.unsetErasureCodingPolicy(dir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSubclusterDown",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testSubclusterDown() throws Exception\n{\r\n    final int totalFiles = 6;\r\n    List<RouterContext> routers = getCluster().getRouters();\r\n    FileSystem fs = getRouterFileSystem();\r\n    FileStatus[] files = fs.listStatus(new Path(\"/\"));\r\n    assertEquals(totalFiles, files.length);\r\n    NameNode nn0 = getCluster().getNamenode(\"ns0\", null).getNamenode();\r\n    FSNamesystem ns0 = nn0.getNamesystem();\r\n    HAContext nn0haCtx = (HAContext) getInternalState(ns0, \"haContext\");\r\n    HAContext mockCtx = mock(HAContext.class);\r\n    doThrow(new StandbyException(\"Mock\")).when(mockCtx).checkOperation(any());\r\n    setInternalState(ns0, \"haContext\", mockCtx);\r\n    RouterContext router0 = routers.get(0);\r\n    RouterRpcServer router0RPCServer = router0.getRouter().getRpcServer();\r\n    RouterClientProtocol router0ClientProtocol = router0RPCServer.getClientProtocolModule();\r\n    setInternalState(router0ClientProtocol, \"allowPartialList\", false);\r\n    try {\r\n        router0.getFileSystem().listStatus(new Path(\"/\"));\r\n        fail(\"I should throw an exception\");\r\n    } catch (RemoteException re) {\r\n        GenericTestUtils.assertExceptionContains(\"No namenode available to invoke getListing\", re);\r\n    }\r\n    RouterContext router1 = routers.get(1);\r\n    files = router1.getFileSystem().listStatus(new Path(\"/\"));\r\n    assertTrue(\"Found \" + files.length + \" items, we should have less\", files.length < totalFiles);\r\n    setInternalState(ns0, \"haContext\", nn0haCtx);\r\n    setInternalState(router0ClientProtocol, \"allowPartialList\", true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testCallerContextWithMultiDestinations",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testCallerContextWithMultiDestinations() throws IOException\n{\r\n    GenericTestUtils.LogCapturer auditLog = GenericTestUtils.LogCapturer.captureLogs(FSNamesystem.auditLog);\r\n    CallerContext.setCurrent(new CallerContext.Builder(\"clientContext\").build());\r\n    assertEquals(\"clientContext\", CallerContext.getCurrent().getContext());\r\n    DistributedFileSystem routerFs = (DistributedFileSystem) getRouterFileSystem();\r\n    Path dirPath = new Path(\"/test_caller_context_with_multi_destinations\");\r\n    routerFs.mkdirs(dirPath);\r\n    routerFs.listStatus(dirPath);\r\n    routerFs.getFileStatus(dirPath);\r\n    String auditFlag = \"src=\" + dirPath.toString();\r\n    String clientIpInfo = \"clientIp:\" + InetAddress.getLocalHost().getHostAddress();\r\n    for (String line : auditLog.getOutput().split(\"\\n\")) {\r\n        if (line.contains(auditFlag)) {\r\n            String callerContext = line.substring(line.indexOf(\"callerContext=\"));\r\n            assertTrue(callerContext.contains(\"clientContext\"));\r\n            assertTrue(callerContext.contains(clientIpInfo));\r\n            assertEquals(callerContext.indexOf(clientIpInfo), callerContext.lastIndexOf(clientIpInfo));\r\n        }\r\n    }\r\n    CallerContext.setCurrent(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "globalSetUp",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void globalSetUp() throws Exception\n{\r\n    Configuration namenodeConf = new Configuration();\r\n    namenodeConf.setBoolean(DFSConfigKeys.HADOOP_CALLER_CONTEXT_ENABLED_KEY, true);\r\n    namenodeConf.set(CommonConfigurationKeys.HADOOP_SECURITY_GROUP_MAPPING, TestRouterFederationRename.MockGroupsMapping.class.getName());\r\n    cluster = new MiniRouterDFSCluster(false, NUM_SUBCLUSTERS);\r\n    cluster.setNumDatanodesPerNameservice(NUM_DNS);\r\n    cluster.addNamenodeOverrides(namenodeConf);\r\n    cluster.setIndependentDNs();\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(DFSConfigKeys.DFS_LIST_LIMIT, 5);\r\n    cluster.addNamenodeOverrides(conf);\r\n    cluster.startCluster();\r\n    String journal = \"hdfs://\" + cluster.getCluster().getNameNode(1).getClientNamenodeAddress() + \"/journal\";\r\n    Configuration routerConf = new RouterConfigBuilder().metrics().rpc().routerRenameOption().set(SCHEDULER_JOURNAL_URI, journal).set(DFS_ROUTER_FEDERATION_RENAME_MAP, \"1\").set(DFS_ROUTER_FEDERATION_RENAME_BANDWIDTH, \"1\").build();\r\n    routerConf.setTimeDuration(RBFConfigKeys.DN_REPORT_CACHE_EXPIRE, 1, TimeUnit.SECONDS);\r\n    routerConf.setBoolean(DFS_ROUTER_ADMIN_ENABLE, true);\r\n    routerConf.setBoolean(DFS_PERMISSIONS_ENABLED_KEY, true);\r\n    routerConf.set(CommonConfigurationKeys.HADOOP_SECURITY_GROUP_MAPPING, TestRouterFederationRename.MockGroupsMapping.class.getName());\r\n    cluster.addRouterOverrides(routerConf);\r\n    cluster.startRouters();\r\n    cluster.registerNamenodes();\r\n    cluster.waitNamenodeRegistration();\r\n    cluster.getCluster().getNamesystem(0).getBlockManager().getDatanodeManager().setHeartbeatInterval(1);\r\n    cluster.getCluster().getNamesystem(1).getBlockManager().getDatanodeManager().setHeartbeatInterval(1);\r\n    cluster.getCluster().getNamesystem(0).getBlockManager().getDatanodeManager().setHeartbeatExpireInterval(3000);\r\n    cluster.getCluster().getNamesystem(1).getBlockManager().getDatanodeManager().setHeartbeatExpireInterval(3000);\r\n    DistCpProcedure.enableForTest();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown()\n{\r\n    cluster.shutdown();\r\n    cluster = null;\r\n    DistCpProcedure.disableForTest();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void setup() throws IOException, InterruptedException\n{\r\n    cluster.installMockLocations();\r\n    cluster.deleteAllFiles();\r\n    cluster.createTestDirectoriesNamenode();\r\n    MiniRouterDFSCluster.RouterContext rndRouter = cluster.getRandomRouter();\r\n    this.setRouter(rndRouter);\r\n    for (MiniRouterDFSCluster.RouterContext rc : cluster.getRouters()) {\r\n        Router r = rc.getRouter();\r\n        MockResolver resolver = (MockResolver) r.getSubclusterResolver();\r\n        List<String> nss = cluster.getNameservices();\r\n        String ns0 = nss.get(0);\r\n        resolver.addLocation(\"/same\", ns0, \"/\");\r\n        resolver.addLocation(\"/same\", ns0, cluster.getNamenodePathForNS(ns0));\r\n    }\r\n    String ns0 = cluster.getNameservices().get(0);\r\n    this.setNs(ns0);\r\n    this.setNamenode(cluster.getNamenode(ns0, null));\r\n    Random rnd = new Random();\r\n    String randomFile = \"testfile-\" + rnd.nextInt();\r\n    this.nnFile = cluster.getNamenodeTestDirectoryForNS(ns) + \"/\" + randomFile;\r\n    createFile(nnFS, nnFile, 32);\r\n    verifyFileExists(nnFS, nnFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setRouter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setRouter(MiniRouterDFSCluster.RouterContext r) throws IOException\n{\r\n    this.router = r;\r\n    this.routerFS = r.getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setNs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNs(String nameservice)\n{\r\n    this.ns = nameservice;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setNamenode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNamenode(MiniRouterDFSCluster.NamenodeContext nn) throws IOException\n{\r\n    this.nnFS = nn.getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterFileSystem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSystem getRouterFileSystem()\n{\r\n    return this.routerFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createDir",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createDir(FileSystem fs, String dir) throws IOException\n{\r\n    fs.mkdirs(new Path(dir));\r\n    String file = dir + \"/file\";\r\n    createFile(fs, file, 32);\r\n    verifyFileExists(fs, dir);\r\n    verifyFileExists(fs, file);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getCluster",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MiniRouterDFSCluster getCluster()\n{\r\n    return cluster;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MiniRouterDFSCluster.RouterContext getRouterContext()\n{\r\n    return router;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void create()\n{\r\n    getConf().setTimeDuration(RBFConfigKeys.FEDERATION_STORE_ROUTER_EXPIRATION_MS, 2, TimeUnit.SECONDS);\r\n    getConf().setTimeDuration(RBFConfigKeys.FEDERATION_STORE_ROUTER_EXPIRATION_DELETION_MS, 2, TimeUnit.SECONDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws IOException, InterruptedException\n{\r\n    if (routerStore == null) {\r\n        routerStore = getStateStore().getRegisteredRecordStore(RouterStore.class);\r\n    }\r\n    assertTrue(clearRecords(getStateStore(), RouterState.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testStateStoreDisconnected",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testStateStoreDisconnected() throws Exception\n{\r\n    getStateStore().closeDriver();\r\n    assertEquals(false, getStateStore().isDriverReady());\r\n    GetRouterRegistrationRequest getSingleRequest = GetRouterRegistrationRequest.newInstance();\r\n    verifyException(routerStore, \"getRouterRegistration\", StateStoreUnavailableException.class, new Class[] { GetRouterRegistrationRequest.class }, new Object[] { getSingleRequest });\r\n    GetRouterRegistrationsRequest getRequest = GetRouterRegistrationsRequest.newInstance();\r\n    routerStore.loadCache(true);\r\n    verifyException(routerStore, \"getRouterRegistrations\", StateStoreUnavailableException.class, new Class[] { GetRouterRegistrationsRequest.class }, new Object[] { getRequest });\r\n    RouterHeartbeatRequest hbRequest = RouterHeartbeatRequest.newInstance(RouterState.newInstance(\"test\", 0, RouterServiceState.UNINITIALIZED));\r\n    verifyException(routerStore, \"routerHeartbeat\", StateStoreUnavailableException.class, new Class[] { RouterHeartbeatRequest.class }, new Object[] { hbRequest });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testUpdateRouterStatus",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testUpdateRouterStatus() throws IllegalStateException, IOException\n{\r\n    long dateStarted = Time.now();\r\n    String address = \"testaddress\";\r\n    RouterHeartbeatRequest request = RouterHeartbeatRequest.newInstance(RouterState.newInstance(address, dateStarted, RouterServiceState.RUNNING));\r\n    assertTrue(routerStore.routerHeartbeat(request).getStatus());\r\n    GetRouterRegistrationRequest getRequest = GetRouterRegistrationRequest.newInstance(address);\r\n    RouterState record = routerStore.getRouterRegistration(getRequest).getRouter();\r\n    assertNotNull(record);\r\n    assertEquals(RouterServiceState.RUNNING, record.getStatus());\r\n    assertEquals(address, record.getAddress());\r\n    assertEquals(FederationUtil.getCompileInfo(), record.getCompileInfo());\r\n    assertFalse(record.getVersion().isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testRouterStateExpiredAndDeletion",
  "errType" : [ "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRouterStateExpiredAndDeletion() throws IOException, InterruptedException, TimeoutException\n{\r\n    long dateStarted = Time.now();\r\n    String address = \"testaddress\";\r\n    RouterHeartbeatRequest request = RouterHeartbeatRequest.newInstance(RouterState.newInstance(address, dateStarted, RouterServiceState.RUNNING));\r\n    assertTrue(routerStore.routerHeartbeat(request).getStatus());\r\n    GetRouterRegistrationRequest getRequest = GetRouterRegistrationRequest.newInstance(address);\r\n    RouterState record = routerStore.getRouterRegistration(getRequest).getRouter();\r\n    assertNotNull(record);\r\n    GenericTestUtils.waitFor(() -> {\r\n        try {\r\n            RouterState routerState = routerStore.getRouterRegistration(getRequest).getRouter();\r\n            return routerState.getStatus() == RouterServiceState.EXPIRED;\r\n        } catch (IOException e) {\r\n            return false;\r\n        }\r\n    }, 100, 3000);\r\n    assertTrue(routerStore.routerHeartbeat(request).getStatus());\r\n    RouterState r = routerStore.getRouterRegistration(getRequest).getRouter();\r\n    assertEquals(RouterServiceState.RUNNING, r.getStatus());\r\n    GenericTestUtils.waitFor(() -> {\r\n        try {\r\n            RouterState routerState = routerStore.getRouterRegistration(getRequest).getRouter();\r\n            return routerState.getStatus() == RouterServiceState.EXPIRED;\r\n        } catch (IOException e) {\r\n            return false;\r\n        }\r\n    }, 100, 3000);\r\n    GenericTestUtils.waitFor(() -> {\r\n        try {\r\n            RouterState routerState = routerStore.getRouterRegistration(getRequest).getRouter();\r\n            return routerState.getStatus() == null;\r\n        } catch (IOException e) {\r\n            return false;\r\n        }\r\n    }, 100, 3000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testGetAllRouterStates",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testGetAllRouterStates() throws StateStoreUnavailableException, IOException\n{\r\n    RouterHeartbeatRequest heartbeatRequest1 = RouterHeartbeatRequest.newInstance(RouterState.newInstance(\"testaddress1\", Time.now(), RouterServiceState.RUNNING));\r\n    assertTrue(routerStore.routerHeartbeat(heartbeatRequest1).getStatus());\r\n    RouterHeartbeatRequest heartbeatRequest2 = RouterHeartbeatRequest.newInstance(RouterState.newInstance(\"testaddress2\", Time.now(), RouterServiceState.RUNNING));\r\n    assertTrue(routerStore.routerHeartbeat(heartbeatRequest2).getStatus());\r\n    routerStore.loadCache(true);\r\n    GetRouterRegistrationsRequest request = GetRouterRegistrationsRequest.newInstance();\r\n    List<RouterState> entries = routerStore.getRouterRegistrations(request).getRouters();\r\n    assertEquals(2, entries.size());\r\n    Collections.sort(entries);\r\n    assertEquals(\"testaddress1\", entries.get(0).getAddress());\r\n    assertEquals(\"testaddress2\", entries.get(1).getAddress());\r\n    assertEquals(RouterServiceState.RUNNING, entries.get(0).getStatus());\r\n    assertEquals(RouterServiceState.RUNNING, entries.get(1).getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    cluster = new MiniRouterDFSCluster(true, 2);\r\n    cluster.startCluster();\r\n    cluster.startRouters();\r\n    cluster.registerNamenodes();\r\n    cluster.waitNamenodeRegistration();\r\n    cluster.installMockLocations();\r\n    if (cluster.isHighAvailability()) {\r\n        for (String ns : cluster.getNameservices()) {\r\n            cluster.switchToActive(ns, NAMENODES[0]);\r\n            cluster.switchToStandby(ns, NAMENODES[1]);\r\n        }\r\n    }\r\n    cluster.waitActiveNamespaces();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardown() throws IOException\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testProxySetSafemode",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testProxySetSafemode() throws Exception\n{\r\n    RouterContext routerContext = cluster.getRandomRouter();\r\n    ClientProtocol routerProtocol = routerContext.getClient().getNamenode();\r\n    routerProtocol.setSafeMode(SafeModeAction.SAFEMODE_GET, true);\r\n    routerProtocol.setSafeMode(SafeModeAction.SAFEMODE_GET, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    createCluster(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster(boolean security) throws IOException\n{\r\n    createCluster(true, 2, security);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void createCluster(boolean ha, int numNameServices, boolean security) throws IOException\n{\r\n    try {\r\n        Configuration conf = null;\r\n        if (security) {\r\n            conf = SecurityConfUtil.initSecurity();\r\n        }\r\n        cluster = new MiniRouterDFSCluster(ha, numNameServices, conf);\r\n        cluster.startCluster(conf);\r\n        cluster.startRouters();\r\n        cluster.registerNamenodes();\r\n        cluster.waitNamenodeRegistration();\r\n        cluster.installMockLocations();\r\n        if (cluster.isHighAvailability()) {\r\n            for (String ns : cluster.getNameservices()) {\r\n                cluster.switchToActive(ns, NAMENODES[0]);\r\n                cluster.switchToStandby(ns, NAMENODES[1]);\r\n            }\r\n        }\r\n        cluster.waitActiveNamespaces();\r\n    } catch (Exception e) {\r\n        destroyCluster();\r\n        throw new IOException(\"Cannot start federated cluster\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "destroyCluster",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void destroyCluster() throws IOException\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n    try {\r\n        SecurityConfUtil.destroy();\r\n    } catch (Exception e) {\r\n        throw new IOException(\"Cannot destroy security context\", e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "getCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MiniDFSCluster getCluster()\n{\r\n    return cluster.getCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "getRouterCluster",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MiniRouterDFSCluster getRouterCluster()\n{\r\n    return cluster;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileSystem getFileSystem() throws IOException\n{\r\n    Assert.assertNotNull(\"cluster not created\", cluster);\r\n    return cluster.getRandomRouter().getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "getTestFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getTestFileSystem() throws IOException\n{\r\n    return getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testHashCode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testHashCode()\n{\r\n    Set<FederationNamespaceInfo> set = new TreeSet<>();\r\n    set.add(new FederationNamespaceInfo(\"\", \"nn1\", \"ns1\"));\r\n    set.add(new FederationNamespaceInfo(\"bp1\", \"nn2\", \"ns1\"));\r\n    assertThat(set).hasSize(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "addRouterOverrides",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addRouterOverrides(Configuration conf)\n{\r\n    if (this.routerOverrides == null) {\r\n        this.routerOverrides = conf;\r\n    } else {\r\n        this.routerOverrides.addResource(conf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "addNamenodeOverrides",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addNamenodeOverrides(Configuration conf)\n{\r\n    if (this.namenodeOverrides == null) {\r\n        this.namenodeOverrides = conf;\r\n    } else {\r\n        this.namenodeOverrides.addResource(conf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "generateNamenodeConfiguration",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "Configuration generateNamenodeConfiguration(String nsId)\n{\r\n    Configuration conf = new HdfsConfiguration();\r\n    conf.set(DFS_NAMESERVICES, getNameservicesKey());\r\n    conf.set(FS_DEFAULT_NAME_KEY, \"hdfs://\" + nsId);\r\n    for (String ns : nameservices) {\r\n        if (highAvailability) {\r\n            conf.set(DFS_HA_NAMENODES_KEY_PREFIX + \".\" + ns, NAMENODES[0] + \",\" + NAMENODES[1]);\r\n        }\r\n        for (NamenodeContext context : getNamenodes(ns)) {\r\n            String suffix = context.getConfSuffix();\r\n            conf.set(DFS_NAMENODE_RPC_ADDRESS_KEY + \".\" + suffix, \"127.0.0.1:\" + context.rpcPort);\r\n            conf.set(DFS_NAMENODE_HTTP_ADDRESS_KEY + \".\" + suffix, \"127.0.0.1:\" + context.httpPort);\r\n            conf.set(DFS_NAMENODE_RPC_BIND_HOST_KEY + \".\" + suffix, \"0.0.0.0\");\r\n            conf.set(DFS_NAMENODE_HTTPS_ADDRESS_KEY + \".\" + suffix, \"127.0.0.1:\" + context.httpsPort);\r\n            conf.set(HdfsClientConfigKeys.Failover.PROXY_PROVIDER_KEY_PREFIX + \".\" + ns, ConfiguredFailoverProxyProvider.class.getName());\r\n            boolean servicePortEnabled = false;\r\n            if (servicePortEnabled) {\r\n                conf.set(DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY + \".\" + suffix, \"127.0.0.1:\" + context.servicePort);\r\n                conf.set(DFS_NAMENODE_SERVICE_RPC_BIND_HOST_KEY + \".\" + suffix, \"0.0.0.0\");\r\n            }\r\n        }\r\n    }\r\n    if (this.namenodeOverrides != null) {\r\n        conf.addResource(this.namenodeOverrides);\r\n    }\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "generateClientConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration generateClientConfiguration()\n{\r\n    Configuration conf = new HdfsConfiguration(false);\r\n    String ns0 = getNameservices().get(0);\r\n    conf.addResource(generateNamenodeConfiguration(ns0));\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "generateRouterConfiguration",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "Configuration generateRouterConfiguration(String nsId, String nnId)\n{\r\n    Configuration conf;\r\n    if (this.routerConf == null) {\r\n        conf = new Configuration(false);\r\n    } else {\r\n        conf = new Configuration(routerConf);\r\n    }\r\n    conf.addResource(generateNamenodeConfiguration(nsId));\r\n    conf.setInt(DFS_ROUTER_HANDLER_COUNT_KEY, 10);\r\n    conf.set(DFS_ROUTER_RPC_ADDRESS_KEY, \"127.0.0.1:0\");\r\n    conf.set(DFS_ROUTER_RPC_BIND_HOST_KEY, \"0.0.0.0\");\r\n    conf.set(DFS_ROUTER_ADMIN_ADDRESS_KEY, \"127.0.0.1:0\");\r\n    conf.set(DFS_ROUTER_ADMIN_BIND_HOST_KEY, \"0.0.0.0\");\r\n    conf.set(DFS_ROUTER_HTTP_ADDRESS_KEY, \"127.0.0.1:0\");\r\n    conf.set(DFS_ROUTER_HTTPS_ADDRESS_KEY, \"127.0.0.1:0\");\r\n    conf.set(DFS_ROUTER_HTTP_BIND_HOST_KEY, \"0.0.0.0\");\r\n    conf.set(DFS_ROUTER_DEFAULT_NAMESERVICE, nameservices.get(0));\r\n    conf.setLong(DFS_ROUTER_HEARTBEAT_INTERVAL_MS, heartbeatInterval);\r\n    conf.setLong(DFS_ROUTER_CACHE_TIME_TO_LIVE_MS, cacheFlushInterval);\r\n    conf.setClass(FEDERATION_NAMENODE_RESOLVER_CLIENT_CLASS, MockResolver.class, ActiveNamenodeResolver.class);\r\n    conf.setClass(FEDERATION_FILE_RESOLVER_CLIENT_CLASS, MockResolver.class, FileSubclusterResolver.class);\r\n    conf.setBoolean(DFS_ROUTER_SAFEMODE_ENABLE, false);\r\n    conf.set(DFS_NAMESERVICE_ID, nsId);\r\n    if (nnId != null) {\r\n        conf.set(DFS_HA_NAMENODE_ID_KEY, nnId);\r\n    }\r\n    StringBuilder sb = new StringBuilder();\r\n    for (String ns : this.nameservices) {\r\n        for (NamenodeContext context : getNamenodes(ns)) {\r\n            String suffix = context.getConfSuffix();\r\n            if (sb.length() != 0) {\r\n                sb.append(\",\");\r\n            }\r\n            sb.append(suffix);\r\n        }\r\n    }\r\n    conf.set(DFS_ROUTER_MONITOR_NAMENODE, sb.toString());\r\n    if (this.routerOverrides != null) {\r\n        for (Entry<String, String> entry : this.routerOverrides) {\r\n            String confKey = entry.getKey();\r\n            String confValue = entry.getValue();\r\n            conf.set(confKey, confValue);\r\n        }\r\n    }\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "configureNameservices",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void configureNameservices(int numNameservices, int numNamenodes, Configuration overrideConf)\n{\r\n    this.nameservices = new ArrayList<>();\r\n    this.namenodes = new ArrayList<>();\r\n    NamenodeContext context = null;\r\n    int nnIndex = 0;\r\n    for (int i = 0; i < numNameservices; i++) {\r\n        String ns = \"ns\" + i;\r\n        this.nameservices.add(\"ns\" + i);\r\n        Configuration nnConf = generateNamenodeConfiguration(ns);\r\n        if (overrideConf != null) {\r\n            nnConf.addResource(overrideConf);\r\n        }\r\n        if (!highAvailability) {\r\n            context = new NamenodeContext(nnConf, ns, null, nnIndex++);\r\n            this.namenodes.add(context);\r\n        } else {\r\n            for (int j = 0; j < numNamenodes; j++) {\r\n                context = new NamenodeContext(nnConf, ns, NAMENODES[j], nnIndex++);\r\n                this.namenodes.add(context);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "setNumDatanodesPerNameservice",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumDatanodesPerNameservice(int num)\n{\r\n    this.numDatanodesPerNameservice = num;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "setStorageTypes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStorageTypes(StorageType[][] storageTypes)\n{\r\n    this.storageTypes = storageTypes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "setRacks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRacks(String[] racks)\n{\r\n    this.racks = racks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "setIndependentDNs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setIndependentDNs()\n{\r\n    this.sharedDNs = false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getNameservicesKey",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getNameservicesKey()\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    for (String nsId : this.nameservices) {\r\n        if (sb.length() > 0) {\r\n            sb.append(\",\");\r\n        }\r\n        sb.append(nsId);\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getRandomNameservice",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getRandomNameservice()\n{\r\n    int randIndex = RND.nextInt(nameservices.size());\r\n    return nameservices.get(randIndex);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getNameservices",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<String> getNameservices()\n{\r\n    return nameservices;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getNamenodes",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<NamenodeContext> getNamenodes(String nameservice)\n{\r\n    List<NamenodeContext> nns = new ArrayList<>();\r\n    for (NamenodeContext c : namenodes) {\r\n        if (c.nameserviceId.equals(nameservice)) {\r\n            nns.add(c);\r\n        }\r\n    }\r\n    return nns;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getRandomNamenode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "NamenodeContext getRandomNamenode()\n{\r\n    Random rand = new Random();\r\n    int i = rand.nextInt(this.namenodes.size());\r\n    return this.namenodes.get(i);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getNamenodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<NamenodeContext> getNamenodes()\n{\r\n    return this.namenodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "isHighAvailability",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isHighAvailability()\n{\r\n    return highAvailability;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getNamenode",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "NamenodeContext getNamenode(String nameservice, String namenode)\n{\r\n    for (NamenodeContext c : this.namenodes) {\r\n        if (c.nameserviceId.equals(nameservice)) {\r\n            if (namenode == null || namenode.isEmpty() || c.namenodeId == null || c.namenodeId.isEmpty()) {\r\n                return c;\r\n            } else if (c.namenodeId.equals(namenode)) {\r\n                return c;\r\n            }\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getRouters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<RouterContext> getRouters(String nameservice)\n{\r\n    List<RouterContext> nns = new ArrayList<>();\r\n    for (RouterContext c : routers) {\r\n        if (c.nameserviceId.equals(nameservice)) {\r\n            nns.add(c);\r\n        }\r\n    }\r\n    return nns;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getRouterContext",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RouterContext getRouterContext(String nsId, String nnId)\n{\r\n    for (RouterContext c : routers) {\r\n        if (nnId == null) {\r\n            return c;\r\n        }\r\n        if (c.namenodeId.equals(nnId) && c.nameserviceId.equals(nsId)) {\r\n            return c;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getRandomRouter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterContext getRandomRouter()\n{\r\n    Random rand = new Random();\r\n    return routers.get(rand.nextInt(routers.size()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getRouters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<RouterContext> getRouters()\n{\r\n    return routers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "buildRouter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterContext buildRouter(String nsId, String nnId) throws URISyntaxException, IOException\n{\r\n    Configuration config = generateRouterConfiguration(nsId, nnId);\r\n    RouterContext rc = new RouterContext(config, nsId, nnId);\r\n    return rc;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "startCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void startCluster()\n{\r\n    startCluster(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "startCluster",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void startCluster(Configuration overrideConf)\n{\r\n    try {\r\n        MiniDFSNNTopology topology = new MiniDFSNNTopology();\r\n        for (String ns : nameservices) {\r\n            NSConf conf = new MiniDFSNNTopology.NSConf(ns);\r\n            if (highAvailability) {\r\n                for (int i = 0; i < namenodes.size() / nameservices.size(); i++) {\r\n                    NNConf nnConf = new MiniDFSNNTopology.NNConf(\"nn\" + i);\r\n                    conf.addNN(nnConf);\r\n                }\r\n            } else {\r\n                NNConf nnConf = new MiniDFSNNTopology.NNConf(null);\r\n                conf.addNN(nnConf);\r\n            }\r\n            topology.addNameservice(conf);\r\n        }\r\n        topology.setFederation(true);\r\n        String ns0 = nameservices.get(0);\r\n        Configuration nnConf = generateNamenodeConfiguration(ns0);\r\n        if (overrideConf != null) {\r\n            nnConf.addResource(overrideConf);\r\n            routerConf = new Configuration(overrideConf);\r\n        }\r\n        int numDNs = nameservices.size() * numDatanodesPerNameservice;\r\n        Configuration[] dnConfs = null;\r\n        if (!sharedDNs) {\r\n            dnConfs = new Configuration[numDNs];\r\n            int dnId = 0;\r\n            for (String nsId : nameservices) {\r\n                Configuration subclusterConf = new Configuration(nnConf);\r\n                subclusterConf.set(DFS_INTERNAL_NAMESERVICES_KEY, nsId);\r\n                for (int i = 0; i < numDatanodesPerNameservice; i++) {\r\n                    dnConfs[dnId] = subclusterConf;\r\n                    dnId++;\r\n                }\r\n            }\r\n        }\r\n        cluster = new MiniDFSCluster.Builder(nnConf).numDataNodes(numDNs).nnTopology(topology).dataNodeConfOverlays(dnConfs).storageTypes(storageTypes).racks(racks).build();\r\n        cluster.waitActive();\r\n        for (int i = 0; i < namenodes.size(); i++) {\r\n            NameNode nn = cluster.getNameNode(i);\r\n            namenodes.get(i).setNamenode(nn);\r\n        }\r\n    } catch (Exception e) {\r\n        LOG.error(\"Cannot start Router DFS cluster: {}\", e.getMessage(), e);\r\n        if (cluster != null) {\r\n            cluster.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "startRouters",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void startRouters() throws InterruptedException, URISyntaxException, IOException\n{\r\n    this.routers = new ArrayList<>();\r\n    for (String ns : this.nameservices) {\r\n        for (NamenodeContext context : getNamenodes(ns)) {\r\n            RouterContext router = buildRouter(ns, context.namenodeId);\r\n            this.routers.add(router);\r\n        }\r\n    }\r\n    for (RouterContext router : this.routers) {\r\n        router.router.start();\r\n    }\r\n    for (RouterContext router : this.routers) {\r\n        waitActive(router);\r\n        router.initRouter();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "waitActive",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitActive(NamenodeContext nn) throws IOException\n{\r\n    cluster.waitActive(nn.index);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "waitActive",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void waitActive(RouterContext router) throws InterruptedException\n{\r\n    for (int loopCount = 0; loopCount < 20; loopCount++) {\r\n        if (router.router.getServiceState() == STATE.STARTED) {\r\n            return;\r\n        }\r\n        Thread.sleep(1000);\r\n    }\r\n    fail(\"Timeout waiting for \" + router.router + \" to activate\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "registerNamenodes",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void registerNamenodes() throws IOException\n{\r\n    for (RouterContext r : this.routers) {\r\n        ActiveNamenodeResolver resolver = r.router.getNamenodeResolver();\r\n        for (NamenodeContext nn : this.namenodes) {\r\n            NamenodeStatusReport report = new NamenodeStatusReport(nn.nameserviceId, nn.namenodeId, nn.getRpcAddress(), nn.getServiceAddress(), nn.getLifelineAddress(), \"http\", nn.getWebAddress());\r\n            FSImage fsImage = nn.namenode.getNamesystem().getFSImage();\r\n            NamespaceInfo nsInfo = fsImage.getStorage().getNamespaceInfo();\r\n            report.setNamespaceInfo(nsInfo);\r\n            String nnState = nn.namenode.getState();\r\n            HAServiceState haState = HAServiceState.ACTIVE;\r\n            for (HAServiceState state : HAServiceState.values()) {\r\n                if (nnState.equalsIgnoreCase(state.name())) {\r\n                    haState = state;\r\n                    break;\r\n                }\r\n            }\r\n            report.setHAServiceState(haState);\r\n            resolver.registerNamenode(report);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "waitNamenodeRegistration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void waitNamenodeRegistration() throws Exception\n{\r\n    for (RouterContext r : this.routers) {\r\n        Router router = r.router;\r\n        for (NamenodeContext nn : this.namenodes) {\r\n            ActiveNamenodeResolver nnResolver = router.getNamenodeResolver();\r\n            waitNamenodeRegistered(nnResolver, nn.nameserviceId, nn.namenodeId, null);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "waitRouterRegistrationQuorum",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void waitRouterRegistrationQuorum(RouterContext router, FederationNamenodeServiceState state, String nsId, String nnId) throws Exception\n{\r\n    LOG.info(\"Waiting for NN {} {} to transition to {}\", nsId, nnId, state);\r\n    ActiveNamenodeResolver nnResolver = router.router.getNamenodeResolver();\r\n    waitNamenodeRegistered(nnResolver, nsId, nnId, state);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "waitActiveNamespaces",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void waitActiveNamespaces() throws Exception\n{\r\n    for (RouterContext r : this.routers) {\r\n        Router router = r.router;\r\n        final ActiveNamenodeResolver resolver = router.getNamenodeResolver();\r\n        for (FederationNamespaceInfo ns : resolver.getNamespaces()) {\r\n            final String nsId = ns.getNameserviceId();\r\n            waitNamenodeRegistered(resolver, nsId, FederationNamenodeServiceState.ACTIVE);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getFederatedPathForNS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getFederatedPathForNS(String nsId)\n{\r\n    return \"/\" + nsId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getNamenodePathForNS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNamenodePathForNS(String nsId)\n{\r\n    return \"/target-\" + nsId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getFederatedTestDirectoryForNS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getFederatedTestDirectoryForNS(String nsId)\n{\r\n    return getFederatedPathForNS(nsId) + \"/\" + TEST_DIR;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getNamenodeTestDirectoryForNS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getNamenodeTestDirectoryForNS(String nsId)\n{\r\n    return getNamenodePathForNS(nsId) + \"/\" + TEST_DIR;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getFederatedTestFileForNS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getFederatedTestFileForNS(String nsId)\n{\r\n    return getFederatedPathForNS(nsId) + \"/\" + TEST_FILE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getNamenodeTestFileForNS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getNamenodeTestFileForNS(String nsId)\n{\r\n    return getNamenodePathForNS(nsId) + \"/\" + TEST_FILE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "switchToActive",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void switchToActive(String nsId, String nnId)\n{\r\n    try {\r\n        int total = cluster.getNumNameNodes();\r\n        NameNodeInfo[] nns = cluster.getNameNodeInfos();\r\n        for (int i = 0; i < total; i++) {\r\n            NameNodeInfo nn = nns[i];\r\n            if (nn.getNameserviceId().equals(nsId) && nn.getNamenodeId().equals(nnId)) {\r\n                cluster.transitionToActive(i);\r\n            }\r\n        }\r\n    } catch (Throwable e) {\r\n        LOG.error(\"Cannot transition to active\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "switchToStandby",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void switchToStandby(String nsId, String nnId)\n{\r\n    try {\r\n        int total = cluster.getNumNameNodes();\r\n        NameNodeInfo[] nns = cluster.getNameNodeInfos();\r\n        for (int i = 0; i < total; i++) {\r\n            NameNodeInfo nn = nns[i];\r\n            if (nn.getNameserviceId().equals(nsId) && nn.getNamenodeId().equals(nnId)) {\r\n                cluster.transitionToStandby(i);\r\n            }\r\n        }\r\n    } catch (Throwable e) {\r\n        LOG.error(\"Cannot transition to standby\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void shutdown()\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n    if (routers != null) {\r\n        for (RouterContext context : routers) {\r\n            stopRouter(context);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "stopRouter",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void stopRouter(RouterContext router)\n{\r\n    try {\r\n        router.router.shutDown();\r\n        int loopCount = 0;\r\n        while (router.router.getServiceState() != STATE.STOPPED) {\r\n            loopCount++;\r\n            Thread.sleep(1000);\r\n            if (loopCount > 20) {\r\n                LOG.error(\"Cannot shutdown router {}\", router.rpcPort);\r\n                break;\r\n            }\r\n        }\r\n    } catch (InterruptedException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "createTestDirectoriesNamenode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createTestDirectoriesNamenode() throws IOException\n{\r\n    for (String ns : getNameservices()) {\r\n        NamenodeContext context = getNamenode(ns, null);\r\n        if (!createTestDirectoriesNamenode(context)) {\r\n            throw new IOException(\"Cannot create test directory for ns \" + ns);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "createTestDirectoriesNamenode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean createTestDirectoriesNamenode(NamenodeContext nn) throws IOException\n{\r\n    FileSystem fs = nn.getFileSystem();\r\n    String testDir = getNamenodeTestDirectoryForNS(nn.nameserviceId);\r\n    return addDirectory(fs, testDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "deleteAllFiles",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void deleteAllFiles() throws IOException\n{\r\n    for (NamenodeContext context : getNamenodes()) {\r\n        FileSystem fs = context.getFileSystem();\r\n        FileStatus[] status = fs.listStatus(new Path(\"/\"));\r\n        for (int i = 0; i < status.length; i++) {\r\n            Path p = status[i].getPath();\r\n            fs.delete(p, true);\r\n        }\r\n        status = fs.listStatus(new Path(\"/\"));\r\n        assertEquals(status.length, 0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "installMockLocations",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void installMockLocations()\n{\r\n    for (RouterContext r : routers) {\r\n        MockResolver resolver = (MockResolver) r.router.getSubclusterResolver();\r\n        for (String nsId : nameservices) {\r\n            String routerPath = getFederatedPathForNS(nsId);\r\n            String nnPath = getNamenodePathForNS(nsId);\r\n            resolver.addLocation(routerPath, nsId, nnPath);\r\n        }\r\n        String ns0 = nameservices.get(0);\r\n        resolver.addLocation(\"/\", ns0, \"/\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getCluster",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MiniDFSCluster getCluster()\n{\r\n    return cluster;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "waitClusterUp",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void waitClusterUp() throws IOException\n{\r\n    cluster.waitClusterUp();\r\n    registerNamenodes();\r\n    try {\r\n        waitNamenodeRegistration();\r\n    } catch (Exception e) {\r\n        throw new IOException(\"Cannot wait for the namenodes\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void create() throws IOException\n{\r\n    nameservices = new ArrayList<>();\r\n    nameservices.add(NAMESERVICES[0]);\r\n    nameservices.add(NAMESERVICES[1]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws IOException, InterruptedException\n{\r\n    mountStore = getStateStore().getRegisteredRecordStore(MountTableStore.class);\r\n    assertTrue(clearRecords(getStateStore(), MountTable.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testStateStoreDisconnected",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testStateStoreDisconnected() throws Exception\n{\r\n    getStateStore().closeDriver();\r\n    assertFalse(getStateStore().isDriverReady());\r\n    MountTable entry = MountTable.newInstance(\"/mnt\", Collections.singletonMap(\"ns0\", \"/tmp\"));\r\n    AddMountTableEntryRequest addRequest = AddMountTableEntryRequest.newInstance(entry);\r\n    verifyException(mountStore, \"addMountTableEntry\", StateStoreUnavailableException.class, new Class[] { AddMountTableEntryRequest.class }, new Object[] { addRequest });\r\n    UpdateMountTableEntryRequest updateRequest = UpdateMountTableEntryRequest.newInstance(entry);\r\n    verifyException(mountStore, \"updateMountTableEntry\", StateStoreUnavailableException.class, new Class[] { UpdateMountTableEntryRequest.class }, new Object[] { updateRequest });\r\n    RemoveMountTableEntryRequest removeRequest = RemoveMountTableEntryRequest.newInstance();\r\n    verifyException(mountStore, \"removeMountTableEntry\", StateStoreUnavailableException.class, new Class[] { RemoveMountTableEntryRequest.class }, new Object[] { removeRequest });\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance();\r\n    mountStore.loadCache(true);\r\n    verifyException(mountStore, \"getMountTableEntries\", StateStoreUnavailableException.class, new Class[] { GetMountTableEntriesRequest.class }, new Object[] { getRequest });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testSynchronizeMountTable",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSynchronizeMountTable() throws IOException\n{\r\n    List<MountTable> entries = createMockMountTable(nameservices);\r\n    assertTrue(synchronizeRecords(getStateStore(), entries, MountTable.class));\r\n    for (MountTable e : entries) {\r\n        mountStore.loadCache(true);\r\n        MountTable entry = getMountTableEntry(e.getSourcePath());\r\n        assertNotNull(entry);\r\n        assertEquals(e.getDefaultLocation().getDest(), entry.getDefaultLocation().getDest());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testAddMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testAddMountTableEntry() throws IOException\n{\r\n    List<MountTable> entries = createMockMountTable(nameservices);\r\n    List<MountTable> entries1 = getMountTableEntries(\"/\").getRecords();\r\n    assertEquals(0, entries1.size());\r\n    MountTable entry0 = entries.get(0);\r\n    AddMountTableEntryRequest request = AddMountTableEntryRequest.newInstance(entry0);\r\n    AddMountTableEntryResponse response = mountStore.addMountTableEntry(request);\r\n    assertTrue(response.getStatus());\r\n    mountStore.loadCache(true);\r\n    List<MountTable> entries2 = getMountTableEntries(\"/\").getRecords();\r\n    assertEquals(1, entries2.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testRemoveMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRemoveMountTableEntry() throws IOException\n{\r\n    List<MountTable> entries = createMockMountTable(nameservices);\r\n    synchronizeRecords(getStateStore(), entries, MountTable.class);\r\n    mountStore.loadCache(true);\r\n    List<MountTable> entries1 = getMountTableEntries(\"/\").getRecords();\r\n    assertEquals(entries.size(), entries1.size());\r\n    RemoveMountTableEntryRequest request = RemoveMountTableEntryRequest.newInstance();\r\n    request.setSrcPath(entries.get(0).getSourcePath());\r\n    assertTrue(mountStore.removeMountTableEntry(request).getStatus());\r\n    mountStore.loadCache(true);\r\n    List<MountTable> entries2 = getMountTableEntries(\"/\").getRecords();\r\n    assertEquals(entries.size() - 1, entries2.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testUpdateMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testUpdateMountTableEntry() throws IOException\n{\r\n    List<MountTable> entries = createMockMountTable(nameservices);\r\n    MountTable entry0 = entries.get(0);\r\n    String srcPath = entry0.getSourcePath();\r\n    String nsId = entry0.getDefaultLocation().getNameserviceId();\r\n    AddMountTableEntryRequest request = AddMountTableEntryRequest.newInstance(entry0);\r\n    AddMountTableEntryResponse response = mountStore.addMountTableEntry(request);\r\n    assertTrue(response.getStatus());\r\n    mountStore.loadCache(true);\r\n    MountTable matchingEntry0 = getMountTableEntry(srcPath);\r\n    assertNotNull(matchingEntry0);\r\n    assertEquals(nsId, matchingEntry0.getDefaultLocation().getNameserviceId());\r\n    Map<String, String> destMap = Collections.singletonMap(\"testnameservice\", \"/\");\r\n    MountTable replacement = MountTable.newInstance(srcPath, destMap);\r\n    UpdateMountTableEntryRequest updateRequest = UpdateMountTableEntryRequest.newInstance(replacement);\r\n    UpdateMountTableEntryResponse updateResponse = mountStore.updateMountTableEntry(updateRequest);\r\n    assertTrue(updateResponse.getStatus());\r\n    mountStore.loadCache(true);\r\n    MountTable matchingEntry1 = getMountTableEntry(srcPath);\r\n    assertNotNull(matchingEntry1);\r\n    assertEquals(\"testnameservice\", matchingEntry1.getDefaultLocation().getNameserviceId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "MountTable getMountTableEntry(String mount) throws IOException\n{\r\n    GetMountTableEntriesRequest request = GetMountTableEntriesRequest.newInstance(mount);\r\n    GetMountTableEntriesResponse response = mountStore.getMountTableEntries(request);\r\n    List<MountTable> results = response.getEntries();\r\n    if (results.size() > 0) {\r\n        return results.get(0);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getMountTableEntries",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "QueryResult<MountTable> getMountTableEntries(String mount) throws IOException\n{\r\n    if (mount == null) {\r\n        throw new IOException(\"Please specify a root search path\");\r\n    }\r\n    GetMountTableEntriesRequest request = GetMountTableEntriesRequest.newInstance();\r\n    request.setSrcPath(mount);\r\n    GetMountTableEntriesResponse response = mountStore.getMountTableEntries(request);\r\n    List<MountTable> records = response.getEntries();\r\n    long timestamp = response.getTimestamp();\r\n    return new QueryResult<MountTable>(records, timestamp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "createMockSecretManager",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createMockSecretManager() throws IOException\n{\r\n    AbstractDelegationTokenSecretManager<DelegationTokenIdentifier> mockDelegationTokenSecretManager = new MockDelegationTokenSecretManager(100, 100, 100, 100);\r\n    mockDelegationTokenSecretManager.startThreads();\r\n    securityManager = new RouterSecurityManager(mockDelegationTokenSecretManager);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "testCreateSecretManagerUsingReflection",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCreateSecretManagerUsingReflection() throws IOException\n{\r\n    Configuration conf = new HdfsConfiguration();\r\n    conf.set(DFS_ROUTER_DELEGATION_TOKEN_DRIVER_CLASS, MockDelegationTokenSecretManager.class.getName());\r\n    conf.set(HADOOP_SECURITY_AUTHENTICATION, UserGroupInformation.AuthenticationMethod.KERBEROS.name());\r\n    RouterSecurityManager routerSecurityManager = new RouterSecurityManager(conf);\r\n    AbstractDelegationTokenSecretManager<DelegationTokenIdentifier> secretManager = routerSecurityManager.getSecretManager();\r\n    assertNotNull(secretManager);\r\n    assertTrue(secretManager.isRunning());\r\n    routerSecurityManager.stop();\r\n    assertFalse(secretManager.isRunning());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "testDelegationTokens",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDelegationTokens() throws IOException\n{\r\n    UserGroupInformation.reset();\r\n    UserGroupInformation.setLoginUser(UserGroupInformation.createUserForTesting(\"router\", getUserGroupForTesting()));\r\n    Token<DelegationTokenIdentifier> token = securityManager.getDelegationToken(new Text(\"some_renewer\"));\r\n    assertNotNull(token);\r\n    UserGroupInformation.setLoginUser(UserGroupInformation.createUserForTesting(\"some_renewer\", getUserGroupForTesting()));\r\n    long updatedExpirationTime = securityManager.renewDelegationToken(token);\r\n    assertTrue(updatedExpirationTime <= token.decodeIdentifier().getMaxDate());\r\n    securityManager.cancelDelegationToken(token);\r\n    String exceptionCause = \"Renewal request for unknown token\";\r\n    exceptionRule.expect(SecretManager.InvalidToken.class);\r\n    exceptionRule.expectMessage(exceptionCause);\r\n    securityManager.renewDelegationToken(token);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "testDelgationTokenTopOwners",
  "errType" : null,
  "containingMethodsNum" : 43,
  "sourceCodeText" : "void testDelgationTokenTopOwners() throws Exception\n{\r\n    UserGroupInformation.reset();\r\n    List<NameValuePair> topOwners;\r\n    UserGroupInformation user = UserGroupInformation.createUserForTesting(\"abc\", new String[] { \"router_group\" });\r\n    UserGroupInformation.setLoginUser(user);\r\n    Token dt = securityManager.getDelegationToken(new Text(\"abc\"));\r\n    topOwners = securityManager.getSecretManager().getTopTokenRealOwners(2);\r\n    assertEquals(1, topOwners.size());\r\n    assertEquals(\"abc\", topOwners.get(0).getName());\r\n    assertEquals(1, topOwners.get(0).getValue());\r\n    securityManager.renewDelegationToken(dt);\r\n    topOwners = securityManager.getSecretManager().getTopTokenRealOwners(2);\r\n    assertEquals(1, topOwners.size());\r\n    assertEquals(\"abc\", topOwners.get(0).getName());\r\n    assertEquals(1, topOwners.get(0).getValue());\r\n    securityManager.cancelDelegationToken(dt);\r\n    topOwners = securityManager.getSecretManager().getTopTokenRealOwners(2);\r\n    assertEquals(0, topOwners.size());\r\n    UserGroupInformation routerUser = UserGroupInformation.createRemoteUser(\"router\");\r\n    UserGroupInformation proxyUser = UserGroupInformation.createProxyUserForTesting(\"abc\", routerUser, new String[] { \"router_group\" });\r\n    UserGroupInformation.setLoginUser(proxyUser);\r\n    Token proxyDT = securityManager.getDelegationToken(new Text(\"router\"));\r\n    topOwners = securityManager.getSecretManager().getTopTokenRealOwners(2);\r\n    assertEquals(1, topOwners.size());\r\n    assertEquals(\"router\", topOwners.get(0).getName());\r\n    assertEquals(1, topOwners.get(0).getValue());\r\n    UserGroupInformation.setLoginUser(routerUser);\r\n    securityManager.renewDelegationToken(proxyDT);\r\n    topOwners = securityManager.getSecretManager().getTopTokenRealOwners(2);\r\n    assertEquals(1, topOwners.size());\r\n    assertEquals(\"router\", topOwners.get(0).getName());\r\n    assertEquals(1, topOwners.get(0).getValue());\r\n    securityManager.cancelDelegationToken(proxyDT);\r\n    topOwners = securityManager.getSecretManager().getTopTokenRealOwners(2);\r\n    assertEquals(0, topOwners.size());\r\n    securityManager.getDelegationToken(new Text(\"router\"));\r\n    securityManager.getDelegationToken(new Text(\"router\"));\r\n    UserGroupInformation.setLoginUser(user);\r\n    securityManager.getDelegationToken(new Text(\"router\"));\r\n    topOwners = securityManager.getSecretManager().getTopTokenRealOwners(2);\r\n    assertEquals(2, topOwners.size());\r\n    assertEquals(\"router\", topOwners.get(0).getName());\r\n    assertEquals(2, topOwners.get(0).getValue());\r\n    assertEquals(\"abc\", topOwners.get(1).getName());\r\n    assertEquals(1, topOwners.get(1).getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "testVerifyToken",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testVerifyToken() throws IOException\n{\r\n    UserGroupInformation.reset();\r\n    UserGroupInformation.setLoginUser(UserGroupInformation.createUserForTesting(\"router\", getUserGroupForTesting()));\r\n    Token<DelegationTokenIdentifier> token = securityManager.getDelegationToken(new Text(\"some_renewer\"));\r\n    assertNotNull(token);\r\n    securityManager.verifyToken(token.decodeIdentifier(), token.getPassword());\r\n    String exceptionCause = \"password doesn't match\";\r\n    exceptionRule.expect(SecretManager.InvalidToken.class);\r\n    exceptionRule.expectMessage(StringContains.containsString(exceptionCause));\r\n    securityManager.verifyToken(token.decodeIdentifier(), new byte[10]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "testCreateCredentials",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testCreateCredentials() throws Exception\n{\r\n    Configuration conf = initSecurity();\r\n    Configuration routerConf = new RouterConfigBuilder().metrics().rpc().build();\r\n    conf.addResource(routerConf);\r\n    Router router = new Router();\r\n    router.init(conf);\r\n    router.start();\r\n    UserGroupInformation ugi = UserGroupInformation.createUserForTesting(\"router\", getUserGroupForTesting());\r\n    Credentials creds = RouterSecurityManager.createCredentials(router, ugi, \"some_renewer\");\r\n    for (Token token : creds.getAllTokens()) {\r\n        assertNotNull(token);\r\n        assertEquals(\"HDFS_DELEGATION_TOKEN\", token.getKind().toString());\r\n        DelegationTokenIdentifier identifier = (DelegationTokenIdentifier) token.decodeIdentifier();\r\n        assertNotNull(identifier);\r\n        String owner = identifier.getOwner().toString();\r\n        String host = Path.WINDOWS ? \"127.0.0.1\" : \"localhost\";\r\n        String expectedOwner = \"router/\" + host + \"@EXAMPLE.COM\";\r\n        assertEquals(expectedOwner, owner);\r\n        assertEquals(\"some_renewer\", identifier.getRenewer().toString());\r\n    }\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "getUserGroupForTesting",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] getUserGroupForTesting()\n{\r\n    String[] groupsForTesting = { \"router_group\" };\r\n    return groupsForTesting;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "testWithoutSecretManager",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testWithoutSecretManager() throws Exception\n{\r\n    Configuration conf = initSecurity();\r\n    conf.set(DFS_ROUTER_DELEGATION_TOKEN_DRIVER_CLASS, ZKDelegationTokenSecretManagerImpl.class.getName());\r\n    Router router = new Router();\r\n    intercept(ServiceStateException.class, \"Failed to create SecretManager\", () -> router.init(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\security",
  "methodName" : "testNotRunningSecretManager",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testNotRunningSecretManager() throws Exception\n{\r\n    Configuration conf = initSecurity();\r\n    conf.set(DFS_ROUTER_DELEGATION_TOKEN_DRIVER_CLASS, MockNotRunningSecretManager.class.getName());\r\n    Router router = new Router();\r\n    intercept(ServiceStateException.class, \"Failed to create SecretManager\", () -> router.init(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    cluster = new StateStoreDFSCluster(false, 2, MultipleDestinationMountTableResolver.class);\r\n    Configuration routerConf = new RouterConfigBuilder().stateStore().admin().quota().rpc().build();\r\n    Configuration hdfsConf = new Configuration(false);\r\n    cluster.addNamenodeOverrides(hdfsConf);\r\n    cluster.addRouterOverrides(routerConf);\r\n    cluster.setNumDatanodesPerNameservice(9);\r\n    cluster.setIndependentDNs();\r\n    cluster.setRacks(new String[] { \"/rack1\", \"/rack1\", \"/rack1\", \"/rack2\", \"/rack2\", \"/rack2\", \"/rack3\", \"/rack3\", \"/rack3\", \"/rack4\", \"/rack4\", \"/rack4\", \"/rack5\", \"/rack5\", \"/rack5\", \"/rack6\", \"/rack6\", \"/rack6\" });\r\n    cluster.startCluster();\r\n    cluster.startRouters();\r\n    cluster.waitClusterUp();\r\n    routerContext = cluster.getRandomRouter();\r\n    routerFs = (DistributedFileSystem) routerContext.getFileSystem();\r\n    nnContext0 = cluster.getNamenode(\"ns0\", null);\r\n    nnContext1 = cluster.getNamenode(\"ns1\", null);\r\n    nnFs0 = (DistributedFileSystem) nnContext0.getFileSystem();\r\n    nnFs1 = (DistributedFileSystem) nnContext1.getFileSystem();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (cluster != null) {\r\n        cluster.stopRouter(routerContext);\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetECTopologyResultForPolicies",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testGetECTopologyResultForPolicies() throws IOException\n{\r\n    routerFs.enableErasureCodingPolicy(\"RS-6-3-1024k\");\r\n    ECTopologyVerifierResult result = routerFs.getECTopologyResultForPolicies();\r\n    assertTrue(result.isSupported());\r\n    result = routerFs.getECTopologyResultForPolicies(\"RS-10-4-1024k\");\r\n    assertFalse(result.isSupported());\r\n    result = routerFs.getECTopologyResultForPolicies(\"RS-10-4-1024k\", \"RS-3-2-1024k\");\r\n    assertFalse(result.isSupported());\r\n    result = routerFs.getECTopologyResultForPolicies(\"XOR-2-1-1024k\", \"RS-3-2-1024k\");\r\n    assertTrue(result.isSupported());\r\n    result = routerFs.getECTopologyResultForPolicies(\"RS-10-4-1024k\", \"RS-3-2-1024k\");\r\n    assertFalse(result.isSupported());\r\n    routerFs.enableErasureCodingPolicy(\"RS-10-4-1024k\");\r\n    result = routerFs.getECTopologyResultForPolicies();\r\n    assertFalse(result.isSupported());\r\n    nnFs0.disableErasureCodingPolicy(\"RS-10-4-1024k\");\r\n    nnFs1.enableErasureCodingPolicy(\"RS-10-4-1024k\");\r\n    result = routerFs.getECTopologyResultForPolicies();\r\n    assertFalse(result.isSupported());\r\n    nnFs1.disableErasureCodingPolicy(\"RS-10-4-1024k\");\r\n    nnFs0.enableErasureCodingPolicy(\"RS-10-4-1024k\");\r\n    result = routerFs.getECTopologyResultForPolicies();\r\n    assertFalse(result.isSupported());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "globalSetUp",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void globalSetUp() throws Exception\n{\r\n    cluster = new StateStoreDFSCluster(false, 1);\r\n    Configuration conf = new RouterConfigBuilder().stateStore().admin().rpc().build();\r\n    cluster.addRouterOverrides(conf);\r\n    cluster.startRouters();\r\n    routerContext = cluster.getRandomRouter();\r\n    Router router = routerContext.getRouter();\r\n    stateStore = router.getStateStore();\r\n    ActiveNamenodeResolver membership = router.getNamenodeResolver();\r\n    membership.registerNamenode(createNamenodeReport(\"ns0\", \"nn1\", HAServiceProtocol.HAServiceState.ACTIVE));\r\n    stateStore.refreshCaches(true);\r\n    routerConf = new Configuration();\r\n    InetSocketAddress routerSocket = router.getAdminServerAddress();\r\n    routerConf.setSocketAddr(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, routerSocket);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "testDisableWrite",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testDisableWrite() throws Exception\n{\r\n    String mount = \"/test-write\";\r\n    MountTable newEntry = MountTable.newInstance(mount, Collections.singletonMap(\"ns0\", mount), Time.now(), Time.now());\r\n    MountTableManager mountTable = routerContext.getAdminClient().getMountTableManager();\r\n    AddMountTableEntryRequest addRequest = AddMountTableEntryRequest.newInstance(newEntry);\r\n    AddMountTableEntryResponse addResponse = mountTable.addMountTableEntry(addRequest);\r\n    assertTrue(addResponse.getStatus());\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    URI address = routerContext.getFileSystemURI();\r\n    DFSClient routerClient = new DFSClient(address, routerConf);\r\n    FedBalanceContext context = new FedBalanceContext.Builder(null, null, mount, routerConf).build();\r\n    RouterDistCpProcedure dcProcedure = new RouterDistCpProcedure();\r\n    executeProcedure(dcProcedure, Stage.FINAL_DISTCP, () -> dcProcedure.disableWrite(context));\r\n    intercept(RemoteException.class, \"is in a read only mount point\", \"Expect readonly exception.\", () -> routerClient.mkdirs(mount + \"/dir\", new FsPermission(020), false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    cluster.stopRouter(routerContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "globalSetUp",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void globalSetUp() throws Exception\n{\r\n    cluster = new StateStoreDFSCluster(false, 1);\r\n    Configuration conf = new RouterConfigBuilder().stateStore().admin().rpc().build();\r\n    conf.setBoolean(RBFConfigKeys.DFS_ROUTER_ADMIN_MOUNT_CHECK_ENABLE, true);\r\n    cluster.addRouterOverrides(conf);\r\n    cluster.startRouters();\r\n    routerContext = cluster.getRandomRouter();\r\n    mockMountTable = cluster.generateMockMountTable();\r\n    Router router = routerContext.getRouter();\r\n    stateStore = router.getStateStore();\r\n    ActiveNamenodeResolver membership = router.getNamenodeResolver();\r\n    membership.registerNamenode(createNamenodeReport(\"ns0\", \"nn1\", HAServiceState.ACTIVE));\r\n    membership.registerNamenode(createNamenodeReport(\"ns1\", \"nn1\", HAServiceState.ACTIVE));\r\n    stateStore.refreshCaches(true);\r\n    setUpMocks();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setUpMocks",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void setUpMocks() throws IOException, NoSuchFieldException\n{\r\n    RouterRpcServer spyRpcServer = Mockito.spy(routerContext.getRouter().createRpcServer());\r\n    FieldSetter.setField(routerContext.getRouter(), Router.class.getDeclaredField(\"rpcServer\"), spyRpcServer);\r\n    Mockito.doReturn(null).when(spyRpcServer).getFileInfo(Mockito.anyString());\r\n    mockRpcClient = Mockito.spy(spyRpcServer.getRPCClient());\r\n    FieldSetter.setField(spyRpcServer, RouterRpcServer.class.getDeclaredField(\"rpcClient\"), mockRpcClient);\r\n    RemoteLocation remoteLocation0 = new RemoteLocation(\"ns0\", \"/testdir\", null);\r\n    RemoteLocation remoteLocation1 = new RemoteLocation(\"ns1\", \"/\", null);\r\n    final Map<RemoteLocation, HdfsFileStatus> mockResponse0 = new HashMap<>();\r\n    final Map<RemoteLocation, HdfsFileStatus> mockResponse1 = new HashMap<>();\r\n    mockResponse0.put(remoteLocation0, new HdfsFileStatus.Builder().build());\r\n    Mockito.doReturn(mockResponse0).when(mockRpcClient).invokeConcurrent(Mockito.eq(Lists.newArrayList(remoteLocation0)), Mockito.any(RemoteMethod.class), Mockito.eq(false), Mockito.eq(false), Mockito.eq(HdfsFileStatus.class));\r\n    mockResponse1.put(remoteLocation1, new HdfsFileStatus.Builder().build());\r\n    Mockito.doReturn(mockResponse1).when(mockRpcClient).invokeConcurrent(Mockito.eq(Lists.newArrayList(remoteLocation1)), Mockito.any(RemoteMethod.class), Mockito.eq(false), Mockito.eq(false), Mockito.eq(HdfsFileStatus.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    cluster.stopRouter(routerContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSetup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSetup() throws Exception\n{\r\n    assertTrue(synchronizeRecords(stateStore, mockMountTable, MountTable.class));\r\n    routerContext.resetAdminClient();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testAddMountTable",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testAddMountTable() throws IOException\n{\r\n    MountTable newEntry = MountTable.newInstance(\"/testpath\", Collections.singletonMap(\"ns0\", \"/testdir\"), Time.now(), Time.now());\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTable = client.getMountTableManager();\r\n    List<MountTable> records = getMountTableEntries(mountTable);\r\n    assertEquals(records.size(), mockMountTable.size());\r\n    AddMountTableEntryRequest addRequest = AddMountTableEntryRequest.newInstance(newEntry);\r\n    AddMountTableEntryResponse addResponse = mountTable.addMountTableEntry(addRequest);\r\n    assertTrue(addResponse.getStatus());\r\n    List<MountTable> records2 = getMountTableEntries(mountTable);\r\n    assertEquals(records2.size(), mockMountTable.size() + 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testAddDuplicateMountTable",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testAddDuplicateMountTable() throws IOException\n{\r\n    MountTable newEntry = MountTable.newInstance(\"/testpath\", Collections.singletonMap(\"ns0\", \"/testdir\"), Time.now(), Time.now());\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTable = client.getMountTableManager();\r\n    List<MountTable> entries1 = getMountTableEntries(mountTable);\r\n    assertEquals(entries1.size(), mockMountTable.size());\r\n    AddMountTableEntryRequest addRequest = AddMountTableEntryRequest.newInstance(newEntry);\r\n    AddMountTableEntryResponse addResponse = mountTable.addMountTableEntry(addRequest);\r\n    assertTrue(addResponse.getStatus());\r\n    List<MountTable> entries2 = getMountTableEntries(mountTable);\r\n    assertEquals(entries2.size(), mockMountTable.size() + 1);\r\n    AddMountTableEntryResponse addResponse2 = mountTable.addMountTableEntry(addRequest);\r\n    assertFalse(addResponse2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testAddReadOnlyMountTable",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testAddReadOnlyMountTable() throws IOException\n{\r\n    MountTable newEntry = MountTable.newInstance(\"/readonly\", Collections.singletonMap(\"ns0\", \"/testdir\"), Time.now(), Time.now());\r\n    newEntry.setReadOnly(true);\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTable = client.getMountTableManager();\r\n    List<MountTable> records = getMountTableEntries(mountTable);\r\n    assertEquals(records.size(), mockMountTable.size());\r\n    AddMountTableEntryRequest addRequest = AddMountTableEntryRequest.newInstance(newEntry);\r\n    AddMountTableEntryResponse addResponse = mountTable.addMountTableEntry(addRequest);\r\n    assertTrue(addResponse.getStatus());\r\n    List<MountTable> records2 = getMountTableEntries(mountTable);\r\n    assertEquals(records2.size(), mockMountTable.size() + 1);\r\n    MountTable record = getMountTableEntry(\"/readonly\");\r\n    assertEquals(\"/readonly\", record.getSourcePath());\r\n    assertTrue(record.isReadOnly());\r\n    RemoveMountTableEntryRequest removeRequest = RemoveMountTableEntryRequest.newInstance(\"/readonly\");\r\n    RemoveMountTableEntryResponse removeResponse = mountTable.removeMountTableEntry(removeRequest);\r\n    assertTrue(removeResponse.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testAddOrderMountTable",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAddOrderMountTable() throws IOException\n{\r\n    testAddOrderMountTable(DestinationOrder.HASH);\r\n    testAddOrderMountTable(DestinationOrder.LOCAL);\r\n    testAddOrderMountTable(DestinationOrder.RANDOM);\r\n    testAddOrderMountTable(DestinationOrder.HASH_ALL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testAddOrderMountTable",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testAddOrderMountTable(final DestinationOrder order) throws IOException\n{\r\n    final String mnt = \"/\" + order;\r\n    MountTable newEntry = MountTable.newInstance(mnt, Collections.singletonMap(\"ns0\", \"/testdir\"), Time.now(), Time.now());\r\n    newEntry.setDestOrder(order);\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTable = client.getMountTableManager();\r\n    AddMountTableEntryRequest addRequest;\r\n    AddMountTableEntryResponse addResponse;\r\n    addRequest = AddMountTableEntryRequest.newInstance(newEntry);\r\n    addResponse = mountTable.addMountTableEntry(addRequest);\r\n    assertTrue(addResponse.getStatus());\r\n    MountTable record = getMountTableEntry(mnt);\r\n    assertEquals(mnt, record.getSourcePath());\r\n    assertEquals(order, record.getDestOrder());\r\n    RemoveMountTableEntryRequest removeRequest = RemoveMountTableEntryRequest.newInstance(mnt);\r\n    RemoveMountTableEntryResponse removeResponse = mountTable.removeMountTableEntry(removeRequest);\r\n    assertTrue(removeResponse.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRemoveMountTable",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRemoveMountTable() throws IOException\n{\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTable = client.getMountTableManager();\r\n    List<MountTable> entries1 = getMountTableEntries(mountTable);\r\n    assertEquals(entries1.size(), mockMountTable.size());\r\n    RemoveMountTableEntryRequest removeRequest = RemoveMountTableEntryRequest.newInstance(\"/\");\r\n    mountTable.removeMountTableEntry(removeRequest);\r\n    List<MountTable> entries2 = getMountTableEntries(mountTable);\r\n    assertEquals(entries2.size(), mockMountTable.size() - 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testEditMountTable",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testEditMountTable() throws IOException\n{\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTable = client.getMountTableManager();\r\n    MountTable entry = getMountTableEntry(\"/\");\r\n    assertEquals(Collections.singletonList(new RemoteLocation(\"ns0\", \"/\", \"/\")), entry.getDestinations());\r\n    MountTable updatedEntry = MountTable.newInstance(\"/\", Collections.singletonMap(\"ns1\", \"/\"), Time.now(), Time.now());\r\n    UpdateMountTableEntryRequest updateRequest = UpdateMountTableEntryRequest.newInstance(updatedEntry);\r\n    mountTable.updateMountTableEntry(updateRequest);\r\n    entry = getMountTableEntry(\"/\");\r\n    assertEquals(Collections.singletonList(new RemoteLocation(\"ns1\", \"/\", \"/\")), entry.getDestinations());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetMountTable",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGetMountTable() throws IOException\n{\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTable = client.getMountTableManager();\r\n    List<MountTable> entries = getMountTableEntries(mountTable);\r\n    assertEquals(mockMountTable.size(), entries.size());\r\n    int matches = 0;\r\n    for (MountTable e : entries) {\r\n        for (MountTable entry : mockMountTable) {\r\n            assertEquals(e.getDestinations().size(), 1);\r\n            assertNotNull(e.getDateCreated());\r\n            assertNotNull(e.getDateModified());\r\n            if (entry.getSourcePath().equals(e.getSourcePath())) {\r\n                matches++;\r\n            }\r\n        }\r\n    }\r\n    assertEquals(matches, mockMountTable.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetSingleMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGetSingleMountTableEntry() throws IOException\n{\r\n    MountTable entry = getMountTableEntry(\"/ns0\");\r\n    assertNotNull(entry);\r\n    assertEquals(entry.getSourcePath(), \"/ns0\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testVerifyFileInDestinations",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testVerifyFileInDestinations() throws IOException\n{\r\n    MountTable newEntry = MountTable.newInstance(\"/testpath\", Collections.singletonMap(\"ns0\", \"/testdir\"), Time.now(), Time.now());\r\n    RouterAdminServer adminServer = this.routerContext.getRouter().getAdminServer();\r\n    List<String> result = adminServer.verifyFileInDestinations(newEntry);\r\n    assertEquals(0, result.size());\r\n    newEntry = MountTable.newInstance(\"/testpath\", Collections.singletonMap(\"ns0\", \"/testdir1\"), Time.now(), Time.now());\r\n    result = adminServer.verifyFileInDestinations(newEntry);\r\n    assertEquals(1, result.size());\r\n    assertEquals(\"ns0\", result.get(0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "MountTable getMountTableEntry(final String mount) throws IOException\n{\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesRequest request = GetMountTableEntriesRequest.newInstance(mount);\r\n    RouterClient client = routerContext.getAdminClient();\r\n    MountTableManager mountTable = client.getMountTableManager();\r\n    List<MountTable> results = getMountTableEntries(mountTable, request);\r\n    if (results.size() > 0) {\r\n        return results.get(0);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTableEntries",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<MountTable> getMountTableEntries(MountTableManager mountTable) throws IOException\n{\r\n    GetMountTableEntriesRequest request = GetMountTableEntriesRequest.newInstance(\"/\");\r\n    return getMountTableEntries(mountTable, request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTableEntries",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<MountTable> getMountTableEntries(MountTableManager mountTable, GetMountTableEntriesRequest request) throws IOException\n{\r\n    stateStore.loadCache(MountTableStoreImpl.class, true);\r\n    GetMountTableEntriesResponse response = mountTable.getMountTableEntries(request);\r\n    return response.getEntries();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNameserviceManager",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testNameserviceManager() throws IOException\n{\r\n    RouterClient client = routerContext.getAdminClient();\r\n    NameserviceManager nsManager = client.getNameserviceManager();\r\n    Set<String> disabled = getDisabledNameservices(nsManager);\r\n    assertTrue(disabled.isEmpty());\r\n    DisableNameserviceRequest disableReq = DisableNameserviceRequest.newInstance(\"ns0\");\r\n    DisableNameserviceResponse disableResp = nsManager.disableNameservice(disableReq);\r\n    assertTrue(disableResp.getStatus());\r\n    disabled = getDisabledNameservices(nsManager);\r\n    assertEquals(1, disabled.size());\r\n    assertTrue(disabled.contains(\"ns0\"));\r\n    EnableNameserviceRequest enableReq = EnableNameserviceRequest.newInstance(\"ns0\");\r\n    EnableNameserviceResponse enableResp = nsManager.enableNameservice(enableReq);\r\n    assertTrue(enableResp.getStatus());\r\n    disabled = getDisabledNameservices(nsManager);\r\n    assertTrue(disabled.isEmpty());\r\n    disableReq = DisableNameserviceRequest.newInstance(\"nsunknown\");\r\n    disableResp = nsManager.disableNameservice(disableReq);\r\n    assertFalse(disableResp.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNameserviceManagerUser",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DisableNameserviceResponse testNameserviceManagerUser(String username) throws Exception\n{\r\n    UserGroupInformation user = UserGroupInformation.createRemoteUser(username);\r\n    return user.doAs((PrivilegedExceptionAction<DisableNameserviceResponse>) () -> {\r\n        RouterClient client = routerContext.getAdminClient();\r\n        NameserviceManager nameservices = client.getNameserviceManager();\r\n        DisableNameserviceRequest disableReq = DisableNameserviceRequest.newInstance(\"ns0\");\r\n        return nameservices.disableNameservice(disableReq);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNameserviceManagerUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNameserviceManagerUnauthorized() throws Exception\n{\r\n    String username = \"baduser\";\r\n    LambdaTestUtils.intercept(IOException.class, username + \" is not a super user\", () -> testNameserviceManagerUser(username));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNameserviceManagerWithRules",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testNameserviceManagerWithRules() throws Exception\n{\r\n    String username = RouterAdminServer.getSuperUser() + \"@Example.com\";\r\n    DisableNameserviceResponse disableResp = testNameserviceManagerUser(username);\r\n    assertTrue(disableResp.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDisabledNameservices",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Set<String> getDisabledNameservices(NameserviceManager nsManager) throws IOException\n{\r\n    stateStore.loadCache(DisabledNameserviceStoreImpl.class, true);\r\n    GetDisabledNameservicesRequest getReq = GetDisabledNameservicesRequest.newInstance();\r\n    GetDisabledNameservicesResponse response = nsManager.getDisabledNameservices(getReq);\r\n    return response.getNameservices();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void create() throws IOException\n{\r\n    conf = new Configuration();\r\n    conf.setInt(RBFConfigKeys.DFS_ROUTER_CACHE_TIME_TO_LIVE_MS, 1);\r\n    conf.setClass(RBFConfigKeys.FEDERATION_NAMENODE_RESOLVER_CLIENT_CLASS, MockResolver.class, ActiveNamenodeResolver.class);\r\n    conf.setClass(RBFConfigKeys.FEDERATION_FILE_RESOLVER_CLIENT_CLASS, MockResolver.class, FileSubclusterResolver.class);\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_RPC_BIND_HOST_KEY, \"0.0.0.0\");\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_RPC_ADDRESS_KEY, \"127.0.0.1:0\");\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, \"127.0.0.1:0\");\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_ADMIN_BIND_HOST_KEY, \"0.0.0.0\");\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_HTTP_ADDRESS_KEY, \"127.0.0.1:0\");\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_HTTPS_ADDRESS_KEY, \"127.0.0.1:0\");\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_HTTP_BIND_HOST_KEY, \"0.0.0.0\");\r\n    conf.set(DFSConfigKeys.DFS_NAMESERVICES, \"ns0\");\r\n    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, \"hdfs://\" + \"ns0\");\r\n    conf.set(DFSConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY + \".\" + \"ns0\", \"127.0.0.1:0\" + 0);\r\n    conf.set(DFSConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY + \".\" + \"ns0\", \"127.0.0.1:\" + 0);\r\n    conf.set(DFSConfigKeys.DFS_NAMENODE_RPC_BIND_HOST_KEY + \".\" + \"ns0\", \"0.0.0.0\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRouterStartup",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testRouterStartup(Configuration routerConfig) throws InterruptedException, IOException\n{\r\n    Router router = new Router();\r\n    assertEquals(STATE.NOTINITED, router.getServiceState());\r\n    assertEquals(RouterServiceState.UNINITIALIZED, router.getRouterState());\r\n    router.init(routerConfig);\r\n    if (routerConfig.getBoolean(RBFConfigKeys.DFS_ROUTER_SAFEMODE_ENABLE, RBFConfigKeys.DFS_ROUTER_SAFEMODE_ENABLE_DEFAULT)) {\r\n        assertEquals(RouterServiceState.SAFEMODE, router.getRouterState());\r\n    } else {\r\n        assertEquals(RouterServiceState.INITIALIZING, router.getRouterState());\r\n    }\r\n    assertEquals(STATE.INITED, router.getServiceState());\r\n    router.start();\r\n    if (routerConfig.getBoolean(RBFConfigKeys.DFS_ROUTER_SAFEMODE_ENABLE, RBFConfigKeys.DFS_ROUTER_SAFEMODE_ENABLE_DEFAULT)) {\r\n        assertEquals(RouterServiceState.SAFEMODE, router.getRouterState());\r\n    } else {\r\n        assertEquals(RouterServiceState.RUNNING, router.getRouterState());\r\n    }\r\n    assertEquals(STATE.STARTED, router.getServiceState());\r\n    router.stop();\r\n    assertEquals(RouterServiceState.SHUTDOWN, router.getRouterState());\r\n    assertEquals(STATE.STOPPED, router.getServiceState());\r\n    router.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRouterService",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRouterService() throws InterruptedException, IOException\n{\r\n    testRouterStartup(new RouterConfigBuilder(conf).admin().build());\r\n    testRouterStartup(new RouterConfigBuilder(conf).http().build());\r\n    testRouterStartup(new RouterConfigBuilder(conf).rpc().build());\r\n    testRouterStartup(new RouterConfigBuilder(conf).rpc().safemode().build());\r\n    testRouterStartup(new RouterConfigBuilder(conf).metrics().build());\r\n    testRouterStartup(new RouterConfigBuilder(conf).stateStore().build());\r\n    testRouterStartup(new RouterConfigBuilder(conf).heartbeat().build());\r\n    testRouterStartup(new RouterConfigBuilder(conf).all().build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRouterRestartRpcService",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRouterRestartRpcService() throws IOException\n{\r\n    Router router = new Router();\r\n    router.init(new RouterConfigBuilder(conf).rpc().build());\r\n    router.start();\r\n    assertNotNull(router.getRpcServerAddress());\r\n    RouterRpcServer rpcServer = router.getRpcServer();\r\n    assertNotNull(rpcServer);\r\n    assertEquals(STATE.STARTED, rpcServer.getServiceState());\r\n    router.stop();\r\n    assertEquals(STATE.STOPPED, rpcServer.getServiceState());\r\n    router.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRouterRpcWithNoSubclusters",
  "errType" : [ "RemoteException", "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRouterRpcWithNoSubclusters() throws IOException\n{\r\n    Router router = new Router();\r\n    router.init(new RouterConfigBuilder(conf).rpc().build());\r\n    router.start();\r\n    InetSocketAddress serverAddress = router.getRpcServerAddress();\r\n    DFSClient dfsClient = new DFSClient(serverAddress, conf);\r\n    try {\r\n        dfsClient.create(\"/test.txt\", false);\r\n        fail(\"Create with no subclusters should fail\");\r\n    } catch (RemoteException e) {\r\n        assertExceptionContains(\"Cannot find locations for /test.txt\", e);\r\n    }\r\n    try {\r\n        dfsClient.datanodeReport(DatanodeReportType.LIVE);\r\n        fail(\"Get datanode reports with no subclusters should fail\");\r\n    } catch (IOException e) {\r\n        assertExceptionContains(\"No remote locations available\", e);\r\n    }\r\n    dfsClient.close();\r\n    router.stop();\r\n    router.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRouterIDInRouterRpcClient",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testRouterIDInRouterRpcClient() throws Exception\n{\r\n    Router router = new Router();\r\n    router.init(new RouterConfigBuilder(conf).rpc().build());\r\n    router.setRouterId(\"Router-0\");\r\n    RemoteMethod remoteMethod = mock(RemoteMethod.class);\r\n    intercept(IOException.class, \"Router-0\", () -> router.getRpcServer().getRPCClient().invokeSingle(\"ns0\", remoteMethod));\r\n    router.stop();\r\n    router.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRouterMetricsWhenDisabled",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRouterMetricsWhenDisabled() throws Exception\n{\r\n    Router router = new Router();\r\n    router.init(new RouterConfigBuilder(conf).rpc().build());\r\n    router.start();\r\n    intercept(IOException.class, \"Namenode metrics is not initialized\", () -> router.getNamenodeMetrics().getCacheCapacity());\r\n    router.stop();\r\n    router.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSwitchRouter",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSwitchRouter() throws IOException\n{\r\n    assertRouterHeartbeater(true, true, true);\r\n    assertRouterHeartbeater(true, true, false);\r\n    assertRouterHeartbeater(true, false, true);\r\n    assertRouterHeartbeater(true, false, false);\r\n    assertRouterHeartbeater(false, true, true);\r\n    assertRouterHeartbeater(false, true, false);\r\n    assertRouterHeartbeater(false, false, true);\r\n    assertRouterHeartbeater(false, false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "assertRouterHeartbeater",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void assertRouterHeartbeater(boolean enableRpcServer, boolean expectedRouterHeartbeat, boolean expectedNNHeartbeat) throws IOException\n{\r\n    final Router router = new Router();\r\n    Configuration baseCfg = new RouterConfigBuilder(conf).rpc(enableRpcServer).build();\r\n    baseCfg.setBoolean(RBFConfigKeys.DFS_ROUTER_HEARTBEAT_ENABLE, expectedRouterHeartbeat);\r\n    baseCfg.setBoolean(RBFConfigKeys.DFS_ROUTER_NAMENODE_HEARTBEAT_ENABLE, expectedNNHeartbeat);\r\n    router.init(baseCfg);\r\n    assertNotNull(router.getRouterId());\r\n    RouterHeartbeatService routerHeartbeatService = router.getRouterHeartbeatService();\r\n    if (expectedRouterHeartbeat) {\r\n        assertNotNull(routerHeartbeatService);\r\n    } else {\r\n        assertNull(routerHeartbeatService);\r\n    }\r\n    Collection<NamenodeHeartbeatService> namenodeHeartbeatServices = router.getNamenodeHeartbeatServices();\r\n    if (expectedNNHeartbeat) {\r\n        assertNotNull(namenodeHeartbeatServices);\r\n    } else {\r\n        assertNull(namenodeHeartbeatServices);\r\n    }\r\n    router.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNamenodeHeartBeatEnableDefault",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testNamenodeHeartBeatEnableDefault() throws IOException\n{\r\n    checkNamenodeHeartBeatEnableDefault(true);\r\n    checkNamenodeHeartBeatEnableDefault(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkNamenodeHeartBeatEnableDefault",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void checkNamenodeHeartBeatEnableDefault(boolean enable) throws IOException\n{\r\n    try (Router router = new Router()) {\r\n        Configuration config = new HdfsConfiguration();\r\n        config.set(RBFConfigKeys.DFS_ROUTER_RPC_BIND_HOST_KEY, \"0.0.0.0\");\r\n        config.set(RBFConfigKeys.DFS_ROUTER_RPC_ADDRESS_KEY, \"127.0.0.1:0\");\r\n        config.set(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, \"127.0.0.1:0\");\r\n        config.set(RBFConfigKeys.DFS_ROUTER_ADMIN_BIND_HOST_KEY, \"0.0.0.0\");\r\n        config.set(RBFConfigKeys.DFS_ROUTER_HTTP_ADDRESS_KEY, \"127.0.0.1:0\");\r\n        config.set(RBFConfigKeys.DFS_ROUTER_HTTPS_ADDRESS_KEY, \"127.0.0.1:0\");\r\n        config.set(RBFConfigKeys.DFS_ROUTER_HTTP_BIND_HOST_KEY, \"0.0.0.0\");\r\n        config.setBoolean(RBFConfigKeys.DFS_ROUTER_HEARTBEAT_ENABLE, enable);\r\n        router.init(config);\r\n        if (enable) {\r\n            assertNotNull(router.getNamenodeHeartbeatServices());\r\n        } else {\r\n            assertNull(router.getNamenodeHeartbeatServices());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterWebHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterWebHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterWebHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws Exception\n{\r\n    RouterHDFSContract.createCluster(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterWebHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterWebHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterWebHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "testOpenReadDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testOpenReadDir() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "testOpenReadDirWithChild",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testOpenReadDirWithChild() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "before",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void before() throws Exception\n{\r\n    globalSetUp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "after",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void after()\n{\r\n    tearDown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSetup",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testSetup() throws Exception\n{\r\n    setup();\r\n    cluster = getCluster();\r\n    List<String> nss = cluster.getNameservices();\r\n    srcNs = nss.get(0);\r\n    dstNs = nss.get(1);\r\n    srcStr = cluster.getFederatedTestDirectoryForNS(srcNs) + \"/d0/\" + getMethodName();\r\n    dstStr = cluster.getFederatedTestDirectoryForNS(dstNs) + \"/d0/\" + getMethodName();\r\n    srcPath = new Path(srcStr);\r\n    dstPath = new Path(dstStr);\r\n    foo = UserGroupInformation.createRemoteUser(\"foo\");\r\n    router = getRouterContext();\r\n    routerFS = getRouterFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRenameSnapshotPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRenameSnapshotPath() throws Exception\n{\r\n    LambdaTestUtils.intercept(IOException.class, \"Router federation rename can't rename snapshot path\", \"Expect IOException.\", () -> RouterFederationRename.checkSnapshotPath(new RemoteLocation(srcNs, \"/foo/.snapshot/src\", \"/src\"), new RemoteLocation(dstNs, \"/foo/dst\", \"/dst\")));\r\n    LambdaTestUtils.intercept(IOException.class, \"Router federation rename can't rename snapshot path\", \"Expect IOException.\", () -> RouterFederationRename.checkSnapshotPath(new RemoteLocation(srcNs, \"/foo/src\", \"/src\"), new RemoteLocation(dstNs, \"/foo/.snapshot/dst\", \"/dst\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testPermission1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testPermission1() throws Exception\n{\r\n    LambdaTestUtils.intercept(RemoteException.class, \"FileNotFoundException\", \"Expect FileNotFoundException.\", () -> {\r\n        DFSClient client = router.getClient(foo);\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename(srcStr, dstStr);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testPermission2",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testPermission2() throws Exception\n{\r\n    createDir(routerFS, srcStr);\r\n    routerFS.setPermission(srcPath.getParent(), FsPermission.createImmutable((short) 0));\r\n    LambdaTestUtils.intercept(RemoteException.class, \"AccessControlException\", \"Expect AccessControlException.\", () -> {\r\n        DFSClient client = router.getClient(foo);\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename(srcStr, dstStr);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testPermission3",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testPermission3() throws Exception\n{\r\n    createDir(routerFS, srcStr);\r\n    routerFS.setPermission(srcPath.getParent(), FsPermission.createImmutable((short) 493));\r\n    LambdaTestUtils.intercept(RemoteException.class, \"AccessControlException\", \"Expect AccessControlException.\", () -> {\r\n        DFSClient client = router.getClient(foo);\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename(srcStr, dstStr);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testPermission4",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testPermission4() throws Exception\n{\r\n    createDir(routerFS, srcStr);\r\n    routerFS.setAcl(srcPath.getParent(), buildAcl(\"not-foo\", ALL));\r\n    LambdaTestUtils.intercept(RemoteException.class, \"AccessControlException\", \"Expect AccessControlException.\", () -> {\r\n        DFSClient client = router.getClient(foo);\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename(srcStr, dstStr);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testPermission5",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testPermission5() throws Exception\n{\r\n    createDir(routerFS, srcStr);\r\n    routerFS.setAcl(srcPath.getParent(), buildAcl(\"foo\", ALL));\r\n    assertFalse(routerFS.exists(dstPath.getParent()));\r\n    LambdaTestUtils.intercept(RemoteException.class, \"FileNotFoundException\", \"Expect FileNotFoundException.\", () -> {\r\n        DFSClient client = router.getClient(foo);\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename(srcStr, dstStr);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testPermission6",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testPermission6() throws Exception\n{\r\n    createDir(routerFS, srcStr);\r\n    routerFS.setAcl(srcPath.getParent(), buildAcl(\"foo\", ALL));\r\n    assertTrue(routerFS.mkdirs(dstPath.getParent()));\r\n    LambdaTestUtils.intercept(RemoteException.class, \"AccessControlException\", \"Expect AccessControlException.\", () -> {\r\n        DFSClient client = router.getClient(foo);\r\n        ClientProtocol clientProtocol = client.getNamenode();\r\n        clientProtocol.rename(srcStr, dstStr);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testPermission7",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testPermission7() throws Exception\n{\r\n    createDir(routerFS, srcStr);\r\n    routerFS.setAcl(srcPath.getParent(), buildAcl(\"foo\", ALL));\r\n    assertTrue(routerFS.mkdirs(dstPath.getParent()));\r\n    routerFS.setOwner(dstPath.getParent(), \"foo\", \"foogroup\");\r\n    DFSClient client = router.getClient(foo);\r\n    ClientProtocol clientProtocol = client.getNamenode();\r\n    clientProtocol.rename(srcStr, dstStr);\r\n    assertFalse(verifyFileExists(routerFS, srcStr));\r\n    assertTrue(verifyFileExists(routerFS, dstStr + \"/file\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "buildAcl",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<AclEntry> buildAcl(String user, FsAction permission)\n{\r\n    List<AclEntry> aclEntryList = Lists.newArrayList();\r\n    aclEntryList.add(new AclEntry.Builder().setName(user).setPermission(permission).setScope(AclEntryScope.ACCESS).setType(AclEntryType.USER).build());\r\n    aclEntryList.add(new AclEntry.Builder().setPermission(FsAction.ALL).setScope(AclEntryScope.ACCESS).setType(AclEntryType.USER).build());\r\n    aclEntryList.add(new AclEntry.Builder().setPermission(FsAction.ALL).setScope(AclEntryScope.ACCESS).setType(AclEntryType.GROUP).build());\r\n    aclEntryList.add(new AclEntry.Builder().setPermission(READ_EXECUTE).setScope(AclEntryScope.ACCESS).setType(AclEntryType.OTHER).build());\r\n    return aclEntryList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createCluster() throws Exception\n{\r\n    RouterHDFSContract.createCluster(true);\r\n    RouterHDFSContract.getFileSystem().getDefaultBlockSize(new Path(\"/\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "all",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterConfigBuilder all()\n{\r\n    this.enableRpcServer = true;\r\n    this.enableAdminServer = true;\r\n    this.enableHttpServer = true;\r\n    this.enableHeartbeat = true;\r\n    this.enableLocalHeartbeat = true;\r\n    this.enableStateStore = true;\r\n    this.enableMetrics = true;\r\n    this.enableSafemode = true;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "enableLocalHeartbeat",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterConfigBuilder enableLocalHeartbeat(boolean enable)\n{\r\n    this.enableLocalHeartbeat = enable;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "rpc",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterConfigBuilder rpc(boolean enable)\n{\r\n    this.enableRpcServer = enable;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "admin",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterConfigBuilder admin(boolean enable)\n{\r\n    this.enableAdminServer = enable;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "http",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterConfigBuilder http(boolean enable)\n{\r\n    this.enableHttpServer = enable;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "heartbeat",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterConfigBuilder heartbeat(boolean enable)\n{\r\n    this.enableHeartbeat = enable;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "stateStore",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterConfigBuilder stateStore(boolean enable)\n{\r\n    this.enableStateStore = enable;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "metrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterConfigBuilder metrics(boolean enable)\n{\r\n    this.enableMetrics = enable;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "routerRenameOption",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterConfigBuilder routerRenameOption(RouterRenameOption option)\n{\r\n    this.routerRenameOption = option;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "quota",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterConfigBuilder quota(boolean enable)\n{\r\n    this.enableQuota = enable;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "safemode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterConfigBuilder safemode(boolean enable)\n{\r\n    this.enableSafemode = enable;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "refreshCache",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterConfigBuilder refreshCache(boolean enable)\n{\r\n    this.enableCacheRefresh = enable;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "rpc",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterConfigBuilder rpc()\n{\r\n    return this.rpc(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "admin",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterConfigBuilder admin()\n{\r\n    return this.admin(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "http",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterConfigBuilder http()\n{\r\n    return this.http(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "heartbeat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterConfigBuilder heartbeat()\n{\r\n    return this.heartbeat(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "stateStore",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RouterConfigBuilder stateStore()\n{\r\n    conf.setClass(RBFConfigKeys.FEDERATION_STORE_DRIVER_CLASS, FederationStateStoreTestUtils.getTestDriverClass(), StateStoreDriver.class);\r\n    return this.stateStore(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "metrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterConfigBuilder metrics()\n{\r\n    return this.metrics(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "routerRenameOption",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterConfigBuilder routerRenameOption()\n{\r\n    return this.routerRenameOption(RouterRenameOption.DISTCP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "quota",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterConfigBuilder quota()\n{\r\n    return this.quota(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "safemode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterConfigBuilder safemode()\n{\r\n    return this.safemode(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "refreshCache",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterConfigBuilder refreshCache()\n{\r\n    return this.refreshCache(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "set",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterConfigBuilder set(String key, String value)\n{\r\n    if (key != null && value != null) {\r\n        innerMap.put(key, value);\r\n    }\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "build",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "Configuration build()\n{\r\n    conf.setBoolean(RBFConfigKeys.DFS_ROUTER_STORE_ENABLE, this.enableStateStore);\r\n    conf.setBoolean(RBFConfigKeys.DFS_ROUTER_RPC_ENABLE, this.enableRpcServer);\r\n    if (this.enableRpcServer) {\r\n        conf.set(RBFConfigKeys.DFS_ROUTER_RPC_ADDRESS_KEY, \"127.0.0.1:0\");\r\n        conf.set(RBFConfigKeys.DFS_ROUTER_RPC_BIND_HOST_KEY, \"0.0.0.0\");\r\n    }\r\n    conf.setBoolean(RBFConfigKeys.DFS_ROUTER_ADMIN_ENABLE, this.enableAdminServer);\r\n    if (this.enableAdminServer) {\r\n        conf.set(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, \"127.0.0.1:0\");\r\n        conf.set(RBFConfigKeys.DFS_ROUTER_ADMIN_BIND_HOST_KEY, \"0.0.0.0\");\r\n    }\r\n    conf.setBoolean(RBFConfigKeys.DFS_ROUTER_HTTP_ENABLE, this.enableHttpServer);\r\n    if (this.enableHttpServer) {\r\n        conf.set(RBFConfigKeys.DFS_ROUTER_HTTP_ADDRESS_KEY, \"127.0.0.1:0\");\r\n        conf.set(RBFConfigKeys.DFS_ROUTER_HTTPS_ADDRESS_KEY, \"127.0.0.1:0\");\r\n        conf.set(RBFConfigKeys.DFS_ROUTER_HTTP_BIND_HOST_KEY, \"0.0.0.0\");\r\n    }\r\n    conf.setBoolean(RBFConfigKeys.DFS_ROUTER_HEARTBEAT_ENABLE, this.enableHeartbeat);\r\n    conf.setBoolean(RBFConfigKeys.DFS_ROUTER_MONITOR_LOCAL_NAMENODE, this.enableLocalHeartbeat);\r\n    conf.setBoolean(RBFConfigKeys.DFS_ROUTER_METRICS_ENABLE, this.enableMetrics);\r\n    conf.setBoolean(RBFConfigKeys.DFS_ROUTER_QUOTA_ENABLE, this.enableQuota);\r\n    conf.setBoolean(RBFConfigKeys.DFS_ROUTER_SAFEMODE_ENABLE, this.enableSafemode);\r\n    conf.setBoolean(RBFConfigKeys.MOUNT_TABLE_CACHE_UPDATE, this.enableCacheRefresh);\r\n    conf.set(DFS_ROUTER_FEDERATION_RENAME_OPTION, routerRenameOption.name());\r\n    for (Map.Entry<String, String> kv : innerMap.entrySet()) {\r\n        conf.set(kv.getKey(), kv.getValue());\r\n    }\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "globalSetUp",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void globalSetUp() throws Exception\n{\r\n    cluster = new MiniRouterDFSCluster(false, 1);\r\n    cluster.setNumDatanodesPerNameservice(2);\r\n    cluster.startCluster();\r\n    Configuration routerConf = new RouterConfigBuilder().metrics().rpc().build();\r\n    routerConf.setTimeDuration(RBFConfigKeys.DN_REPORT_CACHE_EXPIRE, 1, TimeUnit.SECONDS);\r\n    cluster.addRouterOverrides(routerConf);\r\n    cluster.startRouters();\r\n    cluster.registerNamenodes();\r\n    cluster.waitNamenodeRegistration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    cluster.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSetup",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testSetup() throws Exception\n{\r\n    cluster.installMockLocations();\r\n    cluster.deleteAllFiles();\r\n    cluster.createTestDirectoriesNamenode();\r\n    Thread.sleep(100);\r\n    MiniRouterDFSCluster.RouterContext rndRouter = cluster.getRandomRouter();\r\n    this.setRouter(rndRouter);\r\n    String ns0 = cluster.getNameservices().get(0);\r\n    this.setNs(ns0);\r\n    this.setNamenode(cluster.getNamenode(ns0, null));\r\n    Random rnd = new Random();\r\n    String randomFile = \"testfile-\" + rnd.nextInt();\r\n    this.nnFile = cluster.getNamenodeTestDirectoryForNS(ns) + \"/\" + randomFile;\r\n    this.routerFile = cluster.getFederatedTestDirectoryForNS(ns) + \"/\" + randomFile;\r\n    createFile(nnFS, nnFile, 32);\r\n    verifyFileExists(nnFS, nnFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setRouter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setRouter(MiniRouterDFSCluster.RouterContext r) throws IOException, URISyntaxException\n{\r\n    this.router = r;\r\n    this.routerProtocol = r.getClient().getNamenode();\r\n    this.routerFS = r.getFileSystem();\r\n    this.routerNamenodeProtocol = NameNodeProxies.createProxy(router.getConf(), router.getFileSystem().getUri(), NamenodeProtocol.class).getProxy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setNs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNs(String nameservice)\n{\r\n    this.ns = nameservice;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setNamenode",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setNamenode(MiniRouterDFSCluster.NamenodeContext nn) throws IOException, URISyntaxException\n{\r\n    this.namenode = nn;\r\n    this.nnProtocol = nn.getClient().getNamenode();\r\n    this.nnFS = nn.getFileSystem();\r\n    String ns0 = cluster.getNameservices().get(0);\r\n    MiniRouterDFSCluster.NamenodeContext nn0 = cluster.getNamenode(ns0, null);\r\n    this.nnNamenodeProtocol = NameNodeProxies.createProxy(nn0.getConf(), nn0.getFileSystem().getUri(), NamenodeProtocol.class).getProxy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetCurrentTXIDandRollEdits",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGetCurrentTXIDandRollEdits() throws IOException\n{\r\n    Long rollEdits = routerProtocol.rollEdits();\r\n    Long currentTXID = routerProtocol.getCurrentEditLogTxid();\r\n    assertEquals(rollEdits, currentTXID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSaveNamespace",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSaveNamespace() throws IOException\n{\r\n    cluster.getCluster().getFileSystem().setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER);\r\n    Boolean saveNamespace = routerProtocol.saveNamespace(0, 0);\r\n    assertTrue(saveNamespace);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    resolver = new MultipleDestinationMountTableResolver(conf, null);\r\n    Map<String, String> map1 = new HashMap<>();\r\n    map1.put(\"subcluster0\", \"/tmp\");\r\n    resolver.addEntry(MountTable.newInstance(\"/tmp\", map1));\r\n    Map<String, String> mapDefault = new HashMap<>();\r\n    mapDefault.put(\"subcluster0\", \"/\");\r\n    mapDefault.put(\"subcluster1\", \"/\");\r\n    mapDefault.put(\"subcluster2\", \"/\");\r\n    MountTable defaultEntry = MountTable.newInstance(\"/\", mapDefault);\r\n    resolver.addEntry(defaultEntry);\r\n    Map<String, String> mapHash = new HashMap<>();\r\n    mapHash.put(\"subcluster0\", \"/hash\");\r\n    mapHash.put(\"subcluster1\", \"/hash\");\r\n    mapHash.put(\"subcluster2\", \"/hash\");\r\n    MountTable hashEntry = MountTable.newInstance(\"/hash\", mapHash);\r\n    hashEntry.setDestOrder(DestinationOrder.HASH);\r\n    resolver.addEntry(hashEntry);\r\n    Map<String, String> mapHashAll = new HashMap<>();\r\n    mapHashAll.put(\"subcluster0\", \"/hashall\");\r\n    mapHashAll.put(\"subcluster1\", \"/hashall\");\r\n    mapHashAll.put(\"subcluster2\", \"/hashall\");\r\n    MountTable hashEntryAll = MountTable.newInstance(\"/hashall\", mapHashAll);\r\n    hashEntryAll.setDestOrder(DestinationOrder.HASH_ALL);\r\n    resolver.addEntry(hashEntryAll);\r\n    Map<String, String> mapLocal = new HashMap<>();\r\n    mapLocal.put(\"subcluster0\", \"/local\");\r\n    mapLocal.put(\"subcluster1\", \"/local\");\r\n    mapLocal.put(\"subcluster2\", \"/local\");\r\n    MountTable localEntry = MountTable.newInstance(\"/local\", mapLocal);\r\n    localEntry.setDestOrder(DestinationOrder.LOCAL);\r\n    resolver.addEntry(localEntry);\r\n    Map<String, String> mapRandom = new HashMap<>();\r\n    mapRandom.put(\"subcluster0\", \"/random\");\r\n    mapRandom.put(\"subcluster1\", \"/random\");\r\n    mapRandom.put(\"subcluster2\", \"/random\");\r\n    MountTable randomEntry = MountTable.newInstance(\"/random\", mapRandom);\r\n    randomEntry.setDestOrder(DestinationOrder.RANDOM);\r\n    resolver.addEntry(randomEntry);\r\n    Map<String, String> mapReadOnly = new HashMap<>();\r\n    mapReadOnly.put(\"subcluster0\", \"/readonly\");\r\n    mapReadOnly.put(\"subcluster1\", \"/readonly\");\r\n    mapReadOnly.put(\"subcluster2\", \"/readonly\");\r\n    MountTable readOnlyEntry = MountTable.newInstance(\"/readonly\", mapReadOnly);\r\n    readOnlyEntry.setReadOnly(true);\r\n    resolver.addEntry(readOnlyEntry);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testHashEqualDistribution",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testHashEqualDistribution() throws IOException\n{\r\n    testEvenDistribution(\"/hash\");\r\n    testEvenDistribution(\"/hash/folder0\", false);\r\n    testEvenDistribution(\"/hashall\");\r\n    testEvenDistribution(\"/hashall/folder0\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testHashAll",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testHashAll() throws IOException\n{\r\n    PathLocation dest0 = resolver.getDestinationForPath(\"/hashall/file0.txt\");\r\n    assertDest(\"subcluster0\", dest0);\r\n    PathLocation dest1 = resolver.getDestinationForPath(\"/hashall/file1.txt\");\r\n    assertDest(\"subcluster1\", dest1);\r\n    PathLocation dest2 = resolver.getDestinationForPath(\"/hashall/folder0\");\r\n    assertDest(\"subcluster2\", dest2);\r\n    PathLocation dest3 = resolver.getDestinationForPath(\"/hashall/folder0/file0.txt\");\r\n    assertDest(\"subcluster1\", dest3);\r\n    PathLocation dest4 = resolver.getDestinationForPath(\"/hashall/folder0/file1.txt\");\r\n    assertDest(\"subcluster0\", dest4);\r\n    PathLocation dest5 = resolver.getDestinationForPath(\"/hashall/folder0/folder0/file0.txt\");\r\n    assertDest(\"subcluster1\", dest5);\r\n    PathLocation dest6 = resolver.getDestinationForPath(\"/hashall/folder0/folder0/file1.txt\");\r\n    assertDest(\"subcluster1\", dest6);\r\n    PathLocation dest7 = resolver.getDestinationForPath(\"/hashall/folder0/folder0/file2.txt\");\r\n    assertDest(\"subcluster0\", dest7);\r\n    PathLocation dest8 = resolver.getDestinationForPath(\"/hashall/folder1\");\r\n    assertDest(\"subcluster1\", dest8);\r\n    PathLocation dest9 = resolver.getDestinationForPath(\"/hashall/folder1/file0.txt\");\r\n    assertDest(\"subcluster0\", dest9);\r\n    PathLocation dest10 = resolver.getDestinationForPath(\"/hashall/folder1/file1.txt\");\r\n    assertDest(\"subcluster1\", dest10);\r\n    PathLocation dest11 = resolver.getDestinationForPath(\"/hashall/folder2\");\r\n    assertDest(\"subcluster2\", dest11);\r\n    PathLocation dest12 = resolver.getDestinationForPath(\"/hashall/folder2/file0.txt\");\r\n    assertDest(\"subcluster0\", dest12);\r\n    PathLocation dest13 = resolver.getDestinationForPath(\"/hashall/folder2/file1.txt\");\r\n    assertDest(\"subcluster0\", dest13);\r\n    PathLocation dest14 = resolver.getDestinationForPath(\"/hashall/folder2/file2.txt\");\r\n    assertDest(\"subcluster1\", dest14);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testHashFirst",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testHashFirst() throws IOException\n{\r\n    PathLocation dest0 = resolver.getDestinationForPath(\"/hashall/file0.txt\");\r\n    assertDest(\"subcluster0\", dest0);\r\n    PathLocation dest1 = resolver.getDestinationForPath(\"/hashall/file1.txt\");\r\n    assertDest(\"subcluster1\", dest1);\r\n    PathLocation dest2 = resolver.getDestinationForPath(\"/hash/folder0\");\r\n    assertDest(\"subcluster0\", dest2);\r\n    PathLocation dest3 = resolver.getDestinationForPath(\"/hash/folder0/file0.txt\");\r\n    assertDest(\"subcluster0\", dest3);\r\n    PathLocation dest4 = resolver.getDestinationForPath(\"/hash/folder0/file1.txt\");\r\n    assertDest(\"subcluster0\", dest4);\r\n    PathLocation dest5 = resolver.getDestinationForPath(\"/hash/folder0/folder0/file0.txt\");\r\n    assertDest(\"subcluster0\", dest5);\r\n    PathLocation dest6 = resolver.getDestinationForPath(\"/hash/folder0/folder0/file1.txt\");\r\n    assertDest(\"subcluster0\", dest6);\r\n    PathLocation dest7 = resolver.getDestinationForPath(\"/hash/folder1\");\r\n    assertDest(\"subcluster2\", dest7);\r\n    PathLocation dest8 = resolver.getDestinationForPath(\"/hash/folder1/file0.txt\");\r\n    assertDest(\"subcluster2\", dest8);\r\n    PathLocation dest9 = resolver.getDestinationForPath(\"/hash/folder1/file1.txt\");\r\n    assertDest(\"subcluster2\", dest9);\r\n    PathLocation dest10 = resolver.getDestinationForPath(\"/hash/folder2\");\r\n    assertDest(\"subcluster2\", dest10);\r\n    PathLocation dest11 = resolver.getDestinationForPath(\"/hash/folder2/file0.txt\");\r\n    assertDest(\"subcluster2\", dest11);\r\n    PathLocation dest12 = resolver.getDestinationForPath(\"/hash/folder2/file1.txt\");\r\n    assertDest(\"subcluster2\", dest12);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testRandomEqualDistribution",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRandomEqualDistribution() throws IOException\n{\r\n    testEvenDistribution(\"/random\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testSingleDestination",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSingleDestination() throws IOException\n{\r\n    for (int f = 0; f < 100; f++) {\r\n        String filename = \"/tmp/b/c/file\" + f + \".txt\";\r\n        PathLocation destination = resolver.getDestinationForPath(filename);\r\n        RemoteLocation loc = destination.getDefaultLocation();\r\n        assertEquals(\"subcluster0\", loc.getNameserviceId());\r\n        assertEquals(filename, loc.getDest());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testResolveSubdirectories",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testResolveSubdirectories() throws Exception\n{\r\n    Random r = new Random();\r\n    String testDir = \"/sort/testdir\" + r.nextInt();\r\n    String file1 = testDir + \"/file1\" + r.nextInt();\r\n    String file2 = testDir + \"/file2\" + r.nextInt();\r\n    PathLocation testDirLocation = resolver.getDestinationForPath(testDir);\r\n    RemoteLocation defaultLoc = testDirLocation.getDefaultLocation();\r\n    String testDirNamespace = defaultLoc.getNameserviceId();\r\n    PathLocation file1Location = resolver.getDestinationForPath(file1);\r\n    RemoteLocation defaultLoc1 = file1Location.getDefaultLocation();\r\n    assertEquals(testDirNamespace, defaultLoc1.getNameserviceId());\r\n    PathLocation file2Location = resolver.getDestinationForPath(file2);\r\n    RemoteLocation defaultLoc2 = file2Location.getDefaultLocation();\r\n    assertEquals(testDirNamespace, defaultLoc2.getNameserviceId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testExtractTempFileName",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testExtractTempFileName()\n{\r\n    for (String teststring : new String[] { \"testfile1.txt.COPYING\", \"testfile1.txt._COPYING_\", \"testfile1.txt._COPYING_.attempt_1486662804109_0055_m_000042_0\", \"testfile1.txt.tmp\", \"_temp/testfile1.txt\", \"_temporary/testfile1.txt.af77e2ab-4bc5-4959-ae08-299c880ee6b8\", \"_temporary/0/_temporary/attempt_201706281636_0007_m_000003_46/\" + \"testfile1.txt\" }) {\r\n        String finalName = extractTempFileName(teststring);\r\n        assertEquals(\"testfile1.txt\", finalName);\r\n    }\r\n    assertEquals(\"file1.txt.COPYING1\", extractTempFileName(\"file1.txt.COPYING1\"));\r\n    assertEquals(\"file1.txt.tmp2\", extractTempFileName(\"file1.txt.tmp2\"));\r\n    String finalName = extractTempFileName(\"_temporary/part-00007.af77e2ab-4bc5-4959-ae08-299c880ee6b8\");\r\n    assertEquals(\"part-00007\", finalName);\r\n    finalName = extractTempFileName(\"_temporary/0/_temporary/attempt_201706281636_0007_m_000003_46/\" + \"part-00003\");\r\n    assertEquals(\"part-00003\", finalName);\r\n    finalName = extractTempFileName(\"folder0/testfile1.txt._COPYING_\");\r\n    assertEquals(\"folder0/testfile1.txt\", finalName);\r\n    finalName = extractTempFileName(\"folder0/folder1/testfile1.txt._COPYING_\");\r\n    assertEquals(\"folder0/folder1/testfile1.txt\", finalName);\r\n    finalName = extractTempFileName(\"processedHrsData.txt/_temporary/0/_temporary/\" + \"attempt_201706281636_0007_m_000003_46/part-00003\");\r\n    assertEquals(\"processedHrsData.txt/part-00003\", finalName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testReadOnly",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testReadOnly() throws IOException\n{\r\n    MountTable mount = resolver.getMountPoint(\"/readonly\");\r\n    assertTrue(mount.isReadOnly());\r\n    PathLocation dest0 = resolver.getDestinationForPath(\"/readonly/file0.txt\");\r\n    assertDest(\"subcluster1\", dest0);\r\n    PathLocation dest1 = resolver.getDestinationForPath(\"/readonly/file1.txt\");\r\n    assertDest(\"subcluster2\", dest1);\r\n    PathLocation dest2 = resolver.getDestinationForPath(\"/readonly/folder0\");\r\n    assertDest(\"subcluster1\", dest2);\r\n    PathLocation dest3 = resolver.getDestinationForPath(\"/readonly/folder0/file0.txt\");\r\n    assertDest(\"subcluster1\", dest3);\r\n    PathLocation dest4 = resolver.getDestinationForPath(\"/readonly/folder0/file1.txt\");\r\n    assertDest(\"subcluster1\", dest4);\r\n    PathLocation dest5 = resolver.getDestinationForPath(\"/readonly/folder0/folder0/file0.txt\");\r\n    assertDest(\"subcluster1\", dest5);\r\n    PathLocation dest6 = resolver.getDestinationForPath(\"/readonly/folder0/folder0/file1.txt\");\r\n    assertDest(\"subcluster1\", dest6);\r\n    PathLocation dest7 = resolver.getDestinationForPath(\"/readonly/folder1\");\r\n    assertDest(\"subcluster2\", dest7);\r\n    PathLocation dest8 = resolver.getDestinationForPath(\"/readonly/folder1/file0.txt\");\r\n    assertDest(\"subcluster2\", dest8);\r\n    PathLocation dest9 = resolver.getDestinationForPath(\"/readonly/folder1/file1.txt\");\r\n    assertDest(\"subcluster2\", dest9);\r\n    PathLocation dest10 = resolver.getDestinationForPath(\"/readonly/folder2\");\r\n    assertDest(\"subcluster1\", dest10);\r\n    PathLocation dest11 = resolver.getDestinationForPath(\"/readonly/folder2/file0.txt\");\r\n    assertDest(\"subcluster1\", dest11);\r\n    PathLocation dest12 = resolver.getDestinationForPath(\"/readonly/folder2/file1.txt\");\r\n    assertDest(\"subcluster1\", dest12);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testLocalResolver",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testLocalResolver() throws IOException\n{\r\n    PathLocation dest0 = resolver.getDestinationForPath(\"/local/folder0/file0.txt\");\r\n    assertDest(\"subcluster0\", dest0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testRandomResolver",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRandomResolver() throws IOException\n{\r\n    Set<String> destinations = new HashSet<>();\r\n    for (int i = 0; i < 30; i++) {\r\n        PathLocation dest = resolver.getDestinationForPath(\"/random/folder0/file0.txt\");\r\n        RemoteLocation firstDest = dest.getDestinations().get(0);\r\n        String nsId = firstDest.getNameserviceId();\r\n        destinations.add(nsId);\r\n    }\r\n    assertEquals(3, destinations.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testPrioritizeDestination",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testPrioritizeDestination() throws IOException\n{\r\n    PathLocation dest0 = resolver.getDestinationForPath(\"/hashall/file0.txt\");\r\n    assertDest(\"subcluster0\", dest0);\r\n    PathLocation prioritizedDest = PathLocation.prioritizeDestination(dest0, \"subcluster1\");\r\n    assertDest(\"subcluster1\", prioritizedDest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testEvenDistribution",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testEvenDistribution(final String path) throws IOException\n{\r\n    testEvenDistribution(path, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "testEvenDistribution",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testEvenDistribution(final String path, final boolean even) throws IOException\n{\r\n    Map<String, Set<String>> results = new HashMap<>();\r\n    for (int f = 0; f < 10000; f++) {\r\n        String filename = path + \"/file\" + f + \".txt\";\r\n        PathLocation destination = resolver.getDestinationForPath(filename);\r\n        RemoteLocation loc = destination.getDefaultLocation();\r\n        assertEquals(filename, loc.getDest());\r\n        String nsId = loc.getNameserviceId();\r\n        if (!results.containsKey(nsId)) {\r\n            results.put(nsId, new TreeSet<>());\r\n        }\r\n        results.get(nsId).add(filename);\r\n    }\r\n    if (!even) {\r\n        assertEquals(1, results.size());\r\n    } else {\r\n        assertEquals(3, results.size());\r\n        int count = 0;\r\n        for (Set<String> files : results.values()) {\r\n            count = count + files.size();\r\n        }\r\n        int avg = count / results.keySet().size();\r\n        for (Set<String> files : results.values()) {\r\n            int filesCount = files.size();\r\n            assertTrue(filesCount > 0);\r\n            assertTrue(Math.abs(filesCount - avg) < (avg / 5));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "assertDest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertDest(String expectedDest, PathLocation loc)\n{\r\n    assertEquals(expectedDest, loc.getDestinations().get(0).getNameserviceId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    LOG.info(\"Initialize the Mock Namenodes to monitor\");\r\n    for (String nsId : nsIds) {\r\n        nns.put(nsId, new HashMap<>());\r\n        for (String nnId : nnIds) {\r\n            nns.get(nsId).put(nnId, new MockNamenode(nsId));\r\n        }\r\n    }\r\n    LOG.info(\"Set nn0 to active for all nameservices\");\r\n    for (Map<String, MockNamenode> nnNS : nns.values()) {\r\n        nnNS.get(\"nn0\").transitionToActive();\r\n        nnNS.get(\"nn1\").transitionToStandby();\r\n    }\r\n    initializedTime = Time.now();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void cleanup() throws Exception\n{\r\n    for (Map<String, MockNamenode> nnNS : nns.values()) {\r\n        for (MockNamenode nn : nnNS.values()) {\r\n            nn.stop();\r\n        }\r\n    }\r\n    nns.clear();\r\n    if (router != null) {\r\n        router.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodesConfig",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "Configuration getNamenodesConfig()\n{\r\n    final Configuration conf = new HdfsConfiguration();\r\n    conf.set(DFSConfigKeys.DFS_NAMESERVICES, StringUtils.join(\",\", nns.keySet()));\r\n    for (String nsId : nns.keySet()) {\r\n        Set<String> nsNnIds = nns.get(nsId).keySet();\r\n        StringBuilder sb = new StringBuilder();\r\n        sb.append(DFSConfigKeys.DFS_HA_NAMENODES_KEY_PREFIX);\r\n        sb.append(\".\").append(nsId);\r\n        conf.set(sb.toString(), StringUtils.join(\",\", nsNnIds));\r\n        for (String nnId : nsNnIds) {\r\n            final MockNamenode nn = nns.get(nsId).get(nnId);\r\n            sb = new StringBuilder();\r\n            sb.append(DFSConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY);\r\n            sb.append(\".\").append(nsId);\r\n            sb.append(\".\").append(nnId);\r\n            conf.set(sb.toString(), \"localhost:\" + nn.getRPCPort());\r\n            sb = new StringBuilder();\r\n            sb.append(DFSConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY);\r\n            sb.append(\".\").append(nsId);\r\n            sb.append(\".\").append(nnId);\r\n            conf.set(sb.toString(), \"localhost:\" + nn.getHTTPPort());\r\n        }\r\n    }\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNamenodeMonitoring",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testNamenodeMonitoring() throws Exception\n{\r\n    Configuration nsConf = getNamenodesConfig();\r\n    Configuration stateStoreConfig = getStateStoreConfiguration();\r\n    stateStoreConfig.setClass(RBFConfigKeys.FEDERATION_NAMENODE_RESOLVER_CLIENT_CLASS, MembershipNamenodeResolver.class, ActiveNamenodeResolver.class);\r\n    stateStoreConfig.setClass(RBFConfigKeys.FEDERATION_FILE_RESOLVER_CLIENT_CLASS, MountTableResolver.class, FileSubclusterResolver.class);\r\n    Configuration routerConf = new RouterConfigBuilder(nsConf).enableLocalHeartbeat(true).heartbeat().stateStore().rpc().build();\r\n    routerConf.set(RBFConfigKeys.DFS_ROUTER_RPC_ADDRESS_KEY, \"0.0.0.0:0\");\r\n    routerConf.set(RBFConfigKeys.DFS_ROUTER_MONITOR_NAMENODE, \"ns1.nn0,ns1.nn1\");\r\n    routerConf.addResource(stateStoreConfig);\r\n    routerConf.set(DFSConfigKeys.DFS_NAMESERVICE_ID, \"ns0\");\r\n    routerConf.set(DFSConfigKeys.DFS_HA_NAMENODE_ID_KEY, \"nn1\");\r\n    router = new Router();\r\n    router.init(routerConf);\r\n    router.start();\r\n    Collection<NamenodeHeartbeatService> heartbeatServices = router.getNamenodeHeartbeatServices();\r\n    for (NamenodeHeartbeatService service : heartbeatServices) {\r\n        service.periodicInvoke();\r\n    }\r\n    MembershipNamenodeResolver resolver = (MembershipNamenodeResolver) router.getNamenodeResolver();\r\n    resolver.loadCache(true);\r\n    final List<FederationNamenodeContext> namespaceInfo = new ArrayList<>();\r\n    for (String nsId : nns.keySet()) {\r\n        List<? extends FederationNamenodeContext> nnReports = resolver.getNamenodesForNameserviceId(nsId);\r\n        namespaceInfo.addAll(nnReports);\r\n    }\r\n    for (FederationNamenodeContext nnInfo : namespaceInfo) {\r\n        long modTime = nnInfo.getDateModified();\r\n        long diff = modTime - initializedTime;\r\n        if (\"ns0\".equals(nnInfo.getNameserviceId()) && \"nn0\".equals(nnInfo.getNamenodeId())) {\r\n            assertTrue(nnInfo + \" shouldn't be updated: \" + diff, modTime < initializedTime);\r\n        } else {\r\n            assertTrue(nnInfo + \" should be updated: \" + diff, modTime > initializedTime);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNamenodeMonitoringConfig",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testNamenodeMonitoringConfig() throws Exception\n{\r\n    testConfig(asList(), \"\");\r\n    testConfig(asList(\"ns1.nn0\"), \"ns1.nn0\");\r\n    testConfig(asList(\"ns1.nn0\", \"ns1.nn1\"), \"ns1.nn0,ns1.nn1\");\r\n    testConfig(asList(\"ns1.nn0\", \"ns1.nn1\"), \"ns1.nn0, ns1.nn1\");\r\n    testConfig(asList(\"ns1.nn0\", \"ns1.nn1\"), \" ns1.nn0,ns1.nn1\");\r\n    testConfig(asList(\"ns1.nn0\", \"ns1.nn1\"), \"ns1.nn0,ns1.nn1,\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testConfig",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testConfig(Collection<String> expectedNNs, String confNsIds)\n{\r\n    Configuration conf = getNamenodesConfig();\r\n    Configuration routerConf = new RouterConfigBuilder(conf).heartbeat(true).build();\r\n    routerConf.set(RBFConfigKeys.DFS_ROUTER_RPC_ADDRESS_KEY, \"0.0.0.0:0\");\r\n    routerConf.set(RBFConfigKeys.DFS_ROUTER_MONITOR_NAMENODE, confNsIds);\r\n    router = new Router();\r\n    router.init(routerConf);\r\n    Collection<NamenodeHeartbeatService> heartbeatServices = router.getNamenodeHeartbeatServices();\r\n    assertNamenodeHeartbeatService(expectedNNs, heartbeatServices);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "assertNamenodeHeartbeatService",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void assertNamenodeHeartbeatService(Collection<String> expected, Collection<NamenodeHeartbeatService> actual)\n{\r\n    final Set<String> actualSet = new TreeSet<>();\r\n    for (NamenodeHeartbeatService heartbeatService : actual) {\r\n        NamenodeStatusReport report = heartbeatService.getNamenodeStatusReport();\r\n        StringBuilder sb = new StringBuilder();\r\n        sb.append(report.getNameserviceId());\r\n        sb.append(\".\");\r\n        sb.append(report.getNamenodeId());\r\n        actualSet.add(sb.toString());\r\n    }\r\n    assertTrue(expected + \" does not contain all \" + actualSet, expected.containsAll(actualSet));\r\n    assertTrue(actualSet + \" does not contain all \" + expected, actualSet.containsAll(expected));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testJmxUrlHTTP",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJmxUrlHTTP()\n{\r\n    verifyUrlSchemes(HttpConfig.Policy.HTTP_ONLY.name());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testJmxUrlHTTPs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJmxUrlHTTPs()\n{\r\n    verifyUrlSchemes(HttpConfig.Policy.HTTPS_ONLY.name());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifyUrlSchemes",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void verifyUrlSchemes(String scheme)\n{\r\n    final LogVerificationAppender appender = new LogVerificationAppender();\r\n    final org.apache.log4j.Logger logger = org.apache.log4j.Logger.getRootLogger();\r\n    logger.addAppender(appender);\r\n    GenericTestUtils.setRootLogLevel(Level.DEBUG);\r\n    Configuration conf = getNamenodesConfig();\r\n    conf.set(DFSConfigKeys.DFS_HTTP_POLICY_KEY, scheme);\r\n    Configuration routerConf = new RouterConfigBuilder(conf).heartbeat(true).build();\r\n    routerConf.set(RBFConfigKeys.DFS_ROUTER_RPC_ADDRESS_KEY, \"0.0.0.0:0\");\r\n    routerConf.set(RBFConfigKeys.DFS_ROUTER_MONITOR_NAMENODE, \"ns1.nn0\");\r\n    router = new Router();\r\n    router.init(routerConf);\r\n    Collection<NamenodeHeartbeatService> heartbeatServices = router.getNamenodeHeartbeatServices();\r\n    for (NamenodeHeartbeatService heartbeatService : heartbeatServices) {\r\n        heartbeatService.getNamenodeStatusReport();\r\n    }\r\n    if (HttpConfig.Policy.HTTPS_ONLY.name().equals(scheme)) {\r\n        assertEquals(2, appender.countLinesWithMessage(\"JMX URL: https://\"));\r\n        assertEquals(0, appender.countLinesWithMessage(\"JMX URL: http://\"));\r\n    } else {\r\n        assertEquals(2, appender.countLinesWithMessage(\"JMX URL: http://\"));\r\n        assertEquals(0, appender.countLinesWithMessage(\"JMX URL: https://\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testDatanodesView",
  "errType" : null,
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void testDatanodesView() throws IOException\n{\r\n    Configuration routerConf = new RouterConfigBuilder().stateStore().rpc().build();\r\n    router = new Router();\r\n    router.init(routerConf);\r\n    router.start();\r\n    for (String nsId : nsIds) {\r\n        registerSubclusters(router, nns.get(nsId).values());\r\n        for (String nnId : nnIds) {\r\n            MockNamenode nn = nns.get(nsId).get(nnId);\r\n            if (\"nn0\".equals(nnId)) {\r\n                nn.transitionToActive();\r\n            }\r\n            nn.addDatanodeMock();\r\n        }\r\n    }\r\n    long time = Time.now();\r\n    for (String nsId : nsIds) {\r\n        for (String nnId : nnIds) {\r\n            DatanodeInfoBuilder dn0Builder = new DatanodeInfoBuilder().setDatanodeUuid(\"dn0\").setHostName(\"dn0\").setIpAddr(\"dn0\").setXferPort(10000);\r\n            if (\"ns0\".equals(nsId)) {\r\n                dn0Builder.setLastUpdate(time - 1000);\r\n                dn0Builder.setAdminState(AdminStates.NORMAL);\r\n            } else if (\"ns1\".equals(nsId)) {\r\n                dn0Builder.setLastUpdate(time - 500);\r\n                dn0Builder.setAdminState(AdminStates.DECOMMISSIONED);\r\n            }\r\n            DatanodeInfoBuilder dn1Builder = new DatanodeInfoBuilder().setDatanodeUuid(\"dn1\").setHostName(\"dn1\").setIpAddr(\"dn1\").setXferPort(10000);\r\n            if (\"ns0\".equals(nsId)) {\r\n                dn1Builder.setLastUpdate(time - 1000);\r\n                dn1Builder.setAdminState(AdminStates.NORMAL);\r\n            } else if (\"ns1\".equals(nsId)) {\r\n                dn1Builder.setLastUpdate(time - 5 * 1000);\r\n                dn1Builder.setAdminState(AdminStates.DECOMMISSION_INPROGRESS);\r\n            }\r\n            MockNamenode nn = nns.get(nsId).get(nnId);\r\n            List<DatanodeInfo> dns = nn.getDatanodes();\r\n            dns.add(dn0Builder.build());\r\n            dns.add(dn1Builder.build());\r\n        }\r\n    }\r\n    DistributedFileSystem dfs = (DistributedFileSystem) getFileSystem(router);\r\n    DFSClient dfsClient = dfs.getClient();\r\n    DatanodeStorageReport[] dns = dfsClient.getDatanodeStorageReport(DatanodeReportType.ALL);\r\n    assertEquals(2, dns.length);\r\n    for (DatanodeStorageReport dn : dns) {\r\n        DatanodeInfo dnInfo = dn.getDatanodeInfo();\r\n        if (\"dn0\".equals(dnInfo.getHostName())) {\r\n            assertEquals(AdminStates.DECOMMISSIONED, dnInfo.getAdminState());\r\n        } else if (\"dn1\".equals(dnInfo.getHostName())) {\r\n            assertEquals(AdminStates.NORMAL, dnInfo.getAdminState());\r\n        } else {\r\n            fail(\"Unexpected DN: \" + dnInfo.getHostName());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setUpCluster",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUpCluster() throws Exception\n{\r\n    Configuration conf = new RouterConfigBuilder().rpc().admin().build();\r\n    cluster = new MiniRouterDFSCluster(false, 1);\r\n    cluster.addRouterOverrides(conf);\r\n    cluster.startRouters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n    if (tempResource != null) {\r\n        File f = new File(tempResource);\r\n        f.delete();\r\n        tempResource = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "initializeClientConfig",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Configuration initializeClientConfig() throws Exception\n{\r\n    Configuration conf = new Configuration(false);\r\n    Router router = cluster.getRouters().get(0).getRouter();\r\n    conf.set(DFSConfigKeys.DFS_NAMESERVICES, \"ns0,ns1,\" + ROUTER_NS);\r\n    conf.set(DFSConfigKeys.DFS_HA_NAMENODES_KEY_PREFIX + \".\" + ROUTER_NS, \"r1\");\r\n    conf.set(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, LOOPBACK_ADDRESS + \":\" + router.getAdminServerAddress().getPort());\r\n    conf.set(DFSConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY + \".\" + ROUTER_NS + \".r1\", LOOPBACK_ADDRESS + \":\" + router.getRpcServerAddress().getPort());\r\n    conf.set(HdfsClientConfigKeys.Failover.PROXY_PROVIDER_KEY_PREFIX + \".\" + ROUTER_NS, ConfiguredFailoverProxyProvider.class.getCanonicalName());\r\n    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, HDFS_SCHEMA + ROUTER_NS);\r\n    conf.setBoolean(\"dfs.client.failover.random.order\", false);\r\n    conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY, HDFS_SCHEMA + ROUTER_NS);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRefreshSuperUserGroupsConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRefreshSuperUserGroupsConfiguration() throws Exception\n{\r\n    Configuration conf = initializeClientConfig();\r\n    testRefreshSuperUserGroupsConfigurationInternal(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testRefreshSuperUserGroupsConfigurationInternal",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testRefreshSuperUserGroupsConfigurationInternal(Configuration conf) throws Exception\n{\r\n    UserGroupInformation ugi = mock(UserGroupInformation.class);\r\n    UserGroupInformation impersonator = mock(UserGroupInformation.class);\r\n    when(impersonator.getShortUserName()).thenReturn(\"impersonator\");\r\n    when(impersonator.getUserName()).thenReturn(\"impersonator\");\r\n    when(ugi.getRealUser()).thenReturn(impersonator);\r\n    when(ugi.getUserName()).thenReturn(\"victim\");\r\n    when(ugi.getGroups()).thenReturn(Arrays.asList(\"groupVictim\"));\r\n    when(ugi.getGroupsSet()).thenReturn(new LinkedHashSet<>(Arrays.asList(\"groupVictim\")));\r\n    LambdaTestUtils.intercept(AuthorizationException.class, \"User: impersonator is not allowed to impersonate victim\", () -> ProxyUsers.authorize(ugi, LOOPBACK_ADDRESS));\r\n    String tfile = \"testRouterRefreshSuperUserGroupsConfiguration_rsrc.xml\";\r\n    ArrayList<String> keys = new ArrayList<>(Arrays.asList(\"hadoop.proxyuser.impersonator.groups\", \"hadoop.proxyuser.impersonator.hosts\"));\r\n    ArrayList<String> values = new ArrayList<>(Arrays.asList(\"groupVictim\", LOOPBACK_ADDRESS));\r\n    tempResource = addFileBasedConfigResource(tfile, keys, values);\r\n    Configuration.addDefaultResource(tfile);\r\n    RouterAdmin routerAdmin = new RouterAdmin(conf);\r\n    int clientRes = routerAdmin.run(new String[] { \"-refreshSuperUserGroupsConfiguration\" });\r\n    assertEquals(\"CLI command was not successful\", 0, clientRes);\r\n    ProxyUsers.authorize(ugi, LOOPBACK_ADDRESS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addFileBasedConfigResource",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "String addFileBasedConfigResource(String configFileName, ArrayList<String> keyArray, ArrayList<String> valueArray) throws IOException\n{\r\n    if (keyArray.size() != valueArray.size()) {\r\n        throw new IOException(\"keyArray and valueArray should be equal in size\");\r\n    }\r\n    URL url = new Configuration().getResource(\"hdfs-site.xml\");\r\n    Path dir = new Path(URLDecoder.decode(url.getPath(), \"UTF-8\")).getParent();\r\n    String tmp = dir.toString() + \"/\" + configFileName;\r\n    StringBuilder configItems = new StringBuilder();\r\n    configItems.append(\"<configuration>\");\r\n    for (int i = 0; i < keyArray.size(); i++) {\r\n        configItems.append(\"<property>\").append(\"<name>\").append(keyArray.get(i)).append(\"</name>\").append(\"<value>\").append(valueArray.get(i)).append(\"</value>\").append(\"</property>\");\r\n    }\r\n    configItems.append(\"</configuration>\");\r\n    PrintWriter writer = new PrintWriter(new FileOutputStream(tmp));\r\n    writer.println(configItems.toString());\r\n    writer.close();\r\n    return tmp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "testTempOld",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testTempOld()\n{\r\n    assertFalse(isOldTempRecord(\"test.txt\"));\r\n    assertFalse(isOldTempRecord(\"testfolder/test.txt\"));\r\n    long tnow = Time.now();\r\n    String tmpFile1 = \"test.\" + tnow + \".tmp\";\r\n    assertFalse(isOldTempRecord(tmpFile1));\r\n    long told = Time.now() - TimeUnit.MINUTES.toMillis(1);\r\n    String tmpFile2 = \"test.\" + told + \".tmp\";\r\n    assertTrue(isOldTempRecord(tmpFile2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "verifyException",
  "errType" : [ "InvocationTargetException", "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void verifyException(Object obj, String methodName, Class<? extends Exception> exceptionClass, Class<?>[] parameterTypes, Object[] arguments)\n{\r\n    Throwable triggeredException = null;\r\n    try {\r\n        Method m = obj.getClass().getMethod(methodName, parameterTypes);\r\n        m.invoke(obj, arguments);\r\n    } catch (InvocationTargetException ex) {\r\n        triggeredException = ex.getTargetException();\r\n    } catch (Exception e) {\r\n        triggeredException = e;\r\n    }\r\n    if (exceptionClass != null) {\r\n        assertNotNull(\"No exception was triggered, expected exception\" + exceptionClass.getName(), triggeredException);\r\n        assertEquals(exceptionClass, triggeredException.getClass());\r\n    } else {\r\n        assertNull(\"Exception was triggered but no exception was expected\", triggeredException);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "createNamenodeReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NamenodeStatusReport createNamenodeReport(String ns, String nn, HAServiceState state)\n{\r\n    Random rand = new Random();\r\n    return createNamenodeReport(ns, nn, \"localhost:\" + rand.nextInt(10000), state);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "createNamenodeReport",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "NamenodeStatusReport createNamenodeReport(String ns, String nn, String rpcAddress, HAServiceState state)\n{\r\n    Random rand = new Random();\r\n    NamenodeStatusReport report = new NamenodeStatusReport(ns, nn, rpcAddress, \"localhost:\" + rand.nextInt(10000), \"localhost:\" + rand.nextInt(10000), \"http\", \"testwebaddress-\" + ns + nn);\r\n    if (state == null) {\r\n        return report;\r\n    }\r\n    report.setHAServiceState(state);\r\n    NamespaceInfo nsInfo = new NamespaceInfo(1, \"tesclusterid\", ns, 0, \"testbuildvesion\", \"testsoftwareversion\");\r\n    report.setNamespaceInfo(nsInfo);\r\n    return report;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "waitNamenodeRegistered",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitNamenodeRegistered(final ActiveNamenodeResolver resolver, final String nsId, final String nnId, final FederationNamenodeServiceState state) throws Exception\n{\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            try {\r\n                List<? extends FederationNamenodeContext> namenodes = resolver.getNamenodesForNameserviceId(nsId);\r\n                if (namenodes != null) {\r\n                    for (FederationNamenodeContext namenode : namenodes) {\r\n                        if (namenode.getNamenodeId() == nnId || namenode.getNamenodeId().equals(nnId)) {\r\n                            return state == null || namenode.getState().equals(state);\r\n                        }\r\n                    }\r\n                }\r\n            } catch (IOException e) {\r\n            }\r\n            return false;\r\n        }\r\n    }, 1000, 60 * 1000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "waitNamenodeRegistered",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitNamenodeRegistered(final ActiveNamenodeResolver resolver, final String nsId, final FederationNamenodeServiceState state) throws Exception\n{\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            try {\r\n                List<? extends FederationNamenodeContext> nns = resolver.getNamenodesForNameserviceId(nsId);\r\n                for (FederationNamenodeContext nn : nns) {\r\n                    if (nn.getState().equals(state)) {\r\n                        return true;\r\n                    }\r\n                }\r\n            } catch (IOException e) {\r\n            }\r\n            return false;\r\n        }\r\n    }, 1000, 20 * 1000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "verifyDate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean verifyDate(Date d1, Date d2, long precision)\n{\r\n    return Math.abs(d1.getTime() - d2.getTime()) < precision;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getBean",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T getBean(String name, Class<T> obj) throws MalformedObjectNameException\n{\r\n    MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer();\r\n    ObjectName poolName = new ObjectName(name);\r\n    return JMX.newMXBeanProxy(mBeanServer, poolName, obj);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "addDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean addDirectory(FileSystem context, String path) throws IOException\n{\r\n    context.mkdirs(new Path(path), new FsPermission(\"777\"));\r\n    return verifyFileExists(context, path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileStatus getFileStatus(FileSystem context, String path) throws IOException\n{\r\n    return context.getFileStatus(new Path(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "verifyFileExists",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean verifyFileExists(FileSystem context, String path)\n{\r\n    try {\r\n        FileStatus status = getFileStatus(context, path);\r\n        if (status != null) {\r\n            return true;\r\n        }\r\n    } catch (Exception e) {\r\n        return false;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "checkForFileInDirectory",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean checkForFileInDirectory(FileSystem context, String testPath, String targetFile) throws IOException, AccessControlException, FileNotFoundException, UnsupportedFileSystemException, IllegalArgumentException\n{\r\n    FileStatus[] fileStatus = context.listStatus(new Path(testPath));\r\n    String file = null;\r\n    String verifyPath = testPath + \"/\" + targetFile;\r\n    if (testPath.equals(\"/\")) {\r\n        verifyPath = testPath + targetFile;\r\n    }\r\n    Boolean found = false;\r\n    for (int i = 0; i < fileStatus.length; i++) {\r\n        FileStatus f = fileStatus[i];\r\n        file = Path.getPathWithoutSchemeAndAuthority(f.getPath()).toString();\r\n        if (file.equals(verifyPath)) {\r\n            found = true;\r\n        }\r\n    }\r\n    return found;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "countContents",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int countContents(FileSystem context, String testPath) throws IOException\n{\r\n    Path path = new Path(testPath);\r\n    FileStatus[] fileStatus = context.listStatus(path);\r\n    return fileStatus.length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createFile(FileSystem fs, String path, long length) throws IOException\n{\r\n    FsPermission permissions = new FsPermission(\"700\");\r\n    FSDataOutputStream writeStream = fs.create(new Path(path), permissions, true, 1000, (short) 1, DFSConfigKeys.DFS_BLOCK_SIZE_DEFAULT, null);\r\n    for (int i = 0; i < length; i++) {\r\n        writeStream.write(i);\r\n    }\r\n    writeStream.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "readFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String readFile(FileSystem fs, String path) throws IOException\n{\r\n    Path fileName = new Path(path);\r\n    InputStreamReader reader = new InputStreamReader(fs.open(fileName));\r\n    BufferedReader bufferedReader = new BufferedReader(reader);\r\n    StringBuilder data = new StringBuilder();\r\n    String line;\r\n    while ((line = bufferedReader.readLine()) != null) {\r\n        data.append(line);\r\n    }\r\n    bufferedReader.close();\r\n    reader.close();\r\n    return data.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "deleteFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean deleteFile(FileSystem fs, String path) throws IOException\n{\r\n    return fs.delete(new Path(path), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "simulateSlowNamenode",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void simulateSlowNamenode(final NameNode nn, final int seconds) throws Exception\n{\r\n    FSNamesystem namesystem = nn.getNamesystem();\r\n    HAContext haContext = namesystem.getHAContext();\r\n    HAContext spyHAContext = spy(haContext);\r\n    doAnswer(new Answer<Object>() {\r\n\r\n        @Override\r\n        public Object answer(InvocationOnMock invocation) throws Throwable {\r\n            LOG.info(\"Simulating slow namenode {}\", invocation.getMock());\r\n            try {\r\n                Thread.sleep(seconds * 1000);\r\n            } catch (InterruptedException e) {\r\n                LOG.error(\"Simulating a slow namenode aborted\");\r\n            }\r\n            return null;\r\n        }\r\n    }).when(spyHAContext).checkOperation(any(OperationCategory.class));\r\n    Whitebox.setInternalState(namesystem, \"haContext\", spyHAContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "waitRouterRegistered",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitRouterRegistered(RouterStore stateManager, long routerCount, int timeout) throws Exception\n{\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            try {\r\n                List<RouterState> cachedRecords = stateManager.getCachedRecords();\r\n                if (cachedRecords.size() == routerCount) {\r\n                    return true;\r\n                }\r\n            } catch (IOException e) {\r\n            }\r\n            return false;\r\n        }\r\n    }, 100, timeout);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "simulateThrowExceptionRouterRpcServer",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void simulateThrowExceptionRouterRpcServer(final RouterRpcServer server) throws IOException\n{\r\n    RouterRpcClient rpcClient = server.getRPCClient();\r\n    ConnectionManager connectionManager = new ConnectionManager(server.getConfig());\r\n    ConnectionManager spyConnectionManager = spy(connectionManager);\r\n    doAnswer(new Answer() {\r\n\r\n        @Override\r\n        public Object answer(InvocationOnMock invocation) throws Throwable {\r\n            LOG.info(\"Simulating connectionManager throw IOException {}\", invocation.getMock());\r\n            throw new IOException(\"Simulate connectionManager throw IOException\");\r\n        }\r\n    }).when(spyConnectionManager).getConnection(any(UserGroupInformation.class), any(String.class), any(Class.class));\r\n    Whitebox.setInternalState(rpcClient, \"connectionManager\", spyConnectionManager);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "transitionClusterNSToStandby",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void transitionClusterNSToStandby(StateStoreDFSCluster cluster)\n{\r\n    List<String> nameServiceList = cluster.getNameservices();\r\n    for (String nameService : nameServiceList) {\r\n        List<NamenodeContext> nnList = cluster.getNamenodes(nameService);\r\n        for (NamenodeContext namenodeContext : nnList) {\r\n            cluster.switchToStandby(nameService, namenodeContext.getNamenodeId());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "transitionClusterNSToActive",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void transitionClusterNSToActive(StateStoreDFSCluster cluster, int index)\n{\r\n    List<String> nameServiceList = cluster.getNameservices();\r\n    for (String nameService : nameServiceList) {\r\n        List<NamenodeContext> listNamenodeContext = cluster.getNamenodes(nameService);\r\n        cluster.switchToActive(nameService, listNamenodeContext.get(index).getNamenodeId());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileSystem getFileSystem(int rpcPort) throws IOException\n{\r\n    Configuration conf = new HdfsConfiguration();\r\n    URI uri = URI.create(\"hdfs://localhost:\" + rpcPort);\r\n    return DistributedFileSystem.get(uri, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FileSystem getFileSystem(final Router router) throws IOException\n{\r\n    InetSocketAddress rpcAddress = router.getRpcServerAddress();\r\n    int rpcPort = rpcAddress.getPort();\r\n    return getFileSystem(rpcPort);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getAdminClient",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterClient getAdminClient(final Router router) throws IOException\n{\r\n    Configuration conf = new HdfsConfiguration();\r\n    InetSocketAddress routerSocket = router.getAdminServerAddress();\r\n    return new RouterClient(routerSocket, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "createMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createMountTableEntry(final Router router, final String mountPoint, final DestinationOrder order, Collection<String> nsIds) throws Exception\n{\r\n    createMountTableEntry(Collections.singletonList(router), mountPoint, order, nsIds);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "createMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void createMountTableEntry(final List<Router> routers, final String mountPoint, final DestinationOrder order, final Collection<String> nsIds) throws Exception\n{\r\n    Router router = routers.get(0);\r\n    RouterClient admin = getAdminClient(router);\r\n    MountTableManager mountTable = admin.getMountTableManager();\r\n    Map<String, String> destMap = new HashMap<>();\r\n    for (String nsId : nsIds) {\r\n        destMap.put(nsId, mountPoint);\r\n    }\r\n    MountTable newEntry = MountTable.newInstance(mountPoint, destMap);\r\n    newEntry.setDestOrder(order);\r\n    AddMountTableEntryRequest addRequest = AddMountTableEntryRequest.newInstance(newEntry);\r\n    AddMountTableEntryResponse addResponse = mountTable.addMountTableEntry(addRequest);\r\n    boolean created = addResponse.getStatus();\r\n    assertTrue(created);\r\n    refreshRoutersCaches(routers);\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(mountPoint);\r\n    GetMountTableEntriesResponse getResponse = mountTable.getMountTableEntries(getRequest);\r\n    List<MountTable> entries = getResponse.getEntries();\r\n    assertEquals(\"Too many entries: \" + entries, 1, entries.size());\r\n    assertEquals(mountPoint, entries.get(0).getSourcePath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "refreshRoutersCaches",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void refreshRoutersCaches(final List<Router> routers)\n{\r\n    for (final Router router : routers) {\r\n        StateStoreService stateStore = router.getStateStore();\r\n        stateStore.refreshCaches(true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws Exception\n{\r\n    RouterHDFSContract.createCluster(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws Exception\n{\r\n    RouterHDFSContract.createCluster(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterWebHDFSContract.createCluster();\r\n    RouterWebHDFSContract.getFileSystem().getDefaultBlockSize(new Path(\"/\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterWebHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterWebHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws Exception\n{\r\n    RouterHDFSContract.createCluster(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterWebHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterWebHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterWebHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "testNegativeSeek",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNegativeSeek() throws Throwable\n{\r\n    System.out.println(\"Not supported\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "testSeekReadClosedFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekReadClosedFile() throws Throwable\n{\r\n    System.out.println(\"Not supported\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "testSeekPastEndOfFileThenReseekAndRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekPastEndOfFileThenReseekAndRead() throws Throwable\n{\r\n    System.out.println(\"Not supported\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testDefaultConfigs",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testDefaultConfigs()\n{\r\n    Configuration configuration = DFSRouter.getConfiguration();\r\n    String journalUri = configuration.get(FedBalanceConfigs.SCHEDULER_JOURNAL_URI);\r\n    int workerThreads = configuration.getInt(FedBalanceConfigs.WORK_THREAD_NUM, -1);\r\n    Assert.assertEquals(\"hdfs://localhost:8020/tmp/procedure\", journalUri);\r\n    Assert.assertEquals(10, workerThreads);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void create()\n{\r\n    getConf().setLong(RBFConfigKeys.FEDERATION_STORE_MEMBERSHIP_EXPIRATION_MS, TimeUnit.SECONDS.toMillis(2));\r\n    getConf().setLong(RBFConfigKeys.FEDERATION_STORE_MEMBERSHIP_EXPIRATION_DELETION_MS, TimeUnit.SECONDS.toMillis(2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws IOException, InterruptedException\n{\r\n    membershipStore = getStateStore().getRegisteredRecordStore(MembershipStore.class);\r\n    assertTrue(clearRecords(getStateStore(), MembershipState.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testNamenodeStateOverride",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testNamenodeStateOverride() throws Exception\n{\r\n    String ns = \"ns0\";\r\n    String nn = \"nn0\";\r\n    MembershipState report = createRegistration(ns, nn, ROUTERS[1], FederationNamenodeServiceState.STANDBY);\r\n    assertTrue(namenodeHeartbeat(report));\r\n    assertTrue(getStateStore().loadCache(MembershipStore.class, true));\r\n    MembershipState existingState = getNamenodeRegistration(ns, nn);\r\n    assertEquals(FederationNamenodeServiceState.STANDBY, existingState.getState());\r\n    UpdateNamenodeRegistrationRequest request = UpdateNamenodeRegistrationRequest.newInstance(ns, nn, FederationNamenodeServiceState.ACTIVE);\r\n    assertTrue(membershipStore.updateNamenodeRegistration(request).getResult());\r\n    MembershipState newState = getNamenodeRegistration(ns, nn);\r\n    assertEquals(FederationNamenodeServiceState.ACTIVE, newState.getState());\r\n    UpdateNamenodeRegistrationRequest request1 = UpdateNamenodeRegistrationRequest.newInstance(ns, nn, FederationNamenodeServiceState.OBSERVER);\r\n    assertTrue(membershipStore.updateNamenodeRegistration(request1).getResult());\r\n    MembershipState newState1 = getNamenodeRegistration(ns, nn);\r\n    assertEquals(FederationNamenodeServiceState.OBSERVER, newState1.getState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testStateStoreDisconnected",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testStateStoreDisconnected() throws Exception\n{\r\n    getStateStore().closeDriver();\r\n    assertFalse(getStateStore().isDriverReady());\r\n    NamenodeHeartbeatRequest hbRequest = NamenodeHeartbeatRequest.newInstance();\r\n    hbRequest.setNamenodeMembership(createMockRegistrationForNamenode(\"test\", \"test\", FederationNamenodeServiceState.UNAVAILABLE));\r\n    verifyException(membershipStore, \"namenodeHeartbeat\", StateStoreUnavailableException.class, new Class[] { NamenodeHeartbeatRequest.class }, new Object[] { hbRequest });\r\n    GetNamenodeRegistrationsRequest getRequest = GetNamenodeRegistrationsRequest.newInstance();\r\n    verifyException(membershipStore, \"getNamenodeRegistrations\", null, new Class[] { GetNamenodeRegistrationsRequest.class }, new Object[] { getRequest });\r\n    verifyException(membershipStore, \"getExpiredNamenodeRegistrations\", null, new Class[] { GetNamenodeRegistrationsRequest.class }, new Object[] { getRequest });\r\n    UpdateNamenodeRegistrationRequest overrideRequest = UpdateNamenodeRegistrationRequest.newInstance();\r\n    verifyException(membershipStore, \"updateNamenodeRegistration\", null, new Class[] { UpdateNamenodeRegistrationRequest.class }, new Object[] { overrideRequest });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "registerAndLoadRegistrations",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void registerAndLoadRegistrations(List<MembershipState> registrationList) throws IOException\n{\r\n    assertTrue(synchronizeRecords(getStateStore(), registrationList, MembershipState.class));\r\n    assertTrue(getStateStore().loadCache(MembershipStore.class, true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "createRegistration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MembershipState createRegistration(String ns, String nn, String router, FederationNamenodeServiceState state) throws IOException\n{\r\n    MembershipState record = MembershipState.newInstance(router, ns, nn, \"testcluster\", \"testblock-\" + ns, \"testrpc-\" + ns + nn, \"testservice-\" + ns + nn, \"testlifeline-\" + ns + nn, \"http\", \"testweb-\" + ns + nn, state, false);\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testRegistrationMajorityQuorum",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testRegistrationMajorityQuorum() throws InterruptedException, IOException\n{\r\n    String ns = \"ns0\";\r\n    String nn = \"nn0\";\r\n    MembershipState report = createRegistration(ns, nn, ROUTERS[1], FederationNamenodeServiceState.ACTIVE);\r\n    assertTrue(namenodeHeartbeat(report));\r\n    Thread.sleep(1000);\r\n    report = createRegistration(ns, nn, ROUTERS[2], FederationNamenodeServiceState.ACTIVE);\r\n    assertTrue(namenodeHeartbeat(report));\r\n    Thread.sleep(1000);\r\n    report = createRegistration(ns, nn, ROUTERS[3], FederationNamenodeServiceState.ACTIVE);\r\n    assertTrue(namenodeHeartbeat(report));\r\n    report = createRegistration(ns, nn, ROUTERS[0], FederationNamenodeServiceState.STANDBY);\r\n    assertTrue(namenodeHeartbeat(report));\r\n    assertTrue(getStateStore().loadCache(MembershipStore.class, true));\r\n    MembershipState quorumEntry = getNamenodeRegistration(report.getNameserviceId(), report.getNamenodeId());\r\n    assertNotNull(quorumEntry);\r\n    assertEquals(quorumEntry.getRouterId(), ROUTERS[3]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testRegistrationQuorumExcludesExpired",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRegistrationQuorumExcludesExpired() throws InterruptedException, IOException\n{\r\n    List<MembershipState> registrationList = new ArrayList<>();\r\n    String ns = \"ns0\";\r\n    String nn = \"nn0\";\r\n    String rpcAddress = \"testrpcaddress\";\r\n    String serviceAddress = \"testserviceaddress\";\r\n    String lifelineAddress = \"testlifelineaddress\";\r\n    String blockPoolId = \"testblockpool\";\r\n    String clusterId = \"testcluster\";\r\n    String webScheme = \"http\";\r\n    String webAddress = \"testwebaddress\";\r\n    boolean safemode = false;\r\n    MembershipState record = MembershipState.newInstance(ROUTERS[0], ns, nn, clusterId, blockPoolId, rpcAddress, serviceAddress, lifelineAddress, webScheme, webAddress, FederationNamenodeServiceState.ACTIVE, safemode);\r\n    registrationList.add(record);\r\n    record = MembershipState.newInstance(ROUTERS[1], ns, nn, clusterId, blockPoolId, rpcAddress, serviceAddress, lifelineAddress, webScheme, webAddress, FederationNamenodeServiceState.EXPIRED, safemode);\r\n    registrationList.add(record);\r\n    record = MembershipState.newInstance(ROUTERS[2], ns, nn, clusterId, blockPoolId, rpcAddress, serviceAddress, lifelineAddress, webScheme, webAddress, FederationNamenodeServiceState.EXPIRED, safemode);\r\n    registrationList.add(record);\r\n    record = MembershipState.newInstance(ROUTERS[3], ns, nn, clusterId, blockPoolId, rpcAddress, serviceAddress, lifelineAddress, webScheme, webAddress, FederationNamenodeServiceState.EXPIRED, safemode);\r\n    registrationList.add(record);\r\n    registerAndLoadRegistrations(registrationList);\r\n    MembershipState quorumEntry = getNamenodeRegistration(record.getNameserviceId(), record.getNamenodeId());\r\n    assertNotNull(quorumEntry);\r\n    assertEquals(ROUTERS[0], quorumEntry.getRouterId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testRegistrationQuorumAllExpired",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testRegistrationQuorumAllExpired() throws IOException\n{\r\n    List<MembershipState> registrationList = new ArrayList<>();\r\n    String ns = NAMESERVICES[0];\r\n    String nn = NAMENODES[0];\r\n    String rpcAddress = \"testrpcaddress\";\r\n    String serviceAddress = \"testserviceaddress\";\r\n    String lifelineAddress = \"testlifelineaddress\";\r\n    String blockPoolId = \"testblockpool\";\r\n    String clusterId = \"testcluster\";\r\n    String webScheme = \"http\";\r\n    String webAddress = \"testwebaddress\";\r\n    boolean safemode = false;\r\n    long startingTime = Time.now();\r\n    MembershipState record = MembershipState.newInstance(ROUTERS[0], ns, nn, clusterId, blockPoolId, rpcAddress, webAddress, lifelineAddress, webScheme, webAddress, FederationNamenodeServiceState.EXPIRED, safemode);\r\n    record.setDateModified(startingTime - 10000);\r\n    registrationList.add(record);\r\n    record = MembershipState.newInstance(ROUTERS[1], ns, nn, clusterId, blockPoolId, rpcAddress, serviceAddress, lifelineAddress, webScheme, webAddress, FederationNamenodeServiceState.EXPIRED, safemode);\r\n    record.setDateModified(startingTime);\r\n    registrationList.add(record);\r\n    record = MembershipState.newInstance(ROUTERS[2], ns, nn, clusterId, blockPoolId, rpcAddress, serviceAddress, lifelineAddress, webScheme, webAddress, FederationNamenodeServiceState.EXPIRED, safemode);\r\n    record.setDateModified(startingTime);\r\n    registrationList.add(record);\r\n    record = MembershipState.newInstance(ROUTERS[3], ns, nn, clusterId, blockPoolId, rpcAddress, serviceAddress, lifelineAddress, webScheme, webAddress, FederationNamenodeServiceState.EXPIRED, safemode);\r\n    record.setDateModified(startingTime);\r\n    registrationList.add(record);\r\n    registerAndLoadRegistrations(registrationList);\r\n    assertNull(getNamenodeRegistration(record.getNameserviceId(), record.getNamenodeId()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testRegistrationNoQuorum",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testRegistrationNoQuorum() throws InterruptedException, IOException\n{\r\n    MembershipState report1 = createRegistration(NAMESERVICES[0], NAMENODES[0], ROUTERS[1], FederationNamenodeServiceState.STANDBY);\r\n    assertTrue(namenodeHeartbeat(report1));\r\n    Thread.sleep(100);\r\n    MembershipState report2 = createRegistration(NAMESERVICES[0], NAMENODES[0], ROUTERS[2], FederationNamenodeServiceState.ACTIVE);\r\n    assertTrue(namenodeHeartbeat(report2));\r\n    Thread.sleep(100);\r\n    MembershipState report3 = createRegistration(NAMESERVICES[0], NAMENODES[0], ROUTERS[3], FederationNamenodeServiceState.ACTIVE);\r\n    assertTrue(namenodeHeartbeat(report3));\r\n    Thread.sleep(100);\r\n    MembershipState report4 = createRegistration(NAMESERVICES[0], NAMENODES[0], ROUTERS[0], FederationNamenodeServiceState.STANDBY);\r\n    assertTrue(namenodeHeartbeat(report4));\r\n    assertTrue(getStateStore().loadCache(MembershipStore.class, true));\r\n    MembershipState quorumEntry = getNamenodeRegistration(report1.getNameserviceId(), report1.getNamenodeId());\r\n    assertNotNull(quorumEntry);\r\n    assertEquals(ROUTERS[0], quorumEntry.getRouterId());\r\n    assertEquals(FederationNamenodeServiceState.STANDBY, quorumEntry.getState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testRegistrationExpiredAndDeletion",
  "errType" : [ "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testRegistrationExpiredAndDeletion() throws InterruptedException, IOException, TimeoutException\n{\r\n    MembershipState report = createRegistration(NAMESERVICES[0], NAMENODES[0], ROUTERS[0], FederationNamenodeServiceState.ACTIVE);\r\n    assertTrue(namenodeHeartbeat(report));\r\n    assertTrue(getStateStore().loadCache(MembershipStore.class, true));\r\n    MembershipState quorumEntry = getNamenodeRegistration(report.getNameserviceId(), report.getNamenodeId());\r\n    assertNotNull(quorumEntry);\r\n    assertEquals(ROUTERS[0], quorumEntry.getRouterId());\r\n    assertEquals(FederationNamenodeServiceState.ACTIVE, quorumEntry.getState());\r\n    quorumEntry = getExpiredNamenodeRegistration(report.getNameserviceId(), report.getNamenodeId());\r\n    assertNull(quorumEntry);\r\n    GenericTestUtils.waitFor(() -> {\r\n        try {\r\n            assertTrue(getStateStore().loadCache(MembershipStore.class, true));\r\n            return getNamenodeRegistration(NAMESERVICES[0], NAMENODES[0]) == null;\r\n        } catch (IOException e) {\r\n            return false;\r\n        }\r\n    }, 100, 3000);\r\n    quorumEntry = getExpiredNamenodeRegistration(NAMESERVICES[0], NAMENODES[0]);\r\n    assertNotNull(quorumEntry);\r\n    quorumEntry = getNamenodeRegistration(report.getNameserviceId(), report.getNamenodeId());\r\n    assertNull(quorumEntry);\r\n    quorumEntry = getExpiredNamenodeRegistration(report.getNameserviceId(), report.getNamenodeId());\r\n    assertNotNull(quorumEntry);\r\n    assertTrue(namenodeHeartbeat(report));\r\n    assertTrue(getStateStore().loadCache(MembershipStore.class, true));\r\n    quorumEntry = getNamenodeRegistration(report.getNameserviceId(), report.getNamenodeId());\r\n    assertNotNull(quorumEntry);\r\n    assertEquals(ROUTERS[0], quorumEntry.getRouterId());\r\n    assertEquals(FederationNamenodeServiceState.ACTIVE, quorumEntry.getState());\r\n    quorumEntry = getExpiredNamenodeRegistration(report.getNameserviceId(), report.getNamenodeId());\r\n    assertNull(quorumEntry);\r\n    GenericTestUtils.waitFor(() -> {\r\n        try {\r\n            assertTrue(getStateStore().loadCache(MembershipStore.class, true));\r\n            return getNamenodeRegistration(NAMESERVICES[0], NAMENODES[0]) == null;\r\n        } catch (IOException e) {\r\n            return false;\r\n        }\r\n    }, 100, 3000);\r\n    quorumEntry = getExpiredNamenodeRegistration(NAMESERVICES[0], NAMENODES[0]);\r\n    assertNotNull(quorumEntry);\r\n    GenericTestUtils.waitFor(() -> {\r\n        try {\r\n            assertTrue(getStateStore().loadCache(MembershipStore.class, true));\r\n            return getExpiredNamenodeRegistration(NAMESERVICES[0], NAMENODES[0]) == null;\r\n        } catch (IOException e) {\r\n            return false;\r\n        }\r\n    }, 100, 3000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "testNamespaceInfoWithUnavailableNameNodeRegistration",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testNamespaceInfoWithUnavailableNameNodeRegistration() throws IOException\n{\r\n    List<MembershipState> registrationList = new ArrayList<>();\r\n    String router = ROUTERS[0];\r\n    String ns = NAMESERVICES[0];\r\n    String rpcAddress = \"testrpcaddress\";\r\n    String serviceAddress = \"testserviceaddress\";\r\n    String lifelineAddress = \"testlifelineaddress\";\r\n    String blockPoolId = \"testblockpool\";\r\n    String clusterId = \"testcluster\";\r\n    String webScheme = \"http\";\r\n    String webAddress = \"testwebaddress\";\r\n    boolean safemode = false;\r\n    MembershipState record = MembershipState.newInstance(router, ns, NAMENODES[0], clusterId, blockPoolId, rpcAddress, serviceAddress, lifelineAddress, webScheme, webAddress, FederationNamenodeServiceState.ACTIVE, safemode);\r\n    registrationList.add(record);\r\n    record = MembershipState.newInstance(router, ns, NAMENODES[1], \"\", \"\", rpcAddress, serviceAddress, lifelineAddress, webScheme, webAddress, FederationNamenodeServiceState.UNAVAILABLE, safemode);\r\n    registrationList.add(record);\r\n    registerAndLoadRegistrations(registrationList);\r\n    GetNamespaceInfoRequest request = GetNamespaceInfoRequest.newInstance();\r\n    GetNamespaceInfoResponse response = membershipStore.getNamespaceInfo(request);\r\n    Set<FederationNamespaceInfo> namespaces = response.getNamespaceInfo();\r\n    assertEquals(1, namespaces.size());\r\n    FederationNamespaceInfo namespace = namespaces.iterator().next();\r\n    assertEquals(ns, namespace.getNameserviceId());\r\n    assertEquals(clusterId, namespace.getClusterId());\r\n    assertEquals(blockPoolId, namespace.getBlockPoolId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getNamenodeRegistration",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "MembershipState getNamenodeRegistration(final String nsId, final String nnId) throws IOException\n{\r\n    MembershipState partial = MembershipState.newInstance();\r\n    partial.setNameserviceId(nsId);\r\n    partial.setNamenodeId(nnId);\r\n    GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance(partial);\r\n    GetNamenodeRegistrationsResponse response = membershipStore.getNamenodeRegistrations(request);\r\n    List<MembershipState> results = response.getNamenodeMemberships();\r\n    if (results != null && results.size() == 1) {\r\n        MembershipState record = results.get(0);\r\n        return record;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getExpiredNamenodeRegistration",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "MembershipState getExpiredNamenodeRegistration(final String nsId, final String nnId) throws IOException\n{\r\n    MembershipState partial = MembershipState.newInstance();\r\n    partial.setNameserviceId(nsId);\r\n    partial.setNamenodeId(nnId);\r\n    GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance(partial);\r\n    GetNamenodeRegistrationsResponse response = membershipStore.getExpiredNamenodeRegistrations(request);\r\n    List<MembershipState> results = response.getNamenodeMemberships();\r\n    if (results != null && results.size() == 1) {\r\n        MembershipState record = results.get(0);\r\n        return record;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "namenodeHeartbeat",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean namenodeHeartbeat(MembershipState namenode) throws IOException\n{\r\n    NamenodeHeartbeatRequest request = NamenodeHeartbeatRequest.newInstance(namenode);\r\n    NamenodeHeartbeatResponse response = membershipStore.namenodeHeartbeat(request);\r\n    return response.getResult();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "initializeMemberVariables",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initializeMemberVariables()\n{\r\n    xmlFilename = \"hdfs-rbf-default.xml\";\r\n    configurationClasses = new Class[] { RBFConfigKeys.class };\r\n    errorIfMissingConfigProps = true;\r\n    errorIfMissingXmlProps = true;\r\n    configurationPropsToSkipCompare = new HashSet<String>();\r\n    xmlPropsToSkipCompare = new HashSet<String>();\r\n    xmlPrefixToSkipCompare = new HashSet<String>();\r\n    xmlPrefixToSkipCompare.add(RBFConfigKeys.DFS_ROUTER_FAIR_HANDLER_COUNT_KEY_PREFIX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "addLocation",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addLocation(String mount, String nsId, String location)\n{\r\n    List<RemoteLocation> locationsList = this.locations.get(mount);\r\n    if (locationsList == null) {\r\n        locationsList = new LinkedList<>();\r\n        this.locations.put(mount, locationsList);\r\n    }\r\n    final RemoteLocation remoteLocation = new RemoteLocation(nsId, location, mount);\r\n    if (!locationsList.contains(remoteLocation)) {\r\n        locationsList.add(remoteLocation);\r\n    }\r\n    if (this.defaultNamespace == null) {\r\n        this.defaultNamespace = nsId;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "removeLocation",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean removeLocation(String mount, String nsId, String location)\n{\r\n    List<RemoteLocation> locationsList = this.locations.get(mount);\r\n    final RemoteLocation remoteLocation = new RemoteLocation(nsId, location, mount);\r\n    if (locationsList != null) {\r\n        return locationsList.remove(remoteLocation);\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "cleanRegistrations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void cleanRegistrations()\n{\r\n    this.resolver = new HashMap<>();\r\n    this.namespaces = new HashSet<>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "setDisableRegistration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDisableRegistration(boolean isDisable)\n{\r\n    disableRegistration = isDisable;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "updateActiveNamenode",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void updateActiveNamenode(String nsId, InetSocketAddress successfulAddress)\n{\r\n    String address = successfulAddress.getHostName() + \":\" + successfulAddress.getPort();\r\n    String key = nsId;\r\n    if (key != null) {\r\n        @SuppressWarnings(\"unchecked\")\r\n        List<FederationNamenodeContext> namenodes = (List<FederationNamenodeContext>) this.resolver.get(key);\r\n        for (FederationNamenodeContext namenode : namenodes) {\r\n            if (namenode.getRpcAddress().equals(address)) {\r\n                MockNamenodeContext nn = (MockNamenodeContext) namenode;\r\n                nn.setState(FederationNamenodeServiceState.ACTIVE);\r\n                break;\r\n            }\r\n        }\r\n        synchronized (namenodes) {\r\n            Collections.sort(namenodes, new NamenodePriorityComparator());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getNamenodesForNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<? extends FederationNamenodeContext> getNamenodesForNameserviceId(String nameserviceId)\n{\r\n    List<? extends FederationNamenodeContext> namenodes = this.resolver.get(nameserviceId);\r\n    if (namenodes == null) {\r\n        namenodes = new ArrayList<>();\r\n    }\r\n    return Collections.unmodifiableList(new ArrayList<>(namenodes));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getNamenodesForBlockPoolId",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<? extends FederationNamenodeContext> getNamenodesForBlockPoolId(String blockPoolId)\n{\r\n    List<? extends FederationNamenodeContext> namenodes = this.resolver.get(blockPoolId);\r\n    return Collections.unmodifiableList(new ArrayList<>(namenodes));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "registerNamenode",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "boolean registerNamenode(NamenodeStatusReport report) throws IOException\n{\r\n    if (disableRegistration) {\r\n        return false;\r\n    }\r\n    MockNamenodeContext context = new MockNamenodeContext(report.getRpcAddress(), report.getServiceAddress(), report.getLifelineAddress(), report.getWebScheme(), report.getWebAddress(), report.getNameserviceId(), report.getNamenodeId(), report.getState());\r\n    String nsId = report.getNameserviceId();\r\n    String bpId = report.getBlockPoolId();\r\n    String cId = report.getClusterId();\r\n    @SuppressWarnings(\"unchecked\")\r\n    List<MockNamenodeContext> existingItems = (List<MockNamenodeContext>) this.resolver.get(nsId);\r\n    if (existingItems == null) {\r\n        existingItems = new ArrayList<>();\r\n        this.resolver.put(bpId, existingItems);\r\n        this.resolver.put(nsId, existingItems);\r\n    }\r\n    boolean added = false;\r\n    for (int i = 0; i < existingItems.size() && !added; i++) {\r\n        MockNamenodeContext existing = existingItems.get(i);\r\n        if (existing.getNamenodeKey().equals(context.getNamenodeKey())) {\r\n            existingItems.set(i, context);\r\n            added = true;\r\n        }\r\n    }\r\n    if (!added) {\r\n        existingItems.add(context);\r\n    }\r\n    Collections.sort(existingItems, new NamenodePriorityComparator());\r\n    FederationNamespaceInfo info = new FederationNamespaceInfo(bpId, cId, nsId);\r\n    this.namespaces.add(info);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getNamespaces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<FederationNamespaceInfo> getNamespaces() throws IOException\n{\r\n    return Collections.unmodifiableSet(this.namespaces);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getDisabledNamespaces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<String> getDisabledNamespaces() throws IOException\n{\r\n    return new TreeSet<>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getDestinationForPath",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "PathLocation getDestinationForPath(String path) throws IOException\n{\r\n    List<RemoteLocation> remoteLocations = new LinkedList<>();\r\n    List<String> keys = new ArrayList<>(this.locations.keySet());\r\n    Collections.sort(keys, Collections.reverseOrder());\r\n    for (String key : keys) {\r\n        if (path.startsWith(key)) {\r\n            for (RemoteLocation location : this.locations.get(key)) {\r\n                String finalPath = location.getDest();\r\n                String extraPath = path.substring(key.length());\r\n                if (finalPath.endsWith(\"/\") && extraPath.startsWith(\"/\")) {\r\n                    extraPath = extraPath.substring(1);\r\n                }\r\n                finalPath += extraPath;\r\n                String nameservice = location.getNameserviceId();\r\n                RemoteLocation remoteLocation = new RemoteLocation(nameservice, finalPath, path);\r\n                remoteLocations.add(remoteLocation);\r\n            }\r\n            break;\r\n        }\r\n    }\r\n    if (remoteLocations.isEmpty()) {\r\n        return null;\r\n    }\r\n    return new PathLocation(path, remoteLocations);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getMountPoints",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<String> getMountPoints(String path) throws IOException\n{\r\n    List<String> mountPoints = new ArrayList<>();\r\n    for (String mp : this.locations.keySet()) {\r\n        if (mp.startsWith(path)) {\r\n            mountPoints.add(mp);\r\n        }\r\n    }\r\n    return FileSubclusterResolver.getMountPoints(path, mountPoints);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "setRouterId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRouterId(String router)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "setDisableNamespace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDisableNamespace(boolean b)\n{\r\n    this.disableDefaultNamespace = b;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation",
  "methodName" : "getDefaultNamespace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDefaultNamespace()\n{\r\n    if (disableDefaultNamespace) {\r\n        return \"\";\r\n    }\r\n    return defaultNamespace;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testStartupWithoutSpnegoPrincipal",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testStartupWithoutSpnegoPrincipal() throws Exception\n{\r\n    Configuration conf = initSecurity();\r\n    conf.unset(HTTP_KERBEROS_PRINCIPAL_CONF_KEY);\r\n    RouterWebHDFSContract.createCluster(conf);\r\n    assertNotNull(RouterWebHDFSContract.getCluster());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testStartupWithoutKeytab",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testStartupWithoutKeytab() throws Exception\n{\r\n    testCluster(DFS_ROUTER_KEYTAB_FILE_KEY, \"Running in secure mode, but config doesn't have a keytab\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testSuccessfulStartup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSuccessfulStartup() throws Exception\n{\r\n    Configuration conf = initSecurity();\r\n    RouterWebHDFSContract.createCluster(conf);\r\n    assertNotNull(RouterWebHDFSContract.getCluster());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testCluster",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCluster(String configToTest, String message) throws Exception\n{\r\n    Configuration conf = initSecurity();\r\n    conf.unset(configToTest);\r\n    exceptionRule.expect(IOException.class);\r\n    exceptionRule.expectMessage(message);\r\n    RouterWebHDFSContract.createCluster(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    RouterHDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    RouterHDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new RouterHDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    cluster = new StateStoreDFSCluster(false, 2);\r\n    Configuration routerConf = new RouterConfigBuilder().stateStore().metrics().admin().rpc().build();\r\n    routerConf.setInt(RBFConfigKeys.DFS_ROUTER_HANDLER_COUNT_KEY, 8);\r\n    routerConf.setInt(RBFConfigKeys.DFS_ROUTER_CLIENT_THREADS_SIZE, 4);\r\n    cluster.setIndependentDNs();\r\n    cluster.addRouterOverrides(routerConf);\r\n    cluster.startCluster();\r\n    cluster.startRouters();\r\n    cluster.waitClusterUp();\r\n    routerContext = cluster.getRandomRouter();\r\n    routerProtocol = routerContext.getClient().getNamenode();\r\n    routerAdminClient = routerContext.getAdminClient();\r\n    setupNamespace();\r\n    MiniDFSCluster dfsCluster = cluster.getCluster();\r\n    NameNode nn0 = dfsCluster.getNameNode(0);\r\n    simulateSlowNamenode(nn0, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setupNamespace",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void setupNamespace() throws IOException\n{\r\n    MountTableManager mountTable = routerAdminClient.getMountTableManager();\r\n    Map<String, String> destinations = new TreeMap<>();\r\n    destinations.put(\"ns0\", \"/dirns0\");\r\n    MountTable newEntry = MountTable.newInstance(\"/dirns0\", destinations);\r\n    AddMountTableEntryRequest request = AddMountTableEntryRequest.newInstance(newEntry);\r\n    mountTable.addMountTableEntry(request);\r\n    destinations = new TreeMap<>();\r\n    destinations.put(\"ns1\", \"/dirns1\");\r\n    newEntry = MountTable.newInstance(\"/dirns1\", destinations);\r\n    request = AddMountTableEntryRequest.newInstance(newEntry);\r\n    mountTable.addMountTableEntry(request);\r\n    Router router = routerContext.getRouter();\r\n    MountTableResolver mountTableResolver = (MountTableResolver) router.getSubclusterResolver();\r\n    mountTableResolver.loadCache(true);\r\n    NamenodeContext nn0 = cluster.getNamenode(\"ns0\", null);\r\n    nn0.getFileSystem().mkdirs(new Path(\"/dirns0/0\"));\r\n    nn0.getFileSystem().mkdirs(new Path(\"/dir-ns\"));\r\n    NamenodeContext nn1 = cluster.getNamenode(\"ns1\", null);\r\n    nn1.getFileSystem().mkdirs(new Path(\"/dirns1/1\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (cluster != null) {\r\n        cluster.stopRouter(routerContext);\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void cleanup() throws IOException\n{\r\n    Router router = routerContext.getRouter();\r\n    StateStoreService stateStore = router.getStateStore();\r\n    DisabledNameserviceStore store = stateStore.getRegisteredRecordStore(DisabledNameserviceStore.class);\r\n    store.loadCache(true);\r\n    Set<String> disabled = store.getDisabledNameservices();\r\n    for (String nsId : disabled) {\r\n        store.enableNameservice(nsId);\r\n    }\r\n    store.loadCache(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testWithoutDisabling",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testWithoutDisabling() throws IOException\n{\r\n    long t0 = monotonicNow();\r\n    routerProtocol.renewLease(\"client0\");\r\n    long t = monotonicNow() - t0;\r\n    assertTrue(\"It took too little: \" + t + \"ms\", t > TimeUnit.SECONDS.toMillis(1));\r\n    FileSystem routerFs = routerContext.getFileSystem();\r\n    FileStatus[] filesStatus = routerFs.listStatus(new Path(\"/\"));\r\n    assertEquals(3, filesStatus.length);\r\n    assertEquals(\"dir-ns\", filesStatus[0].getPath().getName());\r\n    assertEquals(\"dirns0\", filesStatus[1].getPath().getName());\r\n    assertEquals(\"dirns1\", filesStatus[2].getPath().getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testDisabling",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testDisabling() throws Exception\n{\r\n    disableNameservice(\"ns0\");\r\n    long t0 = monotonicNow();\r\n    routerProtocol.renewLease(\"client0\");\r\n    long t = monotonicNow() - t0;\r\n    assertTrue(\"It took too long: \" + t + \"ms\", t < TimeUnit.SECONDS.toMillis(1));\r\n    FileSystem routerFs = routerContext.getFileSystem();\r\n    FileStatus[] filesStatus = routerFs.listStatus(new Path(\"/\"));\r\n    assertEquals(2, filesStatus.length);\r\n    assertEquals(\"dirns0\", filesStatus[0].getPath().getName());\r\n    assertEquals(\"dirns1\", filesStatus[1].getPath().getName());\r\n    filesStatus = routerFs.listStatus(new Path(\"/dirns1\"));\r\n    assertEquals(1, filesStatus.length);\r\n    assertEquals(\"1\", filesStatus[0].getPath().getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testMetrics",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testMetrics() throws Exception\n{\r\n    disableNameservice(\"ns0\");\r\n    int numActive = 0;\r\n    int numDisabled = 0;\r\n    Router router = routerContext.getRouter();\r\n    RBFMetrics metrics = router.getMetrics();\r\n    String jsonString = metrics.getNameservices();\r\n    JSONObject jsonObject = new JSONObject(jsonString);\r\n    Iterator<?> keys = jsonObject.keys();\r\n    while (keys.hasNext()) {\r\n        String key = (String) keys.next();\r\n        JSONObject json = jsonObject.getJSONObject(key);\r\n        String nsId = json.getString(\"nameserviceId\");\r\n        String state = json.getString(\"state\");\r\n        if (nsId.equals(\"ns0\")) {\r\n            assertEquals(\"DISABLED\", state);\r\n            numDisabled++;\r\n        } else {\r\n            assertEquals(\"ACTIVE\", state);\r\n            numActive++;\r\n        }\r\n    }\r\n    assertEquals(1, numActive);\r\n    assertEquals(1, numDisabled);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "disableNameservice",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void disableNameservice(final String nsId) throws IOException\n{\r\n    NameserviceManager nsManager = routerAdminClient.getNameserviceManager();\r\n    DisableNameserviceRequest req = DisableNameserviceRequest.newInstance(nsId);\r\n    nsManager.disableNameservice(req);\r\n    Router router = routerContext.getRouter();\r\n    StateStoreService stateStore = router.getStateStore();\r\n    DisabledNameserviceStore store = stateStore.getRegisteredRecordStore(DisabledNameserviceStore.class);\r\n    store.loadCache(true);\r\n    MembershipNamenodeResolver resolver = (MembershipNamenodeResolver) router.getNamenodeResolver();\r\n    resolver.loadCache(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setup()\n{\r\n    manager = new RouterQuotaManager();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup()\n{\r\n    manager.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetChildrenPaths",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testGetChildrenPaths()\n{\r\n    RouterQuotaUsage quotaUsage = new RouterQuotaUsage.Builder().build();\r\n    manager.put(\"/path1\", quotaUsage);\r\n    manager.put(\"/path2\", quotaUsage);\r\n    manager.put(\"/path1/subdir\", quotaUsage);\r\n    manager.put(\"/path1/subdir/subdir\", quotaUsage);\r\n    Set<String> childrenPaths = manager.getPaths(\"/path1\");\r\n    assertEquals(3, childrenPaths.size());\r\n    assertTrue(childrenPaths.contains(\"/path1/subdir\") && childrenPaths.contains(\"/path1/subdir/subdir\") && childrenPaths.contains(\"/path1\"));\r\n    manager.put(\"/path3\", quotaUsage);\r\n    manager.put(\"/path3/subdir\", quotaUsage);\r\n    manager.put(\"/path3-subdir\", quotaUsage);\r\n    childrenPaths = manager.getPaths(\"/path3\");\r\n    assertEquals(2, childrenPaths.size());\r\n    assertTrue(childrenPaths.contains(\"/path3\") && childrenPaths.contains(\"/path3/subdir\") && !childrenPaths.contains(\"/path3-subdir\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testGetQuotaUsage",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testGetQuotaUsage()\n{\r\n    RouterQuotaUsage quotaGet;\r\n    quotaGet = manager.getQuotaUsage(\"/non-exist-path\");\r\n    assertNull(quotaGet);\r\n    RouterQuotaUsage.Builder quota = new RouterQuotaUsage.Builder().quota(HdfsConstants.QUOTA_RESET).spaceQuota(HdfsConstants.QUOTA_RESET);\r\n    manager.put(\"/noQuotaSet\", quota.build());\r\n    quotaGet = manager.getQuotaUsage(\"/noQuotaSet\");\r\n    assertNull(quotaGet);\r\n    quota.quota(1);\r\n    quota.spaceQuota(HdfsConstants.QUOTA_RESET);\r\n    manager.put(\"/hasQuotaSet\", quota.build());\r\n    quotaGet = manager.getQuotaUsage(\"/hasQuotaSet\");\r\n    assertEquals(1, quotaGet.getQuota());\r\n    assertEquals(HdfsConstants.QUOTA_RESET, quotaGet.getSpaceQuota());\r\n    quotaGet = manager.getQuotaUsage(\"/hasQuotaSet/file\");\r\n    assertEquals(1, quotaGet.getQuota());\r\n    assertEquals(HdfsConstants.QUOTA_RESET, quotaGet.getSpaceQuota());\r\n    quota.quota(HdfsConstants.QUOTA_RESET);\r\n    quota.spaceQuota(HdfsConstants.QUOTA_RESET);\r\n    manager.put(\"/hasQuotaSet/noQuotaSet\", quota.build());\r\n    quotaGet = manager.getQuotaUsage(\"/hasQuotaSet/noQuotaSet/file\");\r\n    assertEquals(1, quotaGet.getQuota());\r\n    assertEquals(HdfsConstants.QUOTA_RESET, quotaGet.getSpaceQuota());\r\n    quota.quota(2);\r\n    quota.spaceQuota(HdfsConstants.QUOTA_RESET);\r\n    manager.put(\"/hasQuotaSet/hasQuotaSet\", quota.build());\r\n    quotaGet = manager.getQuotaUsage(\"/hasQuotaSet/hasQuotaSet/file\");\r\n    assertEquals(2, quotaGet.getQuota());\r\n    assertEquals(HdfsConstants.QUOTA_RESET, quotaGet.getSpaceQuota());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "generateRecord",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "RouterState generateRecord() throws IOException\n{\r\n    RouterState record = RouterState.newInstance(ADDRESS, START_TIME, STATE);\r\n    record.setVersion(VERSION);\r\n    record.setCompileInfo(COMPILE_INFO);\r\n    record.setDateCreated(DATE_CREATED);\r\n    record.setDateModified(DATE_MODIFIED);\r\n    StateStoreVersion version = StateStoreVersion.newInstance();\r\n    version.setMountTableVersion(FILE_RESOLVER_VERSION);\r\n    record.setStateStoreVersion(version);\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "validateRecord",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void validateRecord(RouterState record) throws IOException\n{\r\n    assertEquals(ADDRESS, record.getAddress());\r\n    assertEquals(START_TIME, record.getDateStarted());\r\n    assertEquals(STATE, record.getStatus());\r\n    assertEquals(COMPILE_INFO, record.getCompileInfo());\r\n    assertEquals(VERSION, record.getVersion());\r\n    StateStoreVersion version = record.getStateStoreVersion();\r\n    assertEquals(FILE_RESOLVER_VERSION, version.getMountTableVersion());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "testGetterSetter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetterSetter() throws IOException\n{\r\n    RouterState record = generateRecord();\r\n    validateRecord(record);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "testSerialization",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSerialization() throws IOException\n{\r\n    RouterState record = generateRecord();\r\n    StateStoreSerializer serializer = StateStoreSerializer.getSerializer();\r\n    String serializedString = serializer.serializeString(record);\r\n    RouterState newRecord = serializer.deserialize(serializedString, RouterState.class);\r\n    validateRecord(newRecord);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup()\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setupCluster",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setupCluster(boolean overloadControl, boolean ha) throws Exception\n{\r\n    cluster = new StateStoreDFSCluster(ha, 2);\r\n    Configuration routerConf = new RouterConfigBuilder().stateStore().metrics().admin().rpc().heartbeat().build();\r\n    routerConf.setInt(RBFConfigKeys.DFS_ROUTER_CLIENT_THREADS_SIZE, 4);\r\n    routerConf.setBoolean(RBFConfigKeys.DFS_ROUTER_CLIENT_REJECT_OVERLOAD, overloadControl);\r\n    cluster.setNumDatanodesPerNameservice(0);\r\n    cluster.addRouterOverrides(routerConf);\r\n    cluster.startCluster();\r\n    cluster.startRouters();\r\n    cluster.waitClusterUp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testWithoutOverloadControl",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testWithoutOverloadControl() throws Exception\n{\r\n    setupCluster(false, false);\r\n    testOverloaded(0);\r\n    MiniDFSCluster dfsCluster = cluster.getCluster();\r\n    NameNode nn0 = dfsCluster.getNameNode(0);\r\n    simulateSlowNamenode(nn0, 1);\r\n    testOverloaded(0);\r\n    for (RouterContext router : cluster.getRouters()) {\r\n        FederationRPCMetrics rpcMetrics = router.getRouter().getRpcServer().getRPCMetrics();\r\n        assertEquals(0, rpcMetrics.getProxyOpFailureClientOverloaded());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testOverloadControl",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testOverloadControl() throws Exception\n{\r\n    setupCluster(true, false);\r\n    List<RouterContext> routers = cluster.getRouters();\r\n    FederationRPCMetrics rpcMetrics0 = routers.get(0).getRouter().getRpcServer().getRPCMetrics();\r\n    FederationRPCMetrics rpcMetrics1 = routers.get(1).getRouter().getRpcServer().getRPCMetrics();\r\n    testOverloaded(0);\r\n    assertEquals(0, rpcMetrics0.getProxyOpFailureClientOverloaded());\r\n    assertEquals(0, rpcMetrics1.getProxyOpFailureClientOverloaded());\r\n    MiniDFSCluster dfsCluster = cluster.getCluster();\r\n    NameNode nn0 = dfsCluster.getNameNode(0);\r\n    simulateSlowNamenode(nn0, 1);\r\n    testOverloaded(4, 6);\r\n    assertTrue(rpcMetrics0.getProxyOpFailureClientOverloaded() + rpcMetrics1.getProxyOpFailureClientOverloaded() >= 4);\r\n    Configuration clientConf = cluster.getRouterClientConf();\r\n    long iniProxyOps0 = rpcMetrics0.getProxyOps();\r\n    long iniProxyOps1 = rpcMetrics1.getProxyOps();\r\n    testOverloaded(0, 0, new URI(\"hdfs://fed/\"), clientConf, 10);\r\n    long proxyOps0 = rpcMetrics0.getProxyOps() - iniProxyOps0;\r\n    long proxyOps1 = rpcMetrics1.getProxyOps() - iniProxyOps1;\r\n    assertEquals(2 * 10, proxyOps0 + proxyOps1);\r\n    assertTrue(proxyOps0 + \" operations: not distributed\", proxyOps0 >= 8);\r\n    assertTrue(proxyOps1 + \" operations: not distributed\", proxyOps1 >= 8);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testOverloaded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testOverloaded(int expOverload) throws Exception\n{\r\n    testOverloaded(expOverload, expOverload);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testOverloaded",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testOverloaded(int expOverloadMin, int expOverloadMax) throws Exception\n{\r\n    RouterContext routerContext = cluster.getRandomRouter();\r\n    URI address = routerContext.getFileSystemURI();\r\n    Configuration conf = new HdfsConfiguration();\r\n    testOverloaded(expOverloadMin, expOverloadMax, address, conf, 10);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testOverloaded",
  "errType" : [ "RemoteException", "IOException", "InterruptedException", "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testOverloaded(int expOverloadMin, int expOverloadMax, final URI address, final Configuration conf, final int numOps) throws Exception\n{\r\n    final AtomicInteger overloadException = new AtomicInteger();\r\n    ExecutorService exec = Executors.newFixedThreadPool(numOps);\r\n    List<Future<?>> futures = new ArrayList<>();\r\n    for (int i = 0; i < numOps; i++) {\r\n        final int sleepTime = i * 50;\r\n        Future<?> future = exec.submit(new Runnable() {\r\n\r\n            @Override\r\n            public void run() {\r\n                DFSClient routerClient = null;\r\n                try {\r\n                    Thread.sleep(sleepTime);\r\n                    routerClient = new DFSClient(address, conf);\r\n                    String clientName = routerClient.getClientName();\r\n                    ClientProtocol routerProto = routerClient.getNamenode();\r\n                    routerProto.renewLease(clientName);\r\n                } catch (RemoteException re) {\r\n                    IOException ioe = re.unwrapRemoteException();\r\n                    assertTrue(\"Wrong exception: \" + ioe, ioe instanceof StandbyException);\r\n                    assertExceptionContains(\"is overloaded\", ioe);\r\n                    overloadException.incrementAndGet();\r\n                } catch (IOException e) {\r\n                    fail(\"Unexpected exception: \" + e);\r\n                } catch (InterruptedException e) {\r\n                    fail(\"Cannot sleep: \" + e);\r\n                } finally {\r\n                    if (routerClient != null) {\r\n                        try {\r\n                            routerClient.close();\r\n                        } catch (IOException e) {\r\n                            LOG.error(\"Cannot close the client\");\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        });\r\n        futures.add(future);\r\n    }\r\n    while (!futures.isEmpty()) {\r\n        futures.remove(0).get();\r\n    }\r\n    exec.shutdown();\r\n    int num = overloadException.get();\r\n    if (expOverloadMin == expOverloadMax) {\r\n        assertEquals(expOverloadMin, num);\r\n    } else {\r\n        assertTrue(\"Expected >=\" + expOverloadMin + \" but was \" + num, num >= expOverloadMin);\r\n        assertTrue(\"Expected <=\" + expOverloadMax + \" but was \" + num, num <= expOverloadMax);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testConnectionNullException",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testConnectionNullException() throws Exception\n{\r\n    setupCluster(false, false);\r\n    RouterContext routerContext = cluster.getRouters().get(0);\r\n    Router router = routerContext.getRouter();\r\n    simulateThrowExceptionRouterRpcServer(router.getRpcServer());\r\n    Configuration conf = cluster.getRouterClientConf();\r\n    conf.setBoolean(\"dfs.client.failover.random.order\", false);\r\n    DFSClient routerClient = new DFSClient(new URI(\"hdfs://fed\"), conf);\r\n    FederationRPCMetrics rpcMetrics0 = cluster.getRouters().get(0).getRouter().getRpcServer().getRPCMetrics();\r\n    FederationRPCMetrics rpcMetrics1 = cluster.getRouters().get(1).getRouter().getRpcServer().getRPCMetrics();\r\n    long originalRouter0Failures = rpcMetrics0.getProxyOpFailureCommunicate();\r\n    long originalRouter1Failures = rpcMetrics1.getProxyOpFailureCommunicate();\r\n    routerClient.getFileInfo(\"/\");\r\n    assertEquals(originalRouter0Failures + 1, rpcMetrics0.getProxyOpFailureCommunicate());\r\n    assertEquals(originalRouter1Failures, rpcMetrics1.getProxyOpFailureCommunicate());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testNoNamenodesAvailable",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testNoNamenodesAvailable() throws Exception\n{\r\n    setupCluster(false, true);\r\n    transitionClusterNSToStandby(cluster);\r\n    Configuration conf = cluster.getRouterClientConf();\r\n    conf.setBoolean(\"dfs.client.failover.random.order\", false);\r\n    conf.setInt(\"dfs.client.retry.max.attempts\", 2);\r\n    DFSClient routerClient = new DFSClient(new URI(\"hdfs://fed\"), conf);\r\n    FederationRPCMetrics rpcMetrics0 = cluster.getRouters().get(0).getRouter().getRpcServer().getRPCMetrics();\r\n    FederationRPCMetrics rpcMetrics1 = cluster.getRouters().get(1).getRouter().getRpcServer().getRPCMetrics();\r\n    long originalRouter0Failures = rpcMetrics0.getProxyOpNoNamenodes();\r\n    long originalRouter1Failures = rpcMetrics1.getProxyOpNoNamenodes();\r\n    String exceptionMessage = \"org.apache.hadoop.hdfs.server.federation.\" + \"router.NoNamenodesAvailableException: No namenodes available \" + \"under nameservice ns0\";\r\n    exceptionRule.expect(RemoteException.class);\r\n    exceptionRule.expectMessage(exceptionMessage);\r\n    routerClient.getFileInfo(\"/\");\r\n    assertEquals(originalRouter0Failures + 4, rpcMetrics0.getProxyOpNoNamenodes());\r\n    assertEquals(originalRouter1Failures, rpcMetrics1.getProxyOpNoNamenodes());\r\n    transitionClusterNSToActive(cluster, 0);\r\n    for (RouterContext routerContext : cluster.getRouters()) {\r\n        Collection<NamenodeHeartbeatService> heartbeatServices = routerContext.getRouter().getNamenodeHeartbeatServices();\r\n        for (NamenodeHeartbeatService service : heartbeatServices) {\r\n            service.periodicInvoke();\r\n        }\r\n        routerContext.getRouter().getStateStore().refreshCaches(true);\r\n    }\r\n    originalRouter0Failures = rpcMetrics0.getProxyOpNoNamenodes();\r\n    routerClient.getFileInfo(\"/\");\r\n    assertEquals(originalRouter0Failures, rpcMetrics0.getProxyOpNoNamenodes());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testAsyncCallerPoolMetrics",
  "errType" : [ "Exception", "IOException", "Exception" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testAsyncCallerPoolMetrics() throws Exception\n{\r\n    setupCluster(true, false);\r\n    simulateSlowNamenode(cluster.getCluster().getNameNode(0), 2);\r\n    final ObjectMapper objectMapper = new ObjectMapper();\r\n    cluster.getRouters().remove(1);\r\n    FederationRPCMetrics metrics = cluster.getRouters().get(0).getRouter().getRpcServer().getRPCMetrics();\r\n    Map<String, Integer> result = objectMapper.readValue(metrics.getAsyncCallerPool(), Map.class);\r\n    assertEquals(0, result.get(\"active\").intValue());\r\n    assertEquals(0, result.get(\"total\").intValue());\r\n    assertEquals(4, result.get(\"max\").intValue());\r\n    ExecutorService exec = Executors.newSingleThreadExecutor();\r\n    try {\r\n        exec.submit(() -> {\r\n            DFSClient routerClient = null;\r\n            try {\r\n                routerClient = new DFSClient(new URI(\"hdfs://fed\"), cluster.getRouterClientConf());\r\n                String clientName = routerClient.getClientName();\r\n                ClientProtocol routerProto = routerClient.getNamenode();\r\n                routerProto.renewLease(clientName);\r\n            } catch (Exception e) {\r\n                fail(\"Client request failed: \" + e);\r\n            } finally {\r\n                if (routerClient != null) {\r\n                    try {\r\n                        routerClient.close();\r\n                    } catch (IOException e) {\r\n                        LOG.error(\"Cannot close the client\");\r\n                    }\r\n                }\r\n            }\r\n        });\r\n        GenericTestUtils.waitFor(() -> {\r\n            try {\r\n                Map<String, Integer> newResult = objectMapper.readValue(metrics.getAsyncCallerPool(), Map.class);\r\n                if (newResult.get(\"active\") != 1) {\r\n                    return false;\r\n                }\r\n                if (newResult.get(\"max\") != 4) {\r\n                    return false;\r\n                }\r\n                int total = newResult.get(\"total\");\r\n                return total >= 1 && total <= 4;\r\n            } catch (Exception e) {\r\n                LOG.error(\"Not able to parse metrics result: \" + e);\r\n            }\r\n            return false;\r\n        }, 100, 2000);\r\n    } finally {\r\n        exec.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    createCluster(new HdfsConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "createCluster",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void createCluster(Configuration conf) throws IOException\n{\r\n    try {\r\n        conf.addResource(CONTRACT_HDFS_XML);\r\n        conf.addResource(CONTRACT_WEBHDFS_XML);\r\n        cluster = new MiniRouterDFSCluster(true, 2, conf);\r\n        cluster.setIndependentDNs();\r\n        cluster.setNumDatanodesPerNameservice(3);\r\n        cluster.startCluster(conf);\r\n        cluster.startRouters();\r\n        cluster.registerNamenodes();\r\n        cluster.waitNamenodeRegistration();\r\n        cluster.installMockLocations();\r\n        if (cluster.isHighAvailability()) {\r\n            for (String ns : cluster.getNameservices()) {\r\n                cluster.switchToActive(ns, NAMENODES[0]);\r\n                cluster.switchToStandby(ns, NAMENODES[1]);\r\n            }\r\n        }\r\n        cluster.waitActiveNamespaces();\r\n    } catch (Exception e) {\r\n        cluster = null;\r\n        throw new IOException(e.getCause());\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "destroyCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void destroyCluster()\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "getCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MiniDFSCluster getCluster()\n{\r\n    return cluster.getCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "getTestFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getTestFileSystem() throws IOException\n{\r\n    return getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "getFileSystem",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "FileSystem getFileSystem() throws IOException\n{\r\n    Assert.assertNotNull(\"cluster not created\", cluster);\r\n    try {\r\n        RouterContext router = cluster.getRandomRouter();\r\n        String uriStr = WebHdfsConstants.WEBHDFS_SCHEME + \"://\" + router.getHttpAddress();\r\n        URI uri = new URI(uriStr);\r\n        Configuration conf = new HdfsConfiguration();\r\n        return FileSystem.get(uri, conf);\r\n    } catch (URISyntaxException e) {\r\n        LOG.error(\"Cannot create URI for the WebHDFS filesystem\", e);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\fs\\contract\\router\\web",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return WebHdfsConstants.WEBHDFS_SCHEME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "testInstanceCreation",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testInstanceCreation()\n{\r\n    Configuration conf = new HdfsConfiguration();\r\n    conf.setClass(FEDERATION_NAMENODE_RESOLVER_CLIENT_CLASS, MockResolver.class, ActiveNamenodeResolver.class);\r\n    conf.setClass(FEDERATION_FILE_RESOLVER_CLIENT_CLASS, MockResolver.class, FileSubclusterResolver.class);\r\n    Router router = new Router();\r\n    StateStoreService stateStore = new StateStoreService();\r\n    ActiveNamenodeResolver namenodeResolverWithContext = FederationUtil.newActiveNamenodeResolver(conf, stateStore);\r\n    ActiveNamenodeResolver namenodeResolverWithoutContext = FederationUtil.newActiveNamenodeResolver(conf, null);\r\n    FileSubclusterResolver subclusterResolverWithContext = FederationUtil.newFileSubclusterResolver(conf, router);\r\n    FileSubclusterResolver subclusterResolverWithoutContext = FederationUtil.newFileSubclusterResolver(conf, null);\r\n    assertNotNull(namenodeResolverWithContext);\r\n    assertNotNull(namenodeResolverWithoutContext);\r\n    assertNotNull(subclusterResolverWithContext);\r\n    assertNotNull(subclusterResolverWithoutContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "setSeed",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void setSeed() throws Exception\n{\r\n    if (fBASE.exists() && !FileUtil.fullyDelete(fBASE)) {\r\n        throw new IOException(\"Could not fully delete \" + fBASE);\r\n    }\r\n    long seed = r.nextLong();\r\n    r.setSeed(seed);\r\n    System.out.println(name.getMethodName() + \" seed: \" + seed);\r\n    conf = new HdfsConfiguration();\r\n    conf.set(SingleUGIResolver.USER, singleUser);\r\n    conf.set(SingleUGIResolver.GROUP, singleGroup);\r\n    conf.set(DFSConfigKeys.DFS_PROVIDER_STORAGEUUID, DFSConfigKeys.DFS_PROVIDER_STORAGEUUID_DEFAULT);\r\n    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_PROVIDED_ENABLED, true);\r\n    conf.setClass(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_CLASS, TextFileRegionAliasMap.class, BlockAliasMap.class);\r\n    conf.set(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_TEXT_WRITE_DIR, nnDirPath.toString());\r\n    conf.set(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_TEXT_READ_FILE, new Path(nnDirPath, fileNameFromBlockPoolID(bpid)).toString());\r\n    conf.set(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_TEXT_DELIMITER, \"\\t\");\r\n    conf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR_PROVIDED, new File(providedPath.toUri()).toString());\r\n    File imageDir = new File(providedPath.toUri());\r\n    if (!imageDir.exists()) {\r\n        LOG.info(\"Creating directory: \" + imageDir);\r\n        imageDir.mkdirs();\r\n    }\r\n    File nnDir = new File(nnDirPath.toUri());\r\n    if (!nnDir.exists()) {\r\n        nnDir.mkdirs();\r\n    }\r\n    for (int i = 0; i < numFiles; i++) {\r\n        File newFile = new File(new Path(providedPath, filePrefix + i + fileSuffix).toUri());\r\n        if (!newFile.exists()) {\r\n            try {\r\n                LOG.info(\"Creating \" + newFile.toString());\r\n                newFile.createNewFile();\r\n                Writer writer = new OutputStreamWriter(new FileOutputStream(newFile.getAbsolutePath()), \"utf-8\");\r\n                for (int j = 0; j < baseFileLen * i; j++) {\r\n                    writer.write(\"0\");\r\n                }\r\n                writer.flush();\r\n                writer.close();\r\n                providedDataSize += newFile.length();\r\n            } catch (IOException e) {\r\n                e.printStackTrace();\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void shutdown() throws Exception\n{\r\n    try {\r\n        if (cluster != null) {\r\n            cluster.shutdown(true, true);\r\n        }\r\n    } finally {\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "createImage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createImage(TreeWalk t, Path out, Class<? extends BlockResolver> blockIdsClass) throws Exception\n{\r\n    createImage(t, out, blockIdsClass, \"\", TextFileRegionAliasMap.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "createImage",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createImage(TreeWalk t, Path out, Class<? extends BlockResolver> blockIdsClass, String clusterID, Class<? extends BlockAliasMap> aliasMapClass) throws Exception\n{\r\n    ImageWriter.Options opts = ImageWriter.defaults();\r\n    opts.setConf(conf);\r\n    opts.output(out.toString()).blocks(aliasMapClass).blockIds(blockIdsClass).clusterID(clusterID).blockPoolID(bpid);\r\n    try (ImageWriter w = new ImageWriter(opts)) {\r\n        for (TreePath e : t) {\r\n            w.accept(e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "startCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void startCluster(Path nspath, int numDatanodes, StorageType[] storageTypes, StorageType[][] storageTypesPerDatanode, boolean doFormat) throws IOException\n{\r\n    startCluster(nspath, numDatanodes, storageTypes, storageTypesPerDatanode, doFormat, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "startCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void startCluster(Path nspath, int numDatanodes, StorageType[] storageTypes, StorageType[][] storageTypesPerDatanode, boolean doFormat, String[] racks) throws IOException\n{\r\n    startCluster(nspath, numDatanodes, storageTypes, storageTypesPerDatanode, doFormat, racks, null, new MiniDFSCluster.Builder(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "startCluster",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void startCluster(Path nspath, int numDatanodes, StorageType[] storageTypes, StorageType[][] storageTypesPerDatanode, boolean doFormat, String[] racks, MiniDFSNNTopology topo, MiniDFSCluster.Builder builder) throws IOException\n{\r\n    conf.set(DFS_NAMENODE_NAME_DIR_KEY, nspath.toString());\r\n    builder.format(doFormat).manageNameDfsDirs(doFormat).numDataNodes(numDatanodes).racks(racks);\r\n    if (storageTypesPerDatanode != null) {\r\n        builder.storageTypes(storageTypesPerDatanode);\r\n    } else if (storageTypes != null) {\r\n        builder.storagesPerDatanode(storageTypes.length).storageTypes(storageTypes);\r\n    }\r\n    if (topo != null) {\r\n        builder.nnTopology(topo);\r\n        if ((topo.isHA() || topo.isFederated()) && !doFormat) {\r\n            builder.manageNameDfsDirs(true);\r\n            builder.enableManagedDfsDirsRedundancy(false);\r\n            builder.manageNameDfsSharedDirs(true);\r\n            List<File> nnDirs = getProvidedNamenodeDirs(conf.get(HDFS_MINIDFS_BASEDIR), topo);\r\n            for (File nnDir : nnDirs) {\r\n                MiniDFSCluster.copyNameDirs(Collections.singletonList(nspath.toUri()), Collections.singletonList(fileAsURI(nnDir)), conf);\r\n            }\r\n        }\r\n    }\r\n    cluster = builder.build();\r\n    cluster.waitActive();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "getProvidedNamenodeDirs",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<File> getProvidedNamenodeDirs(String baseDir, MiniDFSNNTopology topo)\n{\r\n    List<File> nnDirs = new ArrayList<>();\r\n    int nsCounter = 0;\r\n    for (MiniDFSNNTopology.NSConf nsConf : topo.getNameservices()) {\r\n        int nnCounter = nsCounter;\r\n        for (MiniDFSNNTopology.NNConf nnConf : nsConf.getNNs()) {\r\n            if (providedNameservice.equals(nsConf.getId())) {\r\n                File[] nnFiles = MiniDFSCluster.getNameNodeDirectory(baseDir, nsCounter, nnCounter);\r\n                if (nnFiles == null || nnFiles.length == 0) {\r\n                    throw new RuntimeException(\"Failed to get a location for the\" + \"Namenode directory for namespace: \" + nsConf.getId() + \" and namenodeId: \" + nnConf.getNnId());\r\n                }\r\n                nnDirs.add(nnFiles[0]);\r\n            }\r\n            nnCounter++;\r\n        }\r\n        nsCounter = nnCounter;\r\n    }\r\n    return nnDirs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testLoadImage",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testLoadImage() throws Exception\n{\r\n    final long seed = r.nextLong();\r\n    LOG.info(\"providedPath: \" + providedPath);\r\n    createImage(new RandomTreeWalk(seed), nnDirPath, FixedBlockResolver.class);\r\n    startCluster(nnDirPath, 0, new StorageType[] { StorageType.PROVIDED, StorageType.DISK }, null, false);\r\n    FileSystem fs = cluster.getFileSystem();\r\n    for (TreePath e : new RandomTreeWalk(seed)) {\r\n        FileStatus rs = e.getFileStatus();\r\n        Path hp = new Path(rs.getPath().toUri().getPath());\r\n        assertTrue(fs.exists(hp));\r\n        FileStatus hs = fs.getFileStatus(hp);\r\n        assertEquals(rs.getPath().toUri().getPath(), hs.getPath().toUri().getPath());\r\n        assertEquals(rs.getPermission(), hs.getPermission());\r\n        assertEquals(rs.getLen(), hs.getLen());\r\n        assertEquals(singleUser, hs.getOwner());\r\n        assertEquals(singleGroup, hs.getGroup());\r\n        assertEquals(rs.getAccessTime(), hs.getAccessTime());\r\n        assertEquals(rs.getModificationTime(), hs.getModificationTime());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testProvidedReporting",
  "errType" : null,
  "containingMethodsNum" : 36,
  "sourceCodeText" : "void testProvidedReporting() throws Exception\n{\r\n    conf.setClass(ImageWriter.Options.UGI_CLASS, SingleUGIResolver.class, UGIResolver.class);\r\n    createImage(new FSTreeWalk(providedPath, conf), nnDirPath, FixedBlockResolver.class);\r\n    int numDatanodes = 10;\r\n    startCluster(nnDirPath, numDatanodes, new StorageType[] { StorageType.PROVIDED, StorageType.DISK }, null, false);\r\n    long diskCapacity = 1000;\r\n    for (DataNode dn : cluster.getDataNodes()) {\r\n        for (FsVolumeSpi ref : dn.getFSDataset().getFsVolumeReferences()) {\r\n            if (ref.getStorageType() == StorageType.DISK) {\r\n                ((FsVolumeImpl) ref).setCapacityForTesting(diskCapacity);\r\n            }\r\n        }\r\n    }\r\n    cluster.triggerHeartbeats();\r\n    Thread.sleep(10000);\r\n    FSNamesystem namesystem = cluster.getNameNode().getNamesystem();\r\n    DatanodeStatistics dnStats = namesystem.getBlockManager().getDatanodeManager().getDatanodeStatistics();\r\n    assertEquals(diskCapacity * numDatanodes, namesystem.getTotal());\r\n    assertEquals(providedDataSize, dnStats.getProvidedCapacity());\r\n    assertEquals(providedDataSize, namesystem.getProvidedCapacityTotal());\r\n    assertEquals(providedDataSize, dnStats.getStorageTypeStats().get(StorageType.PROVIDED).getCapacityTotal());\r\n    assertEquals(providedDataSize, dnStats.getStorageTypeStats().get(StorageType.PROVIDED).getCapacityUsed());\r\n    for (DataNode dn : cluster.getDataNodes()) {\r\n        for (StorageReport report : dn.getFSDataset().getStorageReports(namesystem.getBlockPoolId())) {\r\n            if (report.getStorage().getStorageType() == StorageType.PROVIDED) {\r\n                assertEquals(providedDataSize, report.getCapacity());\r\n                assertEquals(providedDataSize, report.getDfsUsed());\r\n                assertEquals(providedDataSize, report.getBlockPoolUsed());\r\n                assertEquals(0, report.getNonDfsUsed());\r\n                assertEquals(0, report.getRemaining());\r\n            }\r\n        }\r\n    }\r\n    DFSClient client = new DFSClient(new InetSocketAddress(\"localhost\", cluster.getNameNodePort()), cluster.getConfiguration(0));\r\n    BlockManager bm = namesystem.getBlockManager();\r\n    for (int fileId = 0; fileId < numFiles; fileId++) {\r\n        String filename = \"/\" + filePrefix + fileId + fileSuffix;\r\n        LocatedBlocks locatedBlocks = client.getLocatedBlocks(filename, 0, baseFileLen);\r\n        for (LocatedBlock locatedBlock : locatedBlocks.getLocatedBlocks()) {\r\n            BlockInfo blockInfo = bm.getStoredBlock(locatedBlock.getBlock().getLocalBlock());\r\n            Iterator<DatanodeStorageInfo> storagesItr = blockInfo.getStorageInfos();\r\n            DatanodeStorageInfo info = storagesItr.next();\r\n            assertEquals(StorageType.PROVIDED, info.getStorageType());\r\n            DatanodeDescriptor dnDesc = info.getDatanodeDescriptor();\r\n            assertEquals(ProvidedStorageMap.ProvidedDescriptor.NETWORK_LOCATION + PATH_SEPARATOR_STR + ProvidedStorageMap.ProvidedDescriptor.NAME, NodeBase.getPath(dnDesc));\r\n            assertFalse(storagesItr.hasNext());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testDefaultReplication",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testDefaultReplication() throws Exception\n{\r\n    int targetReplication = 2;\r\n    conf.setInt(FixedBlockMultiReplicaResolver.REPLICATION, targetReplication);\r\n    createImage(new FSTreeWalk(providedPath, conf), nnDirPath, FixedBlockMultiReplicaResolver.class);\r\n    startCluster(nnDirPath, 3, null, new StorageType[][] { { StorageType.PROVIDED, StorageType.DISK }, { StorageType.PROVIDED, StorageType.DISK }, { StorageType.DISK } }, false);\r\n    Thread.sleep(50000);\r\n    FileSystem fs = cluster.getFileSystem();\r\n    int count = 0;\r\n    for (TreePath e : new FSTreeWalk(providedPath, conf)) {\r\n        FileStatus rs = e.getFileStatus();\r\n        Path hp = removePrefix(providedPath, rs.getPath());\r\n        LOG.info(\"path: \" + hp.toUri().getPath());\r\n        e.accept(count++);\r\n        assertTrue(fs.exists(hp));\r\n        FileStatus hs = fs.getFileStatus(hp);\r\n        if (rs.isFile()) {\r\n            BlockLocation[] bl = fs.getFileBlockLocations(hs.getPath(), 0, hs.getLen());\r\n            int i = 0;\r\n            for (; i < bl.length; i++) {\r\n                int currentRep = bl[i].getHosts().length;\r\n                assertEquals(targetReplication, currentRep);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "removePrefix",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "Path removePrefix(Path base, Path walk)\n{\r\n    Path wpath = new Path(walk.toUri().getPath());\r\n    Path bpath = new Path(base.toUri().getPath());\r\n    Path ret = new Path(\"/\");\r\n    while (!(bpath.equals(wpath) || \"\".equals(wpath.getName()))) {\r\n        ret = \"\".equals(ret.getName()) ? new Path(\"/\", wpath.getName()) : new Path(new Path(\"/\", wpath.getName()), new Path(ret.toString().substring(1)));\r\n        wpath = wpath.getParent();\r\n    }\r\n    if (!bpath.equals(wpath)) {\r\n        throw new IllegalArgumentException(base + \" not a prefix of \" + walk);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "verifyFileSystemContents",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void verifyFileSystemContents(int nnIndex) throws Exception\n{\r\n    FileSystem fs = cluster.getFileSystem(nnIndex);\r\n    int count = 0;\r\n    for (TreePath e : new FSTreeWalk(providedPath, conf)) {\r\n        FileStatus rs = e.getFileStatus();\r\n        Path hp = removePrefix(providedPath, rs.getPath());\r\n        LOG.info(\"path: \" + hp.toUri().getPath());\r\n        e.accept(count++);\r\n        assertTrue(fs.exists(hp));\r\n        FileStatus hs = fs.getFileStatus(hp);\r\n        assertEquals(hp.toUri().getPath(), hs.getPath().toUri().getPath());\r\n        assertEquals(rs.getPermission(), hs.getPermission());\r\n        assertEquals(rs.getOwner(), hs.getOwner());\r\n        assertEquals(rs.getGroup(), hs.getGroup());\r\n        if (rs.isFile()) {\r\n            assertEquals(rs.getLen(), hs.getLen());\r\n            try (ReadableByteChannel i = Channels.newChannel(new FileInputStream(new File(rs.getPath().toUri())))) {\r\n                try (ReadableByteChannel j = Channels.newChannel(fs.open(hs.getPath()))) {\r\n                    ByteBuffer ib = ByteBuffer.allocate(4096);\r\n                    ByteBuffer jb = ByteBuffer.allocate(4096);\r\n                    while (true) {\r\n                        int il = i.read(ib);\r\n                        int jl = j.read(jb);\r\n                        if (il < 0 || jl < 0) {\r\n                            assertEquals(il, jl);\r\n                            break;\r\n                        }\r\n                        ib.flip();\r\n                        jb.flip();\r\n                        int cmp = Math.min(ib.remaining(), jb.remaining());\r\n                        for (int k = 0; k < cmp; ++k) {\r\n                            assertEquals(ib.get(), jb.get());\r\n                        }\r\n                        ib.compact();\r\n                        jb.compact();\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "BlockLocation[] createFile(Path path, short replication, long fileLen, long blockLen) throws IOException\n{\r\n    FileSystem fs = cluster.getFileSystem();\r\n    DFSTestUtil.createFile(fs, path, false, (int) blockLen, fileLen, blockLen, replication, 0, true);\r\n    return fs.getFileBlockLocations(path, 0, fileLen);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testClusterWithEmptyImage",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testClusterWithEmptyImage() throws IOException\n{\r\n    startCluster(nnDirPath, 2, null, new StorageType[][] { { StorageType.DISK }, { StorageType.DISK } }, true);\r\n    assertTrue(cluster.isClusterUp());\r\n    assertTrue(cluster.isDataNodeUp());\r\n    BlockLocation[] locations = createFile(new Path(\"/testFile1.dat\"), (short) 2, 1024 * 1024, 1024 * 1024);\r\n    assertEquals(1, locations.length);\r\n    assertEquals(2, locations[0].getHosts().length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "getAndCheckBlockLocations",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "DatanodeInfo[] getAndCheckBlockLocations(DFSClient client, String filename, long fileLen, long expectedBlocks, int expectedLocations) throws IOException\n{\r\n    LocatedBlocks locatedBlocks = client.getLocatedBlocks(filename, 0, fileLen);\r\n    assertEquals(expectedBlocks, locatedBlocks.getLocatedBlocks().size());\r\n    DatanodeInfo[] locations = locatedBlocks.getLocatedBlocks().get(0).getLocations();\r\n    assertEquals(expectedLocations, locations.length);\r\n    checkUniqueness(locations);\r\n    return locations;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "checkUniqueness",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkUniqueness(DatanodeInfo[] locations)\n{\r\n    Set<String> set = new HashSet<>();\r\n    for (DatanodeInfo info : locations) {\r\n        assertFalse(\"All locations should be unique\", set.contains(info.getDatanodeUuid()));\r\n        set.add(info.getDatanodeUuid());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testSetReplicationForProvidedFiles",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSetReplicationForProvidedFiles() throws Exception\n{\r\n    createImage(new FSTreeWalk(providedPath, conf), nnDirPath, FixedBlockResolver.class);\r\n    startCluster(nnDirPath, 10, new StorageType[] { StorageType.PROVIDED, StorageType.DISK }, null, false);\r\n    setAndUnsetReplication(\"/\" + filePrefix + (numFiles - 1) + fileSuffix);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "setAndUnsetReplication",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setAndUnsetReplication(String filename) throws Exception\n{\r\n    Path file = new Path(filename);\r\n    FileSystem fs = cluster.getFileSystem();\r\n    short newReplication = 4;\r\n    LOG.info(\"Setting replication of file {} to {}\", filename, newReplication);\r\n    fs.setReplication(file, newReplication);\r\n    DFSTestUtil.waitForReplication((DistributedFileSystem) fs, file, newReplication, 10000);\r\n    DFSClient client = new DFSClient(new InetSocketAddress(\"localhost\", cluster.getNameNodePort()), cluster.getConfiguration(0));\r\n    getAndCheckBlockLocations(client, filename, baseFileLen, 1, newReplication);\r\n    newReplication = 1;\r\n    LOG.info(\"Setting replication of file {} back to {}\", filename, newReplication);\r\n    fs.setReplication(file, newReplication);\r\n    int defaultReplication = conf.getInt(DFSConfigKeys.DFS_REPLICATION_KEY, DFSConfigKeys.DFS_REPLICATION_DEFAULT);\r\n    DFSTestUtil.waitForReplication((DistributedFileSystem) fs, file, (short) defaultReplication, 10000);\r\n    getAndCheckBlockLocations(client, filename, baseFileLen, 1, defaultReplication);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testProvidedDatanodeFailures",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testProvidedDatanodeFailures() throws Exception\n{\r\n    createImage(new FSTreeWalk(providedPath, conf), nnDirPath, FixedBlockResolver.class);\r\n    startCluster(nnDirPath, 3, null, new StorageType[][] { { StorageType.PROVIDED, StorageType.DISK }, { StorageType.PROVIDED, StorageType.DISK }, { StorageType.DISK } }, false);\r\n    DataNode providedDatanode1 = cluster.getDataNodes().get(0);\r\n    DataNode providedDatanode2 = cluster.getDataNodes().get(1);\r\n    DFSClient client = new DFSClient(new InetSocketAddress(\"localhost\", cluster.getNameNodePort()), cluster.getConfiguration(0));\r\n    DatanodeStorageInfo providedDNInfo = getProvidedDatanodeStorageInfo();\r\n    if (numFiles >= 1) {\r\n        String filename = \"/\" + filePrefix + (numFiles - 1) + fileSuffix;\r\n        DatanodeInfo[] dnInfos = getAndCheckBlockLocations(client, filename, baseFileLen, 1, 2);\r\n        assertTrue(dnInfos[0].getDatanodeUuid().equals(providedDatanode1.getDatanodeUuid()) || dnInfos[0].getDatanodeUuid().equals(providedDatanode2.getDatanodeUuid()));\r\n        MiniDFSCluster.DataNodeProperties providedDNProperties1 = cluster.stopDataNode(0);\r\n        BlockManagerTestUtil.noticeDeadDatanode(cluster.getNameNode(), providedDatanode1.getDatanodeId().getXferAddr());\r\n        dnInfos = getAndCheckBlockLocations(client, filename, baseFileLen, 1, 1);\r\n        assertEquals(providedDatanode2.getDatanodeUuid(), dnInfos[0].getDatanodeUuid());\r\n        MiniDFSCluster.DataNodeProperties providedDNProperties2 = cluster.stopDataNode(0);\r\n        BlockManagerTestUtil.noticeDeadDatanode(cluster.getNameNode(), providedDatanode2.getDatanodeId().getXferAddr());\r\n        getAndCheckBlockLocations(client, filename, baseFileLen, 1, 0);\r\n        assertEquals(0, providedDNInfo.getBlockReportCount());\r\n        cluster.restartDataNode(providedDNProperties1, true);\r\n        cluster.waitActive();\r\n        assertEquals(1, providedDNInfo.getBlockReportCount());\r\n        dnInfos = getAndCheckBlockLocations(client, filename, baseFileLen, 1, 1);\r\n        assertEquals(providedDatanode1.getDatanodeId().getXferAddr(), dnInfos[0].getXferAddr());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testTransientDeadDatanodes",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTransientDeadDatanodes() throws Exception\n{\r\n    createImage(new FSTreeWalk(providedPath, conf), nnDirPath, FixedBlockResolver.class);\r\n    startCluster(nnDirPath, 3, null, new StorageType[][] { { StorageType.PROVIDED, StorageType.DISK }, { StorageType.PROVIDED, StorageType.DISK }, { StorageType.DISK } }, false);\r\n    DataNode providedDatanode = cluster.getDataNodes().get(0);\r\n    DatanodeStorageInfo providedDNInfo = getProvidedDatanodeStorageInfo();\r\n    int initialBRCount = providedDNInfo.getBlockReportCount();\r\n    for (int i = 0; i < numFiles; i++) {\r\n        verifyFileLocation(i, 2);\r\n        BlockManagerTestUtil.noticeDeadDatanode(cluster.getNameNode(), providedDatanode.getDatanodeId().getXferAddr());\r\n        cluster.waitActive();\r\n        cluster.triggerHeartbeats();\r\n        Thread.sleep(1000);\r\n        assertEquals(initialBRCount + i + 1, providedDNInfo.getBlockReportCount());\r\n        verifyFileLocation(i, 2);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "getProvidedDatanodeStorageInfo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DatanodeStorageInfo getProvidedDatanodeStorageInfo()\n{\r\n    ProvidedStorageMap providedStorageMap = cluster.getNamesystem().getBlockManager().getProvidedStorageMap();\r\n    return providedStorageMap.getProvidedStorageInfo();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testNamenodeRestart",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testNamenodeRestart() throws Exception\n{\r\n    createImage(new FSTreeWalk(providedPath, conf), nnDirPath, FixedBlockResolver.class);\r\n    startCluster(nnDirPath, 3, null, new StorageType[][] { { StorageType.PROVIDED, StorageType.DISK }, { StorageType.PROVIDED, StorageType.DISK }, { StorageType.DISK } }, false);\r\n    verifyFileLocation(numFiles - 1, 2);\r\n    cluster.restartNameNodes();\r\n    cluster.waitActive();\r\n    verifyFileLocation(numFiles - 1, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "verifyFileLocation",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void verifyFileLocation(int fileIndex, int replication) throws Exception\n{\r\n    DFSClient client = new DFSClient(new InetSocketAddress(\"localhost\", cluster.getNameNodePort()), cluster.getConfiguration(0));\r\n    if (fileIndex < numFiles && fileIndex >= 0) {\r\n        String filename = filePrefix + fileIndex + fileSuffix;\r\n        File file = new File(new Path(providedPath, filename).toUri());\r\n        long fileLen = file.length();\r\n        long blockSize = conf.getLong(FixedBlockResolver.BLOCKSIZE, FixedBlockResolver.BLOCKSIZE_DEFAULT);\r\n        long numLocatedBlocks = fileLen == 0 ? 1 : (long) Math.ceil(fileLen * 1.0 / blockSize);\r\n        getAndCheckBlockLocations(client, \"/\" + filename, fileLen, numLocatedBlocks, replication);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testSetClusterID",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSetClusterID() throws Exception\n{\r\n    String clusterID = \"PROVIDED-CLUSTER\";\r\n    createImage(new FSTreeWalk(providedPath, conf), nnDirPath, FixedBlockResolver.class, clusterID, TextFileRegionAliasMap.class);\r\n    startCluster(nnDirPath, 2, null, new StorageType[][] { { StorageType.PROVIDED, StorageType.DISK }, { StorageType.DISK } }, false);\r\n    NameNode nn = cluster.getNameNode();\r\n    assertEquals(clusterID, nn.getNamesystem().getClusterId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testNumberOfProvidedLocations",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testNumberOfProvidedLocations() throws Exception\n{\r\n    conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY, 4);\r\n    createImage(new FSTreeWalk(providedPath, conf), nnDirPath, FixedBlockResolver.class);\r\n    startCluster(nnDirPath, 4, new StorageType[] { StorageType.PROVIDED, StorageType.DISK }, null, false);\r\n    int expectedLocations = 4;\r\n    for (int i = 0; i < numFiles; i++) {\r\n        verifyFileLocation(i, expectedLocations);\r\n    }\r\n    for (int i = 1; i <= 2; i++) {\r\n        DataNode dn = cluster.getDataNodes().get(0);\r\n        cluster.stopDataNode(0);\r\n        BlockManagerTestUtil.noticeDeadDatanode(cluster.getNameNode(), dn.getDatanodeId().getXferAddr());\r\n        expectedLocations = 4 - i;\r\n        for (int j = 0; j < numFiles; j++) {\r\n            verifyFileLocation(j, expectedLocations);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testNumberOfProvidedLocationsManyBlocks",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testNumberOfProvidedLocationsManyBlocks() throws Exception\n{\r\n    conf.setLong(FixedBlockResolver.BLOCKSIZE, baseFileLen / 10);\r\n    conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY, 4);\r\n    createImage(new FSTreeWalk(providedPath, conf), nnDirPath, FixedBlockResolver.class);\r\n    startCluster(nnDirPath, 4, new StorageType[] { StorageType.PROVIDED, StorageType.DISK }, null, false);\r\n    int expectedLocations = 4;\r\n    for (int i = 0; i < numFiles; i++) {\r\n        verifyFileLocation(i, expectedLocations);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "createInMemoryAliasMapImage",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "File createInMemoryAliasMapImage() throws Exception\n{\r\n    conf.setClass(ImageWriter.Options.UGI_CLASS, FsUGIResolver.class, UGIResolver.class);\r\n    conf.setClass(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_CLASS, InMemoryLevelDBAliasMapClient.class, BlockAliasMap.class);\r\n    conf.set(DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS, \"localhost:32445\");\r\n    File tempDirectory = new File(new Path(nnDirPath, \"in-memory-alias-map\").toUri());\r\n    File levelDBDir = new File(tempDirectory, bpid);\r\n    levelDBDir.mkdirs();\r\n    conf.set(DFS_PROVIDED_ALIASMAP_INMEMORY_LEVELDB_DIR, tempDirectory.getAbsolutePath());\r\n    conf.setInt(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_LOAD_RETRIES, 10);\r\n    conf.set(DFS_PROVIDED_ALIASMAP_LEVELDB_PATH, tempDirectory.getAbsolutePath());\r\n    createImage(new FSTreeWalk(providedPath, conf), nnDirPath, FixedBlockResolver.class, clusterID, LevelDBFileRegionAliasMap.class);\r\n    return tempDirectory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testInMemoryAliasMap",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testInMemoryAliasMap() throws Exception\n{\r\n    File aliasMapImage = createInMemoryAliasMapImage();\r\n    conf.setBoolean(DFS_PROVIDED_ALIASMAP_INMEMORY_ENABLED, true);\r\n    conf.setInt(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_LOAD_RETRIES, 10);\r\n    startCluster(nnDirPath, 2, new StorageType[] { StorageType.PROVIDED, StorageType.DISK }, null, false);\r\n    verifyFileSystemContents(0);\r\n    FileUtils.deleteDirectory(aliasMapImage);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "getUnAssignedPort",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getUnAssignedPort(Set<Integer> usedPorts, int maxTrials)\n{\r\n    int count = 0;\r\n    while (count < maxTrials) {\r\n        int port = NetUtils.getFreeSocketPort();\r\n        if (usedPorts.contains(port)) {\r\n            count++;\r\n        } else {\r\n            return port;\r\n        }\r\n    }\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "configureAliasMapAddresses",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void configureAliasMapAddresses(MiniDFSNNTopology topology, String providedNameservice)\n{\r\n    conf.unset(DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS);\r\n    Set<Integer> assignedPorts = new HashSet<>();\r\n    for (MiniDFSNNTopology.NSConf nsConf : topology.getNameservices()) {\r\n        for (MiniDFSNNTopology.NNConf nnConf : nsConf.getNNs()) {\r\n            if (providedNameservice.equals(nsConf.getId())) {\r\n                String key = DFSUtil.addKeySuffixes(DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS, nsConf.getId(), nnConf.getNnId());\r\n                int port = getUnAssignedPort(assignedPorts, 10);\r\n                if (port == -1) {\r\n                    throw new RuntimeException(\"No free ports available\");\r\n                }\r\n                assignedPorts.add(port);\r\n                conf.set(key, \"127.0.0.1:\" + port);\r\n                String binHostKey = DFSUtil.addKeySuffixes(DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_BIND_HOST, nsConf.getId(), nnConf.getNnId());\r\n                conf.set(binHostKey, \"0.0.0.0\");\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "verifyPathsWithHAFailoverIfNecessary",
  "errType" : [ "RemoteException", "NullPointerException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void verifyPathsWithHAFailoverIfNecessary(MiniDFSNNTopology topology, String providedNameservice) throws Exception\n{\r\n    List<Integer> nnIndexes = cluster.getNNIndexes(providedNameservice);\r\n    if (topology.isHA()) {\r\n        int nn1 = nnIndexes.get(0);\r\n        int nn2 = nnIndexes.get(1);\r\n        try {\r\n            verifyFileSystemContents(nn1);\r\n            fail(\"Read operation should fail as no Namenode is active\");\r\n        } catch (RemoteException e) {\r\n            LOG.info(\"verifyPaths failed!. Expected exception: {}\" + e);\r\n        }\r\n        cluster.transitionToActive(nn1);\r\n        LOG.info(\"Verifying data from NN with index = {}\", nn1);\r\n        verifyFileSystemContents(nn1);\r\n        cluster.transitionToStandby(nn1);\r\n        cluster.transitionToActive(nn2);\r\n        LOG.info(\"Verifying data from NN with index = {}\", nn2);\r\n        verifyFileSystemContents(nn2);\r\n        cluster.shutdownNameNodes();\r\n        try {\r\n            verifyFileSystemContents(nn2);\r\n            fail(\"Read operation should fail as no Namenode is active\");\r\n        } catch (NullPointerException e) {\r\n            LOG.info(\"verifyPaths failed!. Expected exception: {}\" + e);\r\n        }\r\n    } else {\r\n        verifyFileSystemContents(nnIndexes.get(0));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testInMemoryAliasMapMultiTopologies",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testInMemoryAliasMapMultiTopologies() throws Exception\n{\r\n    MiniDFSNNTopology[] topologies = new MiniDFSNNTopology[] { MiniDFSNNTopology.simpleHATopology(), MiniDFSNNTopology.simpleFederatedTopology(3), MiniDFSNNTopology.simpleHAFederatedTopology(3) };\r\n    for (MiniDFSNNTopology topology : topologies) {\r\n        LOG.info(\"Starting test with topology with HA = {}, federation = {}\", topology.isHA(), topology.isFederated());\r\n        setSeed();\r\n        createInMemoryAliasMapImage();\r\n        conf.setBoolean(DFS_PROVIDED_ALIASMAP_INMEMORY_ENABLED, true);\r\n        conf.setInt(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_LOAD_RETRIES, 10);\r\n        providedNameservice = topology.getNameservices().get(0).getId();\r\n        configureAliasMapAddresses(topology, providedNameservice);\r\n        startCluster(nnDirPath, 2, new StorageType[] { StorageType.PROVIDED, StorageType.DISK }, null, false, null, topology, new MiniDFSClusterBuilderAliasMap(conf));\r\n        verifyPathsWithHAFailoverIfNecessary(topology, providedNameservice);\r\n        shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "getDatanodeDescriptor",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DatanodeDescriptor getDatanodeDescriptor(DatanodeManager dnm, int dnIndex) throws Exception\n{\r\n    return dnm.getDatanode(cluster.getDataNodes().get(dnIndex).getDatanodeId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "startDecommission",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void startDecommission(FSNamesystem namesystem, DatanodeManager dnm, int dnIndex) throws Exception\n{\r\n    namesystem.writeLock();\r\n    DatanodeDescriptor dnDesc = getDatanodeDescriptor(dnm, dnIndex);\r\n    dnm.getDatanodeAdminManager().startDecommission(dnDesc);\r\n    namesystem.writeUnlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "startMaintenance",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void startMaintenance(FSNamesystem namesystem, DatanodeManager dnm, int dnIndex) throws Exception\n{\r\n    namesystem.writeLock();\r\n    DatanodeDescriptor dnDesc = getDatanodeDescriptor(dnm, dnIndex);\r\n    dnm.getDatanodeAdminManager().startMaintenance(dnDesc, Long.MAX_VALUE);\r\n    namesystem.writeUnlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "stopMaintenance",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void stopMaintenance(FSNamesystem namesystem, DatanodeManager dnm, int dnIndex) throws Exception\n{\r\n    namesystem.writeLock();\r\n    DatanodeDescriptor dnDesc = getDatanodeDescriptor(dnm, dnIndex);\r\n    dnm.getDatanodeAdminManager().stopMaintenance(dnDesc);\r\n    namesystem.writeUnlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testDatanodeLifeCycle",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testDatanodeLifeCycle() throws Exception\n{\r\n    createImage(new FSTreeWalk(providedPath, conf), nnDirPath, FixedBlockResolver.class);\r\n    startCluster(nnDirPath, 3, new StorageType[] { StorageType.PROVIDED, StorageType.DISK }, null, false);\r\n    int fileIndex = numFiles - 1;\r\n    final BlockManager blockManager = cluster.getNamesystem().getBlockManager();\r\n    final DatanodeManager dnm = blockManager.getDatanodeManager();\r\n    verifyFileLocation(fileIndex, 3);\r\n    startDecommission(cluster.getNamesystem(), dnm, 0);\r\n    verifyFileLocation(fileIndex, 3);\r\n    cluster.triggerHeartbeats();\r\n    verifyFileLocation(fileIndex, 3);\r\n    startMaintenance(cluster.getNamesystem(), dnm, 1);\r\n    verifyFileLocation(fileIndex, 3);\r\n    DataNode dn1 = cluster.getDataNodes().get(0);\r\n    DataNode dn2 = cluster.getDataNodes().get(1);\r\n    MiniDFSCluster.DataNodeProperties dn1Properties = cluster.stopDataNode(0);\r\n    BlockManagerTestUtil.noticeDeadDatanode(cluster.getNameNode(), dn1.getDatanodeId().getXferAddr());\r\n    verifyFileLocation(fileIndex, 2);\r\n    MiniDFSCluster.DataNodeProperties dn2Properties = cluster.stopDataNode(1);\r\n    BlockManagerTestUtil.noticeDeadDatanode(cluster.getNameNode(), dn2.getDatanodeId().getXferAddr());\r\n    verifyFileLocation(fileIndex, 2);\r\n    stopMaintenance(cluster.getNamesystem(), dnm, 0);\r\n    verifyFileLocation(fileIndex, 1);\r\n    cluster.restartDataNode(dn1Properties, true);\r\n    cluster.waitActive();\r\n    verifyFileLocation(fileIndex, 2);\r\n    cluster.restartDataNode(dn2Properties, true);\r\n    cluster.waitActive();\r\n    verifyFileLocation(fileIndex, 3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testProvidedWithHierarchicalTopology",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testProvidedWithHierarchicalTopology() throws Exception\n{\r\n    conf.setClass(ImageWriter.Options.UGI_CLASS, FsUGIResolver.class, UGIResolver.class);\r\n    String packageName = \"org.apache.hadoop.hdfs.server.blockmanagement\";\r\n    String[] policies = new String[] { \"BlockPlacementPolicyDefault\", \"BlockPlacementPolicyRackFaultTolerant\", \"BlockPlacementPolicyWithNodeGroup\", \"BlockPlacementPolicyWithUpgradeDomain\" };\r\n    createImage(new FSTreeWalk(providedPath, conf), nnDirPath, FixedBlockResolver.class);\r\n    String[] racks = { \"/pod0/rack0\", \"/pod0/rack0\", \"/pod0/rack1\", \"/pod0/rack1\", \"/pod1/rack0\", \"/pod1/rack0\", \"/pod1/rack1\", \"/pod1/rack1\" };\r\n    for (String policy : policies) {\r\n        LOG.info(\"Using policy: \" + packageName + \".\" + policy);\r\n        conf.set(DFS_BLOCK_REPLICATOR_CLASSNAME_KEY, packageName + \".\" + policy);\r\n        startCluster(nnDirPath, racks.length, new StorageType[] { StorageType.PROVIDED, StorageType.DISK }, null, false, racks);\r\n        verifyFileSystemContents(0);\r\n        setAndUnsetReplication(\"/\" + filePrefix + (numFiles - 1) + fileSuffix);\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testBootstrapAliasMap",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testBootstrapAliasMap() throws Exception\n{\r\n    int numNamenodes = 3;\r\n    MiniDFSNNTopology topology = MiniDFSNNTopology.simpleHATopology(numNamenodes);\r\n    createInMemoryAliasMapImage();\r\n    conf.setBoolean(DFS_PROVIDED_ALIASMAP_INMEMORY_ENABLED, true);\r\n    conf.setInt(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_LOAD_RETRIES, 10);\r\n    providedNameservice = topology.getNameservices().get(0).getId();\r\n    configureAliasMapAddresses(topology, providedNameservice);\r\n    startCluster(nnDirPath, 2, new StorageType[] { StorageType.PROVIDED, StorageType.DISK }, null, false, null, topology, new MiniDFSClusterBuilderAliasMap(conf));\r\n    cluster.transitionToActive(0);\r\n    verifyFileSystemContents(0);\r\n    for (int nnIndex = 1; nnIndex < numNamenodes; nnIndex++) {\r\n        cluster.shutdownNameNode(nnIndex);\r\n        for (URI u : cluster.getNameDirs(nnIndex)) {\r\n            File dir = new File(u.getPath());\r\n            assertTrue(FileUtil.fullyDelete(dir));\r\n        }\r\n    }\r\n    for (int index = 1; index < numNamenodes; index++) {\r\n        File aliasMapDir = new File(fBASE, \"aliasmap-\" + index);\r\n        if (!new File(aliasMapDir, \"tempDir\").mkdirs()) {\r\n            throw new IOException(\"Unable to create directory \" + aliasMapDir);\r\n        }\r\n        Configuration currNNConf = cluster.getConfiguration(index);\r\n        currNNConf.set(DFS_PROVIDED_ALIASMAP_INMEMORY_LEVELDB_DIR, aliasMapDir.getAbsolutePath());\r\n        int rc = BootstrapStandby.run(new String[] { \"-nonInteractive\" }, currNNConf);\r\n        assertNotEquals(0, rc);\r\n        rc = BootstrapStandby.run(new String[] { \"-nonInteractive\", \"-force\" }, currNNConf);\r\n        assertEquals(0, rc);\r\n    }\r\n    checkInMemoryAliasMapContents(0, numNamenodes);\r\n    for (int i = 1; i < numNamenodes; i++) {\r\n        cluster.restartNameNode(i, false);\r\n    }\r\n    cluster.waitClusterUp();\r\n    cluster.waitActive();\r\n    int nextNN = 1;\r\n    cluster.shutdownNameNode(0);\r\n    cluster.transitionToActive(nextNN);\r\n    verifyFileSystemContents(nextNN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "checkInMemoryAliasMapContents",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkInMemoryAliasMapContents(int baseNN, int numNamenodes) throws Exception\n{\r\n    InMemoryLevelDBAliasMapServer baseAliasMap = cluster.getNameNode(baseNN).getAliasMapServer();\r\n    for (int i = 0; i < numNamenodes; i++) {\r\n        if (baseNN == i) {\r\n            continue;\r\n        }\r\n        InMemoryLevelDBAliasMapServer otherAliasMap = cluster.getNameNode(baseNN).getAliasMapServer();\r\n        verifyAliasMapEquals(baseAliasMap, otherAliasMap);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "verifyAliasMapEquals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyAliasMapEquals(InMemoryLevelDBAliasMapServer aliasMap1, InMemoryLevelDBAliasMapServer aliasMap2) throws Exception\n{\r\n    Set<FileRegion> fileRegions1 = getFileRegions(aliasMap1);\r\n    Set<FileRegion> fileRegions2 = getFileRegions(aliasMap2);\r\n    assertTrue(fileRegions1.equals(fileRegions2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "getFileRegions",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Set<FileRegion> getFileRegions(InMemoryLevelDBAliasMapServer aliasMap) throws IOException\n{\r\n    Set<FileRegion> fileRegions = new HashSet<>();\r\n    Optional<Block> marker = Optional.empty();\r\n    while (true) {\r\n        InMemoryAliasMapProtocol.IterationResult result = aliasMap.list(marker);\r\n        fileRegions.addAll(result.getFileRegions());\r\n        marker = result.getNextBlock();\r\n        if (!marker.isPresent()) {\r\n            break;\r\n        }\r\n    }\r\n    return fileRegions;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup()\n{\r\n    Configuration conf = new Configuration(false);\r\n    conf.setLong(FixedBlockResolver.BLOCKSIZE, 512L * (1L << 20));\r\n    conf.setLong(FixedBlockResolver.START_BLOCK, 512L * (1L << 20));\r\n    blockId.setConf(conf);\r\n    System.out.println(name.getMethodName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testExactBlock",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testExactBlock() throws Exception\n{\r\n    FileStatus f = file(512, 256);\r\n    int nblocks = 0;\r\n    for (BlockProto b : blockId.resolve(f)) {\r\n        ++nblocks;\r\n        assertEquals(512L * (1L << 20), b.getNumBytes());\r\n    }\r\n    assertEquals(1, nblocks);\r\n    FileStatus g = file(1024, 256);\r\n    nblocks = 0;\r\n    for (BlockProto b : blockId.resolve(g)) {\r\n        ++nblocks;\r\n        assertEquals(512L * (1L << 20), b.getNumBytes());\r\n    }\r\n    assertEquals(2, nblocks);\r\n    FileStatus h = file(5120, 256);\r\n    nblocks = 0;\r\n    for (BlockProto b : blockId.resolve(h)) {\r\n        ++nblocks;\r\n        assertEquals(512L * (1L << 20), b.getNumBytes());\r\n    }\r\n    assertEquals(10, nblocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testEmpty",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testEmpty() throws Exception\n{\r\n    FileStatus f = file(0, 100);\r\n    Iterator<BlockProto> b = blockId.resolve(f).iterator();\r\n    assertTrue(b.hasNext());\r\n    assertEquals(0, b.next().getNumBytes());\r\n    assertFalse(b.hasNext());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testRandomFile",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testRandomFile() throws Exception\n{\r\n    Random r = new Random();\r\n    long seed = r.nextLong();\r\n    System.out.println(\"seed: \" + seed);\r\n    r.setSeed(seed);\r\n    int len = r.nextInt(4096) + 512;\r\n    int blk = r.nextInt(len - 128) + 128;\r\n    FileStatus s = file(len, blk);\r\n    long nbytes = 0;\r\n    for (BlockProto b : blockId.resolve(s)) {\r\n        nbytes += b.getNumBytes();\r\n        assertTrue(512L * (1L << 20) >= b.getNumBytes());\r\n    }\r\n    assertEquals(s.getLen(), nbytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "file",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileStatus file(long lenMB, long blocksizeMB)\n{\r\n    Path p = new Path(\"foo://bar:4344/baz/dingo\");\r\n    return new FileStatus(lenMB * (1 << 20), false, 1, blocksizeMB * (1 << 20), 0L, 0L, null, \"hadoop\", \"hadoop\", p);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testImportAcl",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testImportAcl() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(DFSConfigKeys.DFS_PROVIDED_ACLS_IMPORT_ENABLED, true);\r\n    FileSystem fs = mock(FileSystem.class);\r\n    Path root = mock(Path.class);\r\n    when(root.getFileSystem(conf)).thenReturn(fs);\r\n    Map<Path, FileStatus> expectedChildren = new HashMap<>();\r\n    FileStatus child1 = new FileStatus(0, true, 0, 0, 1, new Path(\"/a\"));\r\n    FileStatus child2 = new FileStatus(0, true, 0, 0, 1, new Path(\"/b\"));\r\n    expectedChildren.put(child1.getPath(), child1);\r\n    expectedChildren.put(child2.getPath(), child2);\r\n    when(fs.listStatus(root)).thenReturn(expectedChildren.values().toArray(new FileStatus[1]));\r\n    AclStatus expectedAcls = mock(AclStatus.class);\r\n    when(fs.getAclStatus(any(Path.class))).thenReturn(expectedAcls);\r\n    FSTreeWalk fsTreeWalk = new FSTreeWalk(root, conf);\r\n    FileStatus rootFileStatus = new FileStatus(0, true, 0, 0, 1, root);\r\n    TreePath treePath = new TreePath(rootFileStatus, 1, null);\r\n    Iterable<TreePath> result = fsTreeWalk.getChildren(treePath, 1, null);\r\n    for (TreePath path : result) {\r\n        FileStatus expectedChildStatus = expectedChildren.remove(path.getFileStatus().getPath());\r\n        assertNotNull(expectedChildStatus);\r\n        AclStatus childAcl = path.getAclStatus();\r\n        assertEquals(expectedAcls, childAcl);\r\n    }\r\n    assertEquals(0, expectedChildren.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testACLNotSupported",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testACLNotSupported() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(DFSConfigKeys.DFS_PROVIDED_ACLS_IMPORT_ENABLED, true);\r\n    FileSystem fs = mock(FileSystem.class);\r\n    when(fs.getAclStatus(any())).thenThrow(new UnsupportedOperationException());\r\n    Path root = mock(Path.class);\r\n    when(root.getFileSystem(conf)).thenReturn(fs);\r\n    FileStatus rootFileStatus = new FileStatus(0, true, 0, 0, 1, root);\r\n    when(fs.getFileStatus(root)).thenReturn(rootFileStatus);\r\n    FSTreeWalk fsTreeWalk = new FSTreeWalk(root, conf);\r\n    TreeWalk.TreeIterator iter = fsTreeWalk.iterator();\r\n    fail(\"Unexpected successful creation of iter: \" + iter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testToINodeACLNotSupported",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testToINodeACLNotSupported() throws Exception\n{\r\n    BlockResolver blockResolver = new FixedBlockResolver();\r\n    Path root = new Path(\"/\");\r\n    FileStatus rootFileStatus = new FileStatus(0, false, 0, 0, 1, root);\r\n    AclStatus acls = mock(AclStatus.class);\r\n    TreePath treePath = new TreePath(rootFileStatus, 1, null, null, acls);\r\n    UGIResolver ugiResolver = mock(UGIResolver.class);\r\n    when(ugiResolver.getPermissionsProto(null, acls)).thenReturn(1L);\r\n    treePath.toINode(ugiResolver, blockResolver, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "randomRoot",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path randomRoot(long seed)\n{\r\n    Random r = new Random(seed);\r\n    String scheme;\r\n    do {\r\n        scheme = genName(r, 3, 5).toLowerCase();\r\n    } while (Character.isDigit(scheme.charAt(0)));\r\n    String authority = genName(r, 3, 15).toLowerCase();\r\n    int port = r.nextInt(1 << 13) + 1000;\r\n    return new Path(scheme, authority + \":\" + port, \"/\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "iterator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TreeIterator iterator()\n{\r\n    return new RandomTreeIterator(seed);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "getChildren",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Iterable<TreePath> getChildren(TreePath p, long id, TreeIterator walk)\n{\r\n    final FileStatus pFs = p.getFileStatus();\r\n    if (pFs.isFile()) {\r\n        return Collections.emptyList();\r\n    }\r\n    long cseed = mSeed.get(p.getParentId()) * p.getFileStatus().hashCode();\r\n    mSeed.put(p.getId(), cseed);\r\n    Random r = new Random(cseed);\r\n    int nChildren = r.nextInt(children);\r\n    ArrayList<TreePath> ret = new ArrayList<TreePath>();\r\n    for (int i = 0; i < nChildren; ++i) {\r\n        ret.add(new TreePath(genFileStatus(p, r), p.getId(), walk));\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "genFileStatus",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "FileStatus genFileStatus(TreePath parent, Random r)\n{\r\n    final int blocksize = 128 * (1 << 20);\r\n    final Path name;\r\n    final boolean isDir;\r\n    if (null == parent) {\r\n        name = root;\r\n        isDir = true;\r\n    } else {\r\n        Path p = parent.getFileStatus().getPath();\r\n        name = new Path(p, genName(r, 3, 10));\r\n        isDir = r.nextFloat() < depth;\r\n    }\r\n    final long len = isDir ? 0 : r.nextInt(Integer.MAX_VALUE);\r\n    final int nblocks = 0 == len ? 0 : (((int) ((len - 1) / blocksize)) + 1);\r\n    BlockLocation[] blocks = genBlocks(r, nblocks, blocksize, len);\r\n    return new LocatedFileStatus(new FileStatus(len, isDir, 1, blocksize, 0L, 0L, null, \"hadoop\", \"hadoop\", name), blocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "genBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockLocation[] genBlocks(Random r, int nblocks, int blocksize, long len)\n{\r\n    BlockLocation[] blocks = new BlockLocation[nblocks];\r\n    if (0 == nblocks) {\r\n        return blocks;\r\n    }\r\n    for (int i = 0; i < nblocks - 1; ++i) {\r\n        blocks[i] = new BlockLocation(null, null, i * blocksize, blocksize);\r\n    }\r\n    blocks[nblocks - 1] = new BlockLocation(null, null, (nblocks - 1) * blocksize, 0 == (len % blocksize) ? blocksize : len % blocksize);\r\n    return blocks;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "genName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String genName(Random r, int min, int max)\n{\r\n    int len = r.nextInt(max - min + 1) + min;\r\n    char[] ret = new char[len];\r\n    while (len > 0) {\r\n        int c = r.nextInt() & 0x7F;\r\n        if (Character.isLetterOrDigit(c)) {\r\n            ret[--len] = (char) c;\r\n        }\r\n    }\r\n    return new String(ret);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "setSeed",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setSeed()\n{\r\n    long seed = r.nextLong();\r\n    r.setSeed(seed);\r\n    System.out.println(name.getMethodName() + \" seed: \" + seed);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testRandomTreeWalkRepeat",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testRandomTreeWalkRepeat() throws Exception\n{\r\n    Set<TreePath> ns = new HashSet<>();\r\n    final long seed = r.nextLong();\r\n    RandomTreeWalk t1 = new RandomTreeWalk(seed, 10, .1f);\r\n    int i = 0;\r\n    for (TreePath p : t1) {\r\n        p.accept(i++);\r\n        assertTrue(ns.add(p));\r\n    }\r\n    RandomTreeWalk t2 = new RandomTreeWalk(seed, 10, .1f);\r\n    int j = 0;\r\n    for (TreePath p : t2) {\r\n        p.accept(j++);\r\n        assertTrue(ns.remove(p));\r\n    }\r\n    assertTrue(ns.isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testRandomTreeWalkFork",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testRandomTreeWalkFork() throws Exception\n{\r\n    Set<FileStatus> ns = new HashSet<>();\r\n    final long seed = r.nextLong();\r\n    RandomTreeWalk t1 = new RandomTreeWalk(seed, 10, .15f);\r\n    int i = 0;\r\n    for (TreePath p : t1) {\r\n        p.accept(i++);\r\n        assertTrue(ns.add(p.getFileStatus()));\r\n    }\r\n    RandomTreeWalk t2 = new RandomTreeWalk(seed, 10, .15f);\r\n    int j = 0;\r\n    ArrayList<TreeWalk.TreeIterator> iters = new ArrayList<>();\r\n    iters.add(t2.iterator());\r\n    while (!iters.isEmpty()) {\r\n        for (TreeWalk.TreeIterator sub = iters.remove(iters.size() - 1); sub.hasNext(); ) {\r\n            TreePath p = sub.next();\r\n            if (0 == (r.nextInt() % 4)) {\r\n                iters.add(sub.fork());\r\n                Collections.shuffle(iters, r);\r\n            }\r\n            p.accept(j++);\r\n            assertTrue(ns.remove(p.getFileStatus()));\r\n        }\r\n    }\r\n    assertTrue(ns.isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testRandomRootWalk",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRandomRootWalk() throws Exception\n{\r\n    Set<FileStatus> ns = new HashSet<>();\r\n    final long seed = r.nextLong();\r\n    Path root = new Path(\"foo://bar:4344/dingos\");\r\n    String sroot = root.toString();\r\n    int nroot = sroot.length();\r\n    RandomTreeWalk t1 = new RandomTreeWalk(root, seed, 10, .1f);\r\n    int i = 0;\r\n    for (TreePath p : t1) {\r\n        p.accept(i++);\r\n        FileStatus stat = p.getFileStatus();\r\n        assertTrue(ns.add(stat));\r\n        assertEquals(sroot, stat.getPath().toString().substring(0, nroot));\r\n    }\r\n    RandomTreeWalk t2 = new RandomTreeWalk(root, seed, 10, .1f);\r\n    int j = 0;\r\n    for (TreePath p : t2) {\r\n        p.accept(j++);\r\n        FileStatus stat = p.getFileStatus();\r\n        assertTrue(ns.remove(stat));\r\n        assertEquals(sroot, stat.getPath().toString().substring(0, nroot));\r\n    }\r\n    assertTrue(ns.isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setup()\n{\r\n    Configuration conf = new Configuration(false);\r\n    conf.setInt(SingleUGIResolver.UID, TESTUID);\r\n    conf.setInt(SingleUGIResolver.GID, TESTGID);\r\n    conf.set(SingleUGIResolver.USER, TESTUSER);\r\n    conf.set(SingleUGIResolver.GROUP, TESTGROUP);\r\n    ugi.setConf(conf);\r\n    System.out.println(name.getMethodName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testRewrite",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRewrite()\n{\r\n    FsPermission p1 = new FsPermission((short) 0755);\r\n    match(ugi.resolve(file(\"dingo\", \"dingo\", p1)), p1);\r\n    match(ugi.resolve(file(TESTUSER, \"dingo\", p1)), p1);\r\n    match(ugi.resolve(file(\"dingo\", TESTGROUP, p1)), p1);\r\n    match(ugi.resolve(file(TESTUSER, TESTGROUP, p1)), p1);\r\n    FsPermission p2 = new FsPermission((short) 0x8000);\r\n    match(ugi.resolve(file(\"dingo\", \"dingo\", p2)), p2);\r\n    match(ugi.resolve(file(TESTUSER, \"dingo\", p2)), p2);\r\n    match(ugi.resolve(file(\"dingo\", TESTGROUP, p2)), p2);\r\n    match(ugi.resolve(file(TESTUSER, TESTGROUP, p2)), p2);\r\n    Map<Integer, String> ids = ugi.ugiMap();\r\n    assertEquals(2, ids.size());\r\n    assertEquals(TESTUSER, ids.get(10101));\r\n    assertEquals(TESTGROUP, ids.get(10102));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testDefault",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testDefault()\n{\r\n    String user;\r\n    try {\r\n        user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    } catch (IOException e) {\r\n        user = \"hadoop\";\r\n    }\r\n    Configuration conf = new Configuration(false);\r\n    ugi.setConf(conf);\r\n    Map<Integer, String> ids = ugi.ugiMap();\r\n    assertEquals(2, ids.size());\r\n    assertEquals(user, ids.get(0));\r\n    assertEquals(user, ids.get(1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testAclResolution",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testAclResolution()\n{\r\n    long perm;\r\n    FsPermission p1 = new FsPermission((short) 0755);\r\n    FileStatus fileStatus = file(\"dingo\", \"dingo\", p1);\r\n    perm = ugi.getPermissionsProto(fileStatus, null);\r\n    match(perm, p1);\r\n    AclEntry aclEntry = new AclEntry.Builder().setType(AclEntryType.USER).setScope(AclEntryScope.ACCESS).setPermission(FsAction.ALL).setName(\"dingo\").build();\r\n    AclStatus aclStatus = new AclStatus.Builder().owner(\"dingo\").group((\"dingo\")).addEntry(aclEntry).setPermission(p1).build();\r\n    perm = ugi.getPermissionsProto(null, aclStatus);\r\n    match(perm, p1);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testInvalidUid",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testInvalidUid()\n{\r\n    Configuration conf = ugi.getConf();\r\n    conf.setInt(SingleUGIResolver.UID, (1 << 24) + 1);\r\n    ugi.setConf(conf);\r\n    ugi.resolve(file(TESTUSER, TESTGROUP, new FsPermission((short) 0777)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testInvalidGid",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testInvalidGid()\n{\r\n    Configuration conf = ugi.getConf();\r\n    conf.setInt(SingleUGIResolver.GID, (1 << 24) + 1);\r\n    ugi.setConf(conf);\r\n    ugi.resolve(file(TESTUSER, TESTGROUP, new FsPermission((short) 0777)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "testDuplicateIds",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testDuplicateIds()\n{\r\n    Configuration conf = new Configuration(false);\r\n    conf.setInt(SingleUGIResolver.UID, 4344);\r\n    conf.setInt(SingleUGIResolver.GID, 4344);\r\n    conf.set(SingleUGIResolver.USER, TESTUSER);\r\n    conf.set(SingleUGIResolver.GROUP, TESTGROUP);\r\n    ugi.setConf(conf);\r\n    ugi.ugiMap();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "match",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void match(long encoded, FsPermission p)\n{\r\n    assertEquals(p, new FsPermission((short) (encoded & 0xFFFF)));\r\n    long uid = (encoded >>> UGIResolver.USER_STRID_OFFSET);\r\n    uid &= UGIResolver.USER_GROUP_STRID_MASK;\r\n    assertEquals(TESTUID, uid);\r\n    long gid = (encoded >>> UGIResolver.GROUP_STRID_OFFSET);\r\n    gid &= UGIResolver.USER_GROUP_STRID_MASK;\r\n    assertEquals(TESTGID, gid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-fs2img\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode",
  "methodName" : "file",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileStatus file(String user, String group, FsPermission perm)\n{\r\n    Path p = new Path(\"foo://bar:4344/baz/dingo\");\r\n    return new FileStatus(4344 * (1 << 20), false, 1, 256 * (1 << 20), 0L, 0L, perm, user, group, p);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
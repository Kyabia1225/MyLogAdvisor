[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "fetchHadoopTarball",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "File fetchHadoopTarball(File destinationDir, String version, Configuration conf, Logger log) throws IOException\n{\r\n    log.info(\"Looking for Hadoop tarball for version: \" + version);\r\n    File destinationFile = new File(destinationDir, String.format(HADOOP_TAR_FILENAME_FORMAT, version));\r\n    if (destinationFile.exists()) {\r\n        log.info(\"Found tarball at: \" + destinationFile.getAbsolutePath());\r\n        return destinationFile;\r\n    }\r\n    String apacheMirror = conf.get(APACHE_DOWNLOAD_MIRROR_KEY);\r\n    if (apacheMirror == null) {\r\n        apacheMirror = System.getProperty(APACHE_DOWNLOAD_MIRROR_KEY, APACHE_DOWNLOAD_MIRROR_DEFAULT);\r\n    }\r\n    if (!destinationDir.exists()) {\r\n        if (!destinationDir.mkdirs()) {\r\n            throw new IOException(\"Unable to create local dir: \" + destinationDir);\r\n        }\r\n    }\r\n    URL downloadURL = new URL(apacheMirror + String.format(APACHE_DOWNLOAD_MIRROR_SUFFIX_FORMAT, version, version));\r\n    log.info(\"Downloading tarball from: <{}> to <{}>\", downloadURL, destinationFile.getAbsolutePath());\r\n    FileUtils.copyURLToFile(downloadURL, destinationFile, 10000, 60000);\r\n    log.info(\"Completed downloading of Hadoop tarball\");\r\n    return destinationFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getNameNodeHdfsUri",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI getNameNodeHdfsUri(Properties nameNodeProperties)\n{\r\n    return URI.create(String.format(\"hdfs://%s:%s/\", nameNodeProperties.getProperty(DynoConstants.NN_HOSTNAME), nameNodeProperties.getProperty(DynoConstants.NN_RPC_PORT)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getNameNodeServiceRpcAddr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI getNameNodeServiceRpcAddr(Properties nameNodeProperties)\n{\r\n    return URI.create(String.format(\"hdfs://%s:%s/\", nameNodeProperties.getProperty(DynoConstants.NN_HOSTNAME), nameNodeProperties.getProperty(DynoConstants.NN_SERVICERPC_PORT)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getNameNodeWebUri",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI getNameNodeWebUri(Properties nameNodeProperties)\n{\r\n    return URI.create(String.format(\"http://%s:%s/\", nameNodeProperties.getProperty(DynoConstants.NN_HOSTNAME), nameNodeProperties.getProperty(DynoConstants.NN_HTTP_PORT)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getNameNodeTrackingUri",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI getNameNodeTrackingUri(Properties nameNodeProperties) throws IOException\n{\r\n    return URI.create(String.format(\"http://%s:%s/node/containerlogs/%s/%s/\", nameNodeProperties.getProperty(DynoConstants.NN_HOSTNAME), nameNodeProperties.getProperty(Environment.NM_HTTP_PORT.name()), nameNodeProperties.getProperty(Environment.CONTAINER_ID.name()), UserGroupInformation.getCurrentUser().getShortUserName()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "waitForAndGetNameNodeProperties",
  "errType" : [ "FileNotFoundException", "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Optional<Properties> waitForAndGetNameNodeProperties(Supplier<Boolean> shouldExit, Configuration conf, Path nameNodeInfoPath, Logger log) throws IOException, InterruptedException\n{\r\n    while (!shouldExit.get()) {\r\n        try (FSDataInputStream nnInfoInputStream = nameNodeInfoPath.getFileSystem(conf).open(nameNodeInfoPath)) {\r\n            Properties nameNodeProperties = new Properties();\r\n            nameNodeProperties.load(nnInfoInputStream);\r\n            return Optional.of(nameNodeProperties);\r\n        } catch (FileNotFoundException fnfe) {\r\n            log.debug(\"NameNode host information not yet available\");\r\n            Thread.sleep(1000);\r\n        } catch (IOException ioe) {\r\n            log.warn(\"Unable to fetch NameNode host information; retrying\", ioe);\r\n            Thread.sleep(1000);\r\n        }\r\n    }\r\n    return Optional.empty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "waitForNameNodeStartup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void waitForNameNodeStartup(Properties nameNodeProperties, Supplier<Boolean> shouldExit, Logger log) throws IOException, InterruptedException\n{\r\n    if (shouldExit.get()) {\r\n        return;\r\n    }\r\n    log.info(\"Waiting for NameNode to finish starting up...\");\r\n    waitForNameNodeJMXValue(\"Startup progress\", NAMENODE_STARTUP_PROGRESS_JMX_QUERY, \"PercentComplete\", 1.0, 0.01, false, nameNodeProperties, shouldExit, log);\r\n    log.info(\"NameNode has started!\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "waitForNameNodeReadiness",
  "errType" : [ "InterruptedException", "IOException" ],
  "containingMethodsNum" : 38,
  "sourceCodeText" : "void waitForNameNodeReadiness(final Properties nameNodeProperties, int numTotalDataNodes, boolean triggerBlockReports, Supplier<Boolean> shouldExit, final Configuration conf, final Logger log) throws IOException, InterruptedException\n{\r\n    if (shouldExit.get()) {\r\n        return;\r\n    }\r\n    int minDataNodes = (int) (conf.getFloat(DATANODE_LIVE_MIN_FRACTION_KEY, DATANODE_LIVE_MIN_FRACTION_DEFAULT) * numTotalDataNodes);\r\n    log.info(String.format(\"Waiting for %d DataNodes to register with the NameNode...\", minDataNodes));\r\n    waitForNameNodeJMXValue(\"Number of live DataNodes\", FSNAMESYSTEM_STATE_JMX_QUERY, JMX_LIVE_NODE_COUNT, minDataNodes, numTotalDataNodes * 0.001, false, nameNodeProperties, shouldExit, log);\r\n    final int totalBlocks = Integer.parseInt(fetchNameNodeJMXValue(nameNodeProperties, FSNAMESYSTEM_STATE_JMX_QUERY, JMX_BLOCKS_TOTAL));\r\n    final AtomicBoolean doneWaiting = new AtomicBoolean(false);\r\n    if (triggerBlockReports) {\r\n        final int blockThreshold = totalBlocks / numTotalDataNodes * 2;\r\n        conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, \"simple\");\r\n        conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, \"false\");\r\n        final DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(getNameNodeHdfsUri(nameNodeProperties), conf);\r\n        log.info(\"Launching thread to trigger block reports for Datanodes with <\" + blockThreshold + \" blocks reported\");\r\n        Thread blockReportThread = new Thread(() -> {\r\n            long lastUnderRepBlocks = Long.MAX_VALUE;\r\n            try {\r\n                while (true) {\r\n                    try {\r\n                        Thread.sleep(TimeUnit.MINUTES.toMillis(1));\r\n                        long underRepBlocks = Long.parseLong(fetchNameNodeJMXValue(nameNodeProperties, FSNAMESYSTEM_JMX_QUERY, JMX_MISSING_BLOCKS)) + Long.parseLong(fetchNameNodeJMXValue(nameNodeProperties, FSNAMESYSTEM_STATE_JMX_QUERY, JMX_UNDER_REPLICATED_BLOCKS));\r\n                        long blockDecrease = lastUnderRepBlocks - underRepBlocks;\r\n                        lastUnderRepBlocks = underRepBlocks;\r\n                        if (blockDecrease < 0 || blockDecrease > (totalBlocks * 0.001)) {\r\n                            continue;\r\n                        }\r\n                        String liveNodeListString = fetchNameNodeJMXValue(nameNodeProperties, NAMENODE_INFO_JMX_QUERY, JMX_LIVE_NODES_LIST);\r\n                        Set<String> datanodesToReport = parseStaleDataNodeList(liveNodeListString, blockThreshold, log);\r\n                        if (datanodesToReport.isEmpty() && doneWaiting.get()) {\r\n                            log.info(\"BlockReportThread exiting; all DataNodes have \" + \"reported blocks\");\r\n                            break;\r\n                        }\r\n                        log.info(\"Queueing {} Datanodes for block report: {}\", datanodesToReport.size(), Joiner.on(\",\").join(datanodesToReport));\r\n                        DatanodeInfo[] datanodes = dfs.getDataNodeStats();\r\n                        int cnt = 0;\r\n                        for (DatanodeInfo datanode : datanodes) {\r\n                            if (datanodesToReport.contains(datanode.getXferAddr(true))) {\r\n                                Thread.sleep(1);\r\n                                triggerDataNodeBlockReport(conf, datanode.getIpcAddr(true));\r\n                                cnt++;\r\n                                Thread.sleep(1000);\r\n                            }\r\n                        }\r\n                        if (cnt != datanodesToReport.size()) {\r\n                            log.warn(\"Found {} Datanodes to queue block reports for but \" + \"was only able to trigger {}\", datanodesToReport.size(), cnt);\r\n                        }\r\n                    } catch (IOException ioe) {\r\n                        log.warn(\"Exception encountered in block report thread\", ioe);\r\n                    }\r\n                }\r\n            } catch (InterruptedException ie) {\r\n            }\r\n            log.info(\"Block reporting thread exiting\");\r\n        });\r\n        blockReportThread.setDaemon(true);\r\n        blockReportThread.setUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\r\n        blockReportThread.start();\r\n    }\r\n    float maxMissingBlocks = totalBlocks * conf.getFloat(MISSING_BLOCKS_MAX_FRACTION_KEY, MISSING_BLOCKS_MAX_FRACTION_DEFAULT);\r\n    log.info(\"Waiting for MissingBlocks to fall below {}...\", maxMissingBlocks);\r\n    waitForNameNodeJMXValue(\"Number of missing blocks\", FSNAMESYSTEM_JMX_QUERY, JMX_MISSING_BLOCKS, maxMissingBlocks, totalBlocks * 0.0001, true, nameNodeProperties, shouldExit, log);\r\n    float maxUnderreplicatedBlocks = totalBlocks * conf.getFloat(UNDERREPLICATED_BLOCKS_MAX_FRACTION_KEY, UNDERREPLICATED_BLOCKS_MAX_FRACTION_DEFAULT);\r\n    log.info(\"Waiting for UnderReplicatedBlocks to fall below {}...\", maxUnderreplicatedBlocks);\r\n    waitForNameNodeJMXValue(\"Number of under replicated blocks\", FSNAMESYSTEM_STATE_JMX_QUERY, JMX_UNDER_REPLICATED_BLOCKS, maxUnderreplicatedBlocks, totalBlocks * 0.001, true, nameNodeProperties, shouldExit, log);\r\n    log.info(\"NameNode is ready for use!\");\r\n    doneWaiting.set(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "triggerDataNodeBlockReport",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void triggerDataNodeBlockReport(Configuration conf, String dataNodeTarget) throws IOException\n{\r\n    InetSocketAddress datanodeAddr = NetUtils.createSocketAddr(dataNodeTarget);\r\n    ClientDatanodeProtocol dnProtocol = DFSUtilClient.createClientDatanodeProtocolProxy(datanodeAddr, UserGroupInformation.getCurrentUser(), conf, NetUtils.getSocketFactory(conf, ClientDatanodeProtocol.class));\r\n    dnProtocol.triggerBlockReport(new BlockReportOptions.Factory().build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "waitForNameNodeJMXValue",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void waitForNameNodeJMXValue(String valueName, String jmxBeanQuery, String jmxProperty, double threshold, double printThreshold, boolean decreasing, Properties nameNodeProperties, Supplier<Boolean> shouldExit, Logger log) throws InterruptedException\n{\r\n    double lastPrintedValue = decreasing ? Double.MAX_VALUE : Double.MIN_VALUE;\r\n    double value;\r\n    int retryCount = 0;\r\n    long startTime = Time.monotonicNow();\r\n    while (!shouldExit.get()) {\r\n        try {\r\n            value = Double.parseDouble(fetchNameNodeJMXValue(nameNodeProperties, jmxBeanQuery, jmxProperty));\r\n            if ((decreasing && value <= threshold) || (!decreasing && value >= threshold)) {\r\n                log.info(String.format(\"%s = %.2f; %s threshold of %.2f; done waiting after %d ms.\", valueName, value, decreasing ? \"below\" : \"above\", threshold, Time.monotonicNow() - startTime));\r\n                break;\r\n            } else if (Math.abs(value - lastPrintedValue) >= printThreshold) {\r\n                log.info(String.format(\"%s: %.2f\", valueName, value));\r\n                lastPrintedValue = value;\r\n            }\r\n        } catch (IOException ioe) {\r\n            if (++retryCount % 20 == 0) {\r\n                log.warn(\"Unable to fetch {}; retried {} times / waited {} ms\", valueName, retryCount, Time.monotonicNow() - startTime, ioe);\r\n            }\r\n        }\r\n        Thread.sleep(3000);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "parseStaleDataNodeList",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "Set<String> parseStaleDataNodeList(String liveNodeJsonString, final int blockThreshold, final Logger log) throws IOException\n{\r\n    final Set<String> dataNodesToReport = new HashSet<>();\r\n    JsonFactory fac = new JsonFactory();\r\n    JsonParser parser = fac.createParser(IOUtils.toInputStream(liveNodeJsonString, StandardCharsets.UTF_8.name()));\r\n    int objectDepth = 0;\r\n    String currentNodeAddr = null;\r\n    for (JsonToken tok = parser.nextToken(); tok != null; tok = parser.nextToken()) {\r\n        if (tok == JsonToken.START_OBJECT) {\r\n            objectDepth++;\r\n        } else if (tok == JsonToken.END_OBJECT) {\r\n            objectDepth--;\r\n        } else if (tok == JsonToken.FIELD_NAME) {\r\n            if (objectDepth == 1) {\r\n                currentNodeAddr = parser.getCurrentName();\r\n            } else if (objectDepth == 2) {\r\n                if (parser.getCurrentName().equals(\"numBlocks\")) {\r\n                    JsonToken valueToken = parser.nextToken();\r\n                    if (valueToken != JsonToken.VALUE_NUMBER_INT || currentNodeAddr == null) {\r\n                        throw new IOException(String.format(\"Malformed LiveNodes JSON; \" + \"got token = %s; currentNodeAddr = %s: %s\", valueToken, currentNodeAddr, liveNodeJsonString));\r\n                    }\r\n                    int numBlocks = parser.getIntValue();\r\n                    if (numBlocks < blockThreshold) {\r\n                        log.debug(String.format(\"Queueing Datanode <%s> for block report; numBlocks = %d\", currentNodeAddr, numBlocks));\r\n                        dataNodesToReport.add(currentNodeAddr);\r\n                    } else {\r\n                        log.debug(String.format(\"Not queueing Datanode <%s> for block report; numBlocks = %d\", currentNodeAddr, numBlocks));\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return dataNodesToReport;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "fetchNameNodeJMXValue",
  "errType" : [ "MalformedURLException" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "String fetchNameNodeJMXValue(Properties nameNodeProperties, String jmxBeanQuery, String property) throws IOException\n{\r\n    URI nnWebUri = getNameNodeWebUri(nameNodeProperties);\r\n    URL queryURL;\r\n    try {\r\n        queryURL = new URL(nnWebUri.getScheme(), nnWebUri.getHost(), nnWebUri.getPort(), \"/jmx?qry=\" + jmxBeanQuery);\r\n    } catch (MalformedURLException e) {\r\n        throw new IllegalArgumentException(\"Invalid JMX query: \\\"\" + jmxBeanQuery + \"\\\" against \" + \"NameNode URI: \" + nnWebUri);\r\n    }\r\n    HttpURLConnection conn = (HttpURLConnection) queryURL.openConnection();\r\n    if (conn.getResponseCode() != 200) {\r\n        throw new IOException(\"Unable to retrieve JMX: \" + conn.getResponseMessage());\r\n    }\r\n    InputStream in = conn.getInputStream();\r\n    JsonFactory fac = new JsonFactory();\r\n    JsonParser parser = fac.createParser(in);\r\n    if (parser.nextToken() != JsonToken.START_OBJECT || parser.nextToken() != JsonToken.FIELD_NAME || !parser.getCurrentName().equals(\"beans\") || parser.nextToken() != JsonToken.START_ARRAY || parser.nextToken() != JsonToken.START_OBJECT) {\r\n        throw new IOException(\"Unexpected format of JMX JSON response for: \" + jmxBeanQuery);\r\n    }\r\n    int objectDepth = 1;\r\n    String ret = null;\r\n    while (objectDepth > 0) {\r\n        JsonToken tok = parser.nextToken();\r\n        if (tok == JsonToken.START_OBJECT) {\r\n            objectDepth++;\r\n        } else if (tok == JsonToken.END_OBJECT) {\r\n            objectDepth--;\r\n        } else if (tok == JsonToken.FIELD_NAME) {\r\n            if (parser.getCurrentName().equals(property)) {\r\n                parser.nextToken();\r\n                ret = parser.getText();\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    parser.close();\r\n    in.close();\r\n    conn.disconnect();\r\n    if (ret == null) {\r\n        throw new IOException(\"Property \" + property + \" not found within \" + jmxBeanQuery);\r\n    } else {\r\n        return ret;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "main",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    boolean result = false;\r\n    try {\r\n        ApplicationMaster appMaster = new ApplicationMaster();\r\n        LOG.info(\"Initializing ApplicationMaster\");\r\n        boolean doRun = appMaster.init(args);\r\n        if (!doRun) {\r\n            System.exit(0);\r\n        }\r\n        result = appMaster.run();\r\n    } catch (Throwable t) {\r\n        LOG.error(\"Error running ApplicationMaster\", t);\r\n        System.exit(1);\r\n    }\r\n    if (result) {\r\n        LOG.info(\"Application Master completed successfully. exiting\");\r\n        System.exit(0);\r\n    } else {\r\n        LOG.info(\"Application Master failed. exiting\");\r\n        System.exit(2);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "boolean init(String[] args) throws ParseException\n{\r\n    Options opts = new Options();\r\n    AMOptions.setOptions(opts);\r\n    CommandLine cliParser = new GnuParser().parse(opts, args);\r\n    if (args.length == 0) {\r\n        printUsage(opts);\r\n        throw new IllegalArgumentException(\"No args specified for application master to initialize\");\r\n    }\r\n    if (cliParser.hasOption(\"help\")) {\r\n        printUsage(opts);\r\n        return false;\r\n    }\r\n    Map<String, String> envs = System.getenv();\r\n    remoteStoragePath = new Path(envs.get(DynoConstants.REMOTE_STORAGE_PATH_ENV));\r\n    applicationAcls = new HashMap<>();\r\n    applicationAcls.put(ApplicationAccessType.VIEW_APP, envs.get(DynoConstants.JOB_ACL_VIEW_ENV));\r\n    launchingUser = envs.get(Environment.USER.name());\r\n    if (envs.containsKey(DynoConstants.REMOTE_NN_RPC_ADDR_ENV)) {\r\n        launchNameNode = false;\r\n        namenodeServiceRpcAddress = envs.get(DynoConstants.REMOTE_NN_RPC_ADDR_ENV);\r\n    } else {\r\n        launchNameNode = true;\r\n    }\r\n    ContainerId containerId = ContainerId.fromString(envs.get(Environment.CONTAINER_ID.name()));\r\n    ApplicationAttemptId appAttemptID = containerId.getApplicationAttemptId();\r\n    LOG.info(\"Application master for app: appId={}, clusterTimestamp={}, \" + \"attemptId={}\", appAttemptID.getApplicationId().getId(), appAttemptID.getApplicationId().getClusterTimestamp(), appAttemptID.getAttemptId());\r\n    amOptions = AMOptions.initFromParser(cliParser);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void printUsage(Options opts)\n{\r\n    new HelpFormatter().printHelp(\"ApplicationMaster\", opts);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 39,
  "sourceCodeText" : "boolean run() throws YarnException, IOException, InterruptedException\n{\r\n    LOG.info(\"Starting ApplicationMaster\");\r\n    Credentials credentials = UserGroupInformation.getCurrentUser().getCredentials();\r\n    DataOutputBuffer dob = new DataOutputBuffer();\r\n    credentials.writeTokenStorageToStream(dob);\r\n    credentials.getAllTokens().removeIf((token) -> token.getKind().equals(AMRMTokenIdentifier.KIND_NAME));\r\n    allTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\r\n    AMRMClientAsync.AbstractCallbackHandler allocListener = new RMCallbackHandler();\r\n    amRMClient = AMRMClientAsync.createAMRMClientAsync(1000, allocListener);\r\n    amRMClient.init(conf);\r\n    amRMClient.start();\r\n    containerListener = createNMCallbackHandler();\r\n    nmClientAsync = new NMClientAsyncImpl(containerListener);\r\n    nmClientAsync.init(conf);\r\n    nmClientAsync.start();\r\n    String appMasterHostname = NetUtils.getHostname();\r\n    amRMClient.registerApplicationMaster(appMasterHostname, -1, \"\");\r\n    Supplier<Boolean> exitCritera = this::isComplete;\r\n    Optional<Properties> namenodeProperties = Optional.empty();\r\n    if (launchNameNode) {\r\n        ContainerRequest nnContainerRequest = setupContainerAskForRM(amOptions.getNameNodeMemoryMB(), amOptions.getNameNodeVirtualCores(), 0, amOptions.getNameNodeNodeLabelExpression());\r\n        LOG.info(\"Requested NameNode ask: \" + nnContainerRequest.toString());\r\n        amRMClient.addContainerRequest(nnContainerRequest);\r\n        Path namenodeInfoPath = new Path(remoteStoragePath, DynoConstants.NN_INFO_FILE_NAME);\r\n        LOG.info(\"Waiting on availability of NameNode information at \" + namenodeInfoPath);\r\n        namenodeProperties = DynoInfraUtils.waitForAndGetNameNodeProperties(exitCritera, conf, namenodeInfoPath, LOG);\r\n        if (!namenodeProperties.isPresent()) {\r\n            cleanup();\r\n            return false;\r\n        }\r\n        namenodeServiceRpcAddress = DynoInfraUtils.getNameNodeServiceRpcAddr(namenodeProperties.get()).toString();\r\n        LOG.info(\"NameNode information: \" + namenodeProperties.get());\r\n        LOG.info(\"NameNode can be reached at: \" + DynoInfraUtils.getNameNodeHdfsUri(namenodeProperties.get()).toString());\r\n        DynoInfraUtils.waitForNameNodeStartup(namenodeProperties.get(), exitCritera, LOG);\r\n    } else {\r\n        LOG.info(\"Using remote NameNode with RPC address: \" + namenodeServiceRpcAddress);\r\n    }\r\n    blockListFiles = Collections.synchronizedList(getDataNodeBlockListingFiles());\r\n    numTotalDataNodes = blockListFiles.size();\r\n    if (numTotalDataNodes == 0) {\r\n        LOG.error(\"No block listing files were found! Cannot run with 0 DataNodes.\");\r\n        markCompleted();\r\n        return false;\r\n    }\r\n    numTotalDataNodeContainers = (int) Math.ceil(((double) numTotalDataNodes) / Math.max(1, amOptions.getDataNodesPerCluster()));\r\n    LOG.info(\"Requesting {} DataNode containers with {} MB memory, {} vcores\", numTotalDataNodeContainers, amOptions.getDataNodeMemoryMB(), amOptions.getDataNodeVirtualCores());\r\n    for (int i = 0; i < numTotalDataNodeContainers; ++i) {\r\n        ContainerRequest datanodeAsk = setupContainerAskForRM(amOptions.getDataNodeMemoryMB(), amOptions.getDataNodeVirtualCores(), 1, amOptions.getDataNodeNodeLabelExpression());\r\n        amRMClient.addContainerRequest(datanodeAsk);\r\n        LOG.debug(\"Requested datanode ask: \" + datanodeAsk.toString());\r\n    }\r\n    LOG.info(\"Finished requesting datanode containers\");\r\n    if (launchNameNode) {\r\n        DynoInfraUtils.waitForNameNodeReadiness(namenodeProperties.get(), numTotalDataNodes, true, exitCritera, conf, LOG);\r\n    }\r\n    waitForCompletion();\r\n    return cleanup();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "createNMCallbackHandler",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NMCallbackHandler createNMCallbackHandler()\n{\r\n    return new NMCallbackHandler();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "waitForCompletion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitForCompletion() throws InterruptedException\n{\r\n    synchronized (completionLock) {\r\n        while (!completed) {\r\n            completionLock.wait();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "isComplete",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isComplete()\n{\r\n    synchronized (completionLock) {\r\n        return completed;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "markCompleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void markCompleted()\n{\r\n    synchronized (completionLock) {\r\n        completed = true;\r\n        completionLock.notify();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "cleanup",
  "errType" : [ "InterruptedException", "YarnException|IOException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "boolean cleanup()\n{\r\n    for (Thread launchThread : launchThreads) {\r\n        try {\r\n            launchThread.join(10000);\r\n        } catch (InterruptedException e) {\r\n            LOG.info(\"Exception thrown in thread join: \" + e.getMessage());\r\n            e.printStackTrace();\r\n        }\r\n    }\r\n    LOG.info(\"Application completed. Stopping running containers\");\r\n    nmClientAsync.stop();\r\n    LOG.info(\"Application completed. Signalling finish to RM\");\r\n    FinalApplicationStatus appStatus;\r\n    String appMessage = null;\r\n    boolean success;\r\n    if (numFailedDataNodeContainers.get() == 0 && numCompletedDataNodeContainers.get() == numTotalDataNodes) {\r\n        appStatus = FinalApplicationStatus.SUCCEEDED;\r\n        success = true;\r\n    } else {\r\n        appStatus = FinalApplicationStatus.FAILED;\r\n        appMessage = \"Diagnostics: total=\" + numTotalDataNodeContainers + \", completed=\" + numCompletedDataNodeContainers.get() + \", allocated=\" + numAllocatedDataNodeContainers.get() + \", failed=\" + numFailedDataNodeContainers.get();\r\n        success = false;\r\n    }\r\n    try {\r\n        amRMClient.unregisterApplicationMaster(appStatus, appMessage, null);\r\n    } catch (YarnException | IOException ex) {\r\n        LOG.error(\"Failed to unregister application\", ex);\r\n    }\r\n    amRMClient.stop();\r\n    return success;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getDataNodeBlockListingFiles",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "List<LocalResource> getDataNodeBlockListingFiles() throws IOException\n{\r\n    Path blockListDirPath = new Path(System.getenv().get(DynoConstants.BLOCK_LIST_PATH_ENV));\r\n    LOG.info(\"Looking for block listing files in \" + blockListDirPath);\r\n    FileSystem blockZipFS = blockListDirPath.getFileSystem(conf);\r\n    List<LocalResource> files = new LinkedList<>();\r\n    for (FileStatus stat : blockZipFS.listStatus(blockListDirPath, DynoConstants.BLOCK_LIST_FILE_FILTER)) {\r\n        LocalResource blockListResource = LocalResource.newInstance(URL.fromPath(stat.getPath()), LocalResourceType.FILE, LocalResourceVisibility.APPLICATION, stat.getLen(), stat.getModificationTime());\r\n        files.add(blockListResource);\r\n    }\r\n    return files;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "isNameNode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isNameNode(ContainerId containerId)\n{\r\n    return namenodeContainer != null && namenodeContainer.getId().equals(containerId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "isDataNode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isDataNode(ContainerId containerId)\n{\r\n    return datanodeContainers.containsKey(containerId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "setupContainerAskForRM",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ContainerRequest setupContainerAskForRM(int memory, int vcores, int priority, String nodeLabel)\n{\r\n    Priority pri = Records.newRecord(Priority.class);\r\n    pri.setPriority(priority);\r\n    Resource capability = Records.newRecord(Resource.class);\r\n    capability.setMemorySize(memory);\r\n    capability.setVirtualCores(vcores);\r\n    return new ContainerRequest(capability, null, null, pri, true, nodeLabel);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getPath(Map<String, String> env)\n{\r\n    return new Path(env.get(getLocationEnvVar()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getTimestamp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTimestamp(Map<String, String> env)\n{\r\n    return Long.parseLong(env.get(getTimestampEnvVar()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLength(Map<String, String> env)\n{\r\n    return Long.parseLong(env.get(getLengthEnvVar()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getLocationEnvVar",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLocationEnvVar()\n{\r\n    return name + \"_LOCATION\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getTimestampEnvVar",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTimestampEnvVar()\n{\r\n    return name + \"_TIMESTAMP\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getLengthEnvVar",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLengthEnvVar()\n{\r\n    return name + \"_LENGTH\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LocalResourceType getType()\n{\r\n    return type;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getResourcePath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getResourcePath()\n{\r\n    return resourcePath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    Client client = new Client(ClassUtil.findContainingJar(ApplicationMaster.class));\r\n    System.exit(ToolRunner.run(new YarnConfiguration(), client, args));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "run",
  "errType" : [ "Throwable", "IllegalArgumentException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "int run(String[] args)\n{\r\n    boolean result;\r\n    try {\r\n        LOG.info(\"Initializing Client\");\r\n        try {\r\n            boolean doRun = init(args);\r\n            if (!doRun) {\r\n                return 0;\r\n            }\r\n        } catch (IllegalArgumentException e) {\r\n            System.err.println(e.getLocalizedMessage());\r\n            printUsage();\r\n            return -1;\r\n        }\r\n        result = run();\r\n    } catch (Throwable t) {\r\n        LOG.error(\"Error running Client\", t);\r\n        return 1;\r\n    }\r\n    if (result) {\r\n        LOG.info(\"Application completed successfully\");\r\n        return 0;\r\n    }\r\n    LOG.error(\"Application failed to complete successfully\");\r\n    return 2;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void printUsage()\n{\r\n    HelpFormatter formatter = new HelpFormatter();\r\n    formatter.setWidth(100);\r\n    formatter.printHelp(\"Client\", opts);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 49,
  "sourceCodeText" : "boolean init(String[] args) throws ParseException, IOException\n{\r\n    List<String> list = Arrays.asList(args);\r\n    if (list.contains(\"-h\") || list.contains(\"--help\")) {\r\n        printUsage();\r\n        return false;\r\n    }\r\n    CommandLineParser parser = new GnuParser();\r\n    CommandLine commandLine = parser.parse(opts, args);\r\n    yarnClient = YarnClient.createYarnClient();\r\n    yarnClient.init(getConf());\r\n    LOG.info(\"Starting with arguments: [\\\"{}\\\"]\", Joiner.on(\"\\\" \\\"\").join(args));\r\n    Path fsImageDir = new Path(commandLine.getOptionValue(FS_IMAGE_DIR_ARG, \"\"));\r\n    versionFilePath = new Path(fsImageDir, \"VERSION\").toString();\r\n    if (commandLine.hasOption(NAMENODE_SERVICERPC_ADDR_ARG)) {\r\n        launchNameNode = false;\r\n        remoteNameNodeRpcAddress = commandLine.getOptionValue(NAMENODE_SERVICERPC_ADDR_ARG);\r\n    } else {\r\n        launchNameNode = true;\r\n        FileSystem localFS = FileSystem.getLocal(getConf());\r\n        fsImageDir = fsImageDir.makeQualified(localFS.getUri(), localFS.getWorkingDirectory());\r\n        FileSystem fsImageFS = fsImageDir.getFileSystem(getConf());\r\n        FileStatus[] fsImageFiles = fsImageFS.listStatus(fsImageDir, (path) -> path.getName().matches(\"^fsimage_(\\\\d)+$\"));\r\n        if (fsImageFiles.length != 1) {\r\n            throw new IllegalArgumentException(\"Must be exactly one fsimage file present in fs_image_dir\");\r\n        }\r\n        fsImagePath = fsImageFiles[0].getPath().toString();\r\n        fsImageMD5Path = fsImageFiles[0].getPath().suffix(\".md5\").toString();\r\n    }\r\n    if (amMemory < 0) {\r\n        throw new IllegalArgumentException(\"Invalid memory specified for \" + \"application master, exiting. Specified memory=\" + amMemory);\r\n    }\r\n    if (amVCores < 0) {\r\n        throw new IllegalArgumentException(\"Invalid virtual cores specified for \" + \"application master, exiting. Specified virtual cores=\" + amVCores);\r\n    }\r\n    this.appName = commandLine.getOptionValue(APPNAME_ARG, APPNAME_DEFAULT);\r\n    this.amQueue = commandLine.getOptionValue(QUEUE_ARG, QUEUE_DEFAULT);\r\n    this.amMemory = Integer.parseInt(commandLine.getOptionValue(MASTER_MEMORY_MB_ARG, MASTER_MEMORY_MB_DEFAULT));\r\n    this.amVCores = Integer.parseInt(commandLine.getOptionValue(MASTER_VCORES_ARG, MASTER_VCORES_DEFAULT));\r\n    this.confPath = commandLine.getOptionValue(CONF_PATH_ARG);\r\n    this.blockListPath = commandLine.getOptionValue(BLOCK_LIST_PATH_ARG);\r\n    if (commandLine.hasOption(HADOOP_BINARY_PATH_ARG)) {\r\n        this.hadoopBinary = commandLine.getOptionValue(HADOOP_BINARY_PATH_ARG);\r\n    } else {\r\n        this.hadoopBinary = DynoInfraUtils.fetchHadoopTarball(new File(\".\").getAbsoluteFile(), commandLine.getOptionValue(HADOOP_VERSION_ARG), getConf(), LOG).toString();\r\n    }\r\n    this.amOptions = AMOptions.initFromParser(commandLine);\r\n    this.clientTimeout = Integer.parseInt(commandLine.getOptionValue(TIMEOUT_ARG, TIMEOUT_DEFAULT));\r\n    this.tokenFileLocation = commandLine.getOptionValue(TOKEN_FILE_LOCATION_ARG);\r\n    amOptions.verify();\r\n    Path blockPath = new Path(blockListPath);\r\n    FileSystem blockListFS = blockPath.getFileSystem(getConf());\r\n    if (blockListFS.getUri().equals(FileSystem.getLocal(getConf()).getUri()) || !blockListFS.exists(blockPath)) {\r\n        throw new IllegalArgumentException(\"block list path must already exist on remote fs!\");\r\n    }\r\n    numTotalDataNodes = blockListFS.listStatus(blockPath, DynoConstants.BLOCK_LIST_FILE_FILTER).length;\r\n    if (commandLine.hasOption(WORKLOAD_REPLAY_ENABLE_ARG)) {\r\n        if (!commandLine.hasOption(WORKLOAD_INPUT_PATH_ARG) || !commandLine.hasOption(WORKLOAD_START_DELAY_ARG)) {\r\n            throw new IllegalArgumentException(\"workload_replay_enable was \" + \"specified; must include all required workload_ parameters.\");\r\n        }\r\n        launchWorkloadJob = true;\r\n        workloadInputPath = commandLine.getOptionValue(WORKLOAD_INPUT_PATH_ARG);\r\n        workloadOutputPath = commandLine.getOptionValue(WORKLOAD_OUTPUT_PATH_ARG);\r\n        workloadThreadsPerMapper = Integer.parseInt(commandLine.getOptionValue(WORKLOAD_THREADS_PER_MAPPER_ARG, String.valueOf(AuditReplayMapper.NUM_THREADS_DEFAULT)));\r\n        workloadRateFactor = Double.parseDouble(commandLine.getOptionValue(WORKLOAD_RATE_FACTOR_ARG, WORKLOAD_RATE_FACTOR_DEFAULT));\r\n        workloadExtraConfigs = new HashMap<>();\r\n        if (commandLine.getOptionValues(WORKLOAD_CONFIG_ARG) != null) {\r\n            for (String opt : commandLine.getOptionValues(WORKLOAD_CONFIG_ARG)) {\r\n                Iterator<String> kvPair = Splitter.on(\"=\").trimResults().split(opt).iterator();\r\n                workloadExtraConfigs.put(kvPair.next(), kvPair.next());\r\n            }\r\n        }\r\n        String delayString = commandLine.getOptionValue(WORKLOAD_START_DELAY_ARG, WorkloadDriver.START_TIME_OFFSET_DEFAULT);\r\n        getConf().set(\"___temp___\", delayString);\r\n        workloadStartDelayMs = getConf().getTimeDuration(\"___temp___\", 0, TimeUnit.MILLISECONDS);\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 45,
  "sourceCodeText" : "boolean run() throws IOException, YarnException\n{\r\n    LOG.info(\"Running Client\");\r\n    yarnClient.start();\r\n    YarnClusterMetrics clusterMetrics = yarnClient.getYarnClusterMetrics();\r\n    LOG.info(\"Got Cluster metric info from ASM, numNodeManagers={}\", clusterMetrics.getNumNodeManagers());\r\n    QueueInfo queueInfo = yarnClient.getQueueInfo(this.amQueue);\r\n    LOG.info(\"Queue info: queueName={}, queueCurrentCapacity={}, \" + \"queueMaxCapacity={}, queueApplicationCount={}, \" + \"queueChildQueueCount={}\", queueInfo.getQueueName(), queueInfo.getCurrentCapacity(), queueInfo.getMaximumCapacity(), queueInfo.getApplications().size(), queueInfo.getChildQueues().size());\r\n    YarnClientApplication app = yarnClient.createApplication();\r\n    GetNewApplicationResponse appResponse = app.getNewApplicationResponse();\r\n    long maxMem = appResponse.getMaximumResourceCapability().getMemorySize();\r\n    LOG.info(\"Max mem capabililty of resources in this cluster \" + maxMem);\r\n    int maxVCores = appResponse.getMaximumResourceCapability().getVirtualCores();\r\n    LOG.info(\"Max virtual cores capabililty of resources in this cluster {}\", maxVCores);\r\n    if (amMemory > maxMem || amMemory < 0 || amVCores > maxVCores || amVCores < 0) {\r\n        throw new IllegalArgumentException(\"Invalid AM memory or vcores: memory=\" + amMemory + \", vcores=\" + amVCores);\r\n    }\r\n    amOptions.verify(maxMem, maxVCores);\r\n    ApplicationSubmissionContext appContext = app.getApplicationSubmissionContext();\r\n    infraAppId = appContext.getApplicationId();\r\n    appContext.setApplicationName(appName);\r\n    ContainerLaunchContext amContainer = Records.newRecord(ContainerLaunchContext.class);\r\n    Map<ApplicationAccessType, String> acls = new HashMap<>();\r\n    acls.put(ApplicationAccessType.VIEW_APP, getConf().get(MRJobConfig.JOB_ACL_VIEW_JOB, MRJobConfig.DEFAULT_JOB_ACL_VIEW_JOB));\r\n    amContainer.setApplicationACLs(acls);\r\n    FileSystem fs = FileSystem.get(getConf());\r\n    fs.mkdirs(getRemoteStoragePath(getConf(), infraAppId));\r\n    Map<String, String> env = setupRemoteResourcesGetEnv();\r\n    amContainer.setEnvironment(env);\r\n    Map<String, LocalResource> localResources = new HashMap<>();\r\n    LocalResource scRsrc = LocalResource.newInstance(org.apache.hadoop.yarn.api.records.URL.fromPath(DynoConstants.DYNO_DEPENDENCIES.getPath(env)), LocalResourceType.ARCHIVE, LocalResourceVisibility.APPLICATION, DynoConstants.DYNO_DEPENDENCIES.getLength(env), DynoConstants.DYNO_DEPENDENCIES.getTimestamp(env));\r\n    localResources.put(DynoConstants.DYNO_DEPENDENCIES.getResourcePath(), scRsrc);\r\n    amContainer.setLocalResources(localResources);\r\n    amContainer.setCommands(getAMCommand());\r\n    Resource capability = Records.newRecord(Resource.class);\r\n    capability.setMemorySize(amMemory);\r\n    capability.setVirtualCores(amVCores);\r\n    appContext.setResource(capability);\r\n    if (UserGroupInformation.isSecurityEnabled()) {\r\n        ByteBuffer fsTokens;\r\n        if (tokenFileLocation != null) {\r\n            fsTokens = ByteBuffer.wrap(Files.readAllBytes(Paths.get(tokenFileLocation)));\r\n        } else {\r\n            Credentials credentials = new Credentials();\r\n            String tokenRenewer = getConf().get(YarnConfiguration.RM_PRINCIPAL);\r\n            if (tokenRenewer == null || tokenRenewer.length() == 0) {\r\n                throw new IOException(\"Can't get Master Kerberos principal for the \" + \"RM to use as renewer\");\r\n            }\r\n            final Token<?>[] tokens = fs.addDelegationTokens(tokenRenewer, credentials);\r\n            if (tokens != null) {\r\n                for (Token<?> token : tokens) {\r\n                    LOG.info(\"Got dt for \" + fs.getUri() + \"; \" + token);\r\n                }\r\n            }\r\n            DataOutputBuffer dob = new DataOutputBuffer();\r\n            credentials.writeTokenStorageToStream(dob);\r\n            fsTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\r\n        }\r\n        amContainer.setTokens(fsTokens);\r\n    }\r\n    appContext.setAMContainerSpec(amContainer);\r\n    appContext.setQueue(amQueue);\r\n    LOG.info(\"Submitting application to RM\");\r\n    yarnClient.submitApplication(appContext);\r\n    return monitorInfraApplication();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "setupRemoteResourcesGetEnv",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "Map<String, String> setupRemoteResourcesGetEnv() throws IOException\n{\r\n    LOG.info(\"Set the environment for the application master\");\r\n    Map<String, String> env = new HashMap<>();\r\n    if (launchNameNode) {\r\n        setupRemoteResource(infraAppId, DynoConstants.FS_IMAGE, env, fsImagePath);\r\n        setupRemoteResource(infraAppId, DynoConstants.FS_IMAGE_MD5, env, fsImageMD5Path);\r\n    } else {\r\n        env.put(DynoConstants.REMOTE_NN_RPC_ADDR_ENV, remoteNameNodeRpcAddress);\r\n    }\r\n    setupRemoteResource(infraAppId, DynoConstants.VERSION, env, versionFilePath);\r\n    setupRemoteResource(infraAppId, DynoConstants.CONF_ZIP, env, confPath);\r\n    setupRemoteResource(infraAppId, DynoConstants.START_SCRIPT, env, START_SCRIPT_LOCATION);\r\n    setupRemoteResource(infraAppId, DynoConstants.HADOOP_BINARY, env, hadoopBinary);\r\n    setupRemoteResource(infraAppId, DynoConstants.DYNO_DEPENDENCIES, env, dependencyJars);\r\n    env.put(DynoConstants.BLOCK_LIST_PATH_ENV, blockListPath);\r\n    env.put(DynoConstants.JOB_ACL_VIEW_ENV, getConf().get(MRJobConfig.JOB_ACL_VIEW_JOB, MRJobConfig.DEFAULT_JOB_ACL_VIEW_JOB));\r\n    env.put(DynoConstants.REMOTE_STORAGE_PATH_ENV, getRemoteStoragePath(getConf(), infraAppId).toString());\r\n    env.put(Environment.CLASSPATH.key(), getAMClassPathEnv());\r\n    return env;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getAMClassPathEnv",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "String getAMClassPathEnv()\n{\r\n    StringBuilder classPathEnv = new StringBuilder(Environment.CLASSPATH.$()).append(ApplicationConstants.CLASS_PATH_SEPARATOR).append(\"./\").append(DynoConstants.DYNO_DEPENDENCIES.getResourcePath()).append(\"/*\");\r\n    for (String c : getConf().getStrings(YarnConfiguration.YARN_APPLICATION_CLASSPATH, YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH)) {\r\n        classPathEnv.append(ApplicationConstants.CLASS_PATH_SEPARATOR);\r\n        classPathEnv.append(c.trim());\r\n    }\r\n    classPathEnv.append(ApplicationConstants.CLASS_PATH_SEPARATOR).append(\"./log4j.properties\");\r\n    if (getConf().getBoolean(YarnConfiguration.IS_MINI_YARN_CLUSTER, false)) {\r\n        classPathEnv.append(ApplicationConstants.CLASS_PATH_SEPARATOR);\r\n        classPathEnv.append(System.getProperty(\"java.class.path\"));\r\n    }\r\n    return classPathEnv.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getAMCommand",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "List<String> getAMCommand()\n{\r\n    List<String> vargs = new ArrayList<>();\r\n    vargs.add(Environment.JAVA_HOME.$() + \"/bin/java\");\r\n    long appMasterHeapSize = Math.round(amMemory * 0.85);\r\n    vargs.add(\"-Xmx\" + appMasterHeapSize + \"m\");\r\n    vargs.add(ApplicationMaster.class.getCanonicalName());\r\n    amOptions.addToVargs(vargs);\r\n    vargs.add(\"1>\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/stdout\");\r\n    vargs.add(\"2>\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/stderr\");\r\n    LOG.info(\"Completed setting up app master command: \" + vargs);\r\n    return Lists.newArrayList(Joiner.on(\" \").join(vargs));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "setupRemoteResource",
  "errType" : null,
  "containingMethodsNum" : 42,
  "sourceCodeText" : "void setupRemoteResource(ApplicationId appId, DynoResource resource, Map<String, String> env, String... srcPaths) throws IOException\n{\r\n    FileStatus remoteFileStatus;\r\n    Path dstPath;\r\n    Preconditions.checkArgument(srcPaths.length > 0, \"Must supply at least one source path\");\r\n    Preconditions.checkArgument(resource.getType() == LocalResourceType.ARCHIVE || srcPaths.length == 1, \"Can only specify multiple source paths if using an ARCHIVE type\");\r\n    List<URI> srcURIs = Arrays.stream(srcPaths).map(URI::create).collect(Collectors.toList());\r\n    Set<String> srcSchemes = srcURIs.stream().map(URI::getScheme).collect(Collectors.toSet());\r\n    Preconditions.checkArgument(srcSchemes.size() == 1, \"All source paths must have the same scheme\");\r\n    String srcScheme = srcSchemes.iterator().next();\r\n    String srcPathString = \"[\" + Joiner.on(\",\").join(srcPaths) + \"]\";\r\n    if (srcScheme == null || srcScheme.equals(FileSystem.getLocal(getConf()).getScheme()) || srcScheme.equals(\"jar\")) {\r\n        List<File> srcFiles = srcURIs.stream().map(URI::getSchemeSpecificPart).map(File::new).collect(Collectors.toList());\r\n        Path dstPathBase = getRemoteStoragePath(getConf(), appId);\r\n        boolean shouldArchive = srcFiles.size() > 1 || srcFiles.get(0).isDirectory() || (resource.getType() == LocalResourceType.ARCHIVE && Arrays.stream(ARCHIVE_FILE_TYPES).noneMatch(suffix -> srcFiles.get(0).getName().endsWith(suffix)));\r\n        if (shouldArchive) {\r\n            if (\"jar\".equals(srcScheme)) {\r\n                throw new IllegalArgumentException(String.format(\"Resources in JARs \" + \"can't be zipped; resource %s is ARCHIVE and src is: %s\", resource.getResourcePath(), srcPathString));\r\n            } else if (resource.getType() != LocalResourceType.ARCHIVE) {\r\n                throw new IllegalArgumentException(String.format(\"Resource type is %s but srcPaths were: %s\", resource.getType(), srcPathString));\r\n            }\r\n            dstPath = new Path(dstPathBase, resource.getResourcePath()).suffix(\".zip\");\r\n        } else {\r\n            dstPath = new Path(dstPathBase, srcFiles.get(0).getName());\r\n        }\r\n        FileSystem remoteFS = dstPath.getFileSystem(getConf());\r\n        LOG.info(\"Uploading resource \" + resource + \" from \" + srcPathString + \" to \" + dstPath);\r\n        try (OutputStream outputStream = remoteFS.create(dstPath, true)) {\r\n            if (\"jar\".equals(srcScheme)) {\r\n                try (InputStream inputStream = new URL(srcPaths[0]).openStream()) {\r\n                    IOUtils.copyBytes(inputStream, outputStream, getConf());\r\n                }\r\n            } else if (shouldArchive) {\r\n                List<File> filesToZip;\r\n                if (srcFiles.size() == 1 && srcFiles.get(0).isDirectory()) {\r\n                    File[] childFiles = srcFiles.get(0).listFiles();\r\n                    if (childFiles == null || childFiles.length == 0) {\r\n                        throw new IllegalArgumentException(\"Specified a directory to archive with no contents\");\r\n                    }\r\n                    filesToZip = Lists.newArrayList(childFiles);\r\n                } else {\r\n                    filesToZip = srcFiles;\r\n                }\r\n                ZipOutputStream zout = new ZipOutputStream(outputStream);\r\n                for (File fileToZip : filesToZip) {\r\n                    addFileToZipRecursively(fileToZip.getParentFile(), fileToZip, zout);\r\n                }\r\n                zout.close();\r\n            } else {\r\n                try (InputStream inputStream = new FileInputStream(srcFiles.get(0))) {\r\n                    IOUtils.copyBytes(inputStream, outputStream, getConf());\r\n                }\r\n            }\r\n        }\r\n        remoteFileStatus = remoteFS.getFileStatus(dstPath);\r\n    } else {\r\n        if (srcPaths.length > 1) {\r\n            throw new IllegalArgumentException(\"If resource is on remote, must be \" + \"a single file: \" + srcPathString);\r\n        }\r\n        LOG.info(\"Using resource {} directly from current location: {}\", resource, srcPaths[0]);\r\n        dstPath = new Path(srcPaths[0]);\r\n        remoteFileStatus = FileSystem.get(dstPath.toUri(), getConf()).getFileStatus(dstPath);\r\n        if (remoteFileStatus.isDirectory()) {\r\n            throw new IllegalArgumentException(\"If resource is on remote \" + \"filesystem, must be a file: \" + srcPaths[0]);\r\n        }\r\n    }\r\n    env.put(resource.getLocationEnvVar(), dstPath.toString());\r\n    env.put(resource.getTimestampEnvVar(), String.valueOf(remoteFileStatus.getModificationTime()));\r\n    env.put(resource.getLengthEnvVar(), String.valueOf(remoteFileStatus.getLen()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getRemoteStoragePath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getRemoteStoragePath(Configuration conf, ApplicationId appId) throws IOException\n{\r\n    FileSystem remoteFS = FileSystem.get(conf);\r\n    return remoteFS.makeQualified(new Path(remoteFS.getHomeDirectory(), DynoConstants.DYNAMOMETER_STORAGE_DIR + \"/\" + appId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "addFileToZipRecursively",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void addFileToZipRecursively(File root, File file, ZipOutputStream out) throws IOException\n{\r\n    File[] files = file.listFiles();\r\n    if (files == null) {\r\n        String relativePath = file.getAbsolutePath().substring(root.getAbsolutePath().length() + 1);\r\n        try {\r\n            try (FileInputStream in = new FileInputStream(file.getAbsolutePath())) {\r\n                out.putNextEntry(new ZipEntry(relativePath));\r\n                IOUtils.copyBytes(in, out, getConf(), false);\r\n                out.closeEntry();\r\n            }\r\n        } catch (FileNotFoundException fnfe) {\r\n            LOG.warn(\"Skipping file; it is a symlink with a nonexistent target: {}\", file);\r\n        }\r\n    } else {\r\n        for (File containedFile : files) {\r\n            addFileToZipRecursively(root, containedFile, out);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "monitorInfraApplication",
  "errType" : [ "IOException", "InterruptedException", "InterruptedException", "YarnException|IOException", "InterruptedException" ],
  "containingMethodsNum" : 41,
  "sourceCodeText" : "boolean monitorInfraApplication() throws YarnException, IOException\n{\r\n    boolean loggedApplicationInfo = false;\r\n    boolean success = false;\r\n    Thread namenodeMonitoringThread = new Thread(() -> {\r\n        Supplier<Boolean> exitCritera = () -> Apps.isApplicationFinalState(infraAppState);\r\n        Optional<Properties> namenodeProperties = Optional.empty();\r\n        while (!exitCritera.get()) {\r\n            try {\r\n                if (!namenodeProperties.isPresent()) {\r\n                    namenodeProperties = DynoInfraUtils.waitForAndGetNameNodeProperties(exitCritera, getConf(), getNameNodeInfoPath(), LOG);\r\n                    if (namenodeProperties.isPresent()) {\r\n                        Properties props = namenodeProperties.get();\r\n                        LOG.info(\"NameNode can be reached via HDFS at: {}\", DynoInfraUtils.getNameNodeHdfsUri(props));\r\n                        LOG.info(\"NameNode web UI available at: {}\", DynoInfraUtils.getNameNodeWebUri(props));\r\n                        LOG.info(\"NameNode can be tracked at: {}\", DynoInfraUtils.getNameNodeTrackingUri(props));\r\n                    } else {\r\n                        break;\r\n                    }\r\n                }\r\n                DynoInfraUtils.waitForNameNodeStartup(namenodeProperties.get(), exitCritera, LOG);\r\n                DynoInfraUtils.waitForNameNodeReadiness(namenodeProperties.get(), numTotalDataNodes, false, exitCritera, getConf(), LOG);\r\n                break;\r\n            } catch (IOException ioe) {\r\n                LOG.error(\"Unexpected exception while waiting for NameNode readiness\", ioe);\r\n            } catch (InterruptedException ie) {\r\n                return;\r\n            }\r\n        }\r\n        if (!Apps.isApplicationFinalState(infraAppState) && launchWorkloadJob) {\r\n            launchAndMonitorWorkloadDriver(namenodeProperties.get());\r\n        }\r\n    });\r\n    if (launchNameNode) {\r\n        namenodeMonitoringThread.start();\r\n    }\r\n    while (true) {\r\n        try {\r\n            Thread.sleep(1000);\r\n        } catch (InterruptedException e) {\r\n            LOG.debug(\"Thread sleep in monitoring loop interrupted\");\r\n        }\r\n        ApplicationReport report = yarnClient.getApplicationReport(infraAppId);\r\n        if (report.getTrackingUrl() != null && !loggedApplicationInfo) {\r\n            loggedApplicationInfo = true;\r\n            LOG.info(\"Track the application at: \" + report.getTrackingUrl());\r\n            LOG.info(\"Kill the application using: yarn application -kill \" + report.getApplicationId());\r\n        }\r\n        LOG.debug(\"Got application report from ASM for: appId={}, \" + \"clientToAMToken={}, appDiagnostics={}, appMasterHost={}, \" + \"appQueue={}, appMasterRpcPort={}, appStartTime={}, \" + \"yarnAppState={}, distributedFinalState={}, appTrackingUrl={}, \" + \"appUser={}\", infraAppId.getId(), report.getClientToAMToken(), report.getDiagnostics(), report.getHost(), report.getQueue(), report.getRpcPort(), report.getStartTime(), report.getYarnApplicationState(), report.getFinalApplicationStatus(), report.getTrackingUrl(), report.getUser());\r\n        infraAppState = report.getYarnApplicationState();\r\n        if (infraAppState == YarnApplicationState.KILLED) {\r\n            if (!launchWorkloadJob) {\r\n                success = true;\r\n            } else if (workloadJob == null) {\r\n                LOG.error(\"Infra app was killed before workload job was launched.\");\r\n            } else if (!workloadJob.isComplete()) {\r\n                LOG.error(\"Infra app was killed before workload job completed.\");\r\n            } else if (workloadJob.isSuccessful()) {\r\n                success = true;\r\n            }\r\n            LOG.info(\"Infra app was killed; exiting from client.\");\r\n            break;\r\n        } else if (infraAppState == YarnApplicationState.FINISHED || infraAppState == YarnApplicationState.FAILED) {\r\n            LOG.info(\"Infra app exited unexpectedly. YarnState=\" + infraAppState.toString() + \". Exiting from client.\");\r\n            break;\r\n        }\r\n        if ((clientTimeout != -1) && (System.currentTimeMillis() > (clientStartTime + clientTimeout))) {\r\n            LOG.info(\"Reached client specified timeout of {} ms for application. \" + \"Killing application\", clientTimeout);\r\n            attemptCleanup();\r\n            break;\r\n        }\r\n        if (isCompleted(workloadAppState)) {\r\n            LOG.info(\"Killing infrastructure app\");\r\n            try {\r\n                forceKillApplication(infraAppId);\r\n            } catch (YarnException | IOException e) {\r\n                LOG.error(\"Exception encountered while killing infra app\", e);\r\n            }\r\n        }\r\n    }\r\n    if (launchNameNode) {\r\n        try {\r\n            namenodeMonitoringThread.interrupt();\r\n            namenodeMonitoringThread.join();\r\n        } catch (InterruptedException ie) {\r\n            LOG.warn(\"Interrupted while joining workload job thread; \" + \"continuing to cleanup.\");\r\n        }\r\n    }\r\n    attemptCleanup();\r\n    return success;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getNameNodeInfoPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getNameNodeInfoPath() throws IOException\n{\r\n    return new Path(getRemoteStoragePath(getConf(), infraAppId), DynoConstants.NN_INFO_FILE_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "launchAndMonitorWorkloadDriver",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void launchAndMonitorWorkloadDriver(Properties nameNodeProperties)\n{\r\n    URI nameNodeURI = DynoInfraUtils.getNameNodeHdfsUri(nameNodeProperties);\r\n    LOG.info(\"Launching workload job using input path: \" + workloadInputPath);\r\n    try {\r\n        long workloadStartTime = System.currentTimeMillis() + workloadStartDelayMs;\r\n        Configuration workloadConf = new Configuration(getConf());\r\n        workloadConf.set(AuditReplayMapper.INPUT_PATH_KEY, workloadInputPath);\r\n        workloadConf.set(AuditReplayMapper.OUTPUT_PATH_KEY, workloadOutputPath);\r\n        workloadConf.setInt(AuditReplayMapper.NUM_THREADS_KEY, workloadThreadsPerMapper);\r\n        workloadConf.setDouble(AuditReplayMapper.RATE_FACTOR_KEY, workloadRateFactor);\r\n        for (Map.Entry<String, String> configPair : workloadExtraConfigs.entrySet()) {\r\n            workloadConf.set(configPair.getKey(), configPair.getValue());\r\n        }\r\n        workloadJob = WorkloadDriver.getJobForSubmission(workloadConf, nameNodeURI.toString(), workloadStartTime, AuditReplayMapper.class);\r\n        workloadJob.submit();\r\n        while (!Apps.isApplicationFinalState(infraAppState) && !isCompleted(workloadAppState)) {\r\n            workloadJob.monitorAndPrintJob();\r\n            Thread.sleep(5000);\r\n            workloadAppState = workloadJob.getJobState();\r\n        }\r\n        if (isCompleted(workloadAppState)) {\r\n            LOG.info(\"Workload job completed successfully!\");\r\n        } else {\r\n            LOG.warn(\"Workload job failed.\");\r\n        }\r\n    } catch (Exception e) {\r\n        LOG.error(\"Exception encountered while running workload job\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "attemptCleanup",
  "errType" : [ "IOException", "InterruptedException", "IOException", "YarnException|IOException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void attemptCleanup()\n{\r\n    LOG.info(\"Attempting to clean up remaining running applications.\");\r\n    if (workloadJob != null) {\r\n        try {\r\n            workloadAppState = workloadJob.getJobState();\r\n        } catch (IOException ioe) {\r\n            LOG.warn(\"Unable to fetch completion status of workload job. Will \" + \"proceed to attempt to kill it.\", ioe);\r\n        } catch (InterruptedException ie) {\r\n            Thread.currentThread().interrupt();\r\n            return;\r\n        }\r\n        if (!isCompleted(workloadAppState)) {\r\n            try {\r\n                LOG.info(\"Attempting to kill workload app: {}\", workloadJob.getJobID());\r\n                workloadJob.killJob();\r\n                LOG.info(\"Killed workload app\");\r\n            } catch (IOException ioe) {\r\n                LOG.error(\"Unable to kill workload app ({})\", workloadJob.getJobID(), ioe);\r\n            }\r\n        }\r\n    }\r\n    if (infraAppId != null && !Apps.isApplicationFinalState(infraAppState)) {\r\n        try {\r\n            LOG.info(\"Attempting to kill infrastructure app: \" + infraAppId);\r\n            forceKillApplication(infraAppId);\r\n            LOG.info(\"Killed infrastructure app\");\r\n        } catch (YarnException | IOException e) {\r\n            LOG.error(\"Unable to kill infrastructure app ({})\", infraAppId, e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "isCompleted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isCompleted(JobStatus.State state)\n{\r\n    return state == JobStatus.State.SUCCEEDED || state == JobStatus.State.FAILED || state == JobStatus.State.KILLED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "forceKillApplication",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void forceKillApplication(ApplicationId appId) throws YarnException, IOException\n{\r\n    yarnClient.killApplication(appId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getWorkloadJob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Job getWorkloadJob()\n{\r\n    return workloadJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "printUsageExit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void printUsageExit(String err)\n{\r\n    System.out.println(err);\r\n    System.out.println(USAGE);\r\n    throw new RuntimeException(err);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    SimulatedDataNodes datanodes = new SimulatedDataNodes();\r\n    ToolRunner.run(new HdfsConfiguration(), datanodes, args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "run",
  "errType" : [ "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 37,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length < 2) {\r\n        printUsageExit(\"Not enough arguments\");\r\n    }\r\n    String bpid = args[0];\r\n    List<Path> blockListFiles = new ArrayList<>();\r\n    for (int i = 1; i < args.length; i++) {\r\n        blockListFiles.add(new Path(args[i]));\r\n    }\r\n    URI defaultFS = FileSystem.getDefaultUri(getConf());\r\n    if (!HdfsConstants.HDFS_URI_SCHEME.equals(defaultFS.getScheme())) {\r\n        printUsageExit(\"Must specify an HDFS-based default FS! Got <\" + defaultFS + \">\");\r\n    }\r\n    String nameNodeAdr = defaultFS.getAuthority();\r\n    if (nameNodeAdr == null) {\r\n        printUsageExit(\"No NameNode address and port in config\");\r\n    }\r\n    System.out.println(\"DataNodes will connect to NameNode at \" + nameNodeAdr);\r\n    String loc = DataNode.getStorageLocations(getConf()).get(0).toString();\r\n    loc = loc.substring(loc.indexOf(\"]\") + 1);\r\n    String path = new URI(loc).getPath();\r\n    System.setProperty(MiniDFSCluster.PROP_TEST_BUILD_DATA, path);\r\n    SimulatedFSDataset.setFactory(getConf());\r\n    getConf().setLong(SimulatedFSDataset.CONFIG_PROPERTY_CAPACITY, STORAGE_CAPACITY);\r\n    UserGroupInformation.setConfiguration(getConf());\r\n    MiniDFSCluster mc = new MiniDFSCluster();\r\n    try {\r\n        mc.formatDataNodeDirs();\r\n    } catch (IOException e) {\r\n        System.out.println(\"Error formatting DataNode dirs: \" + e);\r\n        throw new RuntimeException(\"Error formatting DataNode dirs\", e);\r\n    }\r\n    try {\r\n        System.out.println(\"Found \" + blockListFiles.size() + \" block listing files; launching DataNodes accordingly.\");\r\n        mc.startDataNodes(getConf(), blockListFiles.size(), null, false, StartupOption.REGULAR, null, null, null, null, false, true, true, null, null, null);\r\n        long startTime = Time.monotonicNow();\r\n        System.out.println(\"Waiting for DataNodes to connect to NameNode and \" + \"init storage directories.\");\r\n        Set<DataNode> datanodesWithoutFSDataset = new HashSet<>(mc.getDataNodes());\r\n        while (!datanodesWithoutFSDataset.isEmpty()) {\r\n            datanodesWithoutFSDataset.removeIf((dn) -> DataNodeTestUtils.getFSDataset(dn) != null);\r\n            Thread.sleep(100);\r\n        }\r\n        System.out.println(\"Waited \" + (Time.monotonicNow() - startTime) + \" ms for DataNode FSDatasets to be ready\");\r\n        for (int dnIndex = 0; dnIndex < blockListFiles.size(); dnIndex++) {\r\n            Path blockListFile = blockListFiles.get(dnIndex);\r\n            try (FSDataInputStream fsdis = blockListFile.getFileSystem(getConf()).open(blockListFile);\r\n                BufferedReader reader = new BufferedReader(new InputStreamReader(fsdis, StandardCharsets.UTF_8))) {\r\n                List<Block> blockList = new ArrayList<>();\r\n                int cnt = 0;\r\n                for (String line = reader.readLine(); line != null; line = reader.readLine()) {\r\n                    String[] blockInfo = line.split(\",\");\r\n                    blockList.add(new Block(Long.parseLong(blockInfo[0]), Long.parseLong(blockInfo[2]), Long.parseLong(blockInfo[1])));\r\n                    cnt++;\r\n                }\r\n                try {\r\n                    mc.injectBlocks(dnIndex, blockList, bpid);\r\n                } catch (IOException ioe) {\r\n                    System.out.printf(\"Error injecting blocks into DataNode %d for \" + \"block pool %s: %s%n\", dnIndex, bpid, ExceptionUtils.getStackTrace(ioe));\r\n                }\r\n                System.out.printf(\"Injected %d blocks into DataNode %d for block pool %s%n\", cnt, dnIndex, bpid);\r\n            }\r\n        }\r\n    } catch (IOException e) {\r\n        System.out.println(\"Error creating DataNodes: \" + ExceptionUtils.getStackTrace(e));\r\n        return 1;\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void init(String configurationPrefix)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "authorize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void authorize(UserGroupInformation user, InetAddress remoteAddress)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "authorize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void authorize(UserGroupInformation user, String remoteAddress)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "verifyBlockPlacement",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockPlacementStatus verifyBlockPlacement(DatanodeInfo[] locs, int numberOfReplicas)\n{\r\n    return SATISFIED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "verify",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verify(long maxMemory, int maxVcores) throws IllegalArgumentException\n{\r\n    Preconditions.checkArgument(datanodeMemoryMB > 0 && datanodeMemoryMB <= maxMemory, \"datanodeMemoryMB (%s) must be between 0 and %s\", datanodeMemoryMB, maxMemory);\r\n    Preconditions.checkArgument(datanodeVirtualCores > 0 && datanodeVirtualCores <= maxVcores, \"datanodeVirtualCores (%s) must be between 0 and %s\", datanodeVirtualCores, maxVcores);\r\n    Preconditions.checkArgument(namenodeMemoryMB > 0 && namenodeMemoryMB <= maxMemory, \"namenodeMemoryMB (%s) must be between 0 and %s\", namenodeMemoryMB, maxMemory);\r\n    Preconditions.checkArgument(namenodeVirtualCores > 0 && namenodeVirtualCores <= maxVcores, \"namenodeVirtualCores (%s) must be between 0 and %s\", namenodeVirtualCores, maxVcores);\r\n    Preconditions.checkArgument(datanodesPerCluster > 0, \"datanodesPerCluster (%s) must be > 0\", datanodesPerCluster);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "verify",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verify() throws IllegalArgumentException\n{\r\n    verify(Integer.MAX_VALUE, Integer.MAX_VALUE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "addToVargs",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void addToVargs(List<String> vargs)\n{\r\n    vargs.add(\"--\" + DATANODE_MEMORY_MB_ARG + \" \" + datanodeMemoryMB);\r\n    vargs.add(\"--\" + DATANODE_VCORES_ARG + \" \" + datanodeVirtualCores);\r\n    addStringValToVargs(vargs, DATANODE_ARGS_ARG, datanodeArgs);\r\n    addStringValToVargs(vargs, DATANODE_NODELABEL_ARG, datanodeNodeLabelExpression);\r\n    vargs.add(\"--\" + DATANODES_PER_CLUSTER_ARG + \" \" + datanodesPerCluster);\r\n    vargs.add(\"--\" + DATANODE_LAUNCH_DELAY_ARG + \" \" + datanodeLaunchDelay);\r\n    vargs.add(\"--\" + NAMENODE_MEMORY_MB_ARG + \" \" + namenodeMemoryMB);\r\n    vargs.add(\"--\" + NAMENODE_VCORES_ARG + \" \" + namenodeVirtualCores);\r\n    addStringValToVargs(vargs, NAMENODE_ARGS_ARG, namenodeArgs);\r\n    addStringValToVargs(vargs, NAMENODE_NODELABEL_ARG, namenodeNodeLabelExpression);\r\n    vargs.add(\"--\" + NAMENODE_METRICS_PERIOD_ARG + \" \" + namenodeMetricsPeriod);\r\n    addStringValToVargs(vargs, NAMENODE_NAME_DIR_ARG, namenodeNameDir);\r\n    addStringValToVargs(vargs, NAMENODE_EDITS_DIR_ARG, namenodeEditsDir);\r\n    for (Map.Entry<String, String> entry : originalShellEnv.entrySet()) {\r\n        vargs.add(\"--\" + SHELL_ENV_ARG + \" \" + entry.getKey() + \"=\" + entry.getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "addStringValToVargs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addStringValToVargs(List<String> vargs, String optionName, String val)\n{\r\n    if (!val.isEmpty()) {\r\n        vargs.add(\"--\" + optionName + \" \\\\\\\"\" + val + \"\\\\\\\"\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getDataNodeMemoryMB",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getDataNodeMemoryMB()\n{\r\n    return datanodeMemoryMB;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getDataNodeVirtualCores",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getDataNodeVirtualCores()\n{\r\n    return datanodeVirtualCores;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getDataNodeNodeLabelExpression",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDataNodeNodeLabelExpression()\n{\r\n    return datanodeNodeLabelExpression;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getDataNodesPerCluster",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getDataNodesPerCluster()\n{\r\n    return datanodesPerCluster;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getDataNodeLaunchDelaySec",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getDataNodeLaunchDelaySec()\n{\r\n    String tmpConfKey = \"___temp_config_property___\";\r\n    Configuration tmpConf = new Configuration();\r\n    tmpConf.set(tmpConfKey, datanodeLaunchDelay);\r\n    return tmpConf.getTimeDuration(tmpConfKey, 0, TimeUnit.SECONDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getNameNodeMemoryMB",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNameNodeMemoryMB()\n{\r\n    return namenodeMemoryMB;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getNameNodeVirtualCores",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNameNodeVirtualCores()\n{\r\n    return namenodeVirtualCores;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getNameNodeNodeLabelExpression",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNameNodeNodeLabelExpression()\n{\r\n    return namenodeNodeLabelExpression;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "getShellEnv",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, String> getShellEnv()\n{\r\n    return shellEnv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "setOptions",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void setOptions(Options opts)\n{\r\n    opts.addOption(SHELL_ENV_ARG, true, \"Environment for shell script. Specified as env_key=env_val pairs\");\r\n    opts.addOption(NAMENODE_MEMORY_MB_ARG, true, \"Amount of memory in MB to be requested to run the NN (default \" + NAMENODE_MEMORY_MB_DEFAULT + \"). \" + \"Ignored unless the NameNode is run within YARN.\");\r\n    opts.addOption(NAMENODE_VCORES_ARG, true, \"Amount of virtual cores to be requested to run the NN (default \" + NAMENODE_VCORES_DEFAULT + \"). \" + \"Ignored unless the NameNode is run within YARN.\");\r\n    opts.addOption(NAMENODE_ARGS_ARG, true, \"Additional arguments to add when starting the NameNode. \" + \"Ignored unless the NameNode is run within YARN.\");\r\n    opts.addOption(NAMENODE_NODELABEL_ARG, true, \"The node label to specify for the container to use to \" + \"run the NameNode.\");\r\n    opts.addOption(NAMENODE_METRICS_PERIOD_ARG, true, \"The period in seconds for the NameNode's metrics to be emitted to \" + \"file; if <=0, disables this functionality. Otherwise, a \" + \"metrics file will be stored in the container logs for the \" + \"NameNode (default \" + NAMENODE_METRICS_PERIOD_DEFAULT + \").\");\r\n    opts.addOption(NAMENODE_NAME_DIR_ARG, true, \"The directory to use for the NameNode's name data directory. \" + \"If not specified, a location  within the container's working \" + \"directory will be used.\");\r\n    opts.addOption(NAMENODE_EDITS_DIR_ARG, true, \"The directory to use for the NameNode's edits directory. \" + \"If not specified, a location  within the container's working \" + \"directory will be used.\");\r\n    opts.addOption(DATANODE_MEMORY_MB_ARG, true, \"Amount of memory in MB to be requested to run the DNs (default \" + DATANODE_MEMORY_MB_DEFAULT + \")\");\r\n    opts.addOption(DATANODE_VCORES_ARG, true, \"Amount of virtual cores to be requested to run the DNs (default \" + DATANODE_VCORES_DEFAULT + \")\");\r\n    opts.addOption(DATANODE_ARGS_ARG, true, \"Additional arguments to add when starting the DataNodes.\");\r\n    opts.addOption(DATANODE_NODELABEL_ARG, true, \"The node label to specify \" + \"for the container to use to run the DataNode.\");\r\n    opts.addOption(DATANODES_PER_CLUSTER_ARG, true, \"How many simulated DataNodes to run within each YARN container \" + \"(default \" + DATANODES_PER_CLUSTER_DEFAULT + \")\");\r\n    opts.addOption(DATANODE_LAUNCH_DELAY_ARG, true, \"The period over which to launch the DataNodes; this will \" + \"be used as the maximum delay and each DataNode container will \" + \"be launched with some random delay less than  this value. \" + \"Accepts human-readable time durations (e.g. 10s, 1m) (default \" + DATANODE_LAUNCH_DELAY_DEFAULT + \")\");\r\n    opts.addOption(\"help\", false, \"Print usage\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-dynamometer\\hadoop-dynamometer-infra\\src\\main\\java\\org\\apache\\hadoop\\tools\\dynamometer",
  "methodName" : "initFromParser",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "AMOptions initFromParser(CommandLine cliParser)\n{\r\n    Map<String, String> originalShellEnv = new HashMap<>();\r\n    if (cliParser.hasOption(SHELL_ENV_ARG)) {\r\n        for (String env : cliParser.getOptionValues(SHELL_ENV_ARG)) {\r\n            String trimmed = env.trim();\r\n            int index = trimmed.indexOf('=');\r\n            if (index == -1) {\r\n                originalShellEnv.put(trimmed, \"\");\r\n                continue;\r\n            }\r\n            String key = trimmed.substring(0, index);\r\n            String val = \"\";\r\n            if (index < (trimmed.length() - 1)) {\r\n                val = trimmed.substring(index + 1);\r\n            }\r\n            originalShellEnv.put(key, val);\r\n        }\r\n    }\r\n    return new AMOptions(Integer.parseInt(cliParser.getOptionValue(DATANODE_MEMORY_MB_ARG, DATANODE_MEMORY_MB_DEFAULT)), Integer.parseInt(cliParser.getOptionValue(DATANODE_VCORES_ARG, DATANODE_VCORES_DEFAULT)), cliParser.getOptionValue(DATANODE_ARGS_ARG, \"\"), cliParser.getOptionValue(DATANODE_NODELABEL_ARG, \"\"), Integer.parseInt(cliParser.getOptionValue(DATANODES_PER_CLUSTER_ARG, DATANODES_PER_CLUSTER_DEFAULT)), cliParser.getOptionValue(DATANODE_LAUNCH_DELAY_ARG, DATANODE_LAUNCH_DELAY_DEFAULT), Integer.parseInt(cliParser.getOptionValue(NAMENODE_MEMORY_MB_ARG, NAMENODE_MEMORY_MB_DEFAULT)), Integer.parseInt(cliParser.getOptionValue(NAMENODE_VCORES_ARG, NAMENODE_VCORES_DEFAULT)), cliParser.getOptionValue(NAMENODE_ARGS_ARG, \"\"), cliParser.getOptionValue(NAMENODE_NODELABEL_ARG, \"\"), Integer.parseInt(cliParser.getOptionValue(NAMENODE_METRICS_PERIOD_ARG, NAMENODE_METRICS_PERIOD_DEFAULT)), cliParser.getOptionValue(NAMENODE_NAME_DIR_ARG, \"\"), cliParser.getOptionValue(NAMENODE_EDITS_DIR_ARG, \"\"), originalShellEnv);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getLocatedBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LocatedBlock getLocatedBlock()\n{\r\n    return block;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "readObject",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readObject(ObjectInputStream ois) throws IOException, ClassNotFoundException\n{\r\n    ois.defaultReadObject();\r\n    block = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "isStriped",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isStriped()\n{\r\n    return block.isStriped();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TrustedChannelResolver getInstance(Configuration conf)\n{\r\n    Class<? extends TrustedChannelResolver> clazz = conf.getClass(HdfsClientConfigKeys.DFS_TRUSTEDCHANNEL_RESOLVER_CLASS, TrustedChannelResolver.class, TrustedChannelResolver.class);\r\n    return ReflectionUtils.newInstance(clazz, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.conf = conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "isTrusted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isTrusted()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "isTrusted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isTrusted(InetAddress peerAddress)\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "loadWriteByteArrayManagerConf",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ByteArrayManager.Conf loadWriteByteArrayManagerConf(Configuration conf)\n{\r\n    final boolean byteArrayManagerEnabled = conf.getBoolean(Write.ByteArrayManager.ENABLED_KEY, Write.ByteArrayManager.ENABLED_DEFAULT);\r\n    if (!byteArrayManagerEnabled) {\r\n        return null;\r\n    }\r\n    final int countThreshold = conf.getInt(Write.ByteArrayManager.COUNT_THRESHOLD_KEY, Write.ByteArrayManager.COUNT_THRESHOLD_DEFAULT);\r\n    final int countLimit = conf.getInt(Write.ByteArrayManager.COUNT_LIMIT_KEY, Write.ByteArrayManager.COUNT_LIMIT_DEFAULT);\r\n    final long countResetTimePeriodMs = conf.getLong(Write.ByteArrayManager.COUNT_RESET_TIME_PERIOD_MS_KEY, Write.ByteArrayManager.COUNT_RESET_TIME_PERIOD_MS_DEFAULT);\r\n    return new ByteArrayManager.Conf(countThreshold, countLimit, countResetTimePeriodMs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "loadReplicaAccessorBuilderClasses",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "List<Class<? extends ReplicaAccessorBuilder>> loadReplicaAccessorBuilderClasses(Configuration conf)\n{\r\n    String[] classNames = conf.getTrimmedStrings(HdfsClientConfigKeys.REPLICA_ACCESSOR_BUILDER_CLASSES_KEY);\r\n    if (classNames.length == 0) {\r\n        return Collections.emptyList();\r\n    }\r\n    ArrayList<Class<? extends ReplicaAccessorBuilder>> classes = new ArrayList<>();\r\n    ClassLoader classLoader = Thread.currentThread().getContextClassLoader();\r\n    for (String className : classNames) {\r\n        try {\r\n            Class<? extends ReplicaAccessorBuilder> cls = (Class<? extends ReplicaAccessorBuilder>) classLoader.loadClass(className);\r\n            classes.add(cls);\r\n        } catch (Throwable t) {\r\n            LOG.warn(\"Unable to load \" + className, t);\r\n        }\r\n    }\r\n    return classes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getChecksumType",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "DataChecksum.Type getChecksumType(Configuration conf)\n{\r\n    final String checksum = conf.get(DFS_CHECKSUM_TYPE_KEY, DFS_CHECKSUM_TYPE_DEFAULT);\r\n    try {\r\n        return DataChecksum.Type.valueOf(checksum);\r\n    } catch (IllegalArgumentException iae) {\r\n        LOG.warn(\"Bad checksum type: {}. Using default {}\", checksum, DFS_CHECKSUM_TYPE_DEFAULT);\r\n        return DataChecksum.Type.valueOf(DFS_CHECKSUM_TYPE_DEFAULT);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getChecksumCombineModeFromConf",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ChecksumCombineMode getChecksumCombineModeFromConf(Configuration conf)\n{\r\n    final String mode = conf.get(DFS_CHECKSUM_COMBINE_MODE_KEY, DFS_CHECKSUM_COMBINE_MODE_DEFAULT);\r\n    try {\r\n        return ChecksumCombineMode.valueOf(mode);\r\n    } catch (IllegalArgumentException iae) {\r\n        LOG.warn(\"Bad checksum combine mode: {}. Using default {}\", mode, DFS_CHECKSUM_COMBINE_MODE_DEFAULT);\r\n        return ChecksumCombineMode.valueOf(DFS_CHECKSUM_COMBINE_MODE_DEFAULT);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getChecksumOptFromConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ChecksumOpt getChecksumOptFromConf(Configuration conf)\n{\r\n    DataChecksum.Type type = getChecksumType(conf);\r\n    int bytesPerChecksum = conf.getInt(DFS_BYTES_PER_CHECKSUM_KEY, DFS_BYTES_PER_CHECKSUM_DEFAULT);\r\n    return new ChecksumOpt(type, bytesPerChecksum);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "createChecksum",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DataChecksum createChecksum(ChecksumOpt userOpt)\n{\r\n    ChecksumOpt opt = ChecksumOpt.processChecksumOpt(defaultChecksumOpt, userOpt);\r\n    DataChecksum dataChecksum = DataChecksum.newDataChecksum(opt.getChecksumType(), opt.getBytesPerChecksum());\r\n    if (dataChecksum == null) {\r\n        throw new HadoopIllegalArgumentException(\"Invalid checksum type: userOpt=\" + userOpt + \", default=\" + defaultChecksumOpt + \", effective=null\");\r\n    }\r\n    return dataChecksum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getBlockWriteLocateFollowingInitialDelayMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getBlockWriteLocateFollowingInitialDelayMs()\n{\r\n    return blockWriteLocateFollowingInitialDelayMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getBlockWriteLocateFollowingMaxDelayMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getBlockWriteLocateFollowingMaxDelayMs()\n{\r\n    return blockWriteLocateFollowingMaxDelayMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getHdfsTimeout",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getHdfsTimeout()\n{\r\n    return hdfsTimeout;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getMaxFailoverAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxFailoverAttempts()\n{\r\n    return maxFailoverAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getMaxRetryAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxRetryAttempts()\n{\r\n    return maxRetryAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getFailoverSleepBaseMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getFailoverSleepBaseMillis()\n{\r\n    return failoverSleepBaseMillis;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getFailoverSleepMaxMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getFailoverSleepMaxMillis()\n{\r\n    return failoverSleepMaxMillis;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getMaxBlockAcquireFailures",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxBlockAcquireFailures()\n{\r\n    return maxBlockAcquireFailures;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getDatanodeSocketWriteTimeout",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getDatanodeSocketWriteTimeout()\n{\r\n    return datanodeSocketWriteTimeout;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getIoBufferSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getIoBufferSize()\n{\r\n    return ioBufferSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getDefaultChecksumOpt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ChecksumOpt getDefaultChecksumOpt()\n{\r\n    return defaultChecksumOpt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getChecksumCombineMode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ChecksumCombineMode getChecksumCombineMode()\n{\r\n    return checksumCombineMode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getChecksumEcSocketTimeout",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getChecksumEcSocketTimeout()\n{\r\n    return checksumEcSocketTimeout;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getWritePacketSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getWritePacketSize()\n{\r\n    return writePacketSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getWriteMaxPackets",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getWriteMaxPackets()\n{\r\n    return writeMaxPackets;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getWriteByteArrayManagerConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ByteArrayManager.Conf getWriteByteArrayManagerConf()\n{\r\n    return writeByteArrayManagerConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getDataTransferTcpNoDelay",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getDataTransferTcpNoDelay()\n{\r\n    return dataTransferTcpNoDelay;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getSocketTimeout",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getSocketTimeout()\n{\r\n    return socketTimeout;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getSocketSendBufferSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getSocketSendBufferSize()\n{\r\n    return socketSendBufferSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getExcludedNodesCacheExpiry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getExcludedNodesCacheExpiry()\n{\r\n    return excludedNodesCacheExpiry;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getTimeWindow",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTimeWindow()\n{\r\n    return timeWindow;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getNumCachedConnRetry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumCachedConnRetry()\n{\r\n    return numCachedConnRetry;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getNumBlockWriteRetry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumBlockWriteRetry()\n{\r\n    return numBlockWriteRetry;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getNumBlockWriteLocateFollowingRetry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumBlockWriteLocateFollowingRetry()\n{\r\n    return numBlockWriteLocateFollowingRetry;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getDefaultBlockSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDefaultBlockSize()\n{\r\n    return defaultBlockSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getPrefetchSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPrefetchSize()\n{\r\n    return prefetchSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "isUriCacheEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isUriCacheEnabled()\n{\r\n    return uriCacheEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getDefaultReplication",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "short getDefaultReplication()\n{\r\n    return defaultReplication;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskId()\n{\r\n    return taskId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getUMask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FsPermission getUMask()\n{\r\n    return uMask;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "isConnectToDnViaHostname",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isConnectToDnViaHostname()\n{\r\n    return connectToDnViaHostname;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getRetryTimesForGetLastBlockLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRetryTimesForGetLastBlockLength()\n{\r\n    return retryTimesForGetLastBlockLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getRetryIntervalForGetLastBlockLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRetryIntervalForGetLastBlockLength()\n{\r\n    return retryIntervalForGetLastBlockLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getDatanodeRestartTimeout",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDatanodeRestartTimeout()\n{\r\n    return datanodeRestartTimeout;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getSlowIoWarningThresholdMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSlowIoWarningThresholdMs()\n{\r\n    return slowIoWarningThresholdMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getMarkSlowNodeAsBadNodeThreshold",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMarkSlowNodeAsBadNodeThreshold()\n{\r\n    return markSlowNodeAsBadNodeThreshold;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getClientShortCircuitNum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getClientShortCircuitNum()\n{\r\n    return clientShortCircuitNum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getHedgedReadThresholdMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getHedgedReadThresholdMillis()\n{\r\n    return hedgedReadThresholdMillis;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getHedgedReadThreadpoolSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getHedgedReadThreadpoolSize()\n{\r\n    return hedgedReadThreadpoolSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getStripedReadThreadpoolSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getStripedReadThreadpoolSize()\n{\r\n    return stripedReadThreadpoolSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "isDeadNodeDetectionEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isDeadNodeDetectionEnabled()\n{\r\n    return deadNodeDetectionEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getleaseHardLimitPeriod",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getleaseHardLimitPeriod()\n{\r\n    return leaseHardLimitPeriod;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "isReadUseCachePriority",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isReadUseCachePriority()\n{\r\n    return readUseCachePriority;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getReplicaAccessorBuilderClasses",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<Class<? extends ReplicaAccessorBuilder>> getReplicaAccessorBuilderClasses()\n{\r\n    return replicaAccessorBuilderClasses;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "isLocatedBlocksRefresherEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isLocatedBlocksRefresherEnabled()\n{\r\n    return refreshReadBlockLocationsMS > 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getLocatedBlocksRefresherInterval",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLocatedBlocksRefresherInterval()\n{\r\n    return refreshReadBlockLocationsMS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "isRefreshReadBlockLocationsAutomatically",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isRefreshReadBlockLocationsAutomatically()\n{\r\n    return refreshReadBlockLocationsAutomatically;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getShortCircuitConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ShortCircuitConf getShortCircuitConf()\n{\r\n    return shortCircuitConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getMaxPipelineRecoveryRetries",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxPipelineRecoveryRetries()\n{\r\n    return maxPipelineRecoveryRetries;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getTotalBytesRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTotalBytesRead()\n{\r\n    return totalBytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getTotalLocalBytesRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTotalLocalBytesRead()\n{\r\n    return totalLocalBytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getTotalShortCircuitBytesRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTotalShortCircuitBytesRead()\n{\r\n    return totalShortCircuitBytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getTotalZeroCopyBytesRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTotalZeroCopyBytesRead()\n{\r\n    return totalZeroCopyBytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getRemoteBytesRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getRemoteBytesRead()\n{\r\n    return totalBytesRead - totalLocalBytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBlockType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockType getBlockType()\n{\r\n    return blockType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getTotalEcDecodingTimeMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTotalEcDecodingTimeMillis()\n{\r\n    return totalEcDecodingTimeMillis;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addRemoteBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addRemoteBytes(long amt)\n{\r\n    this.totalBytesRead += amt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addLocalBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addLocalBytes(long amt)\n{\r\n    this.totalBytesRead += amt;\r\n    this.totalLocalBytesRead += amt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addShortCircuitBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addShortCircuitBytes(long amt)\n{\r\n    this.totalBytesRead += amt;\r\n    this.totalLocalBytesRead += amt;\r\n    this.totalShortCircuitBytesRead += amt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addZeroCopyBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addZeroCopyBytes(long amt)\n{\r\n    this.totalBytesRead += amt;\r\n    this.totalLocalBytesRead += amt;\r\n    this.totalShortCircuitBytesRead += amt;\r\n    this.totalZeroCopyBytesRead += amt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addErasureCodingDecodingTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addErasureCodingDecodingTime(long millis)\n{\r\n    this.totalEcDecodingTimeMillis += millis;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setBlockType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setBlockType(BlockType blockType)\n{\r\n    this.blockType = blockType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "clear",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void clear()\n{\r\n    this.totalBytesRead = 0;\r\n    this.totalLocalBytesRead = 0;\r\n    this.totalShortCircuitBytesRead = 0;\r\n    this.totalZeroCopyBytesRead = 0;\r\n    this.totalEcDecodingTimeMillis = 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "readAll",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int readAll(BlockReader reader, byte[] buf, int offset, int len) throws IOException\n{\r\n    int n = 0;\r\n    for (; ; ) {\r\n        int nread = reader.read(buf, offset + n, len - n);\r\n        if (nread <= 0)\r\n            return (n == 0) ? nread : n;\r\n        n += nread;\r\n        if (n >= len)\r\n            return n;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "readFully",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readFully(BlockReader reader, byte[] buf, int off, int len) throws IOException\n{\r\n    int toRead = len;\r\n    while (toRead > 0) {\r\n        int ret = reader.read(buf, off, toRead);\r\n        if (ret < 0) {\r\n            throw new IOException(\"Premature EOF from inputStream\");\r\n        }\r\n        toRead -= ret;\r\n        off += ret;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getUsageReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DataNodeUsageReport getUsageReport(long bWritten, long bRead, long wTime, long rTime, long wBlockOp, long rBlockOp, long timeSinceLastReport)\n{\r\n    if (timeSinceLastReport == 0) {\r\n        if (lastReport == null) {\r\n            lastReport = DataNodeUsageReport.EMPTY_REPORT;\r\n        }\r\n        return lastReport;\r\n    }\r\n    DataNodeUsageReport.Builder builder = new DataNodeUsageReport.Builder();\r\n    DataNodeUsageReport report = builder.setBytesWrittenPerSec(getBytesWrittenPerSec(bWritten, timeSinceLastReport)).setBytesReadPerSec(getBytesReadPerSec(bRead, timeSinceLastReport)).setWriteTime(getWriteTime(wTime)).setReadTime(getReadTime(rTime)).setBlocksWrittenPerSec(getWriteBlockOpPerSec(wBlockOp, timeSinceLastReport)).setBlocksReadPerSec(getReadBlockOpPerSec(rBlockOp, timeSinceLastReport)).setTimestamp(Time.monotonicNow()).build();\r\n    this.bytesRead = bRead;\r\n    this.bytesWritten = bWritten;\r\n    this.blocksWritten = wBlockOp;\r\n    this.blocksRead = rBlockOp;\r\n    this.readTime = rTime;\r\n    this.writeTime = wTime;\r\n    lastReport = report;\r\n    return report;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getBytesReadPerSec",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesReadPerSec(long bRead, long timeInSec)\n{\r\n    return (bRead - this.bytesRead) / timeInSec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getBytesWrittenPerSec",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesWrittenPerSec(long bWritten, long timeInSec)\n{\r\n    return (bWritten - this.bytesWritten) / timeInSec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getWriteBlockOpPerSec",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getWriteBlockOpPerSec(long totalWriteBlocks, long timeInSec)\n{\r\n    return (totalWriteBlocks - this.blocksWritten) / timeInSec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getReadBlockOpPerSec",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getReadBlockOpPerSec(long totalReadBlockOp, long timeInSec)\n{\r\n    return (totalReadBlockOp - this.blocksRead) / timeInSec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getReadTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getReadTime(long totalReadTime)\n{\r\n    return totalReadTime - this.readTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getWriteTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getWriteTime(long totalWriteTime)\n{\r\n    return totalWriteTime - this.writeTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getFile()\n{\r\n    return filename;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getOffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getOffset()\n{\r\n    return offset;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLength()\n{\r\n    Long v = getValue();\r\n    return v == null ? -1 : v;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    super.setConf(conf);\r\n    refreshToken = notNull(conf, (OAUTH_REFRESH_TOKEN_KEY));\r\n    accessTokenTimer.setExpiresInMSSinceEpoch(notNull(conf, OAUTH_REFRESH_TOKEN_EXPIRES_KEY));\r\n    clientId = notNull(conf, OAUTH_CLIENT_ID_KEY);\r\n    refreshURL = notNull(conf, OAUTH_REFRESH_URL_KEY);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "getAccessToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getAccessToken() throws IOException\n{\r\n    if (accessTokenTimer.shouldRefresh()) {\r\n        refresh();\r\n    }\r\n    return accessToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "refresh",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void refresh() throws IOException\n{\r\n    try {\r\n        OkHttpClient client = new OkHttpClient();\r\n        client.setConnectTimeout(URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT, TimeUnit.MILLISECONDS);\r\n        client.setReadTimeout(URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT, TimeUnit.MILLISECONDS);\r\n        String bodyString = Utils.postBody(GRANT_TYPE, REFRESH_TOKEN, REFRESH_TOKEN, refreshToken, CLIENT_ID, clientId);\r\n        RequestBody body = RequestBody.create(URLENCODED, bodyString);\r\n        Request request = new Request.Builder().url(refreshURL).post(body).build();\r\n        Response responseBody = client.newCall(request).execute();\r\n        if (responseBody.code() != HttpStatus.SC_OK) {\r\n            throw new IllegalArgumentException(\"Received invalid http response: \" + responseBody.code() + \", text = \" + responseBody.toString());\r\n        }\r\n        Map<?, ?> response = JsonSerialization.mapReader().readValue(responseBody.body().string());\r\n        String newExpiresIn = response.get(EXPIRES_IN).toString();\r\n        accessTokenTimer.setExpiresIn(newExpiresIn);\r\n        accessToken = response.get(ACCESS_TOKEN).toString();\r\n    } catch (Exception e) {\r\n        throw new IOException(\"Exception while refreshing access token\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "getRefreshToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRefreshToken()\n{\r\n    return refreshToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "createdRollbackImages",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean createdRollbackImages()\n{\r\n    return createdRollbackImages;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setCreatedRollbackImages",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCreatedRollbackImages(boolean created)\n{\r\n    this.createdRollbackImages = created;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isStarted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isStarted()\n{\r\n    return startTime != 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isFinalized",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isFinalized()\n{\r\n    return finalizeTime != 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "finalize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void finalize(long finalizeTime)\n{\r\n    if (finalizeTime != 0) {\r\n        this.finalizeTime = finalizeTime;\r\n        createdRollbackImages = false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFinalizeTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinalizeTime()\n{\r\n    return finalizeTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return super.hashCode() ^ (int) startTime ^ (int) finalizeTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (obj == this) {\r\n        return true;\r\n    } else if (!(obj instanceof RollingUpgradeInfo)) {\r\n        return false;\r\n    }\r\n    final RollingUpgradeInfo that = (RollingUpgradeInfo) obj;\r\n    return super.equals(that) && this.startTime == that.startTime && this.finalizeTime == that.finalizeTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    return super.toString() + \"\\n     Start Time: \" + (startTime == 0 ? \"<NOT STARTED>\" : timestamp2String(startTime)) + \"\\n  Finalize Time: \" + (finalizeTime == 0 ? \"<NOT FINALIZED>\" : timestamp2String(finalizeTime));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "timestamp2String",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String timestamp2String(long timestamp)\n{\r\n    return new Date(timestamp) + \" (=\" + timestamp + \")\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockPoolId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockPoolId()\n{\r\n    return blockPoolId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isFinalized",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isFinalized()\n{\r\n    return finalized;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return blockPoolId.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (obj == this) {\r\n        return true;\r\n    } else if (!(obj instanceof RollingUpgradeStatus)) {\r\n        return false;\r\n    }\r\n    final RollingUpgradeStatus that = (RollingUpgradeStatus) obj;\r\n    return this.blockPoolId.equals(that.blockPoolId) && this.isFinalized() == that.isFinalized();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"  Block Pool ID: \" + blockPoolId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "cleanupWithLogger",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanupWithLogger(Logger log, java.io.Closeable... closeables)\n{\r\n    for (java.io.Closeable c : closeables) {\r\n        if (c != null) {\r\n            try {\r\n                c.close();\r\n            } catch (Throwable e) {\r\n                if (log != null && log.isDebugEnabled()) {\r\n                    log.debug(\"Exception in closing \" + c, e);\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "updateReadStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateReadStatistics(ReadStatistics readStatistics, int nRead, BlockReader blockReader)\n{\r\n    updateReadStatistics(readStatistics, nRead, blockReader.isShortCircuit(), blockReader.getNetworkDistance());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "updateReadStatistics",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void updateReadStatistics(ReadStatistics readStatistics, int nRead, boolean isShortCircuit, int networkDistance)\n{\r\n    if (nRead <= 0) {\r\n        return;\r\n    }\r\n    if (isShortCircuit) {\r\n        readStatistics.addShortCircuitBytes(nRead);\r\n    } else if (networkDistance == 0) {\r\n        readStatistics.addLocalBytes(nRead);\r\n    } else {\r\n        readStatistics.addRemoteBytes(nRead);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getOp",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Op getOp(String str)\n{\r\n    try {\r\n        return DOMAIN.parse(str);\r\n    } catch (IllegalArgumentException e) {\r\n        throw new IllegalArgumentException(str + \" is not a valid \" + Type.DELETE + \" operation.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getHeader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "PacketHeader getHeader()\n{\r\n    return curHeader;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getDataSlice",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ByteBuffer getDataSlice()\n{\r\n    return curDataSlice;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getChecksumSlice",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ByteBuffer getChecksumSlice()\n{\r\n    return curChecksumSlice;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "receiveNextPacket",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void receiveNextPacket(ReadableByteChannel in) throws IOException\n{\r\n    doRead(in, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "receiveNextPacket",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void receiveNextPacket(InputStream in) throws IOException\n{\r\n    doRead(null, in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "doRead",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void doRead(ReadableByteChannel ch, InputStream in) throws IOException\n{\r\n    Preconditions.checkState(curHeader == null || !curHeader.isLastPacketInBlock());\r\n    curPacketBuf.clear();\r\n    curPacketBuf.limit(PacketHeader.PKT_LENGTHS_LEN);\r\n    doReadFully(ch, in, curPacketBuf);\r\n    curPacketBuf.flip();\r\n    int payloadLen = curPacketBuf.getInt();\r\n    if (payloadLen < Ints.BYTES) {\r\n        throw new IOException(\"Invalid payload length \" + payloadLen);\r\n    }\r\n    int dataPlusChecksumLen = payloadLen - Ints.BYTES;\r\n    int headerLen = curPacketBuf.getShort();\r\n    if (headerLen < 0) {\r\n        throw new IOException(\"Invalid header length \" + headerLen);\r\n    }\r\n    LOG.trace(\"readNextPacket: dataPlusChecksumLen={}, headerLen={}\", dataPlusChecksumLen, headerLen);\r\n    int totalLen = payloadLen + headerLen;\r\n    if (totalLen < 0 || totalLen > MAX_PACKET_SIZE) {\r\n        throw new IOException(\"Incorrect value for packet payload size: \" + payloadLen);\r\n    }\r\n    reallocPacketBuf(PacketHeader.PKT_LENGTHS_LEN + dataPlusChecksumLen + headerLen);\r\n    curPacketBuf.clear();\r\n    curPacketBuf.position(PacketHeader.PKT_LENGTHS_LEN);\r\n    curPacketBuf.limit(PacketHeader.PKT_LENGTHS_LEN + dataPlusChecksumLen + headerLen);\r\n    doReadFully(ch, in, curPacketBuf);\r\n    curPacketBuf.flip();\r\n    curPacketBuf.position(PacketHeader.PKT_LENGTHS_LEN);\r\n    byte[] headerBuf = new byte[headerLen];\r\n    curPacketBuf.get(headerBuf);\r\n    if (curHeader == null) {\r\n        curHeader = new PacketHeader();\r\n    }\r\n    curHeader.setFieldsFromData(payloadLen, headerBuf);\r\n    int checksumLen = dataPlusChecksumLen - curHeader.getDataLen();\r\n    if (checksumLen < 0) {\r\n        throw new IOException(\"Invalid packet: data length in packet header \" + \"exceeds data length received. dataPlusChecksumLen=\" + dataPlusChecksumLen + \" header: \" + curHeader);\r\n    }\r\n    reslicePacket(headerLen, checksumLen, curHeader.getDataLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "mirrorPacketTo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void mirrorPacketTo(DataOutputStream mirrorOut) throws IOException\n{\r\n    Preconditions.checkState(!useDirectBuffers, \"Currently only supported for non-direct buffers\");\r\n    mirrorOut.write(curPacketBuf.array(), curPacketBuf.arrayOffset(), curPacketBuf.remaining());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "doReadFully",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void doReadFully(ReadableByteChannel ch, InputStream in, ByteBuffer buf) throws IOException\n{\r\n    if (ch != null) {\r\n        readChannelFully(ch, buf);\r\n    } else {\r\n        Preconditions.checkState(!buf.isDirect(), \"Must not use direct buffers with InputStream API\");\r\n        IOUtils.readFully(in, buf.array(), buf.arrayOffset() + buf.position(), buf.remaining());\r\n        buf.position(buf.position() + buf.remaining());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "reslicePacket",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void reslicePacket(int headerLen, int checksumsLen, int dataLen)\n{\r\n    int lenThroughHeader = PacketHeader.PKT_LENGTHS_LEN + headerLen;\r\n    int lenThroughChecksums = lenThroughHeader + checksumsLen;\r\n    int lenThroughData = lenThroughChecksums + dataLen;\r\n    assert dataLen >= 0 : \"invalid datalen: \" + dataLen;\r\n    assert curPacketBuf.position() == lenThroughHeader;\r\n    assert curPacketBuf.limit() == lenThroughData : \"headerLen= \" + headerLen + \" clen=\" + checksumsLen + \" dlen=\" + dataLen + \" rem=\" + curPacketBuf.remaining();\r\n    curPacketBuf.position(lenThroughHeader);\r\n    curPacketBuf.limit(lenThroughChecksums);\r\n    curChecksumSlice = curPacketBuf.slice();\r\n    curPacketBuf.position(lenThroughChecksums);\r\n    curPacketBuf.limit(lenThroughData);\r\n    curDataSlice = curPacketBuf.slice();\r\n    curPacketBuf.position(0);\r\n    curPacketBuf.limit(lenThroughData);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "readChannelFully",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readChannelFully(ReadableByteChannel ch, ByteBuffer buf) throws IOException\n{\r\n    while (buf.remaining() > 0) {\r\n        int n = ch.read(buf);\r\n        if (n < 0) {\r\n            throw new IOException(\"Premature EOF reading from \" + ch);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "reallocPacketBuf",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void reallocPacketBuf(int atLeastCapacity)\n{\r\n    if (curPacketBuf == null || curPacketBuf.capacity() < atLeastCapacity) {\r\n        ByteBuffer newBuf;\r\n        if (useDirectBuffers) {\r\n            newBuf = bufferPool.getBuffer(atLeastCapacity);\r\n        } else {\r\n            newBuf = ByteBuffer.allocate(atLeastCapacity);\r\n        }\r\n        if (curPacketBuf != null) {\r\n            curPacketBuf.flip();\r\n            newBuf.put(curPacketBuf);\r\n        }\r\n        returnPacketBufToPool();\r\n        curPacketBuf = newBuf;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "returnPacketBufToPool",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void returnPacketBufToPool()\n{\r\n    if (curPacketBuf != null && curPacketBuf.isDirect()) {\r\n        bufferPool.returnBuffer(curPacketBuf);\r\n        curPacketBuf = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close()\n{\r\n    returnPacketBufToPool();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "finalize",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void finalize() throws Throwable\n{\r\n    try {\r\n        returnPacketBufToPool();\r\n    } finally {\r\n        super.finalize();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "writeData",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writeData(byte[] inarray, int off, int len) throws ClosedChannelException\n{\r\n    checkBuffer();\r\n    if (dataPos + len > buf.length) {\r\n        throw new BufferOverflowException();\r\n    }\r\n    System.arraycopy(inarray, off, buf, dataPos, len);\r\n    dataPos += len;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "writeData",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeData(ByteBuffer inBuffer, int len) throws ClosedChannelException\n{\r\n    checkBuffer();\r\n    len = len > inBuffer.remaining() ? inBuffer.remaining() : len;\r\n    if (dataPos + len > buf.length) {\r\n        throw new BufferOverflowException();\r\n    }\r\n    for (int i = 0; i < len; i++) {\r\n        buf[dataPos + i] = inBuffer.get();\r\n    }\r\n    dataPos += len;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "writeChecksum",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writeChecksum(byte[] inarray, int off, int len) throws ClosedChannelException\n{\r\n    checkBuffer();\r\n    if (len == 0) {\r\n        return;\r\n    }\r\n    if (checksumPos + len > dataStart) {\r\n        throw new BufferOverflowException();\r\n    }\r\n    System.arraycopy(inarray, off, buf, checksumPos, len);\r\n    checksumPos += len;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "writeTo",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void writeTo(DataOutputStream stm) throws IOException\n{\r\n    checkBuffer();\r\n    final int dataLen = dataPos - dataStart;\r\n    final int checksumLen = checksumPos - checksumStart;\r\n    final int pktLen = HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;\r\n    PacketHeader header = new PacketHeader(pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);\r\n    if (checksumPos != dataStart) {\r\n        System.arraycopy(buf, checksumStart, buf, dataStart - checksumLen, checksumLen);\r\n        checksumPos = dataStart;\r\n        checksumStart = checksumPos - checksumLen;\r\n    }\r\n    final int headerStart = checksumStart - header.getSerializedSize();\r\n    assert checksumStart + 1 >= header.getSerializedSize();\r\n    assert headerStart >= 0;\r\n    assert headerStart + header.getSerializedSize() == checksumStart;\r\n    System.arraycopy(header.getBytes(), 0, buf, headerStart, header.getSerializedSize());\r\n    if (DFSClientFaultInjector.get().corruptPacket()) {\r\n        buf[headerStart + header.getSerializedSize() + checksumLen + dataLen - 1] ^= 0xff;\r\n    }\r\n    stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);\r\n    if (DFSClientFaultInjector.get().uncorruptPacket()) {\r\n        buf[headerStart + header.getSerializedSize() + checksumLen + dataLen - 1] ^= 0xff;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkBuffer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkBuffer() throws ClosedChannelException\n{\r\n    if (buf == null) {\r\n        throw new ClosedChannelException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "releaseBuffer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void releaseBuffer(ByteArrayManager bam)\n{\r\n    bam.release(buf);\r\n    buf = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLastByteOffsetBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLastByteOffsetBlock()\n{\r\n    return offsetInBlock + dataPos - dataStart;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isHeartbeatPacket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isHeartbeatPacket()\n{\r\n    return seqno == HEART_BEAT_SEQNO;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isLastPacketInBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isLastPacketInBlock()\n{\r\n    return lastPacketInBlock;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSeqno",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSeqno()\n{\r\n    return seqno;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getNumChunks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumChunks()\n{\r\n    return numChunks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "incNumChunks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void incNumChunks()\n{\r\n    numChunks++;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getMaxChunks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxChunks()\n{\r\n    return maxChunks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setSyncBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSyncBlock(boolean syncBlock)\n{\r\n    this.syncBlock = syncBlock;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"packet seqno: \" + this.seqno + \" offsetInBlock: \" + this.offsetInBlock + \" lastPacketInBlock: \" + this.lastPacketInBlock + \" lastByteOffsetInBlock: \" + this.getLastByteOffsetBlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addTraceParent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addTraceParent(Span span)\n{\r\n    if (span == null) {\r\n        return;\r\n    }\r\n    addTraceParent(span.getContext());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addTraceParent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addTraceParent(SpanContext ctx)\n{\r\n    if (ctx == null) {\r\n        return;\r\n    }\r\n    if (traceParentsUsed == traceParents.length) {\r\n        int newLength = (traceParents.length == 0) ? 8 : traceParents.length * 2;\r\n        traceParents = Arrays.copyOf(traceParents, newLength);\r\n    }\r\n    traceParents[traceParentsUsed] = ctx;\r\n    traceParentsUsed++;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getTraceParents",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "SpanContext[] getTraceParents()\n{\r\n    int len = traceParentsUsed;\r\n    Arrays.sort(traceParents, 0, len);\r\n    int i = 0, j = 0;\r\n    SpanContext prevVal = null;\r\n    while (true) {\r\n        if (i == len) {\r\n            break;\r\n        }\r\n        SpanContext val = traceParents[i];\r\n        if (!val.equals(prevVal)) {\r\n            traceParents[j] = val;\r\n            j++;\r\n            prevVal = val;\r\n        }\r\n        i++;\r\n    }\r\n    if (j < traceParents.length) {\r\n        traceParents = Arrays.copyOf(traceParents, j);\r\n        traceParentsUsed = traceParents.length;\r\n    }\r\n    return traceParents;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setSpan",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSpan(Span span)\n{\r\n    this.span = span;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSpan",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Span getSpan()\n{\r\n    return span;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "op",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void op(final DataOutput out, final Op op) throws IOException\n{\r\n    out.writeShort(DataTransferProtocol.DATA_TRANSFER_VERSION);\r\n    op.write(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "send",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void send(final DataOutputStream out, final Op opcode, final Message proto) throws IOException\n{\r\n    LOG.trace(\"Sending DataTransferOp {}: {}\", proto.getClass().getSimpleName(), proto);\r\n    op(out, opcode);\r\n    proto.writeDelimitedTo(out);\r\n    out.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getCachingStrategy",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "CachingStrategyProto getCachingStrategy(CachingStrategy cachingStrategy)\n{\r\n    CachingStrategyProto.Builder builder = CachingStrategyProto.newBuilder();\r\n    if (cachingStrategy.getReadahead() != null) {\r\n        builder.setReadahead(cachingStrategy.getReadahead());\r\n    }\r\n    if (cachingStrategy.getDropBehind() != null) {\r\n        builder.setDropBehind(cachingStrategy.getDropBehind());\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "readBlock",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readBlock(final ExtendedBlock blk, final Token<BlockTokenIdentifier> blockToken, final String clientName, final long blockOffset, final long length, final boolean sendChecksum, final CachingStrategy cachingStrategy) throws IOException\n{\r\n    OpReadBlockProto proto = OpReadBlockProto.newBuilder().setHeader(DataTransferProtoUtil.buildClientHeader(blk, clientName, blockToken)).setOffset(blockOffset).setLen(length).setSendChecksums(sendChecksum).setCachingStrategy(getCachingStrategy(cachingStrategy)).build();\r\n    send(out, Op.READ_BLOCK, proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "writeBlock",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void writeBlock(final ExtendedBlock blk, final StorageType storageType, final Token<BlockTokenIdentifier> blockToken, final String clientName, final DatanodeInfo[] targets, final StorageType[] targetStorageTypes, final DatanodeInfo source, final BlockConstructionStage stage, final int pipelineSize, final long minBytesRcvd, final long maxBytesRcvd, final long latestGenerationStamp, DataChecksum requestedChecksum, final CachingStrategy cachingStrategy, final boolean allowLazyPersist, final boolean pinning, final boolean[] targetPinnings, final String storageId, final String[] targetStorageIds) throws IOException\n{\r\n    ClientOperationHeaderProto header = DataTransferProtoUtil.buildClientHeader(blk, clientName, blockToken);\r\n    ChecksumProto checksumProto = DataTransferProtoUtil.toProto(requestedChecksum);\r\n    OpWriteBlockProto.Builder proto = OpWriteBlockProto.newBuilder().setHeader(header).setStorageType(PBHelperClient.convertStorageType(storageType)).addAllTargets(PBHelperClient.convert(targets, 1)).addAllTargetStorageTypes(PBHelperClient.convertStorageTypes(targetStorageTypes, 1)).setStage(toProto(stage)).setPipelineSize(pipelineSize).setMinBytesRcvd(minBytesRcvd).setMaxBytesRcvd(maxBytesRcvd).setLatestGenerationStamp(latestGenerationStamp).setRequestedChecksum(checksumProto).setCachingStrategy(getCachingStrategy(cachingStrategy)).setAllowLazyPersist(allowLazyPersist).setPinning(pinning).addAllTargetPinnings(PBHelperClient.convert(targetPinnings, 1)).addAllTargetStorageIds(PBHelperClient.convert(targetStorageIds, 1));\r\n    if (source != null) {\r\n        proto.setSource(PBHelperClient.convertDatanodeInfo(source));\r\n    }\r\n    if (storageId != null) {\r\n        proto.setStorageId(storageId);\r\n    }\r\n    send(out, Op.WRITE_BLOCK, proto.build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "transferBlock",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void transferBlock(final ExtendedBlock blk, final Token<BlockTokenIdentifier> blockToken, final String clientName, final DatanodeInfo[] targets, final StorageType[] targetStorageTypes, final String[] targetStorageIds) throws IOException\n{\r\n    OpTransferBlockProto proto = OpTransferBlockProto.newBuilder().setHeader(DataTransferProtoUtil.buildClientHeader(blk, clientName, blockToken)).addAllTargets(PBHelperClient.convert(targets)).addAllTargetStorageTypes(PBHelperClient.convertStorageTypes(targetStorageTypes)).addAllTargetStorageIds(Arrays.asList(targetStorageIds)).build();\r\n    send(out, Op.TRANSFER_BLOCK, proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "requestShortCircuitFds",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void requestShortCircuitFds(final ExtendedBlock blk, final Token<BlockTokenIdentifier> blockToken, SlotId slotId, int maxVersion, boolean supportsReceiptVerification) throws IOException\n{\r\n    OpRequestShortCircuitAccessProto.Builder builder = OpRequestShortCircuitAccessProto.newBuilder().setHeader(DataTransferProtoUtil.buildBaseHeader(blk, blockToken)).setMaxVersion(maxVersion);\r\n    if (slotId != null) {\r\n        builder.setSlotId(PBHelperClient.convert(slotId));\r\n    }\r\n    builder.setSupportsReceiptVerification(supportsReceiptVerification);\r\n    OpRequestShortCircuitAccessProto proto = builder.build();\r\n    send(out, Op.REQUEST_SHORT_CIRCUIT_FDS, proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "releaseShortCircuitFds",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void releaseShortCircuitFds(SlotId slotId) throws IOException\n{\r\n    ReleaseShortCircuitAccessRequestProto.Builder builder = ReleaseShortCircuitAccessRequestProto.newBuilder().setSlotId(PBHelperClient.convert(slotId));\r\n    Span span = Tracer.getCurrentSpan();\r\n    if (span != null) {\r\n        DataTransferTraceInfoProto.Builder traceInfoProtoBuilder = DataTransferTraceInfoProto.newBuilder().setSpanContext(TraceUtils.spanContextToByteString(span.getContext()));\r\n        builder.setTraceInfo(traceInfoProtoBuilder);\r\n    }\r\n    ReleaseShortCircuitAccessRequestProto proto = builder.build();\r\n    send(out, Op.RELEASE_SHORT_CIRCUIT_FDS, proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "requestShortCircuitShm",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void requestShortCircuitShm(String clientName) throws IOException\n{\r\n    ShortCircuitShmRequestProto.Builder builder = ShortCircuitShmRequestProto.newBuilder().setClientName(clientName);\r\n    Span span = Tracer.getCurrentSpan();\r\n    if (span != null) {\r\n        DataTransferTraceInfoProto.Builder traceInfoProtoBuilder = DataTransferTraceInfoProto.newBuilder().setSpanContext(TraceUtils.spanContextToByteString(span.getContext()));\r\n        builder.setTraceInfo(traceInfoProtoBuilder);\r\n    }\r\n    ShortCircuitShmRequestProto proto = builder.build();\r\n    send(out, Op.REQUEST_SHORT_CIRCUIT_SHM, proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "replaceBlock",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void replaceBlock(final ExtendedBlock blk, final StorageType storageType, final Token<BlockTokenIdentifier> blockToken, final String delHint, final DatanodeInfo source, final String storageId) throws IOException\n{\r\n    OpReplaceBlockProto.Builder proto = OpReplaceBlockProto.newBuilder().setHeader(DataTransferProtoUtil.buildBaseHeader(blk, blockToken)).setStorageType(PBHelperClient.convertStorageType(storageType)).setDelHint(delHint).setSource(PBHelperClient.convertDatanodeInfo(source));\r\n    if (storageId != null) {\r\n        proto.setStorageId(storageId);\r\n    }\r\n    send(out, Op.REPLACE_BLOCK, proto.build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "copyBlock",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void copyBlock(final ExtendedBlock blk, final Token<BlockTokenIdentifier> blockToken) throws IOException\n{\r\n    OpCopyBlockProto proto = OpCopyBlockProto.newBuilder().setHeader(DataTransferProtoUtil.buildBaseHeader(blk, blockToken)).build();\r\n    send(out, Op.COPY_BLOCK, proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "blockChecksum",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void blockChecksum(final ExtendedBlock blk, final Token<BlockTokenIdentifier> blockToken, BlockChecksumOptions blockChecksumOptions) throws IOException\n{\r\n    OpBlockChecksumProto proto = OpBlockChecksumProto.newBuilder().setHeader(DataTransferProtoUtil.buildBaseHeader(blk, blockToken)).setBlockChecksumOptions(PBHelperClient.convert(blockChecksumOptions)).build();\r\n    send(out, Op.BLOCK_CHECKSUM, proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "blockGroupChecksum",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void blockGroupChecksum(StripedBlockInfo stripedBlockInfo, Token<BlockTokenIdentifier> blockToken, long requestedNumBytes, BlockChecksumOptions blockChecksumOptions) throws IOException\n{\r\n    OpBlockGroupChecksumProto proto = OpBlockGroupChecksumProto.newBuilder().setHeader(DataTransferProtoUtil.buildBaseHeader(stripedBlockInfo.getBlock(), blockToken)).setDatanodes(PBHelperClient.convertToProto(stripedBlockInfo.getDatanodes())).addAllBlockTokens(PBHelperClient.convert(stripedBlockInfo.getBlockTokens())).addAllBlockIndices(PBHelperClient.convertBlockIndices(stripedBlockInfo.getBlockIndices())).setEcPolicy(PBHelperClient.convertErasureCodingPolicy(stripedBlockInfo.getErasureCodingPolicy())).setRequestedNumBytes(requestedNumBytes).setBlockChecksumOptions(PBHelperClient.convert(blockChecksumOptions)).build();\r\n    send(out, Op.BLOCK_GROUP_CHECKSUM, proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPath()\n{\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "verify",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verify(HdfsLocatedFileStatus stat) throws InvalidPathHandleException\n{\r\n    if (null == stat) {\r\n        throw new InvalidPathHandleException(\"Could not resolve handle\");\r\n    }\r\n    if (mtime != null && mtime != stat.getModificationTime()) {\r\n        throw new InvalidPathHandleException(\"Content changed\");\r\n    }\r\n    if (inodeId != null && inodeId != stat.getFileId()) {\r\n        throw new InvalidPathHandleException(\"Wrong file\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "bytes",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ByteBuffer bytes()\n{\r\n    HdfsPathHandleProto.Builder b = HdfsPathHandleProto.newBuilder();\r\n    b.setPath(path);\r\n    if (inodeId != null) {\r\n        b.setInodeId(inodeId);\r\n    }\r\n    if (mtime != null) {\r\n        b.setMtime(mtime);\r\n    }\r\n    return b.build().toByteString().asReadOnlyByteBuffer();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean equals(Object other)\n{\r\n    if (null == other) {\r\n        return false;\r\n    }\r\n    if (!HdfsPathHandle.class.equals(other.getClass())) {\r\n        return false;\r\n    }\r\n    HdfsPathHandle o = (HdfsPathHandle) other;\r\n    return getPath().equals(o.getPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return path.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(\"{ \").append(\"\\\"path\\\" : \\\"\").append(path).append(\"\\\"\");\r\n    if (inodeId != null) {\r\n        sb.append(\",\\\"inodeId\\\" : \").append(inodeId);\r\n    }\r\n    if (mtime != null) {\r\n        sb.append(\",\\\"mtime\\\" : \").append(mtime);\r\n    }\r\n    sb.append(\" }\");\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "string2Bytes",
  "errType" : [ "UnsupportedEncodingException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] string2Bytes(String str)\n{\r\n    try {\r\n        return str.getBytes(UTF8_CSN);\r\n    } catch (UnsupportedEncodingException e) {\r\n        throw new IllegalArgumentException(\"UTF8 decoding is not supported\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "bytes2String",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String bytes2String(byte[] bytes)\n{\r\n    return bytes2String(bytes, 0, bytes.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "bytes2byteArray",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[][] bytes2byteArray(byte[] bytes)\n{\r\n    return bytes2byteArray(bytes, bytes.length, (byte) Path.SEPARATOR_CHAR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "bytes2byteArray",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[][] bytes2byteArray(byte[] bytes, int len, byte separator)\n{\r\n    if (len < 0 || len > bytes.length) {\r\n        throw new IndexOutOfBoundsException(\"Incorrect index [len, size] [\" + len + \", \" + bytes.length + \"]\");\r\n    }\r\n    if (len == 0) {\r\n        return new byte[][] { null };\r\n    }\r\n    int splits = 0;\r\n    for (int i = 1; i < len; i++) {\r\n        if (bytes[i - 1] == separator && bytes[i] != separator) {\r\n            splits++;\r\n        }\r\n    }\r\n    if (splits == 0 && bytes[0] == separator) {\r\n        return new byte[][] { null };\r\n    }\r\n    splits++;\r\n    byte[][] result = new byte[splits][];\r\n    int nextIndex = 0;\r\n    for (int i = 0; i < splits; i++) {\r\n        int startIndex = nextIndex;\r\n        while (nextIndex < len && bytes[nextIndex] != separator) {\r\n            nextIndex++;\r\n        }\r\n        result[i] = (nextIndex > 0) ? Arrays.copyOfRange(bytes, startIndex, nextIndex) : DFSUtilClient.EMPTY_BYTES;\r\n        do {\r\n            nextIndex++;\r\n        } while (nextIndex < len && bytes[nextIndex] == separator);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getPercentUsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getPercentUsed(long used, long capacity)\n{\r\n    return capacity <= 0 ? 100 : (used * 100.0f) / capacity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getPercentRemaining",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getPercentRemaining(long remaining, long capacity)\n{\r\n    return capacity <= 0 ? 0 : (remaining * 100.0f) / capacity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "percent2String",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String percent2String(double percentage)\n{\r\n    return StringUtils.format(\"%.2f%%\", percentage);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getNameServiceIds",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<String> getNameServiceIds(Configuration conf)\n{\r\n    return conf.getTrimmedStringCollection(DFS_NAMESERVICES);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getNameNodeIds",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Collection<String> getNameNodeIds(Configuration conf, String nsId)\n{\r\n    String key = addSuffix(DFS_HA_NAMENODES_KEY_PREFIX, nsId);\r\n    return conf.getTrimmedStringCollection(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addSuffix",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String addSuffix(String key, String suffix)\n{\r\n    if (suffix == null || suffix.isEmpty()) {\r\n        return key;\r\n    }\r\n    assert !suffix.startsWith(\".\") : \"suffix '\" + suffix + \"' should not already have '.' prepended.\";\r\n    return key + \".\" + suffix;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getHaNnRpcAddresses",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, Map<String, InetSocketAddress>> getHaNnRpcAddresses(Configuration conf)\n{\r\n    return DFSUtilClient.getAddresses(conf, null, HdfsClientConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getHaNnWebHdfsAddresses",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<String, Map<String, InetSocketAddress>> getHaNnWebHdfsAddresses(Configuration conf, String scheme)\n{\r\n    if (WebHdfsConstants.WEBHDFS_SCHEME.equals(scheme)) {\r\n        return getAddresses(conf, null, HdfsClientConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY);\r\n    } else if (WebHdfsConstants.SWEBHDFS_SCHEME.equals(scheme)) {\r\n        return getAddresses(conf, null, HdfsClientConfigKeys.DFS_NAMENODE_HTTPS_ADDRESS_KEY);\r\n    } else {\r\n        throw new IllegalArgumentException(\"Unsupported scheme: \" + scheme);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "locatedBlocks2Locations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockLocation[] locatedBlocks2Locations(LocatedBlocks blocks)\n{\r\n    if (blocks == null) {\r\n        return new BlockLocation[0];\r\n    }\r\n    return locatedBlocks2Locations(blocks.getLocatedBlocks());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "locatedBlocks2Locations",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "BlockLocation[] locatedBlocks2Locations(List<LocatedBlock> blocks)\n{\r\n    if (blocks == null) {\r\n        return new BlockLocation[0];\r\n    }\r\n    int nrBlocks = blocks.size();\r\n    BlockLocation[] blkLocations = new BlockLocation[nrBlocks];\r\n    if (nrBlocks == 0) {\r\n        return blkLocations;\r\n    }\r\n    int idx = 0;\r\n    for (LocatedBlock blk : blocks) {\r\n        assert idx < nrBlocks : \"Incorrect index\";\r\n        DatanodeInfo[] locations = blk.getLocations();\r\n        String[] hosts = new String[locations.length];\r\n        String[] xferAddrs = new String[locations.length];\r\n        String[] racks = new String[locations.length];\r\n        for (int hCnt = 0; hCnt < locations.length; hCnt++) {\r\n            hosts[hCnt] = locations[hCnt].getHostName();\r\n            xferAddrs[hCnt] = locations[hCnt].getXferAddr();\r\n            NodeBase node = new NodeBase(xferAddrs[hCnt], locations[hCnt].getNetworkLocation());\r\n            racks[hCnt] = node.toString();\r\n        }\r\n        DatanodeInfo[] cachedLocations = blk.getCachedLocations();\r\n        String[] cachedHosts = new String[cachedLocations.length];\r\n        for (int i = 0; i < cachedLocations.length; i++) {\r\n            cachedHosts[i] = cachedLocations[i].getHostName();\r\n        }\r\n        blkLocations[idx] = new BlockLocation(xferAddrs, hosts, cachedHosts, racks, blk.getStorageIDs(), blk.getStorageTypes(), blk.getStartOffset(), blk.getBlockSize(), blk.isCorrupt());\r\n        idx++;\r\n    }\r\n    return blkLocations;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "compareBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int compareBytes(byte[] left, byte[] right)\n{\r\n    if (left == null) {\r\n        left = EMPTY_BYTES;\r\n    }\r\n    if (right == null) {\r\n        right = EMPTY_BYTES;\r\n    }\r\n    return SignedBytes.lexicographicalComparator().compare(left, right);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "byteArray2bytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] byteArray2bytes(byte[][] pathComponents)\n{\r\n    if (pathComponents.length == 0 || (pathComponents.length == 1 && (pathComponents[0] == null || pathComponents[0].length == 0))) {\r\n        return EMPTY_BYTES;\r\n    }\r\n    int length = 0;\r\n    for (int i = 0; i < pathComponents.length; i++) {\r\n        length += pathComponents[i].length;\r\n        if (i < pathComponents.length - 1) {\r\n            length++;\r\n        }\r\n    }\r\n    byte[] path = new byte[length];\r\n    int index = 0;\r\n    for (int i = 0; i < pathComponents.length; i++) {\r\n        System.arraycopy(pathComponents[i], 0, path, index, pathComponents[i].length);\r\n        index += pathComponents[i].length;\r\n        if (i < pathComponents.length - 1) {\r\n            path[index] = (byte) Path.SEPARATOR_CHAR;\r\n            index++;\r\n        }\r\n    }\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "byteArray2String",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String byteArray2String(byte[][] pathComponents)\n{\r\n    return bytes2String(byteArray2bytes(pathComponents));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "bytes2String",
  "errType" : [ "UnsupportedEncodingException" ],
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String bytes2String(byte[] bytes, int offset, int length)\n{\r\n    try {\r\n        return new String(bytes, offset, length, UTF8_CSN);\r\n    } catch (UnsupportedEncodingException e) {\r\n        throw new IllegalArgumentException(\"UTF8 encoding is not supported\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "emptyAsSingletonNull",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Collection<String> emptyAsSingletonNull(Collection<String> coll)\n{\r\n    if (coll == null || coll.isEmpty()) {\r\n        return Collections.singletonList(null);\r\n    } else {\r\n        return coll;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "concatSuffixes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String concatSuffixes(String... suffixes)\n{\r\n    if (suffixes == null) {\r\n        return null;\r\n    }\r\n    return Joiner.on(\".\").skipNulls().join(suffixes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAddresses",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Map<String, Map<String, InetSocketAddress>> getAddresses(Configuration conf, String defaultAddress, String... keys)\n{\r\n    Collection<String> nameserviceIds = getNameServiceIds(conf);\r\n    return getAddressesForNsIds(conf, nameserviceIds, defaultAddress, keys);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getResolvedAddressesForNsId",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Map<String, InetSocketAddress> getResolvedAddressesForNsId(Configuration conf, String nsId, DomainNameResolver dnr, String defaultValue, String... keys)\n{\r\n    Collection<String> nnIds = getNameNodeIds(conf, nsId);\r\n    Map<String, InetSocketAddress> ret = Maps.newLinkedHashMap();\r\n    for (String nnId : emptyAsSingletonNull(nnIds)) {\r\n        Map<String, InetSocketAddress> resolvedAddressesForNnId = getResolvedAddressesForNnId(conf, nsId, nnId, dnr, defaultValue, keys);\r\n        ret.putAll(resolvedAddressesForNnId);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getResolvedAddressesForNnId",
  "errType" : [ "UnknownHostException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Map<String, InetSocketAddress> getResolvedAddressesForNnId(Configuration conf, String nsId, String nnId, DomainNameResolver dnr, String defaultValue, String... keys)\n{\r\n    String suffix = concatSuffixes(nsId, nnId);\r\n    String address = checkKeysAndProcess(defaultValue, suffix, conf, keys);\r\n    Map<String, InetSocketAddress> ret = Maps.newLinkedHashMap();\r\n    if (address != null) {\r\n        InetSocketAddress isa = NetUtils.createSocketAddr(address);\r\n        try {\r\n            String[] resolvedHostNames = dnr.getAllResolvedHostnameByDomainName(isa.getHostName(), true);\r\n            int port = isa.getPort();\r\n            for (String hostname : resolvedHostNames) {\r\n                InetSocketAddress inetSocketAddress = new InetSocketAddress(hostname, port);\r\n                String concatId = getConcatNnId(nsId, nnId, hostname, port);\r\n                ret.put(concatId, inetSocketAddress);\r\n            }\r\n        } catch (UnknownHostException e) {\r\n            LOG.error(\"Failed to resolve address: {}\", address);\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getConcatNnId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getConcatNnId(String nsId, String nnId, String hostname, int port)\n{\r\n    if (nnId == null || nnId.isEmpty()) {\r\n        return String.join(\"-\", nsId, hostname, String.valueOf(port));\r\n    }\r\n    return String.join(\"-\", nsId, nnId, hostname, String.valueOf(port));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAddressesForNsIds",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Map<String, Map<String, InetSocketAddress>> getAddressesForNsIds(Configuration conf, Collection<String> nsIds, String defaultAddress, String... keys)\n{\r\n    Map<String, Map<String, InetSocketAddress>> ret = Maps.newLinkedHashMap();\r\n    for (String nsId : emptyAsSingletonNull(nsIds)) {\r\n        Map<String, InetSocketAddress> isas = getAddressesForNameserviceId(conf, nsId, defaultAddress, keys);\r\n        if (!isas.isEmpty()) {\r\n            ret.put(nsId, isas);\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAddressesForNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Map<String, InetSocketAddress> getAddressesForNameserviceId(Configuration conf, String nsId, String defaultValue, String... keys)\n{\r\n    Collection<String> nnIds = getNameNodeIds(conf, nsId);\r\n    Map<String, InetSocketAddress> ret = Maps.newLinkedHashMap();\r\n    for (String nnId : emptyAsSingletonNull(nnIds)) {\r\n        String suffix = concatSuffixes(nsId, nnId);\r\n        String address = checkKeysAndProcess(defaultValue, suffix, conf, keys);\r\n        if (address != null) {\r\n            InetSocketAddress isa = NetUtils.createSocketAddr(address);\r\n            if (isa.isUnresolved()) {\r\n                LOG.warn(\"Namenode for {} remains unresolved for ID {}. Check your \" + \"hdfs-site.xml file to ensure namenodes are configured \" + \"properly.\", nsId, nnId);\r\n            }\r\n            ret.put(nnId, isa);\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkKeysAndProcess",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String checkKeysAndProcess(String defaultValue, String suffix, Configuration conf, String... keys)\n{\r\n    String succeededKey = null;\r\n    String address = null;\r\n    for (String key : keys) {\r\n        address = getConfValue(null, suffix, conf, key);\r\n        if (address != null) {\r\n            succeededKey = key;\r\n            break;\r\n        }\r\n    }\r\n    String ret;\r\n    if (address == null) {\r\n        ret = defaultValue;\r\n    } else if (DFS_NAMENODE_RPC_ADDRESS_KEY.equals(succeededKey)) {\r\n        ret = checkRpcAuxiliary(conf, suffix, address);\r\n    } else {\r\n        ret = address;\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkRpcAuxiliary",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String checkRpcAuxiliary(Configuration conf, String suffix, String address)\n{\r\n    String key = DFS_NAMENODE_RPC_ADDRESS_AUXILIARY_KEY;\r\n    key = addSuffix(key, suffix);\r\n    int[] ports = conf.getInts(key);\r\n    if (ports == null || ports.length == 0) {\r\n        return address;\r\n    }\r\n    LOG.info(\"Using server auxiliary ports \" + Arrays.toString(ports));\r\n    URI uri;\r\n    try {\r\n        uri = new URI(address);\r\n    } catch (URISyntaxException e) {\r\n        LOG.warn(\"NameNode address is not a valid uri:\" + address);\r\n        return address;\r\n    }\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(uri.getScheme()).append(\"://\").append(uri.getHost()).append(\":\");\r\n    sb.append(ports[0]);\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getConfValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getConfValue(String defaultValue, String keySuffix, Configuration conf, String... keys)\n{\r\n    String value = null;\r\n    for (String key : keys) {\r\n        key = addSuffix(key, keySuffix);\r\n        value = conf.get(key);\r\n        if (value != null) {\r\n            break;\r\n        }\r\n    }\r\n    if (value == null) {\r\n        value = defaultValue;\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isValidName",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "boolean isValidName(String src)\n{\r\n    if (!src.startsWith(Path.SEPARATOR)) {\r\n        return false;\r\n    }\r\n    String[] components = StringUtils.split(src, '/');\r\n    for (int i = 0; i < components.length; i++) {\r\n        String element = components[i];\r\n        if (element.equals(\".\") || (element.contains(\":\")) || (element.contains(\"/\"))) {\r\n            return false;\r\n        }\r\n        if (element.equals(\"..\")) {\r\n            if (components.length > 4 && components[1].equals(\".reserved\") && components[2].equals(\".inodes\")) {\r\n                continue;\r\n            }\r\n            return false;\r\n        }\r\n        if (element.isEmpty() && i != components.length - 1 && i != 0) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "durationToString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String durationToString(long durationMs)\n{\r\n    boolean negative = false;\r\n    if (durationMs < 0) {\r\n        negative = true;\r\n        durationMs = -durationMs;\r\n    }\r\n    long durationSec = durationMs / 1000;\r\n    final int secondsPerMinute = 60;\r\n    final int secondsPerHour = 60 * 60;\r\n    final int secondsPerDay = 60 * 60 * 24;\r\n    final long days = durationSec / secondsPerDay;\r\n    durationSec -= days * secondsPerDay;\r\n    final long hours = durationSec / secondsPerHour;\r\n    durationSec -= hours * secondsPerHour;\r\n    final long minutes = durationSec / secondsPerMinute;\r\n    durationSec -= minutes * secondsPerMinute;\r\n    final long seconds = durationSec;\r\n    final long milliseconds = durationMs % 1000;\r\n    String format = \"%03d:%02d:%02d:%02d.%03d\";\r\n    if (negative) {\r\n        format = \"-\" + format;\r\n    }\r\n    return String.format(format, days, hours, minutes, seconds, milliseconds);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "dateToIso8601String",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String dateToIso8601String(Date date)\n{\r\n    SimpleDateFormat df = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ssZ\", Locale.ENGLISH);\r\n    return df.format(date);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isLocalAddress",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean isLocalAddress(InetSocketAddress targetAddr) throws IOException\n{\r\n    if (targetAddr.isUnresolved()) {\r\n        throw new IOException(\"Unresolved host: \" + targetAddr);\r\n    }\r\n    InetAddress addr = targetAddr.getAddress();\r\n    Boolean cached = localAddrMap.get(addr.getHostAddress());\r\n    if (cached != null) {\r\n        LOG.trace(\"Address {} is{} local\", targetAddr, (cached ? \"\" : \" not\"));\r\n        return cached;\r\n    }\r\n    boolean local = NetUtils.isLocalAddress(addr);\r\n    LOG.trace(\"Address {} is{} local\", targetAddr, (local ? \"\" : \" not\"));\r\n    localAddrMap.put(addr.getHostAddress(), local);\r\n    return local;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createClientDatanodeProtocolProxy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientDatanodeProtocol createClientDatanodeProtocolProxy(DatanodeID datanodeid, Configuration conf, int socketTimeout, boolean connectToDnViaHostname, LocatedBlock locatedBlock) throws IOException\n{\r\n    return new ClientDatanodeProtocolTranslatorPB(datanodeid, conf, socketTimeout, connectToDnViaHostname, locatedBlock);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createClientDatanodeProtocolProxy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientDatanodeProtocol createClientDatanodeProtocolProxy(DatanodeID datanodeid, Configuration conf, int socketTimeout, boolean connectToDnViaHostname) throws IOException\n{\r\n    return new ClientDatanodeProtocolTranslatorPB(datanodeid, conf, socketTimeout, connectToDnViaHostname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createClientDatanodeProtocolProxy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientDatanodeProtocol createClientDatanodeProtocolProxy(InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory) throws IOException\n{\r\n    return new ClientDatanodeProtocolTranslatorPB(addr, ticket, conf, factory);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createReconfigurationProtocolProxy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReconfigurationProtocol createReconfigurationProtocolProxy(InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory) throws IOException\n{\r\n    return new ReconfigurationProtocolTranslatorPB(addr, ticket, conf, factory);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "peerFromSocket",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Peer peerFromSocket(Socket socket) throws IOException\n{\r\n    Peer peer;\r\n    boolean success = false;\r\n    try {\r\n        socket.setTcpNoDelay(true);\r\n        SocketChannel channel = socket.getChannel();\r\n        if (channel == null) {\r\n            peer = new BasicInetPeer(socket);\r\n        } else {\r\n            peer = new NioInetPeer(socket);\r\n        }\r\n        success = true;\r\n        return peer;\r\n    } finally {\r\n        if (!success) {\r\n            socket.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "peerFromSocketAndKey",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Peer peerFromSocketAndKey(SaslDataTransferClient saslClient, Socket s, DataEncryptionKeyFactory keyFactory, Token<BlockTokenIdentifier> blockToken, DatanodeID datanodeId, int socketTimeoutMs) throws IOException\n{\r\n    Peer peer = null;\r\n    boolean success = false;\r\n    try {\r\n        peer = peerFromSocket(s);\r\n        peer.setReadTimeout(socketTimeoutMs);\r\n        peer.setWriteTimeout(socketTimeoutMs);\r\n        peer = saslClient.peerSend(peer, keyFactory, blockToken, datanodeId);\r\n        success = true;\r\n        return peer;\r\n    } finally {\r\n        if (!success) {\r\n            IOUtilsClient.cleanupWithLogger(LOG, peer);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getIoFileBufferSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getIoFileBufferSize(Configuration conf)\n{\r\n    return conf.getInt(CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_KEY, CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSmallBufferSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getSmallBufferSize(Configuration conf)\n{\r\n    return Math.min(getIoFileBufferSize(conf) / 2, 512);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isHDFSEncryptionEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isHDFSEncryptionEnabled(Configuration conf)\n{\r\n    return !(conf.getTrimmed(CommonConfigurationKeysPublic.HADOOP_SECURITY_KEY_PROVIDER_PATH, \"\").isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getNNAddress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "InetSocketAddress getNNAddress(String address)\n{\r\n    return NetUtils.createSocketAddr(address, HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getNNAddress",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "InetSocketAddress getNNAddress(Configuration conf)\n{\r\n    URI filesystemURI = FileSystem.getDefaultUri(conf);\r\n    return getNNAddressCheckLogical(conf, filesystemURI);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getNNAddress",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "InetSocketAddress getNNAddress(URI filesystemURI)\n{\r\n    String authority = filesystemURI.getAuthority();\r\n    if (authority == null) {\r\n        throw new IllegalArgumentException(String.format(\"Invalid URI for NameNode address (check %s): %s has no authority.\", FileSystem.FS_DEFAULT_NAME_KEY, filesystemURI.toString()));\r\n    }\r\n    if (!HdfsConstants.HDFS_URI_SCHEME.equalsIgnoreCase(filesystemURI.getScheme())) {\r\n        throw new IllegalArgumentException(String.format(\"Invalid URI for NameNode address (check %s): \" + \"%s is not of scheme '%s'.\", FileSystem.FS_DEFAULT_NAME_KEY, filesystemURI.toString(), HdfsConstants.HDFS_URI_SCHEME));\r\n    }\r\n    return getNNAddress(authority);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getNNAddressCheckLogical",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "InetSocketAddress getNNAddressCheckLogical(Configuration conf, URI filesystemURI)\n{\r\n    InetSocketAddress retAddr;\r\n    if (HAUtilClient.isLogicalUri(conf, filesystemURI)) {\r\n        retAddr = InetSocketAddress.createUnresolved(filesystemURI.getAuthority(), HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT);\r\n    } else {\r\n        retAddr = getNNAddress(filesystemURI);\r\n    }\r\n    return retAddr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getNNUri",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "URI getNNUri(InetSocketAddress namenode)\n{\r\n    int port = namenode.getPort();\r\n    String portString = (port == HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT) ? \"\" : (\":\" + port);\r\n    return URI.create(HdfsConstants.HDFS_URI_SCHEME + \"://\" + namenode.getHostName() + portString);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "toInterruptedIOException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "InterruptedIOException toInterruptedIOException(String message, InterruptedException e)\n{\r\n    final InterruptedIOException iioe = new InterruptedIOException(message);\r\n    iioe.initCause(e);\r\n    return iioe;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "connectToDN",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "IOStreamPair connectToDN(DatanodeInfo dn, int timeout, Configuration conf, SaslDataTransferClient saslClient, SocketFactory socketFactory, boolean connectToDnViaHostname, DataEncryptionKeyFactory dekFactory, Token<BlockTokenIdentifier> blockToken) throws IOException\n{\r\n    boolean success = false;\r\n    Socket sock = null;\r\n    try {\r\n        sock = socketFactory.createSocket();\r\n        String dnAddr = dn.getXferAddr(connectToDnViaHostname);\r\n        LOG.debug(\"Connecting to datanode {}\", dnAddr);\r\n        NetUtils.connect(sock, NetUtils.createSocketAddr(dnAddr), timeout);\r\n        sock.setTcpNoDelay(getClientDataTransferTcpNoDelay(conf));\r\n        sock.setSoTimeout(timeout);\r\n        OutputStream unbufOut = NetUtils.getOutputStream(sock);\r\n        InputStream unbufIn = NetUtils.getInputStream(sock);\r\n        IOStreamPair pair = saslClient.newSocketSend(sock, unbufOut, unbufIn, dekFactory, blockToken, dn);\r\n        IOStreamPair result = new IOStreamPair(new DataInputStream(pair.in), new DataOutputStream(new BufferedOutputStream(pair.out, DFSUtilClient.getSmallBufferSize(conf))));\r\n        success = true;\r\n        return result;\r\n    } finally {\r\n        if (!success) {\r\n            IOUtils.closeSocket(sock);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getClientDataTransferTcpNoDelay",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getClientDataTransferTcpNoDelay(Configuration conf)\n{\r\n    return conf.getBoolean(DFS_DATA_TRANSFER_CLIENT_TCPNODELAY_KEY, DFS_DATA_TRANSFER_CLIENT_TCPNODELAY_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getThreadPoolExecutor",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ThreadPoolExecutor getThreadPoolExecutor(int corePoolSize, int maxPoolSize, long keepAliveTimeSecs, String threadNamePrefix, boolean runRejectedExec)\n{\r\n    return getThreadPoolExecutor(corePoolSize, maxPoolSize, keepAliveTimeSecs, new SynchronousQueue<>(), threadNamePrefix, runRejectedExec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getThreadPoolExecutor",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ThreadPoolExecutor getThreadPoolExecutor(int corePoolSize, int maxPoolSize, long keepAliveTimeSecs, BlockingQueue<Runnable> queue, String threadNamePrefix, boolean runRejectedExec)\n{\r\n    Preconditions.checkArgument(corePoolSize > 0);\r\n    ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(corePoolSize, maxPoolSize, keepAliveTimeSecs, TimeUnit.SECONDS, queue, new Daemon.DaemonFactory() {\r\n\r\n        private final AtomicInteger threadIndex = new AtomicInteger(0);\r\n\r\n        @Override\r\n        public Thread newThread(Runnable r) {\r\n            Thread t = super.newThread(r);\r\n            t.setName(threadNamePrefix + threadIndex.getAndIncrement());\r\n            return t;\r\n        }\r\n    });\r\n    if (runRejectedExec) {\r\n        threadPoolExecutor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy() {\r\n\r\n            @Override\r\n            public void rejectedExecution(Runnable runnable, ThreadPoolExecutor e) {\r\n                LOG.info(threadNamePrefix + \" task is rejected by \" + \"ThreadPoolExecutor. Executing it in current thread.\");\r\n                super.rejectedExecution(runnable, e);\r\n            }\r\n        });\r\n    }\r\n    return threadPoolExecutor;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "makePathFromFileId",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path makePathFromFileId(long fileId)\n{\r\n    StringBuilder sb = new StringBuilder(INODE_PATH_MAX_LENGTH);\r\n    sb.append(Path.SEPARATOR).append(HdfsConstants.DOT_RESERVED_STRING).append(Path.SEPARATOR).append(HdfsConstants.DOT_INODES_STRING).append(Path.SEPARATOR).append(fileId);\r\n    return new Path(sb.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getHomeDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getHomeDirectory(Configuration conf, UserGroupInformation ugi)\n{\r\n    String userHomePrefix = HdfsClientConfigKeys.DFS_USER_HOME_DIR_PREFIX_DEFAULT;\r\n    if (conf != null) {\r\n        userHomePrefix = conf.get(HdfsClientConfigKeys.DFS_USER_HOME_DIR_PREFIX_KEY, HdfsClientConfigKeys.DFS_USER_HOME_DIR_PREFIX_DEFAULT);\r\n    }\r\n    return userHomePrefix + Path.SEPARATOR + ugi.getShortUserName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getTrashRoot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTrashRoot(Configuration conf, UserGroupInformation ugi)\n{\r\n    return getHomeDirectory(conf, ugi) + Path.SEPARATOR + FileSystem.TRASH_PREFIX;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getEZTrashRoot",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getEZTrashRoot(EncryptionZone ez, UserGroupInformation ugi)\n{\r\n    String ezpath = ez.getPath();\r\n    return (ezpath.equals(\"/\") ? ezpath : ezpath + Path.SEPARATOR) + FileSystem.TRASH_PREFIX + Path.SEPARATOR + ugi.getShortUserName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSnapshotTrashRoot",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getSnapshotTrashRoot(String ssRoot, UserGroupInformation ugi)\n{\r\n    return (ssRoot.equals(\"/\") ? ssRoot : ssRoot + Path.SEPARATOR) + FileSystem.TRASH_PREFIX + Path.SEPARATOR + ugi.getShortUserName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isValidSnapshotName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isValidSnapshotName(String snapshotName)\n{\r\n    return (snapshotName != null && !snapshotName.isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSnapshotDiffReport",
  "errType" : [ "RpcNoSuchMethodException|UnsupportedOperationException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "SnapshotDiffReport getSnapshotDiffReport(String snapshotDir, String fromSnapshot, String toSnapshot, SnapshotDiffReportFunction withoutListing, SnapshotDiffReportListingFunction withListing) throws IOException\n{\r\n    if (!isValidSnapshotName(fromSnapshot) || !isValidSnapshotName(toSnapshot)) {\r\n        return withoutListing.apply(snapshotDir, fromSnapshot, toSnapshot);\r\n    }\r\n    byte[] startPath = EMPTY_BYTES;\r\n    int index = -1;\r\n    SnapshotDiffReportGenerator snapshotDiffReport;\r\n    List<DiffReportListingEntry> modifiedList = new TreeList();\r\n    List<DiffReportListingEntry> createdList = new ChunkedArrayList<>();\r\n    List<DiffReportListingEntry> deletedList = new ChunkedArrayList<>();\r\n    SnapshotDiffReportListing report;\r\n    do {\r\n        try {\r\n            report = withListing.apply(snapshotDir, fromSnapshot, toSnapshot, startPath, index);\r\n        } catch (RpcNoSuchMethodException | UnsupportedOperationException e) {\r\n            LOG.warn(\"Falling back to getSnapshotDiffReport {}\", e.getMessage());\r\n            return withoutListing.apply(snapshotDir, fromSnapshot, toSnapshot);\r\n        }\r\n        startPath = report.getLastPath();\r\n        index = report.getLastIndex();\r\n        modifiedList.addAll(report.getModifyList());\r\n        createdList.addAll(report.getCreateList());\r\n        deletedList.addAll(report.getDeleteList());\r\n    } while (!(Arrays.equals(startPath, EMPTY_BYTES) && index == -1));\r\n    snapshotDiffReport = new SnapshotDiffReportGenerator(snapshotDir, fromSnapshot, toSnapshot, report.getIsFromEarlier(), modifiedList, createdList, deletedList);\r\n    return snapshotDiffReport.generateReport();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getFixedByteString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ByteString getFixedByteString(String key)\n{\r\n    return ProtobufHelper.getFixedByteString(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getByteString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ByteString getByteString(byte[] bytes)\n{\r\n    return ProtobufHelper.getByteString(bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ShmId convert(ShortCircuitShmIdProto shmId)\n{\r\n    return new ShmId(shmId.getHi(), shmId.getLo());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DataChecksum.Type convert(HdfsProtos.ChecksumTypeProto type)\n{\r\n    return DataChecksum.Type.valueOf(type.getNumber());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsProtos.ChecksumTypeProto convert(DataChecksum.Type type)\n{\r\n    return HdfsProtos.ChecksumTypeProto.forNumber(type.id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HdfsProtos.BlockChecksumTypeProto convert(BlockChecksumType type)\n{\r\n    switch(type) {\r\n        case MD5CRC:\r\n            return HdfsProtos.BlockChecksumTypeProto.MD5CRC;\r\n        case COMPOSITE_CRC:\r\n            return HdfsProtos.BlockChecksumTypeProto.COMPOSITE_CRC;\r\n        default:\r\n            throw new IllegalStateException(\"BUG: BlockChecksumType not found, type=\" + type);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockChecksumType convert(HdfsProtos.BlockChecksumTypeProto blockChecksumTypeProto)\n{\r\n    switch(blockChecksumTypeProto) {\r\n        case MD5CRC:\r\n            return BlockChecksumType.MD5CRC;\r\n        case COMPOSITE_CRC:\r\n            return BlockChecksumType.COMPOSITE_CRC;\r\n        default:\r\n            throw new IllegalStateException(\"BUG: BlockChecksumTypeProto not found, type=\" + blockChecksumTypeProto);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsProtos.BlockChecksumOptionsProto convert(BlockChecksumOptions options)\n{\r\n    return HdfsProtos.BlockChecksumOptionsProto.newBuilder().setBlockChecksumType(convert(options.getBlockChecksumType())).setStripeLength(options.getStripeLength()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BlockChecksumOptions convert(HdfsProtos.BlockChecksumOptionsProto options)\n{\r\n    return new BlockChecksumOptions(convert(options.getBlockChecksumType()), options.getStripeLength());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExtendedBlockProto convert(final ExtendedBlock b)\n{\r\n    if (b == null)\r\n        return null;\r\n    return ExtendedBlockProto.newBuilder().setPoolIdBytes(getFixedByteString(b.getBlockPoolId())).setBlockId(b.getBlockId()).setNumBytes(b.getNumBytes()).setGenerationStamp(b.getGenerationStamp()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TokenProto convert(Token<?> tok)\n{\r\n    return ProtobufHelper.protoFromToken(tok);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ShortCircuitShmIdProto convert(ShmId shmId)\n{\r\n    return ShortCircuitShmIdProto.newBuilder().setHi(shmId.getHi()).setLo(shmId.getLo()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ShortCircuitShmSlotProto convert(SlotId slotId)\n{\r\n    return ShortCircuitShmSlotProto.newBuilder().setShmId(convert(slotId.getShmId())).setSlotIdx(slotId.getSlotIdx()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DatanodeIDProto convert(DatanodeID dn)\n{\r\n    return DatanodeIDProto.newBuilder().setIpAddrBytes(dn.getIpAddrBytes()).setHostNameBytes(dn.getHostNameBytes()).setXferPort(dn.getXferPort()).setDatanodeUuidBytes(dn.getDatanodeUuidBytes()).setInfoPort(dn.getInfoPort()).setInfoSecurePort(dn.getInfoSecurePort()).setIpcPort(dn.getIpcPort()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DatanodeInfoProto.AdminState convert(final DatanodeInfo.AdminStates inAs)\n{\r\n    switch(inAs) {\r\n        case NORMAL:\r\n            return DatanodeInfoProto.AdminState.NORMAL;\r\n        case DECOMMISSION_INPROGRESS:\r\n            return DatanodeInfoProto.AdminState.DECOMMISSION_INPROGRESS;\r\n        case DECOMMISSIONED:\r\n            return DatanodeInfoProto.AdminState.DECOMMISSIONED;\r\n        case ENTERING_MAINTENANCE:\r\n            return DatanodeInfoProto.AdminState.ENTERING_MAINTENANCE;\r\n        case IN_MAINTENANCE:\r\n            return DatanodeInfoProto.AdminState.IN_MAINTENANCE;\r\n        default:\r\n            return DatanodeInfoProto.AdminState.NORMAL;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "DatanodeInfoProto convert(DatanodeInfo info)\n{\r\n    DatanodeInfoProto.Builder builder = DatanodeInfoProto.newBuilder();\r\n    if (info.getNetworkLocation() != null) {\r\n        builder.setLocationBytes(bytestringCache.getUnchecked(info.getNetworkLocation()));\r\n    }\r\n    if (info.getUpgradeDomain() != null) {\r\n        builder.setUpgradeDomain(info.getUpgradeDomain());\r\n    }\r\n    builder.setId(convert((DatanodeID) info)).setCapacity(info.getCapacity()).setDfsUsed(info.getDfsUsed()).setNonDfsUsed(info.getNonDfsUsed()).setRemaining(info.getRemaining()).setBlockPoolUsed(info.getBlockPoolUsed()).setCacheCapacity(info.getCacheCapacity()).setCacheUsed(info.getCacheUsed()).setLastUpdate(info.getLastUpdate()).setLastUpdateMonotonic(info.getLastUpdateMonotonic()).setXceiverCount(info.getXceiverCount()).setAdminState(convert(info.getAdminState())).setLastBlockReportTime(info.getLastBlockReportTime()).setLastBlockReportMonotonic(info.getLastBlockReportMonotonic()).setNumBlocks(info.getNumBlocks()).build();\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<? extends HdfsProtos.DatanodeInfoProto> convert(DatanodeInfo[] dnInfos)\n{\r\n    return convert(dnInfos, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<? extends HdfsProtos.DatanodeInfoProto> convert(DatanodeInfo[] dnInfos, int startIdx)\n{\r\n    if (dnInfos == null)\r\n        return null;\r\n    ArrayList<HdfsProtos.DatanodeInfoProto> protos = Lists.newArrayListWithCapacity(dnInfos.length);\r\n    for (int i = startIdx; i < dnInfos.length; i++) {\r\n        protos.add(convert(dnInfos[i]));\r\n    }\r\n    return protos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<Boolean> convert(boolean[] targetPinnings, int idx)\n{\r\n    List<Boolean> pinnings = new ArrayList<>();\r\n    if (targetPinnings == null) {\r\n        pinnings.add(Boolean.FALSE);\r\n    } else {\r\n        for (; idx < targetPinnings.length; ++idx) {\r\n            pinnings.add(targetPinnings[idx]);\r\n        }\r\n    }\r\n    return pinnings;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> convert(String[] targetIds, int idx)\n{\r\n    List<String> ids = new ArrayList<>();\r\n    if (targetIds != null) {\r\n        for (; idx < targetIds.length; ++idx) {\r\n            ids.add(targetIds[idx]);\r\n        }\r\n    }\r\n    return ids;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ExtendedBlock convert(ExtendedBlockProto eb)\n{\r\n    if (eb == null)\r\n        return null;\r\n    return new ExtendedBlock(eb.getPoolId(), eb.getBlockId(), eb.getNumBytes(), eb.getGenerationStamp());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DatanodeLocalInfo convert(DatanodeLocalInfoProto proto)\n{\r\n    return new DatanodeLocalInfo(proto.getSoftwareVersion(), proto.getConfigVersion(), proto.getUptime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertDatanodeInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di)\n{\r\n    if (di == null)\r\n        return null;\r\n    return convert(di);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertStorageType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StorageTypeProto convertStorageType(StorageType type)\n{\r\n    switch(type) {\r\n        case DISK:\r\n            return StorageTypeProto.DISK;\r\n        case SSD:\r\n            return StorageTypeProto.SSD;\r\n        case ARCHIVE:\r\n            return StorageTypeProto.ARCHIVE;\r\n        case RAM_DISK:\r\n            return StorageTypeProto.RAM_DISK;\r\n        case PROVIDED:\r\n            return StorageTypeProto.PROVIDED;\r\n        case NVDIMM:\r\n            return StorageTypeProto.NVDIMM;\r\n        default:\r\n            throw new IllegalStateException(\"BUG: StorageType not found, type=\" + type);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertStorageType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StorageType convertStorageType(StorageTypeProto type)\n{\r\n    switch(type) {\r\n        case DISK:\r\n            return StorageType.DISK;\r\n        case SSD:\r\n            return StorageType.SSD;\r\n        case ARCHIVE:\r\n            return StorageType.ARCHIVE;\r\n        case RAM_DISK:\r\n            return StorageType.RAM_DISK;\r\n        case PROVIDED:\r\n            return StorageType.PROVIDED;\r\n        case NVDIMM:\r\n            return StorageType.NVDIMM;\r\n        default:\r\n            throw new IllegalStateException(\"BUG: StorageTypeProto not found, type=\" + type);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertStorageTypes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<StorageTypeProto> convertStorageTypes(StorageType[] types)\n{\r\n    return convertStorageTypes(types, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertStorageTypes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<StorageTypeProto> convertStorageTypes(StorageType[] types, int startIdx)\n{\r\n    if (types == null) {\r\n        return null;\r\n    }\r\n    final List<StorageTypeProto> protos = new ArrayList<>(types.length);\r\n    for (int i = startIdx; i < types.length; ++i) {\r\n        protos.add(convertStorageType(types[i]));\r\n    }\r\n    return protos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "vintPrefixed",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "InputStream vintPrefixed(final InputStream input) throws IOException\n{\r\n    final int firstByte = input.read();\r\n    if (firstByte == -1) {\r\n        throw new EOFException(\"Unexpected EOF while trying to read response from server\");\r\n    }\r\n    int size = CodedInputStream.readRawVarint32(firstByte, input);\r\n    assert size >= 0;\r\n    return new LimitInputStream(input, size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "CipherOption convert(HdfsProtos.CipherOptionProto proto)\n{\r\n    if (proto != null) {\r\n        CipherSuite suite = null;\r\n        if (proto.getSuite() != null) {\r\n            suite = convert(proto.getSuite());\r\n        }\r\n        byte[] inKey = null;\r\n        if (proto.getInKey() != null) {\r\n            inKey = proto.getInKey().toByteArray();\r\n        }\r\n        byte[] inIv = null;\r\n        if (proto.getInIv() != null) {\r\n            inIv = proto.getInIv().toByteArray();\r\n        }\r\n        byte[] outKey = null;\r\n        if (proto.getOutKey() != null) {\r\n            outKey = proto.getOutKey().toByteArray();\r\n        }\r\n        byte[] outIv = null;\r\n        if (proto.getOutIv() != null) {\r\n            outIv = proto.getOutIv().toByteArray();\r\n        }\r\n        return new CipherOption(suite, inKey, inIv, outKey, outIv);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CipherSuite convert(HdfsProtos.CipherSuiteProto proto)\n{\r\n    switch(proto) {\r\n        case AES_CTR_NOPADDING:\r\n            return CipherSuite.AES_CTR_NOPADDING;\r\n        case SM4_CTR_NOPADDING:\r\n            return CipherSuite.SM4_CTR_NOPADDING;\r\n        default:\r\n            CipherSuite suite = CipherSuite.UNKNOWN;\r\n            suite.setUnknownValue(proto.getNumber());\r\n            return suite;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "HdfsProtos.CipherOptionProto convert(CipherOption option)\n{\r\n    if (option != null) {\r\n        HdfsProtos.CipherOptionProto.Builder builder = HdfsProtos.CipherOptionProto.newBuilder();\r\n        if (option.getCipherSuite() != null) {\r\n            builder.setSuite(convert(option.getCipherSuite()));\r\n        }\r\n        if (option.getInKey() != null) {\r\n            builder.setInKey(getByteString(option.getInKey()));\r\n        }\r\n        if (option.getInIv() != null) {\r\n            builder.setInIv(getByteString(option.getInIv()));\r\n        }\r\n        if (option.getOutKey() != null) {\r\n            builder.setOutKey(getByteString(option.getOutKey()));\r\n        }\r\n        if (option.getOutIv() != null) {\r\n            builder.setOutIv(getByteString(option.getOutIv()));\r\n        }\r\n        return builder.build();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HdfsProtos.CipherSuiteProto convert(CipherSuite suite)\n{\r\n    switch(suite) {\r\n        case UNKNOWN:\r\n            return HdfsProtos.CipherSuiteProto.UNKNOWN;\r\n        case AES_CTR_NOPADDING:\r\n            return HdfsProtos.CipherSuiteProto.AES_CTR_NOPADDING;\r\n        case SM4_CTR_NOPADDING:\r\n            return HdfsProtos.CipherSuiteProto.SM4_CTR_NOPADDING;\r\n        default:\r\n            return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertCipherOptions",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<HdfsProtos.CipherOptionProto> convertCipherOptions(List<CipherOption> options)\n{\r\n    if (options != null) {\r\n        List<HdfsProtos.CipherOptionProto> protos = Lists.newArrayListWithCapacity(options.size());\r\n        for (CipherOption option : options) {\r\n            protos.add(convert(option));\r\n        }\r\n        return protos;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertCipherOptionProtos",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<CipherOption> convertCipherOptionProtos(List<HdfsProtos.CipherOptionProto> protos)\n{\r\n    if (protos != null) {\r\n        List<CipherOption> options = Lists.newArrayListWithCapacity(protos.size());\r\n        for (HdfsProtos.CipherOptionProto proto : protos) {\r\n            options.add(convert(proto));\r\n        }\r\n        return options;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertLocatedBlockProto",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "LocatedBlock convertLocatedBlockProto(LocatedBlockProto proto)\n{\r\n    if (proto == null)\r\n        return null;\r\n    List<DatanodeInfoProto> locs = proto.getLocsList();\r\n    DatanodeInfo[] targets = new DatanodeInfo[locs.size()];\r\n    for (int i = 0; i < locs.size(); i++) {\r\n        targets[i] = convert(locs.get(i));\r\n    }\r\n    final StorageType[] storageTypes = convertStorageTypes(proto.getStorageTypesList(), locs.size());\r\n    final int storageIDsCount = proto.getStorageIDsCount();\r\n    final String[] storageIDs;\r\n    if (storageIDsCount == 0) {\r\n        storageIDs = null;\r\n    } else {\r\n        Preconditions.checkState(storageIDsCount == locs.size());\r\n        storageIDs = proto.getStorageIDsList().toArray(new String[storageIDsCount]);\r\n    }\r\n    byte[] indices = null;\r\n    if (proto.hasBlockIndices()) {\r\n        indices = proto.getBlockIndices().toByteArray();\r\n    }\r\n    List<DatanodeInfo> cachedLocs = new ArrayList<>(locs.size());\r\n    List<Boolean> isCachedList = proto.getIsCachedList();\r\n    for (int i = 0; i < isCachedList.size(); i++) {\r\n        if (isCachedList.get(i)) {\r\n            cachedLocs.add(targets[i]);\r\n        }\r\n    }\r\n    final LocatedBlock lb;\r\n    if (indices == null) {\r\n        lb = new LocatedBlock(PBHelperClient.convert(proto.getB()), targets, storageIDs, storageTypes, proto.getOffset(), proto.getCorrupt(), cachedLocs.toArray(new DatanodeInfo[cachedLocs.size()]));\r\n    } else {\r\n        lb = new LocatedStripedBlock(PBHelperClient.convert(proto.getB()), targets, storageIDs, storageTypes, indices, proto.getOffset(), proto.getCorrupt(), cachedLocs.toArray(new DatanodeInfo[cachedLocs.size()]));\r\n        List<TokenProto> tokenProtos = proto.getBlockTokensList();\r\n        Token<BlockTokenIdentifier>[] blockTokens = convertTokens(tokenProtos);\r\n        ((LocatedStripedBlock) lb).setBlockTokens(blockTokens);\r\n    }\r\n    lb.setBlockToken(convert(proto.getBlockToken()));\r\n    return lb;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertTokens",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Token<BlockTokenIdentifier>[] convertTokens(List<TokenProto> tokenProtos)\n{\r\n    @SuppressWarnings(\"unchecked\")\r\n    Token<BlockTokenIdentifier>[] blockTokens = new Token[tokenProtos.size()];\r\n    for (int i = 0; i < blockTokens.length; i++) {\r\n        blockTokens[i] = convert(tokenProtos.get(i));\r\n    }\r\n    return blockTokens;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AccessModeProto convert(BlockTokenIdentifier.AccessMode aMode)\n{\r\n    switch(aMode) {\r\n        case READ:\r\n            return AccessModeProto.READ;\r\n        case WRITE:\r\n            return AccessModeProto.WRITE;\r\n        case COPY:\r\n            return AccessModeProto.COPY;\r\n        case REPLACE:\r\n            return AccessModeProto.REPLACE;\r\n        default:\r\n            throw new IllegalArgumentException(\"Unexpected AccessMode: \" + aMode);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockTokenIdentifier.AccessMode convert(AccessModeProto accessModeProto)\n{\r\n    switch(accessModeProto) {\r\n        case READ:\r\n            return BlockTokenIdentifier.AccessMode.READ;\r\n        case WRITE:\r\n            return BlockTokenIdentifier.AccessMode.WRITE;\r\n        case COPY:\r\n            return BlockTokenIdentifier.AccessMode.COPY;\r\n        case REPLACE:\r\n            return BlockTokenIdentifier.AccessMode.REPLACE;\r\n        default:\r\n            throw new IllegalArgumentException(\"Unexpected AccessModeProto: \" + accessModeProto);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "BlockTokenSecretProto convert(BlockTokenIdentifier blockTokenSecret)\n{\r\n    BlockTokenSecretProto.Builder builder = BlockTokenSecretProto.newBuilder();\r\n    builder.setExpiryDate(blockTokenSecret.getExpiryDate());\r\n    builder.setKeyId(blockTokenSecret.getKeyId());\r\n    String userId = blockTokenSecret.getUserId();\r\n    if (userId != null) {\r\n        builder.setUserId(userId);\r\n    }\r\n    String blockPoolId = blockTokenSecret.getBlockPoolId();\r\n    if (blockPoolId != null) {\r\n        builder.setBlockPoolId(blockPoolId);\r\n    }\r\n    builder.setBlockId(blockTokenSecret.getBlockId());\r\n    for (BlockTokenIdentifier.AccessMode aMode : blockTokenSecret.getAccessModes()) {\r\n        builder.addModes(convert(aMode));\r\n    }\r\n    for (StorageType storageType : blockTokenSecret.getStorageTypes()) {\r\n        builder.addStorageTypes(convertStorageType(storageType));\r\n    }\r\n    for (String storageId : blockTokenSecret.getStorageIds()) {\r\n        builder.addStorageIds(storageId);\r\n    }\r\n    byte[] handshake = blockTokenSecret.getHandshakeMsg();\r\n    if (handshake != null && handshake.length > 0) {\r\n        builder.setHandshakeSecret(getByteString(handshake));\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "DatanodeInfo convert(DatanodeInfoProto di)\n{\r\n    if (di == null) {\r\n        return null;\r\n    }\r\n    DatanodeInfoBuilder dinfo = new DatanodeInfoBuilder().setNodeID(convert(di.getId())).setNetworkLocation(di.hasLocation() ? di.getLocation() : null).setCapacity(di.getCapacity()).setDfsUsed(di.getDfsUsed()).setRemaining(di.getRemaining()).setBlockPoolUsed(di.getBlockPoolUsed()).setCacheCapacity(di.getCacheCapacity()).setCacheUsed(di.getCacheUsed()).setLastUpdate(di.getLastUpdate()).setLastUpdateMonotonic(di.getLastUpdateMonotonic()).setXceiverCount(di.getXceiverCount()).setAdminState(convert(di.getAdminState())).setUpgradeDomain(di.hasUpgradeDomain() ? di.getUpgradeDomain() : null).setLastBlockReportTime(di.hasLastBlockReportTime() ? di.getLastBlockReportTime() : 0).setLastBlockReportMonotonic(di.hasLastBlockReportMonotonic() ? di.getLastBlockReportMonotonic() : 0).setNumBlocks(di.getNumBlocks());\r\n    if (di.hasNonDfsUsed()) {\r\n        dinfo.setNonDfsUsed(di.getNonDfsUsed());\r\n    } else {\r\n        long nonDFSUsed = di.getCapacity() - di.getDfsUsed() - di.getRemaining();\r\n        dinfo.setNonDfsUsed(nonDFSUsed < 0 ? 0 : nonDFSUsed);\r\n    }\r\n    return dinfo.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertStorageTypes",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "StorageType[] convertStorageTypes(List<StorageTypeProto> storageTypesList, int expectedSize)\n{\r\n    final StorageType[] storageTypes = new StorageType[expectedSize];\r\n    if (storageTypesList.size() != expectedSize) {\r\n        Preconditions.checkState(storageTypesList.isEmpty());\r\n        Arrays.fill(storageTypes, StorageType.DEFAULT);\r\n    } else {\r\n        for (int i = 0; i < storageTypes.length; ++i) {\r\n            storageTypes[i] = convertStorageType(storageTypesList.get(i));\r\n        }\r\n    }\r\n    return storageTypes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<BlockTokenIdentifier> convert(TokenProto blockToken)\n{\r\n    return (Token<BlockTokenIdentifier>) ProtobufHelper.tokenFromProto(blockToken);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "DatanodeID convert(DatanodeIDProto dn)\n{\r\n    return new DatanodeID(dn.getIpAddr(), dn.getHostName(), dn.getDatanodeUuid(), dn.getXferPort(), dn.getInfoPort(), dn.hasInfoSecurePort() ? dn.getInfoSecurePort() : 0, dn.getIpcPort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AdminStates convert(AdminState adminState)\n{\r\n    switch(adminState) {\r\n        case DECOMMISSION_INPROGRESS:\r\n            return AdminStates.DECOMMISSION_INPROGRESS;\r\n        case DECOMMISSIONED:\r\n            return AdminStates.DECOMMISSIONED;\r\n        case ENTERING_MAINTENANCE:\r\n            return AdminStates.ENTERING_MAINTENANCE;\r\n        case IN_MAINTENANCE:\r\n            return AdminStates.IN_MAINTENANCE;\r\n        case NORMAL:\r\n        default:\r\n            return AdminStates.NORMAL;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "LocatedBlocks convert(LocatedBlocksProto lb)\n{\r\n    return new LocatedBlocks(lb.getFileLength(), lb.getUnderConstruction(), convertLocatedBlocks(lb.getBlocksList()), lb.hasLastBlock() ? convertLocatedBlockProto(lb.getLastBlock()) : null, lb.getIsLastBlockComplete(), lb.hasFileEncryptionInfo() ? convert(lb.getFileEncryptionInfo()) : null, lb.hasEcPolicy() ? convertErasureCodingPolicy(lb.getEcPolicy()) : null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertStoragePolicies",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "BlockStoragePolicy[] convertStoragePolicies(List<BlockStoragePolicyProto> policyProtos)\n{\r\n    if (policyProtos == null || policyProtos.size() == 0) {\r\n        return new BlockStoragePolicy[0];\r\n    }\r\n    BlockStoragePolicy[] policies = new BlockStoragePolicy[policyProtos.size()];\r\n    int i = 0;\r\n    for (BlockStoragePolicyProto proto : policyProtos) {\r\n        policies[i++] = convert(proto);\r\n    }\r\n    return policies;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 32,
  "sourceCodeText" : "EventBatchList convert(GetEditsFromTxidResponseProto resp) throws IOException\n{\r\n    final InotifyProtos.EventsListProto list = resp.getEventsList();\r\n    final long firstTxid = list.getFirstTxid();\r\n    final long lastTxid = list.getLastTxid();\r\n    List<EventBatch> batches = Lists.newArrayList();\r\n    if (list.getEventsList().size() > 0) {\r\n        throw new IOException(\"Can't handle old inotify server response.\");\r\n    }\r\n    for (InotifyProtos.EventBatchProto bp : list.getBatchList()) {\r\n        long txid = bp.getTxid();\r\n        if ((txid != -1) && ((txid < firstTxid) || (txid > lastTxid))) {\r\n            throw new IOException(\"Error converting TxidResponseProto: got a \" + \"transaction id \" + txid + \" that was outside the range of [\" + firstTxid + \", \" + lastTxid + \"].\");\r\n        }\r\n        List<Event> events = Lists.newArrayList();\r\n        for (InotifyProtos.EventProto p : bp.getEventsList()) {\r\n            switch(p.getType()) {\r\n                case EVENT_CLOSE:\r\n                    InotifyProtos.CloseEventProto close = InotifyProtos.CloseEventProto.parseFrom(p.getContents());\r\n                    events.add(new Event.CloseEvent(close.getPath(), close.getFileSize(), close.getTimestamp()));\r\n                    break;\r\n                case EVENT_CREATE:\r\n                    InotifyProtos.CreateEventProto create = InotifyProtos.CreateEventProto.parseFrom(p.getContents());\r\n                    Event.CreateEvent.Builder builder = new Event.CreateEvent.Builder().iNodeType(createTypeConvert(create.getType())).path(create.getPath()).ctime(create.getCtime()).ownerName(create.getOwnerName()).groupName(create.getGroupName()).perms(convert(create.getPerms())).replication(create.getReplication()).symlinkTarget(create.getSymlinkTarget().isEmpty() ? null : create.getSymlinkTarget()).defaultBlockSize(create.getDefaultBlockSize()).overwrite(create.getOverwrite());\r\n                    if (create.hasErasureCoded()) {\r\n                        builder.erasureCoded(create.getErasureCoded());\r\n                    }\r\n                    events.add(builder.build());\r\n                    break;\r\n                case EVENT_METADATA:\r\n                    InotifyProtos.MetadataUpdateEventProto meta = InotifyProtos.MetadataUpdateEventProto.parseFrom(p.getContents());\r\n                    events.add(new Event.MetadataUpdateEvent.Builder().path(meta.getPath()).metadataType(metadataUpdateTypeConvert(meta.getType())).mtime(meta.getMtime()).atime(meta.getAtime()).replication(meta.getReplication()).ownerName(meta.getOwnerName().isEmpty() ? null : meta.getOwnerName()).groupName(meta.getGroupName().isEmpty() ? null : meta.getGroupName()).perms(meta.hasPerms() ? convert(meta.getPerms()) : null).acls(meta.getAclsList().isEmpty() ? null : convertAclEntry(meta.getAclsList())).xAttrs(meta.getXAttrsList().isEmpty() ? null : convertXAttrs(meta.getXAttrsList())).xAttrsRemoved(meta.getXAttrsRemoved()).build());\r\n                    break;\r\n                case EVENT_RENAME:\r\n                    InotifyProtos.RenameEventProto rename = InotifyProtos.RenameEventProto.parseFrom(p.getContents());\r\n                    events.add(new Event.RenameEvent.Builder().srcPath(rename.getSrcPath()).dstPath(rename.getDestPath()).timestamp(rename.getTimestamp()).build());\r\n                    break;\r\n                case EVENT_APPEND:\r\n                    InotifyProtos.AppendEventProto append = InotifyProtos.AppendEventProto.parseFrom(p.getContents());\r\n                    events.add(new Event.AppendEvent.Builder().path(append.getPath()).newBlock(append.hasNewBlock() && append.getNewBlock()).build());\r\n                    break;\r\n                case EVENT_UNLINK:\r\n                    InotifyProtos.UnlinkEventProto unlink = InotifyProtos.UnlinkEventProto.parseFrom(p.getContents());\r\n                    events.add(new Event.UnlinkEvent.Builder().path(unlink.getPath()).timestamp(unlink.getTimestamp()).build());\r\n                    break;\r\n                case EVENT_TRUNCATE:\r\n                    InotifyProtos.TruncateEventProto truncate = InotifyProtos.TruncateEventProto.parseFrom(p.getContents());\r\n                    events.add(new Event.TruncateEvent(truncate.getPath(), truncate.getFileSize(), truncate.getTimestamp()));\r\n                    break;\r\n                default:\r\n                    throw new RuntimeException(\"Unexpected inotify event type: \" + p.getType());\r\n            }\r\n        }\r\n        batches.add(new EventBatch(txid, events.toArray(new Event[0])));\r\n    }\r\n    return new EventBatchList(batches, resp.getEventsList().getFirstTxid(), resp.getEventsList().getLastTxid(), resp.getEventsList().getSyncTxid());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertLocatedBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocatedBlockProto[] convertLocatedBlocks(LocatedBlock[] lb)\n{\r\n    if (lb == null)\r\n        return null;\r\n    return convertLocatedBlocks2(Arrays.asList(lb)).toArray(new LocatedBlockProto[lb.length]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertLocatedBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocatedBlock[] convertLocatedBlocks(LocatedBlockProto[] lb)\n{\r\n    if (lb == null)\r\n        return null;\r\n    return convertLocatedBlocks(Arrays.asList(lb)).toArray(new LocatedBlock[lb.length]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertLocatedBlocks",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<LocatedBlock> convertLocatedBlocks(List<LocatedBlockProto> lb)\n{\r\n    if (lb == null)\r\n        return null;\r\n    final int len = lb.size();\r\n    List<LocatedBlock> result = new ArrayList<>(len);\r\n    for (LocatedBlockProto aLb : lb) {\r\n        result.add(convertLocatedBlockProto(aLb));\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertLocatedBlocks2",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<LocatedBlockProto> convertLocatedBlocks2(List<LocatedBlock> lb)\n{\r\n    if (lb == null)\r\n        return null;\r\n    final int len = lb.size();\r\n    List<LocatedBlockProto> result = new ArrayList<>(len);\r\n    for (LocatedBlock aLb : lb) {\r\n        result.add(convertLocatedBlock(aLb));\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertLocatedBlock",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "LocatedBlockProto convertLocatedBlock(LocatedBlock b)\n{\r\n    if (b == null)\r\n        return null;\r\n    Builder builder = LocatedBlockProto.newBuilder();\r\n    DatanodeInfo[] locs = b.getLocations();\r\n    List<DatanodeInfo> cachedLocs = Lists.newLinkedList(Arrays.asList(b.getCachedLocations()));\r\n    for (int i = 0; i < locs.length; i++) {\r\n        DatanodeInfo loc = locs[i];\r\n        builder.addLocs(i, PBHelperClient.convert(loc));\r\n        boolean locIsCached = cachedLocs.contains(loc);\r\n        builder.addIsCached(locIsCached);\r\n        if (locIsCached) {\r\n            cachedLocs.remove(loc);\r\n        }\r\n    }\r\n    Preconditions.checkArgument(cachedLocs.size() == 0, \"Found additional cached replica locations that are not in the set of\" + \" storage-backed locations!\");\r\n    StorageType[] storageTypes = b.getStorageTypes();\r\n    if (storageTypes != null) {\r\n        for (StorageType storageType : storageTypes) {\r\n            builder.addStorageTypes(convertStorageType(storageType));\r\n        }\r\n    }\r\n    final String[] storageIDs = b.getStorageIDs();\r\n    if (storageIDs != null) {\r\n        builder.addAllStorageIDs(Arrays.asList(storageIDs));\r\n    }\r\n    if (b instanceof LocatedStripedBlock) {\r\n        LocatedStripedBlock sb = (LocatedStripedBlock) b;\r\n        byte[] indices = sb.getBlockIndices();\r\n        builder.setBlockIndices(PBHelperClient.getByteString(indices));\r\n        Token<BlockTokenIdentifier>[] blockTokens = sb.getBlockTokens();\r\n        builder.addAllBlockTokens(convert(blockTokens));\r\n    }\r\n    return builder.setB(PBHelperClient.convert(b.getBlock())).setBlockToken(PBHelperClient.convert(b.getBlockToken())).setCorrupt(b.isCorrupt()).setOffset(b.getStartOffset()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<TokenProto> convert(Token<BlockTokenIdentifier>[] blockTokens)\n{\r\n    List<TokenProto> results = new ArrayList<>(blockTokens.length);\r\n    for (Token<BlockTokenIdentifier> bt : blockTokens) {\r\n        results.add(convert(bt));\r\n    }\r\n    return results;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertBlockIndices",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<Integer> convertBlockIndices(byte[] blockIndices)\n{\r\n    List<Integer> results = new ArrayList<>(blockIndices.length);\r\n    for (byte bt : blockIndices) {\r\n        results.add(Integer.valueOf(bt));\r\n    }\r\n    return results;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertBlockIndices",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "byte[] convertBlockIndices(List<Integer> blockIndices)\n{\r\n    byte[] blkIndices = new byte[blockIndices.size()];\r\n    for (int i = 0; i < blockIndices.size(); i++) {\r\n        blkIndices[i] = (byte) blockIndices.get(i).intValue();\r\n    }\r\n    return blkIndices;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "BlockStoragePolicy convert(BlockStoragePolicyProto proto)\n{\r\n    List<StorageTypeProto> cList = proto.getCreationPolicy().getStorageTypesList();\r\n    StorageType[] creationTypes = convertStorageTypes(cList, cList.size());\r\n    List<StorageTypeProto> cfList = proto.hasCreationFallbackPolicy() ? proto.getCreationFallbackPolicy().getStorageTypesList() : null;\r\n    StorageType[] creationFallbackTypes = cfList == null ? StorageType.EMPTY_ARRAY : convertStorageTypes(cfList, cfList.size());\r\n    List<StorageTypeProto> rfList = proto.hasReplicationFallbackPolicy() ? proto.getReplicationFallbackPolicy().getStorageTypesList() : null;\r\n    StorageType[] replicationFallbackTypes = rfList == null ? StorageType.EMPTY_ARRAY : convertStorageTypes(rfList, rfList.size());\r\n    return new BlockStoragePolicy((byte) proto.getPolicyId(), proto.getName(), creationTypes, creationFallbackTypes, replicationFallbackTypes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FsActionProto convert(FsAction v)\n{\r\n    return FsActionProto.forNumber(v != null ? v.ordinal() : 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertXAttrProto",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "XAttrProto convertXAttrProto(XAttr a)\n{\r\n    XAttrProto.Builder builder = XAttrProto.newBuilder();\r\n    builder.setNamespace(convert(a.getNameSpace()));\r\n    if (a.getName() != null) {\r\n        builder.setName(a.getName());\r\n    }\r\n    if (a.getValue() != null) {\r\n        builder.setValue(getByteString(a.getValue()));\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<XAttr> convert(ListXAttrsResponseProto a)\n{\r\n    final List<XAttrProto> xAttrs = a.getXAttrsList();\r\n    return convertXAttrs(xAttrs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<XAttr> convert(GetXAttrsResponseProto a)\n{\r\n    List<XAttrProto> xAttrs = a.getXAttrsList();\r\n    return convertXAttrs(xAttrs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertXAttrs",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<XAttr> convertXAttrs(List<XAttrProto> xAttrSpec)\n{\r\n    ArrayList<XAttr> xAttrs = Lists.newArrayListWithCapacity(xAttrSpec.size());\r\n    for (XAttrProto a : xAttrSpec) {\r\n        XAttr.Builder builder = new XAttr.Builder();\r\n        builder.setNameSpace(convert(a.getNamespace()));\r\n        if (a.hasName()) {\r\n            builder.setName(a.getName());\r\n        }\r\n        if (a.hasValue()) {\r\n            builder.setValue(a.getValue().toByteArray());\r\n        }\r\n        xAttrs.add(builder.build());\r\n    }\r\n    return xAttrs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "XAttrNamespaceProto convert(XAttr.NameSpace v)\n{\r\n    return XAttrNamespaceProto.forNumber(v.ordinal());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "XAttr.NameSpace convert(XAttrNamespaceProto v)\n{\r\n    return castEnum(v, XATTR_NAMESPACE_VALUES);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "castEnum",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "U castEnum(T from, U[] to)\n{\r\n    return to[from.ordinal()];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "metadataUpdateTypeConvert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InotifyProtos.MetadataUpdateType metadataUpdateTypeConvert(Event.MetadataUpdateEvent.MetadataType type)\n{\r\n    switch(type) {\r\n        case TIMES:\r\n            return InotifyProtos.MetadataUpdateType.META_TYPE_TIMES;\r\n        case REPLICATION:\r\n            return InotifyProtos.MetadataUpdateType.META_TYPE_REPLICATION;\r\n        case OWNER:\r\n            return InotifyProtos.MetadataUpdateType.META_TYPE_OWNER;\r\n        case PERMS:\r\n            return InotifyProtos.MetadataUpdateType.META_TYPE_PERMS;\r\n        case ACLS:\r\n            return InotifyProtos.MetadataUpdateType.META_TYPE_ACLS;\r\n        case XATTRS:\r\n            return InotifyProtos.MetadataUpdateType.META_TYPE_XATTRS;\r\n        default:\r\n            return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "metadataUpdateTypeConvert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Event.MetadataUpdateEvent.MetadataType metadataUpdateTypeConvert(InotifyProtos.MetadataUpdateType type)\n{\r\n    switch(type) {\r\n        case META_TYPE_TIMES:\r\n            return Event.MetadataUpdateEvent.MetadataType.TIMES;\r\n        case META_TYPE_REPLICATION:\r\n            return Event.MetadataUpdateEvent.MetadataType.REPLICATION;\r\n        case META_TYPE_OWNER:\r\n            return Event.MetadataUpdateEvent.MetadataType.OWNER;\r\n        case META_TYPE_PERMS:\r\n            return Event.MetadataUpdateEvent.MetadataType.PERMS;\r\n        case META_TYPE_ACLS:\r\n            return Event.MetadataUpdateEvent.MetadataType.ACLS;\r\n        case META_TYPE_XATTRS:\r\n            return Event.MetadataUpdateEvent.MetadataType.XATTRS;\r\n        default:\r\n            return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "createTypeConvert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InotifyProtos.INodeType createTypeConvert(Event.CreateEvent.INodeType type)\n{\r\n    switch(type) {\r\n        case DIRECTORY:\r\n            return InotifyProtos.INodeType.I_TYPE_DIRECTORY;\r\n        case FILE:\r\n            return InotifyProtos.INodeType.I_TYPE_FILE;\r\n        case SYMLINK:\r\n            return InotifyProtos.INodeType.I_TYPE_SYMLINK;\r\n        default:\r\n            return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertLocatedBlock",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<LocatedBlock> convertLocatedBlock(List<LocatedBlockProto> lb)\n{\r\n    if (lb == null)\r\n        return null;\r\n    final int len = lb.size();\r\n    List<LocatedBlock> result = new ArrayList<>(len);\r\n    for (LocatedBlockProto aLb : lb) {\r\n        result.add(convertLocatedBlockProto(aLb));\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertAclEntry",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<AclEntry> convertAclEntry(List<AclEntryProto> aclSpec)\n{\r\n    ArrayList<AclEntry> r = Lists.newArrayListWithCapacity(aclSpec.size());\r\n    for (AclEntryProto e : aclSpec) {\r\n        AclEntry.Builder builder = new AclEntry.Builder();\r\n        builder.setType(convert(e.getType()));\r\n        builder.setScope(convert(e.getScope()));\r\n        builder.setPermission(convert(e.getPermissions()));\r\n        if (e.hasName()) {\r\n            builder.setName(e.getName());\r\n        }\r\n        r.add(builder.build());\r\n    }\r\n    return r;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AclEntryScopeProto convert(AclEntryScope v)\n{\r\n    return AclEntryScopeProto.forNumber(v.ordinal());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AclEntryScope convert(AclEntryScopeProto v)\n{\r\n    return castEnum(v, ACL_ENTRY_SCOPE_VALUES);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AclEntryTypeProto convert(AclEntryType e)\n{\r\n    return AclEntryTypeProto.forNumber(e.ordinal());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AclEntryType convert(AclEntryTypeProto v)\n{\r\n    return castEnum(v, ACL_ENTRY_TYPE_VALUES);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FsAction convert(FsActionProto v)\n{\r\n    return castEnum(v, FSACTION_VALUES);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FsPermission convert(FsPermissionProto p)\n{\r\n    return new FsPermission((short) p.getPerm());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "createTypeConvert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Event.CreateEvent.INodeType createTypeConvert(InotifyProtos.INodeType type)\n{\r\n    switch(type) {\r\n        case I_TYPE_DIRECTORY:\r\n            return Event.CreateEvent.INodeType.DIRECTORY;\r\n        case I_TYPE_FILE:\r\n            return Event.CreateEvent.INodeType.FILE;\r\n        case I_TYPE_SYMLINK:\r\n            return Event.CreateEvent.INodeType.SYMLINK;\r\n        default:\r\n            return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsProtos.FileEncryptionInfoProto convert(FileEncryptionInfo info)\n{\r\n    if (info == null) {\r\n        return null;\r\n    }\r\n    return HdfsProtos.FileEncryptionInfoProto.newBuilder().setSuite(convert(info.getCipherSuite())).setCryptoProtocolVersion(convert(info.getCryptoProtocolVersion())).setKey(getByteString(info.getEncryptedDataEncryptionKey())).setIv(getByteString(info.getIV())).setEzKeyVersionName(info.getEzKeyVersionName()).setKeyName(info.getKeyName()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CryptoProtocolVersionProto convert(CryptoProtocolVersion version)\n{\r\n    switch(version) {\r\n        case UNKNOWN:\r\n            return CryptoProtocolVersionProto.UNKNOWN_PROTOCOL_VERSION;\r\n        case ENCRYPTION_ZONES:\r\n            return CryptoProtocolVersionProto.ENCRYPTION_ZONES;\r\n        default:\r\n            return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "FileEncryptionInfo convert(HdfsProtos.FileEncryptionInfoProto proto)\n{\r\n    if (proto == null) {\r\n        return null;\r\n    }\r\n    CipherSuite suite = convert(proto.getSuite());\r\n    CryptoProtocolVersion version = convert(proto.getCryptoProtocolVersion());\r\n    byte[] key = proto.getKey().toByteArray();\r\n    byte[] iv = proto.getIv().toByteArray();\r\n    String ezKeyVersionName = proto.getEzKeyVersionName();\r\n    String keyName = proto.getKeyName();\r\n    return new FileEncryptionInfo(suite, version, key, iv, keyName, ezKeyVersionName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CryptoProtocolVersion convert(CryptoProtocolVersionProto proto)\n{\r\n    switch(proto) {\r\n        case ENCRYPTION_ZONES:\r\n            return CryptoProtocolVersion.ENCRYPTION_ZONES;\r\n        default:\r\n            CryptoProtocolVersion version = CryptoProtocolVersion.UNKNOWN;\r\n            version.setUnknownValue(proto.getNumber());\r\n            return version;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertXAttrProto",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "List<XAttrProto> convertXAttrProto(List<XAttr> xAttrSpec)\n{\r\n    if (xAttrSpec == null) {\r\n        return Lists.newArrayListWithCapacity(0);\r\n    }\r\n    ArrayList<XAttrProto> xAttrs = Lists.newArrayListWithCapacity(xAttrSpec.size());\r\n    for (XAttr a : xAttrSpec) {\r\n        XAttrProto.Builder builder = XAttrProto.newBuilder();\r\n        builder.setNamespace(convert(a.getNameSpace()));\r\n        if (a.getName() != null) {\r\n            builder.setName(a.getName());\r\n        }\r\n        if (a.getValue() != null) {\r\n            builder.setValue(getByteString(a.getValue()));\r\n        }\r\n        xAttrs.add(builder.build());\r\n    }\r\n    return xAttrs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int convert(EnumSet<XAttrSetFlag> flag)\n{\r\n    int value = 0;\r\n    if (flag.contains(XAttrSetFlag.CREATE)) {\r\n        value |= XAttrSetFlagProto.XATTR_CREATE.getNumber();\r\n    }\r\n    if (flag.contains(XAttrSetFlag.REPLACE)) {\r\n        value |= XAttrSetFlagProto.XATTR_REPLACE.getNumber();\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "EncryptionZone convert(EncryptionZoneProto proto)\n{\r\n    return new EncryptionZone(proto.getId(), proto.getPath(), convert(proto.getSuite()), convert(proto.getCryptoProtocolVersion()), proto.getKeyName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OpenFilesBatchResponseProto convert(OpenFileEntry openFileEntry)\n{\r\n    return OpenFilesBatchResponseProto.newBuilder().setId(openFileEntry.getId()).setPath(openFileEntry.getFilePath()).setClientName(openFileEntry.getClientName()).setClientMachine(openFileEntry.getClientMachine()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "OpenFileEntry convert(OpenFilesBatchResponseProto proto)\n{\r\n    return new OpenFileEntry(proto.getId(), proto.getPath(), proto.getClientName(), proto.getClientMachine());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AclStatus convert(GetAclStatusResponseProto e)\n{\r\n    AclStatusProto r = e.getResult();\r\n    AclStatus.Builder builder = new AclStatus.Builder();\r\n    builder.owner(r.getOwner()).group(r.getGroup()).stickyBit(r.getSticky()).addEntries(convertAclEntry(r.getEntriesList()));\r\n    if (r.hasPermission()) {\r\n        builder.setPermission(convert(r.getPermission()));\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertAclEntryProto",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "List<AclEntryProto> convertAclEntryProto(List<AclEntry> aclSpec)\n{\r\n    ArrayList<AclEntryProto> r = Lists.newArrayListWithCapacity(aclSpec.size());\r\n    for (AclEntry e : aclSpec) {\r\n        AclEntryProto.Builder builder = AclEntryProto.newBuilder();\r\n        builder.setType(convert(e.getType()));\r\n        builder.setScope(convert(e.getScope()));\r\n        builder.setPermissions(convert(e.getPermission()));\r\n        if (e.getName() != null) {\r\n            builder.setName(e.getName());\r\n        }\r\n        r.add(builder.build());\r\n    }\r\n    return r;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "CachePoolEntry convert(CachePoolEntryProto proto)\n{\r\n    CachePoolInfo info = convert(proto.getInfo());\r\n    CachePoolStats stats = convert(proto.getStats());\r\n    return new CachePoolEntry(info, stats);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "CachePoolInfo convert(CachePoolInfoProto proto)\n{\r\n    String poolName = Preconditions.checkNotNull(proto.getPoolName());\r\n    CachePoolInfo info = new CachePoolInfo(poolName);\r\n    if (proto.hasOwnerName()) {\r\n        info.setOwnerName(proto.getOwnerName());\r\n    }\r\n    if (proto.hasGroupName()) {\r\n        info.setGroupName(proto.getGroupName());\r\n    }\r\n    if (proto.hasMode()) {\r\n        info.setMode(new FsPermission((short) proto.getMode()));\r\n    }\r\n    if (proto.hasLimit()) {\r\n        info.setLimit(proto.getLimit());\r\n    }\r\n    if (proto.hasDefaultReplication()) {\r\n        info.setDefaultReplication(Shorts.checkedCast(proto.getDefaultReplication()));\r\n    }\r\n    if (proto.hasMaxRelativeExpiry()) {\r\n        info.setMaxRelativeExpiryMs(proto.getMaxRelativeExpiry());\r\n    }\r\n    return info;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "CachePoolStats convert(CachePoolStatsProto proto)\n{\r\n    CachePoolStats.Builder builder = new CachePoolStats.Builder();\r\n    builder.setBytesNeeded(proto.getBytesNeeded());\r\n    builder.setBytesCached(proto.getBytesCached());\r\n    builder.setBytesOverlimit(proto.getBytesOverlimit());\r\n    builder.setFilesNeeded(proto.getFilesNeeded());\r\n    builder.setFilesCached(proto.getFilesCached());\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "CachePoolInfoProto convert(CachePoolInfo info)\n{\r\n    CachePoolInfoProto.Builder builder = CachePoolInfoProto.newBuilder();\r\n    builder.setPoolName(info.getPoolName());\r\n    if (info.getOwnerName() != null) {\r\n        builder.setOwnerName(info.getOwnerName());\r\n    }\r\n    if (info.getGroupName() != null) {\r\n        builder.setGroupName(info.getGroupName());\r\n    }\r\n    if (info.getMode() != null) {\r\n        builder.setMode(info.getMode().toShort());\r\n    }\r\n    if (info.getLimit() != null) {\r\n        builder.setLimit(info.getLimit());\r\n    }\r\n    if (info.getDefaultReplication() != null) {\r\n        builder.setDefaultReplication(info.getDefaultReplication());\r\n    }\r\n    if (info.getMaxRelativeExpiryMs() != null) {\r\n        builder.setMaxRelativeExpiry(info.getMaxRelativeExpiryMs());\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "CacheDirectiveInfoProto convert(CacheDirectiveInfo info)\n{\r\n    CacheDirectiveInfoProto.Builder builder = CacheDirectiveInfoProto.newBuilder();\r\n    if (info.getId() != null) {\r\n        builder.setId(info.getId());\r\n    }\r\n    if (info.getPath() != null) {\r\n        builder.setPath(info.getPath().toUri().getPath());\r\n    }\r\n    if (info.getReplication() != null) {\r\n        builder.setReplication(info.getReplication());\r\n    }\r\n    if (info.getPool() != null) {\r\n        builder.setPool(info.getPool());\r\n    }\r\n    if (info.getExpiration() != null) {\r\n        builder.setExpiration(convert(info.getExpiration()));\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CacheDirectiveInfoExpirationProto convert(CacheDirectiveInfo.Expiration expiration)\n{\r\n    return CacheDirectiveInfoExpirationProto.newBuilder().setIsRelative(expiration.isRelative()).setMillis(expiration.getMillis()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "CacheDirectiveEntry convert(CacheDirectiveEntryProto proto)\n{\r\n    CacheDirectiveInfo info = convert(proto.getInfo());\r\n    CacheDirectiveStats stats = convert(proto.getStats());\r\n    return new CacheDirectiveEntry(info, stats);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "CacheDirectiveStats convert(CacheDirectiveStatsProto proto)\n{\r\n    CacheDirectiveStats.Builder builder = new CacheDirectiveStats.Builder();\r\n    builder.setBytesNeeded(proto.getBytesNeeded());\r\n    builder.setBytesCached(proto.getBytesCached());\r\n    builder.setFilesNeeded(proto.getFilesNeeded());\r\n    builder.setFilesCached(proto.getFilesCached());\r\n    builder.setHasExpired(proto.getHasExpired());\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "CacheDirectiveInfo convert(CacheDirectiveInfoProto proto)\n{\r\n    CacheDirectiveInfo.Builder builder = new CacheDirectiveInfo.Builder();\r\n    if (proto.hasId()) {\r\n        builder.setId(proto.getId());\r\n    }\r\n    if (proto.hasPath()) {\r\n        builder.setPath(new Path(proto.getPath()));\r\n    }\r\n    if (proto.hasReplication()) {\r\n        builder.setReplication(Shorts.checkedCast(proto.getReplication()));\r\n    }\r\n    if (proto.hasPool()) {\r\n        builder.setPool(proto.getPool());\r\n    }\r\n    if (proto.hasExpiration()) {\r\n        builder.setExpiration(convert(proto.getExpiration()));\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "CacheDirectiveInfo.Expiration convert(CacheDirectiveInfoExpirationProto proto)\n{\r\n    if (proto.getIsRelative()) {\r\n        return CacheDirectiveInfo.Expiration.newRelative(proto.getMillis());\r\n    }\r\n    return CacheDirectiveInfo.Expiration.newAbsolute(proto.getMillis());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertCacheFlags",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int convertCacheFlags(EnumSet<CacheFlag> flags)\n{\r\n    int value = 0;\r\n    if (flags.contains(CacheFlag.FORCE)) {\r\n        value |= CacheFlagProto.FORCE.getNumber();\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "SnapshotDiffReport convert(SnapshotDiffReportProto reportProto)\n{\r\n    if (reportProto == null) {\r\n        return null;\r\n    }\r\n    String snapshotDir = reportProto.getSnapshotRoot();\r\n    String fromSnapshot = reportProto.getFromSnapshot();\r\n    String toSnapshot = reportProto.getToSnapshot();\r\n    List<SnapshotDiffReportEntryProto> list = reportProto.getDiffReportEntriesList();\r\n    List<DiffReportEntry> entries = new ChunkedArrayList<>();\r\n    for (SnapshotDiffReportEntryProto entryProto : list) {\r\n        DiffReportEntry entry = convert(entryProto);\r\n        if (entry != null)\r\n            entries.add(entry);\r\n    }\r\n    return new SnapshotDiffReport(snapshotDir, fromSnapshot, toSnapshot, entries);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "DiffReportEntry convert(SnapshotDiffReportEntryProto entry)\n{\r\n    if (entry == null) {\r\n        return null;\r\n    }\r\n    DiffType type = DiffType.getTypeFromLabel(entry.getModificationLabel());\r\n    return type == null ? null : new DiffReportEntry(type, entry.getFullpath().toByteArray(), entry.hasTargetPath() ? entry.getTargetPath().toByteArray() : null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "SnapshotDiffReportListing convert(SnapshotDiffReportListingProto reportProto)\n{\r\n    if (reportProto == null) {\r\n        return null;\r\n    }\r\n    List<SnapshotDiffReportListingEntryProto> modifyList = reportProto.getModifiedEntriesList();\r\n    List<DiffReportListingEntry> modifiedEntries = new ChunkedArrayList<>();\r\n    for (SnapshotDiffReportListingEntryProto entryProto : modifyList) {\r\n        DiffReportListingEntry entry = convert(entryProto);\r\n        if (entry != null) {\r\n            modifiedEntries.add(entry);\r\n        }\r\n    }\r\n    List<SnapshotDiffReportListingEntryProto> createList = reportProto.getCreatedEntriesList();\r\n    List<DiffReportListingEntry> createdEntries = new ChunkedArrayList<>();\r\n    for (SnapshotDiffReportListingEntryProto entryProto : createList) {\r\n        DiffReportListingEntry entry = convert(entryProto);\r\n        if (entry != null) {\r\n            createdEntries.add(entry);\r\n        }\r\n    }\r\n    List<SnapshotDiffReportListingEntryProto> deletedList = reportProto.getDeletedEntriesList();\r\n    List<DiffReportListingEntry> deletedEntries = new ChunkedArrayList<>();\r\n    for (SnapshotDiffReportListingEntryProto entryProto : deletedList) {\r\n        DiffReportListingEntry entry = convert(entryProto);\r\n        if (entry != null) {\r\n            deletedEntries.add(entry);\r\n        }\r\n    }\r\n    byte[] startPath = reportProto.getCursor().getStartPath().toByteArray();\r\n    boolean isFromEarlier = reportProto.getIsFromEarlier();\r\n    int index = reportProto.getCursor().getIndex();\r\n    return new SnapshotDiffReportListing(startPath, modifiedEntries, createdEntries, deletedEntries, index, isFromEarlier);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "DiffReportListingEntry convert(SnapshotDiffReportListingEntryProto entry)\n{\r\n    if (entry == null) {\r\n        return null;\r\n    }\r\n    long dirId = entry.getDirId();\r\n    long fileId = entry.getFileId();\r\n    boolean isReference = entry.getIsReference();\r\n    byte[] sourceName = entry.getFullpath().toByteArray();\r\n    byte[] targetName = entry.hasTargetPath() ? entry.getTargetPath().toByteArray() : null;\r\n    return new DiffReportListingEntry(dirId, fileId, sourceName, isReference, targetName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "SnapshottableDirectoryStatus[] convert(SnapshottableDirectoryListingProto sdlp)\n{\r\n    if (sdlp == null)\r\n        return null;\r\n    List<SnapshottableDirectoryStatusProto> list = sdlp.getSnapshottableDirListingList();\r\n    if (list.isEmpty()) {\r\n        return new SnapshottableDirectoryStatus[0];\r\n    } else {\r\n        SnapshottableDirectoryStatus[] result = new SnapshottableDirectoryStatus[list.size()];\r\n        for (int i = 0; i < list.size(); i++) {\r\n            result[i] = convert(list.get(i));\r\n        }\r\n        return result;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "SnapshottableDirectoryStatus convert(SnapshottableDirectoryStatusProto sdirStatusProto)\n{\r\n    if (sdirStatusProto == null) {\r\n        return null;\r\n    }\r\n    final HdfsFileStatusProto status = sdirStatusProto.getDirStatus();\r\n    EnumSet<HdfsFileStatus.Flags> flags = status.hasFlags() ? convertFlags(status.getFlags()) : convertFlags(status.getPermission());\r\n    return new SnapshottableDirectoryStatus(status.getModificationTime(), status.getAccessTime(), convert(status.getPermission()), flags, status.getOwner(), status.getGroup(), status.getPath().toByteArray(), status.getFileId(), status.getChildrenNum(), sdirStatusProto.getSnapshotNumber(), sdirStatusProto.getSnapshotQuota(), sdirStatusProto.getParentFullpath().toByteArray());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "SnapshotStatus[] convert(HdfsProtos.SnapshotListingProto sdlp)\n{\r\n    if (sdlp == null) {\r\n        return null;\r\n    }\r\n    List<HdfsProtos.SnapshotStatusProto> list = sdlp.getSnapshotListingList();\r\n    if (list.isEmpty()) {\r\n        return new SnapshotStatus[0];\r\n    } else {\r\n        SnapshotStatus[] result = new SnapshotStatus[list.size()];\r\n        for (int i = 0; i < list.size(); i++) {\r\n            result[i] = convert(list.get(i));\r\n        }\r\n        return result;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "SnapshotStatus convert(HdfsProtos.SnapshotStatusProto sdirStatusProto)\n{\r\n    if (sdirStatusProto == null) {\r\n        return null;\r\n    }\r\n    final HdfsFileStatusProto status = sdirStatusProto.getDirStatus();\r\n    EnumSet<HdfsFileStatus.Flags> flags = status.hasFlags() ? convertFlags(status.getFlags()) : convertFlags(status.getPermission());\r\n    return new SnapshotStatus(status.getModificationTime(), status.getAccessTime(), convert(status.getPermission()), flags, status.getOwner(), status.getGroup(), status.getPath().toByteArray(), status.getFileId(), status.getChildrenNum(), sdirStatusProto.getSnapshotID(), sdirStatusProto.getIsDeleted(), sdirStatusProto.getParentFullpath().toByteArray());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "DataEncryptionKey convert(DataEncryptionKeyProto bet)\n{\r\n    String encryptionAlgorithm = bet.getEncryptionAlgorithm();\r\n    return new DataEncryptionKey(bet.getKeyId(), bet.getBlockPoolId(), bet.getNonce().toByteArray(), bet.getEncryptionKey().toByteArray(), bet.getExpiryDate(), encryptionAlgorithm.isEmpty() ? null : encryptionAlgorithm);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> convertDelegationToken(TokenProto blockToken)\n{\r\n    return new Token<>(blockToken.getIdentifier().toByteArray(), blockToken.getPassword().toByteArray(), new Text(blockToken.getKind()), new Text(blockToken.getService()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DatanodeIDProto[] convert(DatanodeID[] did)\n{\r\n    if (did == null)\r\n        return null;\r\n    final int len = did.length;\r\n    DatanodeIDProto[] result = new DatanodeIDProto[len];\r\n    for (int i = 0; i < len; ++i) {\r\n        result[i] = convert(did[i]);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FsPermissionProto convert(FsPermission p)\n{\r\n    return FsPermissionProto.newBuilder().setPerm(p.toShort()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HdfsFileStatus convert(HdfsFileStatusProto fs)\n{\r\n    if (fs == null) {\r\n        return null;\r\n    }\r\n    EnumSet<HdfsFileStatus.Flags> flags = fs.hasFlags() ? convertFlags(fs.getFlags()) : convertFlags(fs.getPermission());\r\n    return new HdfsFileStatus.Builder().length(fs.getLength()).isdir(fs.getFileType().equals(FileType.IS_DIR)).replication(fs.getBlockReplication()).blocksize(fs.getBlocksize()).mtime(fs.getModificationTime()).atime(fs.getAccessTime()).perm(convert(fs.getPermission())).flags(flags).owner(fs.getOwner()).group(fs.getGroup()).symlink(FileType.IS_SYMLINK.equals(fs.getFileType()) ? fs.getSymlink().toByteArray() : null).path(fs.getPath().toByteArray()).fileId(fs.hasFileId() ? fs.getFileId() : HdfsConstants.GRANDFATHER_INODE_ID).locations(fs.hasLocations() ? convert(fs.getLocations()) : null).children(fs.hasChildrenNum() ? fs.getChildrenNum() : -1).feInfo(fs.hasFileEncryptionInfo() ? convert(fs.getFileEncryptionInfo()) : null).storagePolicy(fs.hasStoragePolicy() ? (byte) fs.getStoragePolicy() : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED).ecPolicy(fs.hasEcPolicy() ? convertErasureCodingPolicy(fs.getEcPolicy()) : null).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertFlags",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "EnumSet<HdfsFileStatus.Flags> convertFlags(int flags)\n{\r\n    EnumSet<HdfsFileStatus.Flags> f = EnumSet.noneOf(HdfsFileStatus.Flags.class);\r\n    for (HdfsFileStatusProto.Flags pbf : HdfsFileStatusProto.Flags.values()) {\r\n        if ((pbf.getNumber() & flags) != 0) {\r\n            switch(pbf) {\r\n                case HAS_ACL:\r\n                    f.add(HdfsFileStatus.Flags.HAS_ACL);\r\n                    break;\r\n                case HAS_CRYPT:\r\n                    f.add(HdfsFileStatus.Flags.HAS_CRYPT);\r\n                    break;\r\n                case HAS_EC:\r\n                    f.add(HdfsFileStatus.Flags.HAS_EC);\r\n                    break;\r\n                case SNAPSHOT_ENABLED:\r\n                    f.add(HdfsFileStatus.Flags.SNAPSHOT_ENABLED);\r\n                    break;\r\n                default:\r\n                    break;\r\n            }\r\n        }\r\n    }\r\n    return f;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertFlags",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "EnumSet<HdfsFileStatus.Flags> convertFlags(FsPermissionProto pbp)\n{\r\n    EnumSet<HdfsFileStatus.Flags> f = EnumSet.noneOf(HdfsFileStatus.Flags.class);\r\n    FsPermission p = new FsPermissionExtension((short) pbp.getPerm());\r\n    if (p.getAclBit()) {\r\n        f.add(HdfsFileStatus.Flags.HAS_ACL);\r\n    }\r\n    if (p.getEncryptedBit()) {\r\n        f.add(HdfsFileStatus.Flags.HAS_CRYPT);\r\n    }\r\n    if (p.getErasureCodedBit()) {\r\n        f.add(HdfsFileStatus.Flags.HAS_EC);\r\n    }\r\n    return f;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "CorruptFileBlocks convert(CorruptFileBlocksProto c)\n{\r\n    if (c == null)\r\n        return null;\r\n    List<String> fileList = c.getFilesList();\r\n    return new CorruptFileBlocks(fileList.toArray(new String[fileList.size()]), c.getCookie());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ContentSummary convert(ContentSummaryProto cs)\n{\r\n    if (cs == null)\r\n        return null;\r\n    ContentSummary.Builder builder = new ContentSummary.Builder();\r\n    builder.length(cs.getLength()).fileCount(cs.getFileCount()).directoryCount(cs.getDirectoryCount()).snapshotLength(cs.getSnapshotLength()).snapshotFileCount(cs.getSnapshotFileCount()).snapshotDirectoryCount(cs.getSnapshotDirectoryCount()).snapshotSpaceConsumed(cs.getSnapshotSpaceConsumed()).quota(cs.getQuota()).spaceConsumed(cs.getSpaceConsumed()).spaceQuota(cs.getSpaceQuota()).erasureCodingPolicy(cs.getErasureCodingPolicy());\r\n    if (cs.hasTypeQuotaInfos()) {\r\n        addStorageTypes(cs.getTypeQuotaInfos(), builder);\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "QuotaUsage convert(QuotaUsageProto qu)\n{\r\n    if (qu == null) {\r\n        return null;\r\n    }\r\n    QuotaUsage.Builder builder = new QuotaUsage.Builder();\r\n    builder.fileAndDirectoryCount(qu.getFileAndDirectoryCount()).quota(qu.getQuota()).spaceConsumed(qu.getSpaceConsumed()).spaceQuota(qu.getSpaceQuota());\r\n    if (qu.hasTypeQuotaInfos()) {\r\n        addStorageTypes(qu.getTypeQuotaInfos(), builder);\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "QuotaUsageProto convert(QuotaUsage qu)\n{\r\n    if (qu == null) {\r\n        return null;\r\n    }\r\n    QuotaUsageProto.Builder builder = QuotaUsageProto.newBuilder();\r\n    builder.setFileAndDirectoryCount(qu.getFileAndDirectoryCount()).setQuota(qu.getQuota()).setSpaceConsumed(qu.getSpaceConsumed()).setSpaceQuota(qu.getSpaceQuota());\r\n    if (qu.isTypeQuotaSet() || qu.isTypeConsumedAvailable()) {\r\n        builder.setTypeQuotaInfos(getBuilder(qu));\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReencryptActionProto convert(ReencryptAction a)\n{\r\n    switch(a) {\r\n        case CANCEL:\r\n            return ReencryptActionProto.CANCEL_REENCRYPT;\r\n        case START:\r\n            return ReencryptActionProto.START_REENCRYPT;\r\n        default:\r\n            throw new IllegalArgumentException(\"Unexpected value: \" + a);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RollingUpgradeActionProto convert(RollingUpgradeAction a)\n{\r\n    switch(a) {\r\n        case QUERY:\r\n            return RollingUpgradeActionProto.QUERY;\r\n        case PREPARE:\r\n            return RollingUpgradeActionProto.START;\r\n        case FINALIZE:\r\n            return RollingUpgradeActionProto.FINALIZE;\r\n        default:\r\n            throw new IllegalArgumentException(\"Unexpected value: \" + a);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "RollingUpgradeInfo convert(RollingUpgradeInfoProto proto)\n{\r\n    RollingUpgradeStatusProto status = proto.getStatus();\r\n    return new RollingUpgradeInfo(status.getBlockPoolId(), proto.getCreatedRollbackImages(), proto.getStartTime(), proto.getFinalizeTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertDatanodeStorageReports",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DatanodeStorageReport[] convertDatanodeStorageReports(List<DatanodeStorageReportProto> protos)\n{\r\n    final DatanodeStorageReport[] reports = new DatanodeStorageReport[protos.size()];\r\n    for (int i = 0; i < reports.length; i++) {\r\n        reports[i] = convertDatanodeStorageReport(protos.get(i));\r\n    }\r\n    return reports;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertDatanodeStorageReport",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DatanodeStorageReport convertDatanodeStorageReport(DatanodeStorageReportProto proto)\n{\r\n    return new DatanodeStorageReport(convert(proto.getDatanodeInfo()), convertStorageReports(proto.getStorageReportsList()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertStorageReports",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "StorageReport[] convertStorageReports(List<StorageReportProto> list)\n{\r\n    final StorageReport[] report = new StorageReport[list.size()];\r\n    for (int i = 0; i < report.length; i++) {\r\n        report[i] = convert(list.get(i));\r\n    }\r\n    return report;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "StorageReport convert(StorageReportProto p)\n{\r\n    long nonDfsUsed = p.hasNonDfsUsed() ? p.getNonDfsUsed() : p.getCapacity() - p.getDfsUsed() - p.getRemaining();\r\n    return new StorageReport(p.hasStorage() ? convert(p.getStorage()) : new DatanodeStorage(p.getStorageUuid()), p.getFailed(), p.getCapacity(), p.getDfsUsed(), p.getRemaining(), p.getBlockPoolUsed(), nonDfsUsed, p.getMount());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DatanodeStorage convert(DatanodeStorageProto s)\n{\r\n    return new DatanodeStorage(s.getStorageUuid(), convertState(s.getState()), convertStorageType(s.getStorageType()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "State convertState(StorageState state)\n{\r\n    switch(state) {\r\n        case READ_ONLY_SHARED:\r\n            return State.READ_ONLY_SHARED;\r\n        case NORMAL:\r\n        default:\r\n            return State.NORMAL;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SafeModeActionProto convert(SafeModeAction a)\n{\r\n    switch(a) {\r\n        case SAFEMODE_LEAVE:\r\n            return SafeModeActionProto.SAFEMODE_LEAVE;\r\n        case SAFEMODE_ENTER:\r\n            return SafeModeActionProto.SAFEMODE_ENTER;\r\n        case SAFEMODE_GET:\r\n            return SafeModeActionProto.SAFEMODE_GET;\r\n        case SAFEMODE_FORCE_EXIT:\r\n            return SafeModeActionProto.SAFEMODE_FORCE_EXIT;\r\n        default:\r\n            throw new IllegalArgumentException(\"Unexpected SafeModeAction :\" + a);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DatanodeInfo[] convert(List<DatanodeInfoProto> list)\n{\r\n    DatanodeInfo[] info = new DatanodeInfo[list.size()];\r\n    for (int i = 0; i < info.length; i++) {\r\n        info[i] = convert(list.get(i));\r\n    }\r\n    return info;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "long[] convert(GetFsStatsResponseProto res)\n{\r\n    long[] result = new long[ClientProtocol.STATS_ARRAY_LENGTH];\r\n    result[ClientProtocol.GET_STATS_CAPACITY_IDX] = res.getCapacity();\r\n    result[ClientProtocol.GET_STATS_USED_IDX] = res.getUsed();\r\n    result[ClientProtocol.GET_STATS_REMAINING_IDX] = res.getRemaining();\r\n    result[ClientProtocol.GET_STATS_UNDER_REPLICATED_IDX] = res.getUnderReplicated();\r\n    result[ClientProtocol.GET_STATS_CORRUPT_BLOCKS_IDX] = res.getCorruptBlocks();\r\n    result[ClientProtocol.GET_STATS_MISSING_BLOCKS_IDX] = res.getMissingBlocks();\r\n    result[ClientProtocol.GET_STATS_MISSING_REPL_ONE_BLOCKS_IDX] = res.getMissingReplOneBlocks();\r\n    result[ClientProtocol.GET_STATS_BYTES_IN_FUTURE_BLOCKS_IDX] = res.hasBlocksInFuture() ? res.getBlocksInFuture() : 0;\r\n    result[ClientProtocol.GET_STATS_PENDING_DELETION_BLOCKS_IDX] = res.getPendingDeletionBlocks();\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "ReplicatedBlockStats convert(GetFsReplicatedBlockStatsResponseProto res)\n{\r\n    if (res.hasHighestPrioLowRedundancyBlocks()) {\r\n        return new ReplicatedBlockStats(res.getLowRedundancy(), res.getCorruptBlocks(), res.getMissingBlocks(), res.getMissingReplOneBlocks(), res.getBlocksInFuture(), res.getPendingDeletionBlocks(), res.getHighestPrioLowRedundancyBlocks());\r\n    }\r\n    return new ReplicatedBlockStats(res.getLowRedundancy(), res.getCorruptBlocks(), res.getMissingBlocks(), res.getMissingReplOneBlocks(), res.getBlocksInFuture(), res.getPendingDeletionBlocks());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "ECBlockGroupStats convert(GetFsECBlockGroupStatsResponseProto res)\n{\r\n    if (res.hasHighestPrioLowRedundancyBlocks()) {\r\n        return new ECBlockGroupStats(res.getLowRedundancy(), res.getCorruptBlocks(), res.getMissingBlocks(), res.getBlocksInFuture(), res.getPendingDeletionBlocks(), res.getHighestPrioLowRedundancyBlocks());\r\n    }\r\n    return new ECBlockGroupStats(res.getLowRedundancy(), res.getCorruptBlocks(), res.getMissingBlocks(), res.getBlocksInFuture(), res.getPendingDeletionBlocks());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DatanodeReportTypeProto convert(DatanodeReportType t)\n{\r\n    switch(t) {\r\n        case ALL:\r\n            return DatanodeReportTypeProto.ALL;\r\n        case LIVE:\r\n            return DatanodeReportTypeProto.LIVE;\r\n        case DEAD:\r\n            return DatanodeReportTypeProto.DEAD;\r\n        case DECOMMISSIONING:\r\n            return DatanodeReportTypeProto.DECOMMISSIONING;\r\n        case ENTERING_MAINTENANCE:\r\n            return DatanodeReportTypeProto.ENTERING_MAINTENANCE;\r\n        case IN_MAINTENANCE:\r\n            return DatanodeReportTypeProto.IN_MAINTENANCE;\r\n        default:\r\n            throw new IllegalArgumentException(\"Unexpected data type report:\" + t);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "DirectoryListing convert(DirectoryListingProto dl)\n{\r\n    if (dl == null)\r\n        return null;\r\n    List<HdfsFileStatusProto> partList = dl.getPartialListingList();\r\n    return new DirectoryListing(partList.isEmpty() ? new HdfsFileStatus[0] : convert(partList.toArray(new HdfsFileStatusProto[partList.size()])), dl.getRemainingEntries());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsFileStatus[] convert(HdfsFileStatusProto[] fs)\n{\r\n    if (fs == null)\r\n        return null;\r\n    final int len = fs.length;\r\n    HdfsFileStatus[] result = new HdfsFileStatus[len];\r\n    for (int i = 0; i < len; ++i) {\r\n        result[i] = convert(fs[i]);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertHdfsFileStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<HdfsFileStatus> convertHdfsFileStatus(List<HdfsFileStatusProto> fs)\n{\r\n    if (fs == null) {\r\n        return null;\r\n    }\r\n    List<HdfsFileStatus> result = Lists.newArrayListWithCapacity(fs.size());\r\n    for (HdfsFileStatusProto proto : fs) {\r\n        result.add(convert(proto));\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertCreateFlag",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "int convertCreateFlag(EnumSetWritable<CreateFlag> flag)\n{\r\n    int value = 0;\r\n    if (flag.contains(CreateFlag.APPEND)) {\r\n        value |= CreateFlagProto.APPEND.getNumber();\r\n    }\r\n    if (flag.contains(CreateFlag.CREATE)) {\r\n        value |= CreateFlagProto.CREATE.getNumber();\r\n    }\r\n    if (flag.contains(CreateFlag.OVERWRITE)) {\r\n        value |= CreateFlagProto.OVERWRITE.getNumber();\r\n    }\r\n    if (flag.contains(CreateFlag.LAZY_PERSIST)) {\r\n        value |= CreateFlagProto.LAZY_PERSIST.getNumber();\r\n    }\r\n    if (flag.contains(CreateFlag.NEW_BLOCK)) {\r\n        value |= CreateFlagProto.NEW_BLOCK.getNumber();\r\n    }\r\n    if (flag.contains(CreateFlag.SHOULD_REPLICATE)) {\r\n        value |= CreateFlagProto.SHOULD_REPLICATE.getNumber();\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "FsServerDefaults convert(FsServerDefaultsProto fs)\n{\r\n    if (fs == null)\r\n        return null;\r\n    return new FsServerDefaults(fs.getBlockSize(), fs.getBytesPerChecksum(), fs.getWritePacketSize(), (short) fs.getReplication(), fs.getFileBufferSize(), fs.getEncryptDataTransfer(), fs.getTrashInterval(), convert(fs.getChecksumType()), fs.hasKeyProviderUri() ? fs.getKeyProviderUri() : null, (byte) fs.getPolicyId(), fs.getSnapshotTrashRootEnabled());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<CryptoProtocolVersionProto> convert(CryptoProtocolVersion[] versions)\n{\r\n    List<CryptoProtocolVersionProto> protos = Lists.newArrayListWithCapacity(versions.length);\r\n    for (CryptoProtocolVersion v : versions) {\r\n        protos.add(convert(v));\r\n    }\r\n    return protos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<StorageTypesProto> convert(StorageType[][] types)\n{\r\n    List<StorageTypesProto> list = Lists.newArrayList();\r\n    if (types != null) {\r\n        for (StorageType[] ts : types) {\r\n            StorageTypesProto.Builder builder = StorageTypesProto.newBuilder();\r\n            builder.addAllStorageTypes(convertStorageTypes(ts));\r\n            list.add(builder.build());\r\n        }\r\n    }\r\n    return list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "BlockStoragePolicyProto convert(BlockStoragePolicy policy)\n{\r\n    BlockStoragePolicyProto.Builder builder = BlockStoragePolicyProto.newBuilder().setPolicyId(policy.getId()).setName(policy.getName());\r\n    StorageTypesProto creationProto = convert(policy.getStorageTypes());\r\n    Preconditions.checkArgument(creationProto != null);\r\n    builder.setCreationPolicy(creationProto);\r\n    StorageTypesProto creationFallbackProto = convert(policy.getCreationFallbacks());\r\n    if (creationFallbackProto != null) {\r\n        builder.setCreationFallbackPolicy(creationFallbackProto);\r\n    }\r\n    StorageTypesProto replicationFallbackProto = convert(policy.getReplicationFallbacks());\r\n    if (replicationFallbackProto != null) {\r\n        builder.setReplicationFallbackPolicy(replicationFallbackProto);\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "StorageTypesProto convert(StorageType[] types)\n{\r\n    if (types == null || types.length == 0) {\r\n        return null;\r\n    }\r\n    List<StorageTypeProto> list = convertStorageTypes(types);\r\n    return StorageTypesProto.newBuilder().addAllStorageTypes(list).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DatanodeID[] convert(DatanodeIDProto[] did)\n{\r\n    if (did == null)\r\n        return null;\r\n    final int len = did.length;\r\n    DatanodeID[] result = new DatanodeID[len];\r\n    for (int i = 0; i < len; ++i) {\r\n        result[i] = convert(did[i]);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockProto convert(Block b)\n{\r\n    return BlockProto.newBuilder().setBlockId(b.getBlockId()).setGenStamp(b.getGenerationStamp()).setNumBytes(b.getNumBytes()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Block convert(BlockProto b)\n{\r\n    return new Block(b.getBlockId(), b.getNumBytes(), b.getGenStamp());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockTypeProto convert(BlockType blockType)\n{\r\n    switch(blockType) {\r\n        case CONTIGUOUS:\r\n            return BlockTypeProto.CONTIGUOUS;\r\n        case STRIPED:\r\n            return BlockTypeProto.STRIPED;\r\n        default:\r\n            throw new IllegalArgumentException(\"Unexpected block type: \" + blockType);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockType convert(BlockTypeProto blockType)\n{\r\n    switch(blockType.getNumber()) {\r\n        case BlockTypeProto.CONTIGUOUS_VALUE:\r\n            return BlockType.CONTIGUOUS;\r\n        case BlockTypeProto.STRIPED_VALUE:\r\n            return BlockType.STRIPED;\r\n        default:\r\n            throw new IllegalArgumentException(\"Unexpected block type: \" + blockType);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DatanodeInfo[] convert(DatanodeInfoProto[] di)\n{\r\n    if (di == null)\r\n        return null;\r\n    DatanodeInfo[] result = new DatanodeInfo[di.length];\r\n    for (int i = 0; i < di.length; i++) {\r\n        result[i] = convert(di[i]);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertDatanodeStorageReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DatanodeStorageReportProto convertDatanodeStorageReport(DatanodeStorageReport report)\n{\r\n    return DatanodeStorageReportProto.newBuilder().setDatanodeInfo(convert(report.getDatanodeInfo())).addAllStorageReports(convertStorageReports(report.getStorageReports())).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertDatanodeStorageReports",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<DatanodeStorageReportProto> convertDatanodeStorageReports(DatanodeStorageReport[] reports)\n{\r\n    final List<DatanodeStorageReportProto> protos = new ArrayList<>(reports.length);\r\n    for (DatanodeStorageReport report : reports) {\r\n        protos.add(convertDatanodeStorageReport(report));\r\n    }\r\n    return protos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertLocatedBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocatedBlock[] convertLocatedBlock(LocatedBlockProto[] lb)\n{\r\n    if (lb == null)\r\n        return null;\r\n    return convertLocatedBlock(Arrays.asList(lb)).toArray(new LocatedBlock[lb.length]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "LocatedBlocksProto convert(LocatedBlocks lb)\n{\r\n    if (lb == null) {\r\n        return null;\r\n    }\r\n    LocatedBlocksProto.Builder builder = LocatedBlocksProto.newBuilder();\r\n    if (lb.getLastLocatedBlock() != null) {\r\n        builder.setLastBlock(convertLocatedBlock(lb.getLastLocatedBlock()));\r\n    }\r\n    if (lb.getFileEncryptionInfo() != null) {\r\n        builder.setFileEncryptionInfo(convert(lb.getFileEncryptionInfo()));\r\n    }\r\n    if (lb.getErasureCodingPolicy() != null) {\r\n        builder.setEcPolicy(convertErasureCodingPolicy(lb.getErasureCodingPolicy()));\r\n    }\r\n    return builder.setFileLength(lb.getFileLength()).setUnderConstruction(lb.isUnderConstruction()).addAllBlocks(convertLocatedBlocks2(lb.getLocatedBlocks())).setIsLastBlockComplete(lb.isLastBlockComplete()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DataEncryptionKeyProto convert(DataEncryptionKey bet)\n{\r\n    DataEncryptionKeyProto.Builder b = DataEncryptionKeyProto.newBuilder().setKeyId(bet.keyId).setBlockPoolId(bet.blockPoolId).setNonce(getByteString(bet.nonce)).setEncryptionKey(getByteString(bet.encryptionKey)).setExpiryDate(bet.expiryDate);\r\n    if (bet.encryptionAlgorithm != null) {\r\n        b.setEncryptionAlgorithm(bet.encryptionAlgorithm);\r\n    }\r\n    return b.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FsServerDefaultsProto convert(FsServerDefaults fs)\n{\r\n    if (fs == null)\r\n        return null;\r\n    FsServerDefaultsProto.Builder builder = FsServerDefaultsProto.newBuilder().setBlockSize(fs.getBlockSize()).setBytesPerChecksum(fs.getBytesPerChecksum()).setWritePacketSize(fs.getWritePacketSize()).setReplication(fs.getReplication()).setFileBufferSize(fs.getFileBufferSize()).setEncryptDataTransfer(fs.getEncryptDataTransfer()).setTrashInterval(fs.getTrashInterval()).setChecksumType(convert(fs.getChecksumType())).setPolicyId(fs.getDefaultStoragePolicyId()).setSnapshotTrashRootEnabled(fs.getSnapshotTrashRootEnabled());\r\n    if (fs.getKeyProviderUri() != null) {\r\n        builder.setKeyProviderUri(fs.getKeyProviderUri());\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertCreateFlag",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "EnumSetWritable<CreateFlag> convertCreateFlag(int flag)\n{\r\n    EnumSet<CreateFlag> result = EnumSet.noneOf(CreateFlag.class);\r\n    if ((flag & CreateFlagProto.APPEND_VALUE) == CreateFlagProto.APPEND_VALUE) {\r\n        result.add(CreateFlag.APPEND);\r\n    }\r\n    if ((flag & CreateFlagProto.CREATE_VALUE) == CreateFlagProto.CREATE_VALUE) {\r\n        result.add(CreateFlag.CREATE);\r\n    }\r\n    if ((flag & CreateFlagProto.OVERWRITE_VALUE) == CreateFlagProto.OVERWRITE_VALUE) {\r\n        result.add(CreateFlag.OVERWRITE);\r\n    }\r\n    if ((flag & CreateFlagProto.LAZY_PERSIST_VALUE) == CreateFlagProto.LAZY_PERSIST_VALUE) {\r\n        result.add(CreateFlag.LAZY_PERSIST);\r\n    }\r\n    if ((flag & CreateFlagProto.NEW_BLOCK_VALUE) == CreateFlagProto.NEW_BLOCK_VALUE) {\r\n        result.add(CreateFlag.NEW_BLOCK);\r\n    }\r\n    if ((flag & CreateFlagProto.SHOULD_REPLICATE.getNumber()) == CreateFlagProto.SHOULD_REPLICATE.getNumber()) {\r\n        result.add(CreateFlag.SHOULD_REPLICATE);\r\n    }\r\n    return new EnumSetWritable<>(result, CreateFlag.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertCacheFlags",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "EnumSet<CacheFlag> convertCacheFlags(int flags)\n{\r\n    EnumSet<CacheFlag> result = EnumSet.noneOf(CacheFlag.class);\r\n    if ((flags & CacheFlagProto.FORCE_VALUE) == CacheFlagProto.FORCE_VALUE) {\r\n        result.add(CacheFlag.FORCE);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "HdfsFileStatusProto convert(HdfsFileStatus fs)\n{\r\n    if (fs == null)\r\n        return null;\r\n    FileType fType = FileType.IS_FILE;\r\n    if (fs.isDirectory()) {\r\n        fType = FileType.IS_DIR;\r\n    } else if (fs.isSymlink()) {\r\n        fType = FileType.IS_SYMLINK;\r\n    }\r\n    HdfsFileStatusProto.Builder builder = HdfsFileStatusProto.newBuilder().setLength(fs.getLen()).setFileType(fType).setBlockReplication(fs.getReplication()).setBlocksize(fs.getBlockSize()).setModificationTime(fs.getModificationTime()).setAccessTime(fs.getAccessTime()).setPermission(convert(fs.getPermission())).setOwnerBytes(getFixedByteString(fs.getOwner())).setGroupBytes(getFixedByteString(fs.getGroup())).setFileId(fs.getFileId()).setChildrenNum(fs.getChildrenNum()).setPath(getByteString(fs.getLocalNameInBytes())).setStoragePolicy(fs.getStoragePolicy());\r\n    if (fs.isSymlink()) {\r\n        builder.setSymlink(getByteString(fs.getSymlinkInBytes()));\r\n    }\r\n    if (fs.getFileEncryptionInfo() != null) {\r\n        builder.setFileEncryptionInfo(convert(fs.getFileEncryptionInfo()));\r\n    }\r\n    if (fs instanceof HdfsLocatedFileStatus) {\r\n        final HdfsLocatedFileStatus lfs = (HdfsLocatedFileStatus) fs;\r\n        LocatedBlocks locations = lfs.getLocatedBlocks();\r\n        if (locations != null) {\r\n            builder.setLocations(convert(locations));\r\n        }\r\n    }\r\n    if (fs.getErasureCodingPolicy() != null) {\r\n        builder.setEcPolicy(convertErasureCodingPolicy(fs.getErasureCodingPolicy()));\r\n    }\r\n    int flags = fs.hasAcl() ? HdfsFileStatusProto.Flags.HAS_ACL_VALUE : 0;\r\n    flags |= fs.isEncrypted() ? HdfsFileStatusProto.Flags.HAS_CRYPT_VALUE : 0;\r\n    flags |= fs.isErasureCoded() ? HdfsFileStatusProto.Flags.HAS_EC_VALUE : 0;\r\n    flags |= fs.isSnapshotEnabled() ? HdfsFileStatusProto.Flags.SNAPSHOT_ENABLED_VALUE : 0;\r\n    builder.setFlags(flags);\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "SnapshottableDirectoryStatusProto convert(SnapshottableDirectoryStatus status)\n{\r\n    if (status == null) {\r\n        return null;\r\n    }\r\n    int snapshotNumber = status.getSnapshotNumber();\r\n    int snapshotQuota = status.getSnapshotQuota();\r\n    byte[] parentFullPath = status.getParentFullPath();\r\n    ByteString parentFullPathBytes = getByteString(parentFullPath == null ? DFSUtilClient.EMPTY_BYTES : parentFullPath);\r\n    HdfsFileStatusProto fs = convert(status.getDirStatus());\r\n    SnapshottableDirectoryStatusProto.Builder builder = SnapshottableDirectoryStatusProto.newBuilder().setSnapshotNumber(snapshotNumber).setSnapshotQuota(snapshotQuota).setParentFullpath(parentFullPathBytes).setDirStatus(fs);\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "HdfsProtos.SnapshotStatusProto convert(SnapshotStatus status)\n{\r\n    if (status == null) {\r\n        return null;\r\n    }\r\n    byte[] parentFullPath = status.getParentFullPath();\r\n    ByteString parentFullPathBytes = getByteString(parentFullPath == null ? DFSUtilClient.EMPTY_BYTES : parentFullPath);\r\n    HdfsFileStatusProto fs = convert(status.getDirStatus());\r\n    HdfsProtos.SnapshotStatusProto.Builder builder = HdfsProtos.SnapshotStatusProto.newBuilder().setSnapshotID(status.getSnapshotID()).setParentFullpath(parentFullPathBytes).setIsDeleted(status.isDeleted()).setDirStatus(fs);\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsFileStatusProto[] convert(HdfsFileStatus[] fs)\n{\r\n    if (fs == null)\r\n        return null;\r\n    final int len = fs.length;\r\n    HdfsFileStatusProto[] result = new HdfsFileStatusProto[len];\r\n    for (int i = 0; i < len; ++i) {\r\n        result[i] = convert(fs[i]);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DirectoryListingProto convert(DirectoryListing d)\n{\r\n    if (d == null)\r\n        return null;\r\n    return DirectoryListingProto.newBuilder().addAllPartialListing(Arrays.asList(convert(d.getPartialListing()))).setRemainingEntries(d.getRemainingEntries()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "GetFsStatsResponseProto convert(long[] fsStats)\n{\r\n    GetFsStatsResponseProto.Builder result = GetFsStatsResponseProto.newBuilder();\r\n    if (fsStats.length >= ClientProtocol.GET_STATS_CAPACITY_IDX + 1)\r\n        result.setCapacity(fsStats[ClientProtocol.GET_STATS_CAPACITY_IDX]);\r\n    if (fsStats.length >= ClientProtocol.GET_STATS_USED_IDX + 1)\r\n        result.setUsed(fsStats[ClientProtocol.GET_STATS_USED_IDX]);\r\n    if (fsStats.length >= ClientProtocol.GET_STATS_REMAINING_IDX + 1)\r\n        result.setRemaining(fsStats[ClientProtocol.GET_STATS_REMAINING_IDX]);\r\n    if (fsStats.length >= ClientProtocol.GET_STATS_UNDER_REPLICATED_IDX + 1)\r\n        result.setUnderReplicated(fsStats[ClientProtocol.GET_STATS_UNDER_REPLICATED_IDX]);\r\n    if (fsStats.length >= ClientProtocol.GET_STATS_CORRUPT_BLOCKS_IDX + 1)\r\n        result.setCorruptBlocks(fsStats[ClientProtocol.GET_STATS_CORRUPT_BLOCKS_IDX]);\r\n    if (fsStats.length >= ClientProtocol.GET_STATS_MISSING_BLOCKS_IDX + 1)\r\n        result.setMissingBlocks(fsStats[ClientProtocol.GET_STATS_MISSING_BLOCKS_IDX]);\r\n    if (fsStats.length >= ClientProtocol.GET_STATS_MISSING_REPL_ONE_BLOCKS_IDX + 1)\r\n        result.setMissingReplOneBlocks(fsStats[ClientProtocol.GET_STATS_MISSING_REPL_ONE_BLOCKS_IDX]);\r\n    if (fsStats.length >= ClientProtocol.GET_STATS_BYTES_IN_FUTURE_BLOCKS_IDX + 1) {\r\n        result.setBlocksInFuture(fsStats[ClientProtocol.GET_STATS_BYTES_IN_FUTURE_BLOCKS_IDX]);\r\n    }\r\n    if (fsStats.length >= ClientProtocol.GET_STATS_PENDING_DELETION_BLOCKS_IDX + 1) {\r\n        result.setPendingDeletionBlocks(fsStats[ClientProtocol.GET_STATS_PENDING_DELETION_BLOCKS_IDX]);\r\n    }\r\n    return result.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "GetFsReplicatedBlockStatsResponseProto convert(ReplicatedBlockStats replicatedBlockStats)\n{\r\n    GetFsReplicatedBlockStatsResponseProto.Builder result = GetFsReplicatedBlockStatsResponseProto.newBuilder();\r\n    result.setLowRedundancy(replicatedBlockStats.getLowRedundancyBlocks());\r\n    result.setCorruptBlocks(replicatedBlockStats.getCorruptBlocks());\r\n    result.setMissingBlocks(replicatedBlockStats.getMissingReplicaBlocks());\r\n    result.setMissingReplOneBlocks(replicatedBlockStats.getMissingReplicationOneBlocks());\r\n    result.setBlocksInFuture(replicatedBlockStats.getBytesInFutureBlocks());\r\n    result.setPendingDeletionBlocks(replicatedBlockStats.getPendingDeletionBlocks());\r\n    if (replicatedBlockStats.hasHighestPriorityLowRedundancyBlocks()) {\r\n        result.setHighestPrioLowRedundancyBlocks(replicatedBlockStats.getHighestPriorityLowRedundancyBlocks());\r\n    }\r\n    return result.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "GetFsECBlockGroupStatsResponseProto convert(ECBlockGroupStats ecBlockGroupStats)\n{\r\n    GetFsECBlockGroupStatsResponseProto.Builder result = GetFsECBlockGroupStatsResponseProto.newBuilder();\r\n    result.setLowRedundancy(ecBlockGroupStats.getLowRedundancyBlockGroups());\r\n    result.setCorruptBlocks(ecBlockGroupStats.getCorruptBlockGroups());\r\n    result.setMissingBlocks(ecBlockGroupStats.getMissingBlockGroups());\r\n    result.setBlocksInFuture(ecBlockGroupStats.getBytesInFutureBlockGroups());\r\n    result.setPendingDeletionBlocks(ecBlockGroupStats.getPendingDeletionBlocks());\r\n    if (ecBlockGroupStats.hasHighestPriorityLowRedundancyBlocks()) {\r\n        result.setHighestPrioLowRedundancyBlocks(ecBlockGroupStats.getHighestPriorityLowRedundancyBlocks());\r\n    }\r\n    return result.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DatanodeReportType convert(DatanodeReportTypeProto t)\n{\r\n    switch(t) {\r\n        case ALL:\r\n            return DatanodeReportType.ALL;\r\n        case LIVE:\r\n            return DatanodeReportType.LIVE;\r\n        case DEAD:\r\n            return DatanodeReportType.DEAD;\r\n        case DECOMMISSIONING:\r\n            return DatanodeReportType.DECOMMISSIONING;\r\n        case ENTERING_MAINTENANCE:\r\n            return DatanodeReportType.ENTERING_MAINTENANCE;\r\n        case IN_MAINTENANCE:\r\n            return DatanodeReportType.IN_MAINTENANCE;\r\n        default:\r\n            throw new IllegalArgumentException(\"Unexpected data type report:\" + t);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SafeModeAction convert(SafeModeActionProto a)\n{\r\n    switch(a) {\r\n        case SAFEMODE_LEAVE:\r\n            return SafeModeAction.SAFEMODE_LEAVE;\r\n        case SAFEMODE_ENTER:\r\n            return SafeModeAction.SAFEMODE_ENTER;\r\n        case SAFEMODE_GET:\r\n            return SafeModeAction.SAFEMODE_GET;\r\n        case SAFEMODE_FORCE_EXIT:\r\n            return SafeModeAction.SAFEMODE_FORCE_EXIT;\r\n        default:\r\n            throw new IllegalArgumentException(\"Unexpected SafeModeAction :\" + a);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReencryptAction convert(ReencryptActionProto a)\n{\r\n    switch(a) {\r\n        case CANCEL_REENCRYPT:\r\n            return ReencryptAction.CANCEL;\r\n        case START_REENCRYPT:\r\n            return ReencryptAction.START;\r\n        default:\r\n            throw new IllegalArgumentException(\"Unexpected value: \" + a);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RollingUpgradeAction convert(RollingUpgradeActionProto a)\n{\r\n    switch(a) {\r\n        case QUERY:\r\n            return RollingUpgradeAction.QUERY;\r\n        case START:\r\n            return RollingUpgradeAction.PREPARE;\r\n        case FINALIZE:\r\n            return RollingUpgradeAction.FINALIZE;\r\n        default:\r\n            throw new IllegalArgumentException(\"Unexpected value: \" + a);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertRollingUpgradeStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RollingUpgradeStatusProto convertRollingUpgradeStatus(RollingUpgradeStatus status)\n{\r\n    return RollingUpgradeStatusProto.newBuilder().setBlockPoolId(status.getBlockPoolId()).setFinalized(status.isFinalized()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RollingUpgradeStatus convert(RollingUpgradeStatusProto proto)\n{\r\n    return new RollingUpgradeStatus(proto.getBlockPoolId(), proto.getFinalized());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RollingUpgradeInfoProto convert(RollingUpgradeInfo info)\n{\r\n    return RollingUpgradeInfoProto.newBuilder().setStatus(convertRollingUpgradeStatus(info)).setCreatedRollbackImages(info.createdRollbackImages()).setStartTime(info.getStartTime()).setFinalizeTime(info.getFinalizeTime()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CorruptFileBlocksProto convert(CorruptFileBlocks c)\n{\r\n    if (c == null)\r\n        return null;\r\n    return CorruptFileBlocksProto.newBuilder().addAllFiles(Arrays.asList(c.getFiles())).setCookie(c.getCookie()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ContentSummaryProto convert(ContentSummary cs)\n{\r\n    if (cs == null)\r\n        return null;\r\n    ContentSummaryProto.Builder builder = ContentSummaryProto.newBuilder();\r\n    builder.setLength(cs.getLength()).setFileCount(cs.getFileCount()).setDirectoryCount(cs.getDirectoryCount()).setSnapshotLength(cs.getSnapshotLength()).setSnapshotFileCount(cs.getSnapshotFileCount()).setSnapshotDirectoryCount(cs.getSnapshotDirectoryCount()).setSnapshotSpaceConsumed(cs.getSnapshotSpaceConsumed()).setQuota(cs.getQuota()).setSpaceConsumed(cs.getSpaceConsumed()).setSpaceQuota(cs.getSpaceQuota()).setErasureCodingPolicy(cs.getErasureCodingPolicy());\r\n    if (cs.isTypeQuotaSet() || cs.isTypeConsumedAvailable()) {\r\n        builder.setTypeQuotaInfos(getBuilder(cs));\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "addStorageTypes",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addStorageTypes(HdfsProtos.StorageTypeQuotaInfosProto typeQuotaInfos, QuotaUsage.Builder builder)\n{\r\n    for (HdfsProtos.StorageTypeQuotaInfoProto info : typeQuotaInfos.getTypeQuotaInfoList()) {\r\n        StorageType type = convertStorageType(info.getType());\r\n        builder.typeConsumed(type, info.getConsumed());\r\n        builder.typeQuota(type, info.getQuota());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getBuilder",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HdfsProtos.StorageTypeQuotaInfosProto.Builder getBuilder(QuotaUsage qu)\n{\r\n    HdfsProtos.StorageTypeQuotaInfosProto.Builder isb = HdfsProtos.StorageTypeQuotaInfosProto.newBuilder();\r\n    for (StorageType t : StorageType.getTypesSupportingQuota()) {\r\n        HdfsProtos.StorageTypeQuotaInfoProto info = HdfsProtos.StorageTypeQuotaInfoProto.newBuilder().setType(convertStorageType(t)).setConsumed(qu.getTypeConsumed(t)).setQuota(qu.getTypeQuota(t)).build();\r\n        isb.addTypeQuotaInfo(info);\r\n    }\r\n    return isb;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DatanodeStorageProto convert(DatanodeStorage s)\n{\r\n    return DatanodeStorageProto.newBuilder().setState(convertState(s.getState())).setStorageType(convertStorageType(s.getStorageType())).setStorageUuid(s.getStorageID()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StorageState convertState(State state)\n{\r\n    switch(state) {\r\n        case READ_ONLY_SHARED:\r\n            return StorageState.READ_ONLY_SHARED;\r\n        case NORMAL:\r\n        default:\r\n            return StorageState.NORMAL;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "StorageReportProto convert(StorageReport r)\n{\r\n    StorageReportProto.Builder builder = StorageReportProto.newBuilder().setBlockPoolUsed(r.getBlockPoolUsed()).setCapacity(r.getCapacity()).setDfsUsed(r.getDfsUsed()).setRemaining(r.getRemaining()).setStorageUuid(r.getStorage().getStorageID()).setStorage(convert(r.getStorage())).setNonDfsUsed(r.getNonDfsUsed()).setMount(r.getMount());\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertStorageReports",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<StorageReportProto> convertStorageReports(StorageReport[] storages)\n{\r\n    final List<StorageReportProto> protos = new ArrayList<>(storages.length);\r\n    for (StorageReport storage : storages) {\r\n        protos.add(convert(storage));\r\n    }\r\n    return protos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "SnapshottableDirectoryListingProto convert(SnapshottableDirectoryStatus[] status)\n{\r\n    if (status == null)\r\n        return null;\r\n    SnapshottableDirectoryStatusProto[] protos = new SnapshottableDirectoryStatusProto[status.length];\r\n    for (int i = 0; i < status.length; i++) {\r\n        protos[i] = convert(status[i]);\r\n    }\r\n    List<SnapshottableDirectoryStatusProto> protoList = Arrays.asList(protos);\r\n    return SnapshottableDirectoryListingProto.newBuilder().addAllSnapshottableDirListing(protoList).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "HdfsProtos.SnapshotListingProto convert(SnapshotStatus[] status)\n{\r\n    if (status == null) {\r\n        return null;\r\n    }\r\n    HdfsProtos.SnapshotStatusProto[] protos = new HdfsProtos.SnapshotStatusProto[status.length];\r\n    for (int i = 0; i < status.length; i++) {\r\n        protos[i] = convert(status[i]);\r\n    }\r\n    List<SnapshotStatusProto> protoList = Arrays.asList(protos);\r\n    return SnapshotListingProto.newBuilder().addAllSnapshotListing(protoList).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "SnapshotDiffReportEntryProto convert(DiffReportEntry entry)\n{\r\n    if (entry == null) {\r\n        return null;\r\n    }\r\n    ByteString sourcePath = getByteString(entry.getSourcePath() == null ? DFSUtilClient.EMPTY_BYTES : entry.getSourcePath());\r\n    String modification = entry.getType().getLabel();\r\n    SnapshotDiffReportEntryProto.Builder builder = SnapshotDiffReportEntryProto.newBuilder().setFullpath(sourcePath).setModificationLabel(modification);\r\n    if (entry.getType() == DiffType.RENAME) {\r\n        ByteString targetPath = getByteString(entry.getTargetPath() == null ? DFSUtilClient.EMPTY_BYTES : entry.getTargetPath());\r\n        builder.setTargetPath(targetPath);\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "SnapshotDiffReportListingEntryProto convert(DiffReportListingEntry entry)\n{\r\n    if (entry == null) {\r\n        return null;\r\n    }\r\n    ByteString sourcePath = getByteString(entry.getSourcePath() == null ? DFSUtilClient.EMPTY_BYTES : DFSUtilClient.byteArray2bytes(entry.getSourcePath()));\r\n    long dirId = entry.getDirId();\r\n    long fileId = entry.getFileId();\r\n    boolean isReference = entry.isReference();\r\n    ByteString targetPath = getByteString(entry.getTargetPath() == null ? DFSUtilClient.EMPTY_BYTES : DFSUtilClient.byteArray2bytes(entry.getTargetPath()));\r\n    SnapshotDiffReportListingEntryProto.Builder builder = SnapshotDiffReportListingEntryProto.newBuilder().setFullpath(sourcePath).setDirId(dirId).setFileId(fileId).setIsReference(isReference).setTargetPath(targetPath);\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "SnapshotDiffReportListingProto convert(SnapshotDiffReportListing report)\n{\r\n    if (report == null) {\r\n        return null;\r\n    }\r\n    ByteString startPath = getByteString(report.getLastPath() == null ? DFSUtilClient.EMPTY_BYTES : report.getLastPath());\r\n    List<DiffReportListingEntry> modifiedEntries = report.getModifyList();\r\n    List<DiffReportListingEntry> createdEntries = report.getCreateList();\r\n    List<DiffReportListingEntry> deletedEntries = report.getDeleteList();\r\n    List<SnapshotDiffReportListingEntryProto> modifiedEntryProtos = new ChunkedArrayList<>();\r\n    for (DiffReportListingEntry entry : modifiedEntries) {\r\n        SnapshotDiffReportListingEntryProto entryProto = convert(entry);\r\n        if (entryProto != null) {\r\n            modifiedEntryProtos.add(entryProto);\r\n        }\r\n    }\r\n    List<SnapshotDiffReportListingEntryProto> createdEntryProtos = new ChunkedArrayList<>();\r\n    for (DiffReportListingEntry entry : createdEntries) {\r\n        SnapshotDiffReportListingEntryProto entryProto = convert(entry);\r\n        if (entryProto != null) {\r\n            createdEntryProtos.add(entryProto);\r\n        }\r\n    }\r\n    List<SnapshotDiffReportListingEntryProto> deletedEntryProtos = new ChunkedArrayList<>();\r\n    for (DiffReportListingEntry entry : deletedEntries) {\r\n        SnapshotDiffReportListingEntryProto entryProto = convert(entry);\r\n        if (entryProto != null) {\r\n            deletedEntryProtos.add(entryProto);\r\n        }\r\n    }\r\n    return SnapshotDiffReportListingProto.newBuilder().addAllModifiedEntries(modifiedEntryProtos).addAllCreatedEntries(createdEntryProtos).addAllDeletedEntries(deletedEntryProtos).setIsFromEarlier(report.getIsFromEarlier()).setCursor(HdfsProtos.SnapshotDiffReportCursorProto.newBuilder().setStartPath(startPath).setIndex(report.getLastIndex()).build()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "SnapshotDiffReportProto convert(SnapshotDiffReport report)\n{\r\n    if (report == null) {\r\n        return null;\r\n    }\r\n    List<DiffReportEntry> entries = report.getDiffList();\r\n    List<SnapshotDiffReportEntryProto> entryProtos = new ChunkedArrayList<>();\r\n    for (DiffReportEntry entry : entries) {\r\n        SnapshotDiffReportEntryProto entryProto = convert(entry);\r\n        if (entryProto != null)\r\n            entryProtos.add(entryProto);\r\n    }\r\n    return SnapshotDiffReportProto.newBuilder().setSnapshotRoot(report.getSnapshotRoot()).setFromSnapshot(report.getFromSnapshot()).setToSnapshot(report.getLaterSnapshotName()).addAllDiffReportEntries(entryProtos).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "CacheDirectiveStatsProto convert(CacheDirectiveStats stats)\n{\r\n    CacheDirectiveStatsProto.Builder builder = CacheDirectiveStatsProto.newBuilder();\r\n    builder.setBytesNeeded(stats.getBytesNeeded());\r\n    builder.setBytesCached(stats.getBytesCached());\r\n    builder.setFilesNeeded(stats.getFilesNeeded());\r\n    builder.setFilesCached(stats.getFilesCached());\r\n    builder.setHasExpired(stats.hasExpired());\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "CacheDirectiveEntryProto convert(CacheDirectiveEntry entry)\n{\r\n    CacheDirectiveEntryProto.Builder builder = CacheDirectiveEntryProto.newBuilder();\r\n    builder.setInfo(convert(entry.getInfo()));\r\n    builder.setStats(convert(entry.getStats()));\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertBooleanList",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean[] convertBooleanList(List<Boolean> targetPinningsList)\n{\r\n    final boolean[] targetPinnings = new boolean[targetPinningsList.size()];\r\n    for (int i = 0; i < targetPinningsList.size(); i++) {\r\n        targetPinnings[i] = targetPinningsList.get(i);\r\n    }\r\n    return targetPinnings;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "CachePoolStatsProto convert(CachePoolStats stats)\n{\r\n    CachePoolStatsProto.Builder builder = CachePoolStatsProto.newBuilder();\r\n    builder.setBytesNeeded(stats.getBytesNeeded());\r\n    builder.setBytesCached(stats.getBytesCached());\r\n    builder.setBytesOverlimit(stats.getBytesOverlimit());\r\n    builder.setFilesNeeded(stats.getFilesNeeded());\r\n    builder.setFilesCached(stats.getFilesCached());\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "CachePoolEntryProto convert(CachePoolEntry entry)\n{\r\n    CachePoolEntryProto.Builder builder = CachePoolEntryProto.newBuilder();\r\n    builder.setInfo(convert(entry.getInfo()));\r\n    builder.setStats(convert(entry.getStats()));\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "DatanodeLocalInfoProto convert(DatanodeLocalInfo info)\n{\r\n    DatanodeLocalInfoProto.Builder builder = DatanodeLocalInfoProto.newBuilder();\r\n    builder.setSoftwareVersion(info.getSoftwareVersion());\r\n    builder.setConfigVersion(info.getConfigVersion());\r\n    builder.setUptime(info.getUptime());\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "GetAclStatusResponseProto convert(AclStatus e)\n{\r\n    AclStatusProto.Builder builder = AclStatusProto.newBuilder();\r\n    builder.setOwner(e.getOwner()).setGroup(e.getGroup()).setSticky(e.isStickyBit()).addAllEntries(convertAclEntryProto(e.getEntries()));\r\n    if (e.getPermission() != null) {\r\n        builder.setPermission(convert(e.getPermission()));\r\n    }\r\n    AclStatusProto r = builder.build();\r\n    return GetAclStatusResponseProto.newBuilder().setResult(r).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "EnumSet<XAttrSetFlag> convert(int flag)\n{\r\n    EnumSet<XAttrSetFlag> result = EnumSet.noneOf(XAttrSetFlag.class);\r\n    if ((flag & XAttrSetFlagProto.XATTR_CREATE_VALUE) == XAttrSetFlagProto.XATTR_CREATE_VALUE) {\r\n        result.add(XAttrSetFlag.CREATE);\r\n    }\r\n    if ((flag & XAttrSetFlagProto.XATTR_REPLACE_VALUE) == XAttrSetFlagProto.XATTR_REPLACE_VALUE) {\r\n        result.add(XAttrSetFlag.REPLACE);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertXAttr",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "XAttr convertXAttr(XAttrProto a)\n{\r\n    XAttr.Builder builder = new XAttr.Builder();\r\n    builder.setNameSpace(convert(a.getNamespace()));\r\n    if (a.hasName()) {\r\n        builder.setName(a.getName());\r\n    }\r\n    if (a.hasValue()) {\r\n        builder.setValue(a.getValue().toByteArray());\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertXAttrsResponse",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "GetXAttrsResponseProto convertXAttrsResponse(List<XAttr> xAttrs)\n{\r\n    GetXAttrsResponseProto.Builder builder = GetXAttrsResponseProto.newBuilder();\r\n    if (xAttrs != null) {\r\n        builder.addAllXAttrs(convertXAttrProto(xAttrs));\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertListXAttrsResponse",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ListXAttrsResponseProto convertListXAttrsResponse(List<XAttr> names)\n{\r\n    ListXAttrsResponseProto.Builder builder = ListXAttrsResponseProto.newBuilder();\r\n    if (names != null) {\r\n        builder.addAllXAttrs(convertXAttrProto(names));\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EncryptionZoneProto convert(EncryptionZone zone)\n{\r\n    return EncryptionZoneProto.newBuilder().setId(zone.getId()).setPath(zone.getPath()).setSuite(convert(zone.getSuite())).setCryptoProtocolVersion(convert(zone.getVersion())).setKeyName(zone.getKeyName()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SlotId convert(ShortCircuitShmSlotProto slotId)\n{\r\n    return new SlotId(convert(slotId.getShmId()), slotId.getSlotIdx());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertEditsResponse",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "GetEditsFromTxidResponseProto convertEditsResponse(EventBatchList el)\n{\r\n    InotifyProtos.EventsListProto.Builder builder = InotifyProtos.EventsListProto.newBuilder();\r\n    for (EventBatch b : el.getBatches()) {\r\n        List<InotifyProtos.EventProto> events = Lists.newArrayList();\r\n        for (Event e : b.getEvents()) {\r\n            switch(e.getEventType()) {\r\n                case CLOSE:\r\n                    Event.CloseEvent ce = (Event.CloseEvent) e;\r\n                    events.add(InotifyProtos.EventProto.newBuilder().setType(InotifyProtos.EventType.EVENT_CLOSE).setContents(InotifyProtos.CloseEventProto.newBuilder().setPath(ce.getPath()).setFileSize(ce.getFileSize()).setTimestamp(ce.getTimestamp()).build().toByteString()).build());\r\n                    break;\r\n                case CREATE:\r\n                    Event.CreateEvent ce2 = (Event.CreateEvent) e;\r\n                    InotifyProtos.CreateEventProto.Builder pB = (InotifyProtos.CreateEventProto.newBuilder());\r\n                    pB.setType(createTypeConvert(ce2.getiNodeType())).setPath(ce2.getPath()).setCtime(ce2.getCtime()).setOwnerName(ce2.getOwnerName()).setGroupName(ce2.getGroupName()).setPerms(convert(ce2.getPerms())).setReplication(ce2.getReplication()).setSymlinkTarget(ce2.getSymlinkTarget() == null ? \"\" : ce2.getSymlinkTarget()).setDefaultBlockSize(ce2.getDefaultBlockSize()).setOverwrite(ce2.getOverwrite());\r\n                    if (ce2.isErasureCoded().isPresent()) {\r\n                        pB.setErasureCoded(ce2.isErasureCoded().get());\r\n                    }\r\n                    events.add(InotifyProtos.EventProto.newBuilder().setType(InotifyProtos.EventType.EVENT_CREATE).setContents(pB.build().toByteString()).build());\r\n                    break;\r\n                case METADATA:\r\n                    Event.MetadataUpdateEvent me = (Event.MetadataUpdateEvent) e;\r\n                    InotifyProtos.MetadataUpdateEventProto.Builder metaB = InotifyProtos.MetadataUpdateEventProto.newBuilder().setPath(me.getPath()).setType(metadataUpdateTypeConvert(me.getMetadataType())).setMtime(me.getMtime()).setAtime(me.getAtime()).setReplication(me.getReplication()).setOwnerName(me.getOwnerName() == null ? \"\" : me.getOwnerName()).setGroupName(me.getGroupName() == null ? \"\" : me.getGroupName()).addAllAcls(me.getAcls() == null ? Lists.<AclEntryProto>newArrayList() : convertAclEntryProto(me.getAcls())).addAllXAttrs(me.getxAttrs() == null ? Lists.<XAttrProto>newArrayList() : convertXAttrProto(me.getxAttrs())).setXAttrsRemoved(me.isxAttrsRemoved());\r\n                    if (me.getPerms() != null) {\r\n                        metaB.setPerms(convert(me.getPerms()));\r\n                    }\r\n                    events.add(InotifyProtos.EventProto.newBuilder().setType(InotifyProtos.EventType.EVENT_METADATA).setContents(metaB.build().toByteString()).build());\r\n                    break;\r\n                case RENAME:\r\n                    Event.RenameEvent re = (Event.RenameEvent) e;\r\n                    events.add(InotifyProtos.EventProto.newBuilder().setType(InotifyProtos.EventType.EVENT_RENAME).setContents(InotifyProtos.RenameEventProto.newBuilder().setSrcPath(re.getSrcPath()).setDestPath(re.getDstPath()).setTimestamp(re.getTimestamp()).build().toByteString()).build());\r\n                    break;\r\n                case APPEND:\r\n                    Event.AppendEvent re2 = (Event.AppendEvent) e;\r\n                    events.add(InotifyProtos.EventProto.newBuilder().setType(InotifyProtos.EventType.EVENT_APPEND).setContents(InotifyProtos.AppendEventProto.newBuilder().setPath(re2.getPath()).setNewBlock(re2.toNewBlock()).build().toByteString()).build());\r\n                    break;\r\n                case UNLINK:\r\n                    Event.UnlinkEvent ue = (Event.UnlinkEvent) e;\r\n                    events.add(InotifyProtos.EventProto.newBuilder().setType(InotifyProtos.EventType.EVENT_UNLINK).setContents(InotifyProtos.UnlinkEventProto.newBuilder().setPath(ue.getPath()).setTimestamp(ue.getTimestamp()).build().toByteString()).build());\r\n                    break;\r\n                case TRUNCATE:\r\n                    Event.TruncateEvent te = (Event.TruncateEvent) e;\r\n                    events.add(InotifyProtos.EventProto.newBuilder().setType(InotifyProtos.EventType.EVENT_TRUNCATE).setContents(InotifyProtos.TruncateEventProto.newBuilder().setPath(te.getPath()).setFileSize(te.getFileSize()).setTimestamp(te.getTimestamp()).build().toByteString()).build());\r\n                    break;\r\n                default:\r\n                    throw new RuntimeException(\"Unexpected inotify event: \" + e);\r\n            }\r\n        }\r\n        builder.addBatch(InotifyProtos.EventBatchProto.newBuilder().setTxid(b.getTxid()).addAllEvents(events));\r\n    }\r\n    builder.setFirstTxid(el.getFirstTxid());\r\n    builder.setLastTxid(el.getLastTxid());\r\n    builder.setSyncTxid(el.getSyncTxid());\r\n    return GetEditsFromTxidResponseProto.newBuilder().setEventsList(builder.build()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertCryptoProtocolVersions",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "CryptoProtocolVersion[] convertCryptoProtocolVersions(List<CryptoProtocolVersionProto> protos)\n{\r\n    List<CryptoProtocolVersion> versions = Lists.newArrayListWithCapacity(protos.size());\r\n    for (CryptoProtocolVersionProto p : protos) {\r\n        versions.add(convert(p));\r\n    }\r\n    return versions.toArray(new CryptoProtocolVersion[] {});\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertPerFileEncInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsProtos.PerFileEncryptionInfoProto convertPerFileEncInfo(FileEncryptionInfo info)\n{\r\n    if (info == null) {\r\n        return null;\r\n    }\r\n    return HdfsProtos.PerFileEncryptionInfoProto.newBuilder().setKey(getByteString(info.getEncryptedDataEncryptionKey())).setIv(getByteString(info.getIV())).setEzKeyVersionName(info.getEzKeyVersionName()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ZoneEncryptionInfoProto convert(CipherSuite suite, CryptoProtocolVersion version, String keyName)\n{\r\n    return convert(suite, version, keyName, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ZoneEncryptionInfoProto convert(CipherSuite suite, CryptoProtocolVersion version, String keyName, ReencryptionInfoProto proto)\n{\r\n    if (suite == null || version == null || keyName == null) {\r\n        return null;\r\n    }\r\n    ZoneEncryptionInfoProto.Builder builder = ZoneEncryptionInfoProto.newBuilder().setSuite(convert(suite)).setCryptoProtocolVersion(convert(version)).setKeyName(keyName);\r\n    if (proto != null) {\r\n        builder.setReencryptionProto(proto);\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FileEncryptionInfo convert(HdfsProtos.PerFileEncryptionInfoProto fileProto, CipherSuite suite, CryptoProtocolVersion version, String keyName)\n{\r\n    if (fileProto == null || suite == null || version == null || keyName == null) {\r\n        return null;\r\n    }\r\n    byte[] key = fileProto.getKey().toByteArray();\r\n    byte[] iv = fileProto.getIv().toByteArray();\r\n    String ezKeyVersionName = fileProto.getEzKeyVersionName();\r\n    return new FileEncryptionInfo(suite, version, key, iv, keyName, ezKeyVersionName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ReencryptionInfoProto convert(String ezkvn, Long submissionTime, boolean isCanceled, long numReencrypted, long numFailures, Long completionTime, String lastFile)\n{\r\n    if (ezkvn == null || submissionTime == null) {\r\n        return null;\r\n    }\r\n    ReencryptionInfoProto.Builder builder = ReencryptionInfoProto.newBuilder().setEzKeyVersionName(ezkvn).setSubmissionTime(submissionTime).setCanceled(isCanceled).setNumReencrypted(numReencrypted).setNumFailures(numFailures);\r\n    if (completionTime != null) {\r\n        builder.setCompletionTime(completionTime);\r\n    }\r\n    if (lastFile != null) {\r\n        builder.setLastFile(lastFile);\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "ZoneReencryptionStatusProto convert(ZoneReencryptionStatus zs)\n{\r\n    ZoneReencryptionStatusProto.Builder builder = ZoneReencryptionStatusProto.newBuilder().setId(zs.getId()).setPath(zs.getZoneName()).setEzKeyVersionName(zs.getEzKeyVersionName()).setSubmissionTime(zs.getSubmissionTime()).setCanceled(zs.isCanceled()).setNumReencrypted(zs.getFilesReencrypted()).setNumFailures(zs.getNumReencryptionFailures());\r\n    switch(zs.getState()) {\r\n        case Submitted:\r\n            builder.setState(ReencryptionStateProto.SUBMITTED);\r\n            break;\r\n        case Processing:\r\n            builder.setState(ReencryptionStateProto.PROCESSING);\r\n            break;\r\n        case Completed:\r\n            builder.setState(ReencryptionStateProto.COMPLETED);\r\n            break;\r\n        default:\r\n            throw new IllegalArgumentException(\"Unknown state \" + zs.getState());\r\n    }\r\n    final long completion = zs.getCompletionTime();\r\n    if (completion != 0) {\r\n        builder.setCompletionTime(completion);\r\n    }\r\n    final String file = zs.getLastCheckpointFile();\r\n    if (file != null) {\r\n        builder.setLastFile(file);\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ZoneReencryptionStatus convert(ZoneReencryptionStatusProto proto)\n{\r\n    ZoneReencryptionStatus.State state;\r\n    switch(proto.getState()) {\r\n        case SUBMITTED:\r\n            state = ZoneReencryptionStatus.State.Submitted;\r\n            break;\r\n        case PROCESSING:\r\n            state = ZoneReencryptionStatus.State.Processing;\r\n            break;\r\n        case COMPLETED:\r\n            state = ZoneReencryptionStatus.State.Completed;\r\n            break;\r\n        default:\r\n            throw new IllegalArgumentException(\"Unknown state \" + proto.getState());\r\n    }\r\n    ZoneReencryptionStatus.Builder builder = new ZoneReencryptionStatus.Builder().id(proto.getId()).zoneName(proto.getPath()).state(state).ezKeyVersionName(proto.getEzKeyVersionName()).submissionTime(proto.getSubmissionTime()).canceled(proto.getCanceled()).filesReencrypted(proto.getNumReencrypted()).fileReencryptionFailures(proto.getNumFailures());\r\n    if (proto.hasCompletionTime()) {\r\n        builder.completionTime(proto.getCompletionTime());\r\n    }\r\n    if (proto.hasLastFile()) {\r\n        builder.lastCheckpointFile(proto.getLastFile());\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DatanodeInfo[] convert(DatanodeInfosProto datanodeInfosProto)\n{\r\n    List<DatanodeInfoProto> proto = datanodeInfosProto.getDatanodesList();\r\n    DatanodeInfo[] infos = new DatanodeInfo[proto.size()];\r\n    for (int i = 0; i < infos.length; i++) {\r\n        infos[i] = convert(proto.get(i));\r\n    }\r\n    return infos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<DatanodeInfosProto> convert(DatanodeInfo[][] targets)\n{\r\n    DatanodeInfosProto[] ret = new DatanodeInfosProto[targets.length];\r\n    for (int i = 0; i < targets.length; i++) {\r\n        ret[i] = DatanodeInfosProto.newBuilder().addAllDatanodes(convert(targets[i])).build();\r\n    }\r\n    return Arrays.asList(ret);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertECSchema",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ECSchema convertECSchema(HdfsProtos.ECSchemaProto schema)\n{\r\n    List<HdfsProtos.ECSchemaOptionEntryProto> optionsList = schema.getOptionsList();\r\n    Map<String, String> options = new HashMap<>(optionsList.size());\r\n    for (HdfsProtos.ECSchemaOptionEntryProto option : optionsList) {\r\n        options.put(option.getKey(), option.getValue());\r\n    }\r\n    return new ECSchema(schema.getCodecName(), schema.getDataUnits(), schema.getParityUnits(), options);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertECSchema",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HdfsProtos.ECSchemaProto convertECSchema(ECSchema schema)\n{\r\n    HdfsProtos.ECSchemaProto.Builder builder = HdfsProtos.ECSchemaProto.newBuilder().setCodecName(schema.getCodecName()).setDataUnits(schema.getNumDataUnits()).setParityUnits(schema.getNumParityUnits());\r\n    Set<Map.Entry<String, String>> entrySet = schema.getExtraOptions().entrySet();\r\n    for (Map.Entry<String, String> entry : entrySet) {\r\n        builder.addOptions(HdfsProtos.ECSchemaOptionEntryProto.newBuilder().setKey(entry.getKey()).setValue(entry.getValue()).build());\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertECState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ErasureCodingPolicyState convertECState(HdfsProtos.ErasureCodingPolicyState state)\n{\r\n    return ErasureCodingPolicyState.fromValue(state.getNumber());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertECState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsProtos.ErasureCodingPolicyState convertECState(ErasureCodingPolicyState state)\n{\r\n    return HdfsProtos.ErasureCodingPolicyState.forNumber(state.getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ErasureCodingPolicy convertErasureCodingPolicy(ErasureCodingPolicyProto proto)\n{\r\n    final byte id = (byte) (proto.getId() & 0xFF);\r\n    ErasureCodingPolicy policy = SystemErasureCodingPolicies.getByID(id);\r\n    if (policy == null) {\r\n        Preconditions.checkArgument(proto.hasName(), \"Missing name field in ErasureCodingPolicy proto\");\r\n        Preconditions.checkArgument(proto.hasSchema(), \"Missing schema field in ErasureCodingPolicy proto\");\r\n        Preconditions.checkArgument(proto.hasCellSize(), \"Missing cellsize field in ErasureCodingPolicy proto\");\r\n        return new ErasureCodingPolicy(proto.getName(), convertECSchema(proto.getSchema()), proto.getCellSize(), id);\r\n    }\r\n    return policy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertErasureCodingPolicyInfo",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ErasureCodingPolicyInfo convertErasureCodingPolicyInfo(ErasureCodingPolicyProto proto)\n{\r\n    ErasureCodingPolicy policy = convertErasureCodingPolicy(proto);\r\n    ErasureCodingPolicyInfo info = new ErasureCodingPolicyInfo(policy);\r\n    Preconditions.checkArgument(proto.hasState(), \"Missing state field in ErasureCodingPolicy proto\");\r\n    info.setState(convertECState(proto.getState()));\r\n    return info;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "createECPolicyProtoBuilder",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ErasureCodingPolicyProto.Builder createECPolicyProtoBuilder(ErasureCodingPolicy policy)\n{\r\n    final ErasureCodingPolicyProto.Builder builder = ErasureCodingPolicyProto.newBuilder().setId(policy.getId());\r\n    if (SystemErasureCodingPolicies.getByID(policy.getId()) == null) {\r\n        builder.setName(policy.getName()).setSchema(convertECSchema(policy.getSchema())).setCellSize(policy.getCellSize());\r\n    }\r\n    return builder;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ErasureCodingPolicyProto convertErasureCodingPolicy(ErasureCodingPolicy policy)\n{\r\n    return createECPolicyProtoBuilder(policy).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ErasureCodingPolicyProto convertErasureCodingPolicy(ErasureCodingPolicyInfo info)\n{\r\n    final ErasureCodingPolicyProto.Builder builder = createECPolicyProtoBuilder(info.getPolicy());\r\n    builder.setState(convertECState(info.getState()));\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertErasureCodingCodec",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "CodecProto convertErasureCodingCodec(String codec, String coders)\n{\r\n    CodecProto.Builder builder = CodecProto.newBuilder().setCodec(codec).setCoders(coders);\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertAddErasureCodingPolicyResponse",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AddErasureCodingPolicyResponseProto convertAddErasureCodingPolicyResponse(AddErasureCodingPolicyResponse response)\n{\r\n    AddErasureCodingPolicyResponseProto.Builder builder = AddErasureCodingPolicyResponseProto.newBuilder().setPolicy(convertErasureCodingPolicy(response.getPolicy())).setSucceed(response.isSucceed());\r\n    if (!response.isSucceed()) {\r\n        builder.setErrorMsg(response.getErrorMsg());\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertAddErasureCodingPolicyResponse",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AddErasureCodingPolicyResponse convertAddErasureCodingPolicyResponse(AddErasureCodingPolicyResponseProto proto)\n{\r\n    ErasureCodingPolicy policy = convertErasureCodingPolicy(proto.getPolicy());\r\n    if (proto.getSucceed()) {\r\n        return new AddErasureCodingPolicyResponse(policy);\r\n    } else {\r\n        return new AddErasureCodingPolicyResponse(policy, proto.getErrorMsg());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertToProto",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "HdfsProtos.DatanodeInfosProto convertToProto(DatanodeInfo[] datanodeInfos)\n{\r\n    HdfsProtos.DatanodeInfosProto.Builder builder = HdfsProtos.DatanodeInfosProto.newBuilder();\r\n    for (DatanodeInfo datanodeInfo : datanodeInfos) {\r\n        builder.addDatanodes(PBHelperClient.convert(datanodeInfo));\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertECTopologyVerifierResultProto",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ECTopologyVerifierResult convertECTopologyVerifierResultProto(HdfsProtos.ECTopologyVerifierResultProto resp)\n{\r\n    return new ECTopologyVerifierResult(resp.getIsSupported(), resp.getResultMessage());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertECTopologyVerifierResult",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "HdfsProtos.ECTopologyVerifierResultProto convertECTopologyVerifierResult(ECTopologyVerifierResult resp)\n{\r\n    final HdfsProtos.ECTopologyVerifierResultProto.Builder builder = HdfsProtos.ECTopologyVerifierResultProto.newBuilder().setIsSupported(resp.isSupported()).setResultMessage(resp.getResultMessage());\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertAddBlockFlags",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "EnumSet<AddBlockFlag> convertAddBlockFlags(List<AddBlockFlagProto> addBlockFlags)\n{\r\n    EnumSet<AddBlockFlag> flags = EnumSet.noneOf(AddBlockFlag.class);\r\n    for (AddBlockFlagProto af : addBlockFlags) {\r\n        AddBlockFlag flag = AddBlockFlag.valueOf((short) af.getNumber());\r\n        if (flag != null) {\r\n            flags.add(flag);\r\n        }\r\n    }\r\n    return flags;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertAddBlockFlags",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<AddBlockFlagProto> convertAddBlockFlags(EnumSet<AddBlockFlag> flags)\n{\r\n    List<AddBlockFlagProto> ret = new ArrayList<>();\r\n    for (AddBlockFlag flag : flags) {\r\n        AddBlockFlagProto abfp = AddBlockFlagProto.forNumber(flag.getMode());\r\n        if (abfp != null) {\r\n            ret.add(abfp);\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ProvidedStorageLocation convert(HdfsProtos.ProvidedStorageLocationProto providedStorageLocationProto)\n{\r\n    if (providedStorageLocationProto == null) {\r\n        return null;\r\n    }\r\n    String path = providedStorageLocationProto.getPath();\r\n    long length = providedStorageLocationProto.getLength();\r\n    long offset = providedStorageLocationProto.getOffset();\r\n    ByteString nonce = providedStorageLocationProto.getNonce();\r\n    if (path == null || length == -1 || offset == -1 || nonce == null) {\r\n        return null;\r\n    } else {\r\n        return new ProvidedStorageLocation(new Path(path), offset, length, nonce.toByteArray());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "HdfsProtos.ProvidedStorageLocationProto convert(ProvidedStorageLocation providedStorageLocation)\n{\r\n    String path = providedStorageLocation.getPath().toString();\r\n    return HdfsProtos.ProvidedStorageLocationProto.newBuilder().setPath(path).setLength(providedStorageLocation.getLength()).setOffset(providedStorageLocation.getOffset()).setNonce(ByteString.copyFrom(providedStorageLocation.getNonce())).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertOpenFileTypes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "EnumSet<OpenFilesType> convertOpenFileTypes(List<OpenFilesTypeProto> openFilesTypeProtos)\n{\r\n    EnumSet<OpenFilesType> types = EnumSet.noneOf(OpenFilesType.class);\r\n    for (OpenFilesTypeProto af : openFilesTypeProtos) {\r\n        OpenFilesType type = OpenFilesType.valueOf((short) af.getNumber());\r\n        if (type != null) {\r\n            types.add(type);\r\n        }\r\n    }\r\n    return types;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "convertOpenFileTypes",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<OpenFilesTypeProto> convertOpenFileTypes(EnumSet<OpenFilesType> types)\n{\r\n    List<OpenFilesTypeProto> typeProtos = new ArrayList<>();\r\n    for (OpenFilesType type : types) {\r\n        OpenFilesTypeProto typeProto = OpenFilesTypeProto.forNumber(type.getMode());\r\n        if (typeProto != null) {\r\n            typeProtos.add(typeProto);\r\n        }\r\n    }\r\n    return typeProtos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getDatanodeInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DatanodeInfo getDatanodeInfo()\n{\r\n    return datanodeInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getStorageReports",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StorageReport[] getStorageReports()\n{\r\n    return storageReports;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOException get()\n{\r\n    return thrown;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "set",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void set(Throwable t)\n{\r\n    assert t != null;\r\n    this.thrown = t instanceof IOException ? (IOException) t : new IOException(t);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "clear",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void clear()\n{\r\n    thrown = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "check",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void check(boolean resetToNull) throws IOException\n{\r\n    if (thrown != null) {\r\n        final IOException e = thrown;\r\n        if (resetToNull) {\r\n            thrown = null;\r\n        }\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "throwException4Close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void throwException4Close() throws IOException\n{\r\n    check(false);\r\n    throw new ClosedChannelException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getParentIdx",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getParentIdx()\n{\r\n    return parentIdx;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getPartialListing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<HdfsFileStatus> getPartialListing()\n{\r\n    return partialListing;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RemoteException getException()\n{\r\n    return exception;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return new ToStringBuilder(this).append(\"partialListing\", partialListing).append(\"parentIdx\", parentIdx).append(\"exception\", exception).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Result getResult()\n{\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getPlanID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPlanID()\n{\r\n    return planID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getPlanFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPlanFile()\n{\r\n    return planFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getCurrentState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<DiskBalancerWorkEntry> getCurrentState()\n{\r\n    return currentState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "currentStateString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String currentStateString() throws IOException\n{\r\n    return MAPPER_WITH_INDENT_OUTPUT.writeValueAsString(currentState);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "toJsonString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toJsonString() throws IOException\n{\r\n    return MAPPER.writeValueAsString(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "parseJson",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DiskBalancerWorkStatus parseJson(String json) throws IOException\n{\r\n    return READER_WORKSTATUS.readValue(json);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "addWorkEntry",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addWorkEntry(DiskBalancerWorkEntry entry)\n{\r\n    Preconditions.checkNotNull(entry);\r\n    currentState.add(entry);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getValueString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getValueString()\n{\r\n    return value.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DFSClientFaultInjector get()\n{\r\n    return instance;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "set",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void set(DFSClientFaultInjector instance)\n{\r\n    DFSClientFaultInjector.instance = instance;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "corruptPacket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean corruptPacket()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "uncorruptPacket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean uncorruptPacket()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "failPacket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean failPacket()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "startFetchFromDatanode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void startFetchFromDatanode()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "fetchFromDatanodeException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void fetchFromDatanodeException()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readFromDatanodeDelay",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void readFromDatanodeDelay()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "skipRollingRestartWait",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean skipRollingRestartWait()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "sleepBeforeHedgedGet",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void sleepBeforeHedgedGet()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "delayWhenRenewLeaseTimeout",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void delayWhenRenewLeaseTimeout()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getMessage",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getMessage()\n{\r\n    String msg = super.getMessage();\r\n    if (msg == null) {\r\n        return \"Quota by storage type : \" + type.toString() + \" on path : \" + (pathName == null ? \"\" : pathName) + \" is exceeded. quota = \" + long2String(quota, \"B\", 2) + \" but space consumed = \" + long2String(count, \"B\", 2);\r\n    } else {\r\n        return msg;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl\\metrics",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BlockReaderLocalMetrics create()\n{\r\n    MetricsSystem ms = DefaultMetricsSystem.instance();\r\n    BlockReaderLocalMetrics metrics = new BlockReaderLocalMetrics();\r\n    ms.register(SHORT_CIRCUIT_READ_METRIC_REGISTERED_NAME, null, metrics);\r\n    return metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl\\metrics",
  "methodName" : "addShortCircuitReadLatency",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addShortCircuitReadLatency(final long latency)\n{\r\n    shortCircuitReadRollingAverages.add(SHORT_CIRCUIT_LOCAL_READS_METRIC_VALUE_NAME, latency);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl\\metrics",
  "methodName" : "collectThreadLocalStates",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void collectThreadLocalStates()\n{\r\n    shortCircuitReadRollingAverages.collectThreadLocalStates();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl\\metrics",
  "methodName" : "getShortCircuitReadRollingAverages",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MutableRollingAverages getShortCircuitReadRollingAverages()\n{\r\n    return shortCircuitReadRollingAverages;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getReplica",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ShortCircuitReplica getReplica()\n{\r\n    return replica;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getInvalidTokenException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InvalidToken getInvalidTokenException()\n{\r\n    return exc;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder builder = new StringBuilder();\r\n    String prefix = \"\";\r\n    builder.append(\"ShortCircuitReplicaInfo{\");\r\n    if (replica != null) {\r\n        builder.append(prefix).append(replica);\r\n        prefix = \", \";\r\n    }\r\n    if (exc != null) {\r\n        builder.append(prefix).append(exc);\r\n    }\r\n    builder.append(\"}\");\r\n    return builder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "get",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "KeyProvider get(final Configuration conf, final URI serverProviderUri)\n{\r\n    if (serverProviderUri == null) {\r\n        return null;\r\n    }\r\n    try {\r\n        return cache.get(serverProviderUri, new Callable<KeyProvider>() {\r\n\r\n            @Override\r\n            public KeyProvider call() throws Exception {\r\n                return KMSUtil.createKeyProviderFromUri(conf, serverProviderUri);\r\n            }\r\n        });\r\n    } catch (Exception e) {\r\n        LOG.error(\"Could not create KeyProvider for DFSClient !!\", e);\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "invalidateCache",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void invalidateCache()\n{\r\n    LOG.debug(\"Invalidating all cached KeyProviders.\");\r\n    if (cache != null) {\r\n        cache.invalidateAll();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createKeyProviderURI",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "URI createKeyProviderURI(Configuration conf)\n{\r\n    final String providerUriStr = conf.getTrimmed(CommonConfigurationKeysPublic.HADOOP_SECURITY_KEY_PROVIDER_PATH);\r\n    if (providerUriStr == null || providerUriStr.isEmpty()) {\r\n        LOG.error(\"Could not find uri with key [\" + CommonConfigurationKeysPublic.HADOOP_SECURITY_KEY_PROVIDER_PATH + \"] to create a keyProvider !!\");\r\n        return null;\r\n    }\r\n    final URI providerUri;\r\n    try {\r\n        providerUri = new URI(providerUriStr);\r\n    } catch (URISyntaxException e) {\r\n        LOG.error(\"KeyProvider URI string is invalid [\" + providerUriStr + \"]!!\", e.getCause());\r\n        return null;\r\n    }\r\n    return providerUri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setKeyProvider",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setKeyProvider(Configuration conf, KeyProvider keyProvider)\n{\r\n    URI uri = createKeyProviderURI(conf);\r\n    assert uri != null;\r\n    cache.put(uri, keyProvider);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "getKind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getKind()\n{\r\n    return KIND_NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "UserGroupInformation getUser()\n{\r\n    if (userId == null || \"\".equals(userId)) {\r\n        String user = blockPoolId + \":\" + Long.toString(blockId);\r\n        return UserGroupInformation.createRemoteUser(user);\r\n    }\r\n    return UserGroupInformation.createRemoteUser(userId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "getExpiryDate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getExpiryDate()\n{\r\n    return expiryDate;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "setExpiryDate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setExpiryDate(long expiryDate)\n{\r\n    this.cache = null;\r\n    this.expiryDate = expiryDate;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "getKeyId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getKeyId()\n{\r\n    return this.keyId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "setKeyId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setKeyId(int keyId)\n{\r\n    this.cache = null;\r\n    this.keyId = keyId;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "getUserId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUserId()\n{\r\n    return userId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "getBlockPoolId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockPoolId()\n{\r\n    return blockPoolId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "getBlockId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBlockId()\n{\r\n    return blockId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "getAccessModes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EnumSet<AccessMode> getAccessModes()\n{\r\n    return modes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "getStorageTypes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StorageType[] getStorageTypes()\n{\r\n    return storageTypes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "getStorageIds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] getStorageIds()\n{\r\n    return storageIds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "getHandshakeMsg",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getHandshakeMsg()\n{\r\n    return handshakeMsg;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "setHandshakeMsg",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHandshakeMsg(byte[] bytes)\n{\r\n    cache = null;\r\n    handshakeMsg = bytes;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String toString()\n{\r\n    return \"block_token_identifier (expiryDate=\" + this.getExpiryDate() + \", keyId=\" + this.getKeyId() + \", userId=\" + this.getUserId() + \", blockPoolId=\" + this.getBlockPoolId() + \", blockId=\" + this.getBlockId() + \", access modes=\" + this.getAccessModes() + \", storageTypes= \" + Arrays.toString(this.getStorageTypes()) + \", storageIds= \" + Arrays.toString(this.getStorageIds()) + \")\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "isEqual",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isEqual(Object a, Object b)\n{\r\n    return a == null ? b == null : a.equals(b);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (obj == this) {\r\n        return true;\r\n    }\r\n    if (obj instanceof BlockTokenIdentifier) {\r\n        BlockTokenIdentifier that = (BlockTokenIdentifier) obj;\r\n        return this.expiryDate == that.expiryDate && this.keyId == that.keyId && isEqual(this.userId, that.userId) && isEqual(this.blockPoolId, that.blockPoolId) && this.blockId == that.blockId && isEqual(this.modes, that.modes) && Arrays.equals(this.storageTypes, that.storageTypes) && Arrays.equals(this.storageIds, that.storageIds);\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int hashCode()\n{\r\n    return (int) expiryDate ^ keyId ^ (int) blockId ^ modes.hashCode() ^ (userId == null ? 0 : userId.hashCode()) ^ (blockPoolId == null ? 0 : blockPoolId.hashCode()) ^ (storageTypes == null ? 0 : Arrays.hashCode(storageTypes)) ^ (storageIds == null ? 0 : Arrays.hashCode(storageIds));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    this.cache = null;\r\n    final DataInputStream dis = (DataInputStream) in;\r\n    if (!dis.markSupported()) {\r\n        throw new IOException(\"Could not peek first byte.\");\r\n    }\r\n    this.cache = IOUtils.readFullyToByteArray(dis);\r\n    dis.reset();\r\n    dis.mark(1);\r\n    final byte firstByte = dis.readByte();\r\n    dis.reset();\r\n    if (firstByte <= 0) {\r\n        readFieldsLegacy(dis);\r\n    } else {\r\n        readFieldsProtobuf(dis);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "readFieldsLegacy",
  "errType" : [ "EOFException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void readFieldsLegacy(DataInput in) throws IOException\n{\r\n    expiryDate = WritableUtils.readVLong(in);\r\n    keyId = WritableUtils.readVInt(in);\r\n    userId = WritableUtils.readString(in);\r\n    blockPoolId = WritableUtils.readString(in);\r\n    blockId = WritableUtils.readVLong(in);\r\n    int length = WritableUtils.readVIntInRange(in, 0, AccessMode.class.getEnumConstants().length);\r\n    for (int i = 0; i < length; i++) {\r\n        modes.add(WritableUtils.readEnum(in, AccessMode.class));\r\n    }\r\n    try {\r\n        length = WritableUtils.readVInt(in);\r\n        StorageType[] readStorageTypes = new StorageType[length];\r\n        for (int i = 0; i < length; i++) {\r\n            readStorageTypes[i] = WritableUtils.readEnum(in, StorageType.class);\r\n        }\r\n        storageTypes = readStorageTypes;\r\n        length = WritableUtils.readVInt(in);\r\n        String[] readStorageIds = new String[length];\r\n        for (int i = 0; i < length; i++) {\r\n            readStorageIds[i] = WritableUtils.readString(in);\r\n        }\r\n        storageIds = readStorageIds;\r\n        int handshakeMsgLen = WritableUtils.readVInt(in);\r\n        if (handshakeMsgLen != 0) {\r\n            handshakeMsg = new byte[handshakeMsgLen];\r\n            in.readFully(handshakeMsg);\r\n        }\r\n    } catch (EOFException eof) {\r\n    }\r\n    useProto = false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "readFieldsProtobuf",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void readFieldsProtobuf(DataInput in) throws IOException\n{\r\n    BlockTokenSecretProto blockTokenSecretProto = BlockTokenSecretProto.parseFrom((DataInputStream) in);\r\n    expiryDate = blockTokenSecretProto.getExpiryDate();\r\n    keyId = blockTokenSecretProto.getKeyId();\r\n    if (blockTokenSecretProto.hasUserId()) {\r\n        userId = blockTokenSecretProto.getUserId();\r\n    } else {\r\n        userId = null;\r\n    }\r\n    if (blockTokenSecretProto.hasBlockPoolId()) {\r\n        blockPoolId = blockTokenSecretProto.getBlockPoolId();\r\n    } else {\r\n        blockPoolId = null;\r\n    }\r\n    blockId = blockTokenSecretProto.getBlockId();\r\n    for (int i = 0; i < blockTokenSecretProto.getModesCount(); i++) {\r\n        AccessModeProto accessModeProto = blockTokenSecretProto.getModes(i);\r\n        modes.add(PBHelperClient.convert(accessModeProto));\r\n    }\r\n    storageTypes = blockTokenSecretProto.getStorageTypesList().stream().map(PBHelperClient::convertStorageType).toArray(StorageType[]::new);\r\n    storageIds = blockTokenSecretProto.getStorageIdsList().stream().toArray(String[]::new);\r\n    useProto = true;\r\n    if (blockTokenSecretProto.hasHandshakeSecret()) {\r\n        handshakeMsg = blockTokenSecretProto.getHandshakeSecret().toByteArray();\r\n    } else {\r\n        handshakeMsg = new byte[0];\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    if (useProto) {\r\n        writeProtobuf(out);\r\n    } else {\r\n        writeLegacy(out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "writeLegacy",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void writeLegacy(DataOutput out) throws IOException\n{\r\n    WritableUtils.writeVLong(out, expiryDate);\r\n    WritableUtils.writeVInt(out, keyId);\r\n    WritableUtils.writeString(out, userId);\r\n    WritableUtils.writeString(out, blockPoolId);\r\n    WritableUtils.writeVLong(out, blockId);\r\n    WritableUtils.writeVInt(out, modes.size());\r\n    for (AccessMode aMode : modes) {\r\n        WritableUtils.writeEnum(out, aMode);\r\n    }\r\n    if (storageTypes != null) {\r\n        WritableUtils.writeVInt(out, storageTypes.length);\r\n        for (StorageType type : storageTypes) {\r\n            WritableUtils.writeEnum(out, type);\r\n        }\r\n    }\r\n    if (storageIds != null) {\r\n        WritableUtils.writeVInt(out, storageIds.length);\r\n        for (String id : storageIds) {\r\n            WritableUtils.writeString(out, id);\r\n        }\r\n    }\r\n    if (handshakeMsg != null && handshakeMsg.length > 0) {\r\n        WritableUtils.writeVInt(out, handshakeMsg.length);\r\n        out.write(handshakeMsg);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "writeProtobuf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writeProtobuf(DataOutput out) throws IOException\n{\r\n    BlockTokenSecretProto secret = PBHelperClient.convert(this);\r\n    out.write(secret.toByteArray());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "getBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getBytes()\n{\r\n    if (cache == null)\r\n        cache = super.getBytes();\r\n    return cache;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String toString(EnumSet<E> set)\n{\r\n    if (set == null || set.isEmpty()) {\r\n        return \"\";\r\n    } else {\r\n        final StringBuilder b = new StringBuilder();\r\n        final Iterator<E> i = set.iterator();\r\n        b.append(i.next());\r\n        for (; i.hasNext(); ) {\r\n            b.append(',').append(i.next());\r\n        }\r\n        return b.toString();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "toEnumSet",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "EnumSet<E> toEnumSet(final Class<E> clazz, final E[] values)\n{\r\n    final EnumSet<E> set = EnumSet.noneOf(clazz);\r\n    set.addAll(Arrays.asList(values));\r\n    return set;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return getName() + \"=\" + toString(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getValueString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getValueString()\n{\r\n    return toString(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "checkSaslComplete",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkSaslComplete(SaslParticipant sasl, Map<String, String> saslProps) throws IOException\n{\r\n    if (!sasl.isComplete()) {\r\n        throw new IOException(\"Failed to complete SASL handshake\");\r\n    }\r\n    Set<String> requestedQop = ImmutableSet.copyOf(Arrays.asList(saslProps.get(Sasl.QOP).split(\",\")));\r\n    String negotiatedQop = sasl.getNegotiatedQop();\r\n    LOG.debug(\"Verifying QOP, requested QOP = {}, negotiated QOP = {}\", requestedQop, negotiatedQop);\r\n    if (!requestedQop.contains(negotiatedQop)) {\r\n        throw new IOException(String.format(\"SASL handshake completed, but \" + \"channel does not have acceptable quality of protection, \" + \"requested = %s, negotiated = %s\", requestedQop, negotiatedQop));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "requestedQopContainsPrivacy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean requestedQopContainsPrivacy(Map<String, String> saslProps)\n{\r\n    Set<String> requestedQop = ImmutableSet.copyOf(Arrays.asList(saslProps.get(Sasl.QOP).split(\",\")));\r\n    return requestedQop.contains(\"auth-conf\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "createSaslPropertiesForEncryption",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<String, String> createSaslPropertiesForEncryption(String encryptionAlgorithm)\n{\r\n    Map<String, String> saslProps = Maps.newHashMapWithExpectedSize(3);\r\n    saslProps.put(Sasl.QOP, QualityOfProtection.PRIVACY.getSaslQop());\r\n    saslProps.put(Sasl.SERVER_AUTH, \"true\");\r\n    saslProps.put(\"com.sun.security.sasl.digest.cipher\", encryptionAlgorithm);\r\n    return saslProps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "encryptionKeyToPassword",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "char[] encryptionKeyToPassword(byte[] encryptionKey)\n{\r\n    return new String(Base64.encodeBase64(encryptionKey, false), Charsets.UTF_8).toCharArray();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "getPeerAddress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "InetAddress getPeerAddress(Peer peer)\n{\r\n    String remoteAddr = peer.getRemoteAddressString().split(\":\")[0];\r\n    int slashIdx = remoteAddr.indexOf('/');\r\n    return InetAddresses.forString(slashIdx != -1 ? remoteAddr.substring(slashIdx + 1, remoteAddr.length()) : remoteAddr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "getSaslPropertiesResolver",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "SaslPropertiesResolver getSaslPropertiesResolver(Configuration conf)\n{\r\n    String qops = conf.get(DFS_DATA_TRANSFER_PROTECTION_KEY);\r\n    if (qops == null || qops.isEmpty()) {\r\n        LOG.debug(\"DataTransferProtocol not using SaslPropertiesResolver, no \" + \"QOP found in configuration for {}\", DFS_DATA_TRANSFER_PROTECTION_KEY);\r\n        return null;\r\n    }\r\n    Configuration saslPropsResolverConf = new Configuration(conf);\r\n    saslPropsResolverConf.set(HADOOP_RPC_PROTECTION, qops);\r\n    Class<? extends SaslPropertiesResolver> resolverClass = conf.getClass(HADOOP_SECURITY_SASL_PROPS_RESOLVER_CLASS, SaslPropertiesResolver.class, SaslPropertiesResolver.class);\r\n    resolverClass = conf.getClass(DFS_DATA_TRANSFER_SASL_PROPS_RESOLVER_CLASS_KEY, resolverClass, SaslPropertiesResolver.class);\r\n    saslPropsResolverConf.setClass(HADOOP_SECURITY_SASL_PROPS_RESOLVER_CLASS, resolverClass, SaslPropertiesResolver.class);\r\n    SaslPropertiesResolver resolver = SaslPropertiesResolver.getInstance(saslPropsResolverConf);\r\n    LOG.debug(\"DataTransferProtocol using SaslPropertiesResolver, configured \" + \"QOP {} = {}, configured class {} = {}\", DFS_DATA_TRANSFER_PROTECTION_KEY, qops, DFS_DATA_TRANSFER_SASL_PROPS_RESOLVER_CLASS_KEY, resolverClass);\r\n    return resolver;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "readSaslMessage",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "T readSaslMessage(InputStream in, Function<DataTransferEncryptorMessageProto, ? extends T> handler) throws IOException\n{\r\n    DataTransferEncryptorMessageProto proto = DataTransferEncryptorMessageProto.parseFrom(vintPrefixed(in));\r\n    switch(proto.getStatus()) {\r\n        case ERROR_UNKNOWN_KEY:\r\n            throw new InvalidEncryptionKeyException(proto.getMessage());\r\n        case ERROR:\r\n            if (proto.hasAccessTokenError() && proto.getAccessTokenError()) {\r\n                throw new InvalidBlockTokenException(proto.getMessage());\r\n            }\r\n            throw new IOException(proto.getMessage());\r\n        case SUCCESS:\r\n            return handler.apply(proto);\r\n        default:\r\n            throw new IOException(\"Unknown status: \" + proto.getStatus() + \", message: \" + proto.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "readSaslMessage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] readSaslMessage(InputStream in) throws IOException\n{\r\n    return readSaslMessage(in, proto -> proto.getPayload().toByteArray());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "readSaslMessageAndNegotiationCipherOptions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] readSaslMessageAndNegotiationCipherOptions(InputStream in, List<CipherOption> cipherOptions) throws IOException\n{\r\n    return readSaslMessage(in, proto -> {\r\n        List<CipherOptionProto> optionProtos = proto.getCipherOptionList();\r\n        if (optionProtos != null) {\r\n            for (CipherOptionProto optionProto : optionProtos) {\r\n                cipherOptions.add(PBHelperClient.convert(optionProto));\r\n            }\r\n        }\r\n        return proto.getPayload().toByteArray();\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "readSaslMessageWithHandshakeSecret",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SaslMessageWithHandshake readSaslMessageWithHandshakeSecret(InputStream in) throws IOException\n{\r\n    return readSaslMessage(in, proto -> {\r\n        byte[] payload = proto.getPayload().toByteArray();\r\n        byte[] secret = null;\r\n        String bpid = null;\r\n        if (proto.hasHandshakeSecret()) {\r\n            HandshakeSecretProto handshakeSecret = proto.getHandshakeSecret();\r\n            secret = handshakeSecret.getSecret().toByteArray();\r\n            bpid = handshakeSecret.getBpid();\r\n        }\r\n        return new SaslMessageWithHandshake(payload, secret, bpid);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "negotiateCipherOption",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "CipherOption negotiateCipherOption(Configuration conf, List<CipherOption> options) throws IOException\n{\r\n    String cipherSuites = conf.get(DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY);\r\n    if (cipherSuites == null || cipherSuites.isEmpty()) {\r\n        return null;\r\n    }\r\n    if (!cipherSuites.equals(CipherSuite.AES_CTR_NOPADDING.getName()) && !cipherSuites.equals(CipherSuite.SM4_CTR_NOPADDING.getName())) {\r\n        throw new IOException(String.format(\"Invalid cipher suite, %s=%s\", DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY, cipherSuites));\r\n    }\r\n    if (options != null) {\r\n        for (CipherOption option : options) {\r\n            CipherSuite suite = option.getCipherSuite();\r\n            if (suite == CipherSuite.AES_CTR_NOPADDING || suite == CipherSuite.SM4_CTR_NOPADDING) {\r\n                int keyLen = conf.getInt(DFS_ENCRYPT_DATA_TRANSFER_CIPHER_KEY_BITLENGTH_KEY, DFS_ENCRYPT_DATA_TRANSFER_CIPHER_KEY_BITLENGTH_DEFAULT) / 8;\r\n                CryptoCodec codec = CryptoCodec.getInstance(conf, suite);\r\n                byte[] inKey = new byte[keyLen];\r\n                byte[] inIv = new byte[suite.getAlgorithmBlockSize()];\r\n                byte[] outKey = new byte[keyLen];\r\n                byte[] outIv = new byte[suite.getAlgorithmBlockSize()];\r\n                assert codec != null;\r\n                codec.generateSecureRandom(inKey);\r\n                codec.generateSecureRandom(inIv);\r\n                codec.generateSecureRandom(outKey);\r\n                codec.generateSecureRandom(outIv);\r\n                codec.close();\r\n                return new CipherOption(suite, inKey, inIv, outKey, outIv);\r\n            }\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "sendSaslMessageAndNegotiatedCipherOption",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option) throws IOException\n{\r\n    DataTransferEncryptorMessageProto.Builder builder = DataTransferEncryptorMessageProto.newBuilder();\r\n    builder.setStatus(DataTransferEncryptorStatus.SUCCESS);\r\n    if (payload != null) {\r\n        builder.setPayload(ByteString.copyFrom(payload));\r\n    }\r\n    if (option != null) {\r\n        builder.addCipherOption(PBHelperClient.convert(option));\r\n    }\r\n    DataTransferEncryptorMessageProto proto = builder.build();\r\n    proto.writeDelimitedTo(out);\r\n    out.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "createStreamPair",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer) throws IOException\n{\r\n    LOG.debug(\"Creating IOStreamPair of CryptoInputStream and \" + \"CryptoOutputStream.\");\r\n    CryptoCodec codec = CryptoCodec.getInstance(conf, cipherOption.getCipherSuite());\r\n    byte[] inKey = cipherOption.getInKey();\r\n    byte[] inIv = cipherOption.getInIv();\r\n    byte[] outKey = cipherOption.getOutKey();\r\n    byte[] outIv = cipherOption.getOutIv();\r\n    InputStream cIn = new CryptoInputStream(in, codec, isServer ? inKey : outKey, isServer ? inIv : outIv);\r\n    OutputStream cOut = new CryptoOutputStream(out, codec, isServer ? outKey : inKey, isServer ? outIv : inIv);\r\n    return new IOStreamPair(cIn, cOut);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "sendGenericSaslErrorMessage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sendGenericSaslErrorMessage(OutputStream out, String message) throws IOException\n{\r\n    sendSaslMessage(out, DataTransferEncryptorStatus.ERROR, null, message);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "sendSaslMessage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sendSaslMessage(OutputStream out, byte[] payload) throws IOException\n{\r\n    sendSaslMessage(out, DataTransferEncryptorStatus.SUCCESS, payload, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "sendSaslMessageHandshakeSecret",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sendSaslMessageHandshakeSecret(OutputStream out, byte[] payload, byte[] secret, String bpid) throws IOException\n{\r\n    sendSaslMessageHandshakeSecret(out, DataTransferEncryptorStatus.SUCCESS, payload, null, secret, bpid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "sendSaslMessageAndNegotiationCipherOptions",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List<CipherOption> options) throws IOException\n{\r\n    DataTransferEncryptorMessageProto.Builder builder = DataTransferEncryptorMessageProto.newBuilder();\r\n    builder.setStatus(DataTransferEncryptorStatus.SUCCESS);\r\n    if (payload != null) {\r\n        builder.setPayload(ByteString.copyFrom(payload));\r\n    }\r\n    if (options != null) {\r\n        builder.addAllCipherOption(PBHelperClient.convertCipherOptions(options));\r\n    }\r\n    DataTransferEncryptorMessageProto proto = builder.build();\r\n    proto.writeDelimitedTo(out);\r\n    out.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "readSaslMessageAndNegotiatedCipherOption",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SaslResponseWithNegotiatedCipherOption readSaslMessageAndNegotiatedCipherOption(InputStream in) throws IOException\n{\r\n    return readSaslMessage(in, proto -> {\r\n        byte[] response = proto.getPayload().toByteArray();\r\n        List<CipherOption> options = PBHelperClient.convertCipherOptionProtos(proto.getCipherOptionList());\r\n        CipherOption option = null;\r\n        if (options != null && !options.isEmpty()) {\r\n            option = options.get(0);\r\n        }\r\n        return new SaslResponseWithNegotiatedCipherOption(response, option);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "wrap",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "CipherOption wrap(CipherOption option, SaslParticipant sasl) throws IOException\n{\r\n    if (option != null) {\r\n        byte[] inKey = option.getInKey();\r\n        if (inKey != null) {\r\n            inKey = sasl.wrap(inKey, 0, inKey.length);\r\n        }\r\n        byte[] outKey = option.getOutKey();\r\n        if (outKey != null) {\r\n            outKey = sasl.wrap(outKey, 0, outKey.length);\r\n        }\r\n        return new CipherOption(option.getCipherSuite(), inKey, option.getInIv(), outKey, option.getOutIv());\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "unwrap",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "CipherOption unwrap(CipherOption option, SaslParticipant sasl) throws IOException\n{\r\n    if (option != null) {\r\n        byte[] inKey = option.getInKey();\r\n        if (inKey != null) {\r\n            inKey = sasl.unwrap(inKey, 0, inKey.length);\r\n        }\r\n        byte[] outKey = option.getOutKey();\r\n        if (outKey != null) {\r\n            outKey = sasl.unwrap(outKey, 0, outKey.length);\r\n        }\r\n        return new CipherOption(option.getCipherSuite(), inKey, option.getInIv(), outKey, option.getOutIv());\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "sendSaslMessage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message) throws IOException\n{\r\n    sendSaslMessage(out, status, payload, message, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "sendSaslMessage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message, HandshakeSecretProto handshakeSecret) throws IOException\n{\r\n    sendSaslMessage(out, status, payload, message, handshakeSecret, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "sendSaslMessage",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message, HandshakeSecretProto handshakeSecret, boolean accessTokenError) throws IOException\n{\r\n    DataTransferEncryptorMessageProto.Builder builder = DataTransferEncryptorMessageProto.newBuilder();\r\n    builder.setStatus(status);\r\n    if (payload != null) {\r\n        builder.setPayload(ByteString.copyFrom(payload));\r\n    }\r\n    if (message != null) {\r\n        builder.setMessage(message);\r\n    }\r\n    if (handshakeSecret != null) {\r\n        builder.setHandshakeSecret(handshakeSecret);\r\n    }\r\n    if (accessTokenError) {\r\n        builder.setAccessTokenError(true);\r\n    }\r\n    DataTransferEncryptorMessageProto proto = builder.build();\r\n    proto.writeDelimitedTo(out);\r\n    out.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "sendSaslMessageHandshakeSecret",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void sendSaslMessageHandshakeSecret(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message, byte[] secret, String bpid) throws IOException\n{\r\n    HandshakeSecretProto.Builder builder = HandshakeSecretProto.newBuilder();\r\n    builder.setSecret(ByteString.copyFrom(secret));\r\n    builder.setBpid(bpid);\r\n    sendSaslMessage(out, status, payload, message, builder.build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void init()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addDeprecatedKeys",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addDeprecatedKeys()\n{\r\n    Configuration.addDeprecations(new DeprecationDelta[] { new DeprecationDelta(\"dfs.backup.address\", DeprecatedKeys.DFS_NAMENODE_BACKUP_ADDRESS_KEY), new DeprecationDelta(\"dfs.backup.http.address\", DeprecatedKeys.DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY), new DeprecationDelta(\"dfs.balance.bandwidthPerSec\", DeprecatedKeys.DFS_DATANODE_BALANCE_BANDWIDTHPERSEC_KEY), new DeprecationDelta(\"dfs.data.dir\", DeprecatedKeys.DFS_DATANODE_DATA_DIR_KEY), new DeprecationDelta(\"dfs.http.address\", HdfsClientConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY), new DeprecationDelta(\"dfs.https.address\", HdfsClientConfigKeys.DFS_NAMENODE_HTTPS_ADDRESS_KEY), new DeprecationDelta(\"dfs.max.objects\", DeprecatedKeys.DFS_NAMENODE_MAX_OBJECTS_KEY), new DeprecationDelta(\"dfs.name.dir\", DeprecatedKeys.DFS_NAMENODE_NAME_DIR_KEY), new DeprecationDelta(\"dfs.name.dir.restore\", DeprecatedKeys.DFS_NAMENODE_NAME_DIR_RESTORE_KEY), new DeprecationDelta(\"dfs.name.edits.dir\", DeprecatedKeys.DFS_NAMENODE_EDITS_DIR_KEY), new DeprecationDelta(\"dfs.read.prefetch.size\", HdfsClientConfigKeys.Read.PREFETCH_SIZE_KEY), new DeprecationDelta(\"dfs.safemode.extension\", DeprecatedKeys.DFS_NAMENODE_SAFEMODE_EXTENSION_KEY), new DeprecationDelta(\"dfs.safemode.threshold.pct\", DeprecatedKeys.DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_KEY), new DeprecationDelta(\"dfs.secondary.http.address\", DeprecatedKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY), new DeprecationDelta(\"dfs.socket.timeout\", HdfsClientConfigKeys.DFS_CLIENT_SOCKET_TIMEOUT_KEY), new DeprecationDelta(\"fs.checkpoint.dir\", DeprecatedKeys.DFS_NAMENODE_CHECKPOINT_DIR_KEY), new DeprecationDelta(\"fs.checkpoint.edits.dir\", DeprecatedKeys.DFS_NAMENODE_CHECKPOINT_EDITS_DIR_KEY), new DeprecationDelta(\"fs.checkpoint.period\", DeprecatedKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_KEY), new DeprecationDelta(\"heartbeat.recheck.interval\", DeprecatedKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY), new DeprecationDelta(\"dfs.https.client.keystore.resource\", DeprecatedKeys.DFS_CLIENT_HTTPS_KEYSTORE_RESOURCE_KEY), new DeprecationDelta(\"dfs.https.need.client.auth\", DeprecatedKeys.DFS_CLIENT_HTTPS_NEED_AUTH_KEY), new DeprecationDelta(\"slave.host.name\", DeprecatedKeys.DFS_DATANODE_HOST_NAME_KEY), new DeprecationDelta(\"session.id\", DeprecatedKeys.DFS_METRICS_SESSION_ID_KEY), new DeprecationDelta(\"dfs.access.time.precision\", DeprecatedKeys.DFS_NAMENODE_ACCESSTIME_PRECISION_KEY), new DeprecationDelta(\"dfs.replication.considerLoad\", DeprecatedKeys.DFS_NAMENODE_REDUNDANCY_CONSIDERLOAD_KEY), new DeprecationDelta(\"dfs.namenode.replication.considerLoad\", DeprecatedKeys.DFS_NAMENODE_REDUNDANCY_CONSIDERLOAD_KEY), new DeprecationDelta(\"dfs.namenode.replication.considerLoad.factor\", DeprecatedKeys.DFS_NAMENODE_REDUNDANCY_CONSIDERLOAD_FACTOR), new DeprecationDelta(\"dfs.replication.interval\", DeprecatedKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY), new DeprecationDelta(\"dfs.namenode.replication.interval\", DeprecatedKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY), new DeprecationDelta(\"dfs.replication.min\", DeprecatedKeys.DFS_NAMENODE_REPLICATION_MIN_KEY), new DeprecationDelta(\"dfs.replication.pending.timeout.sec\", DeprecatedKeys.DFS_NAMENODE_RECONSTRUCTION_PENDING_TIMEOUT_SEC_KEY), new DeprecationDelta(\"dfs.namenode.replication.pending.timeout-sec\", DeprecatedKeys.DFS_NAMENODE_RECONSTRUCTION_PENDING_TIMEOUT_SEC_KEY), new DeprecationDelta(\"dfs.max-repl-streams\", DeprecatedKeys.DFS_NAMENODE_REPLICATION_MAX_STREAMS_KEY), new DeprecationDelta(\"dfs.permissions\", DeprecatedKeys.DFS_PERMISSIONS_ENABLED_KEY), new DeprecationDelta(\"dfs.permissions.supergroup\", DeprecatedKeys.DFS_PERMISSIONS_SUPERUSERGROUP_KEY), new DeprecationDelta(\"dfs.write.packet.size\", HdfsClientConfigKeys.DFS_CLIENT_WRITE_PACKET_SIZE_KEY), new DeprecationDelta(\"dfs.block.size\", HdfsClientConfigKeys.DFS_BLOCK_SIZE_KEY), new DeprecationDelta(\"dfs.datanode.max.xcievers\", DeprecatedKeys.DFS_DATANODE_MAX_RECEIVER_THREADS_KEY), new DeprecationDelta(\"io.bytes.per.checksum\", HdfsClientConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY), new DeprecationDelta(\"dfs.federation.nameservices\", HdfsClientConfigKeys.DFS_NAMESERVICES), new DeprecationDelta(\"dfs.federation.nameservice.id\", DeprecatedKeys.DFS_NAMESERVICE_ID), new DeprecationDelta(\"dfs.encryption.key.provider.uri\", CommonConfigurationKeysPublic.HADOOP_SECURITY_KEY_PROVIDER_PATH) });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    init();\r\n    Configuration.dumpDeprecatedKeys();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "getAccessToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAccessToken() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration configuration)\n{\r\n    this.conf = configuration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "parseJson",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DiskBalancerWorkItem parseJson(String json) throws IOException\n{\r\n    Preconditions.checkNotNull(json);\r\n    return READER.readValue(json);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getErrMsg",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getErrMsg()\n{\r\n    return errMsg;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "setErrMsg",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setErrMsg(String errMsg)\n{\r\n    this.errMsg = errMsg;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getErrorCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getErrorCount()\n{\r\n    return errorCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "incErrorCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void incErrorCount()\n{\r\n    this.errorCount++;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getBytesCopied",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesCopied()\n{\r\n    return bytesCopied;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "setBytesCopied",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setBytesCopied(long bytesCopied)\n{\r\n    this.bytesCopied = bytesCopied;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "incCopiedSoFar",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void incCopiedSoFar(long delta)\n{\r\n    this.bytesCopied += delta;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getBytesToCopy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesToCopy()\n{\r\n    return bytesToCopy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getBlocksCopied",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBlocksCopied()\n{\r\n    return blocksCopied;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "incBlocksCopied",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void incBlocksCopied()\n{\r\n    blocksCopied++;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "toJson",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toJson() throws IOException\n{\r\n    return MAPPER.writeValueAsString(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "setErrorCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setErrorCount(long errorCount)\n{\r\n    this.errorCount = errorCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "setBlocksCopied",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setBlocksCopied(long blocksCopied)\n{\r\n    this.blocksCopied = blocksCopied;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getMaxDiskErrors",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMaxDiskErrors()\n{\r\n    return maxDiskErrors;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "setMaxDiskErrors",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMaxDiskErrors(long maxDiskErrors)\n{\r\n    this.maxDiskErrors = maxDiskErrors;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getTolerancePercent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTolerancePercent()\n{\r\n    return tolerancePercent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "setTolerancePercent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTolerancePercent(long tolerancePercent)\n{\r\n    this.tolerancePercent = tolerancePercent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getBandwidth",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBandwidth()\n{\r\n    return bandwidth;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "setBandwidth",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setBandwidth(long bandwidth)\n{\r\n    this.bandwidth = bandwidth;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "setStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStartTime(long startTime)\n{\r\n    this.startTime = startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getSecondsElapsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSecondsElapsed()\n{\r\n    return secondsElapsed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "setSecondsElapsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSecondsElapsed(long secondsElapsed)\n{\r\n    this.secondsElapsed = secondsElapsed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getInputStreamChannel",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReadableByteChannel getInputStreamChannel()\n{\r\n    return channel;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "setReadTimeout",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setReadTimeout(int timeoutMs) throws IOException\n{\r\n    socket.setAttribute(DomainSocket.RECEIVE_TIMEOUT, timeoutMs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getReceiveBufferSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getReceiveBufferSize() throws IOException\n{\r\n    return socket.getAttribute(DomainSocket.RECEIVE_BUFFER_SIZE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getTcpNoDelay",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getTcpNoDelay() throws IOException\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "setWriteTimeout",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setWriteTimeout(int timeoutMs) throws IOException\n{\r\n    socket.setAttribute(DomainSocket.SEND_TIMEOUT, timeoutMs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "isClosed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isClosed()\n{\r\n    return !socket.isOpen();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    socket.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getRemoteAddressString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRemoteAddressString()\n{\r\n    return \"unix:\" + socket.getPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getLocalAddressString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLocalAddressString()\n{\r\n    return \"<local>\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getInputStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InputStream getInputStream() throws IOException\n{\r\n    return in;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getOutputStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OutputStream getOutputStream() throws IOException\n{\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "isLocal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isLocal()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"DomainPeer(\" + getRemoteAddressString() + \")\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getDomainSocket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DomainSocket getDomainSocket()\n{\r\n    return socket;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "hasSecureChannel",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasSecureChannel()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getProxy",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ProxyInfo<T> getProxy()\n{\r\n    if (currentUsedHandler != null) {\r\n        return currentUsedHandler;\r\n    }\r\n    Map<String, ProxyInfo<T>> targetProxyInfos = new HashMap<>();\r\n    StringBuilder combinedInfo = new StringBuilder(\"[\");\r\n    for (int i = 0; i < proxies.size(); i++) {\r\n        ProxyInfo<T> pInfo = super.getProxy();\r\n        incrementProxyIndex();\r\n        targetProxyInfos.put(pInfo.proxyInfo, pInfo);\r\n        combinedInfo.append(pInfo.proxyInfo).append(',');\r\n    }\r\n    combinedInfo.append(']');\r\n    T wrappedProxy = (T) Proxy.newProxyInstance(RequestHedgingInvocationHandler.class.getClassLoader(), new Class<?>[] { xface }, new RequestHedgingInvocationHandler(targetProxyInfos));\r\n    currentUsedHandler = new ProxyInfo<T>(wrappedProxy, combinedInfo.toString());\r\n    return currentUsedHandler;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "performFailover",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void performFailover(T currentProxy)\n{\r\n    toIgnore = ((RequestHedgingInvocationHandler) Proxy.getInvocationHandler(currentUsedHandler.proxy)).currentUsedProxy.proxyInfo;\r\n    this.currentUsedHandler = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "logProxyException",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void logProxyException(Exception ex, String proxyInfo)\n{\r\n    if (isStandbyException(ex)) {\r\n        LOG.debug(\"Invocation returned standby exception on [{}]\", proxyInfo, ex);\r\n    } else {\r\n        LOG.warn(\"Invocation returned exception on [{}]\", proxyInfo, ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "isStandbyException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isStandbyException(Exception exception)\n{\r\n    if (exception instanceof RemoteException) {\r\n        return ((RemoteException) exception).unwrapRemoteException() instanceof StandbyException;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "unwrapExecutionException",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Exception unwrapExecutionException(ExecutionException ex)\n{\r\n    if (ex != null) {\r\n        Throwable cause = ex.getCause();\r\n        if (cause instanceof InvocationTargetException) {\r\n            return unwrapInvocationTargetException((InvocationTargetException) cause);\r\n        }\r\n    }\r\n    return ex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "unwrapInvocationTargetException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Exception unwrapInvocationTargetException(InvocationTargetException ex)\n{\r\n    if (ex != null) {\r\n        Throwable cause = ex.getCause();\r\n        if (cause instanceof Exception) {\r\n            return (Exception) cause;\r\n        }\r\n    }\r\n    return ex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getOffset",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Long getOffset()\n{\r\n    Long offset = getValue();\r\n    return (offset == null) ? Long.valueOf(0) : offset;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getResolvedPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getResolvedPath()\n{\r\n    boolean noRemainder = (remainder == null || \"\".equals(remainder));\r\n    Path target = new Path(linkTarget);\r\n    if (target.isUriPathAbsolute()) {\r\n        return noRemainder ? target : new Path(target, remainder);\r\n    } else {\r\n        return noRemainder ? new Path(preceding, target) : new Path(new Path(preceding, linkTarget), remainder);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getMessage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getMessage()\n{\r\n    String msg = super.getMessage();\r\n    if (msg != null) {\r\n        return msg;\r\n    }\r\n    return getResolvedPath().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "prepareDecodeInputs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void prepareDecodeInputs()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "prepareParityChunk",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean prepareParityChunk(int index)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "decode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void decode() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "updateState4SuccessRead",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateState4SuccessRead(StripingChunkReadResult result)\n{\r\n    Preconditions.checkArgument(result.state == StripingChunkReadResult.SUCCESSFUL);\r\n    readerInfos[result.index].setOffset(alignedStripe.getOffsetInBlock() + alignedStripe.getSpanInBlock());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkMissingBlocks",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkMissingBlocks() throws IOException\n{\r\n    if (alignedStripe.missingChunksNum > parityBlkNum) {\r\n        clearFutures();\r\n        throw new IOException(alignedStripe.missingChunksNum + \" missing blocks, the stripe is: \" + alignedStripe + \"; locatedBlocks is: \" + dfsStripedInputStream.getLocatedBlocks());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readDataForDecoding",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void readDataForDecoding() throws IOException\n{\r\n    prepareDecodeInputs();\r\n    for (int i = 0; i < dataBlkNum; i++) {\r\n        Preconditions.checkNotNull(alignedStripe.chunks[i]);\r\n        if (alignedStripe.chunks[i].state == StripingChunk.REQUESTED) {\r\n            if (!readChunk(targetBlocks[i], i)) {\r\n                alignedStripe.missingChunksNum++;\r\n            }\r\n        }\r\n    }\r\n    checkMissingBlocks();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readParityChunks",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void readParityChunks(int num) throws IOException\n{\r\n    for (int i = dataBlkNum, j = 0; i < dataBlkNum + parityBlkNum && j < num; i++) {\r\n        if (alignedStripe.chunks[i] == null) {\r\n            if (prepareParityChunk(i) && readChunk(targetBlocks[i], i)) {\r\n                j++;\r\n            } else {\r\n                alignedStripe.missingChunksNum++;\r\n            }\r\n        }\r\n    }\r\n    checkMissingBlocks();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getReadStrategies",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ByteBufferStrategy[] getReadStrategies(StripingChunk chunk)\n{\r\n    if (chunk.useByteBuffer()) {\r\n        ByteBufferStrategy strategy = new ByteBufferStrategy(chunk.getByteBuffer(), dfsStripedInputStream.getReadStatistics(), dfsStripedInputStream.getDFSClient());\r\n        return new ByteBufferStrategy[] { strategy };\r\n    }\r\n    ByteBufferStrategy[] strategies = new ByteBufferStrategy[chunk.getChunkBuffer().getSlices().size()];\r\n    for (int i = 0; i < strategies.length; i++) {\r\n        ByteBuffer buffer = chunk.getChunkBuffer().getSlice(i);\r\n        strategies[i] = new ByteBufferStrategy(buffer, dfsStripedInputStream.getReadStatistics(), dfsStripedInputStream.getDFSClient());\r\n    }\r\n    return strategies;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readToBuffer",
  "errType" : [ "ChecksumException", "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int readToBuffer(BlockReader blockReader, DatanodeInfo currentNode, ByteBufferStrategy strategy, ExtendedBlock currentBlock) throws IOException\n{\r\n    final int targetLength = strategy.getTargetLength();\r\n    int length = 0;\r\n    try {\r\n        while (length < targetLength) {\r\n            int ret = strategy.readFromBlock(blockReader);\r\n            if (ret < 0) {\r\n                throw new IOException(\"Unexpected EOS from the reader\");\r\n            }\r\n            length += ret;\r\n        }\r\n        return length;\r\n    } catch (ChecksumException ce) {\r\n        DFSClient.LOG.warn(\"Found Checksum error for \" + currentBlock + \" from \" + currentNode + \" at \" + ce.getPos());\r\n        strategy.getReadBuffer().clear();\r\n        corruptedBlocks.addCorruptedBlock(currentBlock, currentNode);\r\n        throw ce;\r\n    } catch (IOException e) {\r\n        DFSClient.LOG.warn(\"Exception while reading from \" + currentBlock + \" of \" + dfsStripedInputStream.getSrc() + \" from \" + currentNode, e);\r\n        strategy.getReadBuffer().clear();\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readCells",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Callable<BlockReadStats> readCells(final BlockReader reader, final DatanodeInfo datanode, final long currentReaderOffset, final long targetReaderOffset, final ByteBufferStrategy[] strategies, final ExtendedBlock currentBlock)\n{\r\n    return () -> {\r\n        if (reader == null) {\r\n            throw new IOException(\"The BlockReader is null. \" + \"The BlockReader creation failed or the reader hit exception.\");\r\n        }\r\n        Preconditions.checkState(currentReaderOffset <= targetReaderOffset);\r\n        if (currentReaderOffset < targetReaderOffset) {\r\n            long skipped = reader.skip(targetReaderOffset - currentReaderOffset);\r\n            Preconditions.checkState(skipped == targetReaderOffset - currentReaderOffset);\r\n        }\r\n        int ret = 0;\r\n        for (ByteBufferStrategy strategy : strategies) {\r\n            int bytesReead = readToBuffer(reader, datanode, strategy, currentBlock);\r\n            ret += bytesReead;\r\n        }\r\n        return new BlockReadStats(ret, reader.isShortCircuit(), reader.getNetworkDistance());\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readChunk",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean readChunk(final LocatedBlock block, int chunkIndex) throws IOException\n{\r\n    final StripingChunk chunk = alignedStripe.chunks[chunkIndex];\r\n    if (block == null) {\r\n        chunk.state = StripingChunk.MISSING;\r\n        return false;\r\n    }\r\n    if (readerInfos[chunkIndex] == null) {\r\n        if (!dfsStripedInputStream.createBlockReader(block, alignedStripe.getOffsetInBlock(), targetBlocks, readerInfos, chunkIndex)) {\r\n            chunk.state = StripingChunk.MISSING;\r\n            return false;\r\n        }\r\n    } else if (readerInfos[chunkIndex].shouldSkip) {\r\n        chunk.state = StripingChunk.MISSING;\r\n        return false;\r\n    }\r\n    chunk.state = StripingChunk.PENDING;\r\n    Callable<BlockReadStats> readCallable = readCells(readerInfos[chunkIndex].reader, readerInfos[chunkIndex].datanode, readerInfos[chunkIndex].blockReaderOffset, alignedStripe.getOffsetInBlock(), getReadStrategies(chunk), block.getBlock());\r\n    Future<BlockReadStats> request = service.submit(readCallable);\r\n    futures.put(request, chunkIndex);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readStripe",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void readStripe() throws IOException\n{\r\n    for (int i = 0; i < dataBlkNum; i++) {\r\n        if (alignedStripe.chunks[i] != null && alignedStripe.chunks[i].state != StripingChunk.ALLZERO) {\r\n            if (!readChunk(targetBlocks[i], i)) {\r\n                alignedStripe.missingChunksNum++;\r\n            }\r\n        }\r\n    }\r\n    if (alignedStripe.missingChunksNum > 0) {\r\n        checkMissingBlocks();\r\n        readDataForDecoding();\r\n        readParityChunks(alignedStripe.missingChunksNum);\r\n    }\r\n    while (!futures.isEmpty()) {\r\n        try {\r\n            StripingChunkReadResult r = StripedBlockUtil.getNextCompletedStripedRead(service, futures, 0);\r\n            dfsStripedInputStream.updateReadStats(r.getReadStats());\r\n            DFSClient.LOG.debug(\"Read task returned: {}, for stripe {}\", r, alignedStripe);\r\n            StripingChunk returnedChunk = alignedStripe.chunks[r.index];\r\n            Preconditions.checkNotNull(returnedChunk);\r\n            Preconditions.checkState(returnedChunk.state == StripingChunk.PENDING);\r\n            if (r.state == StripingChunkReadResult.SUCCESSFUL) {\r\n                returnedChunk.state = StripingChunk.FETCHED;\r\n                alignedStripe.fetchedChunksNum++;\r\n                updateState4SuccessRead(r);\r\n                if (alignedStripe.fetchedChunksNum == dataBlkNum) {\r\n                    clearFutures();\r\n                    break;\r\n                }\r\n            } else {\r\n                returnedChunk.state = StripingChunk.MISSING;\r\n                dfsStripedInputStream.closeReader(readerInfos[r.index]);\r\n                final int missing = alignedStripe.missingChunksNum;\r\n                alignedStripe.missingChunksNum++;\r\n                checkMissingBlocks();\r\n                readDataForDecoding();\r\n                readParityChunks(alignedStripe.missingChunksNum - missing);\r\n            }\r\n        } catch (InterruptedException ie) {\r\n            String err = \"Read request interrupted\";\r\n            DFSClient.LOG.error(err);\r\n            clearFutures();\r\n            throw new InterruptedIOException(err);\r\n        }\r\n    }\r\n    if (alignedStripe.missingChunksNum > 0) {\r\n        decode();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "finalizeDecodeInputs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void finalizeDecodeInputs()\n{\r\n    for (int i = 0; i < alignedStripe.chunks.length; i++) {\r\n        final StripingChunk chunk = alignedStripe.chunks[i];\r\n        if (chunk != null && chunk.state == StripingChunk.FETCHED) {\r\n            if (chunk.useChunkBuffer()) {\r\n                chunk.getChunkBuffer().copyTo(decodeInputs[i].getBuffer());\r\n            } else {\r\n                chunk.getByteBuffer().flip();\r\n            }\r\n        } else if (chunk != null && chunk.state == StripingChunk.ALLZERO) {\r\n            decodeInputs[i].setAllZero(true);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "decodeAndFillBuffer",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void decodeAndFillBuffer(boolean fillBuffer) throws IOException\n{\r\n    int[] decodeIndices = prepareErasedIndices();\r\n    final int decodeChunkNum = decodeIndices.length;\r\n    ECChunk[] outputs = new ECChunk[decodeChunkNum];\r\n    for (int i = 0; i < decodeChunkNum; i++) {\r\n        outputs[i] = decodeInputs[decodeIndices[i]];\r\n        decodeInputs[decodeIndices[i]] = null;\r\n    }\r\n    long start = Time.monotonicNow();\r\n    decoder.decode(decodeInputs, decodeIndices, outputs);\r\n    if (fillBuffer) {\r\n        for (int i = 0; i < decodeIndices.length; i++) {\r\n            int missingBlkIdx = decodeIndices[i];\r\n            StripingChunk chunk = alignedStripe.chunks[missingBlkIdx];\r\n            if (chunk.state == StripingChunk.MISSING && chunk.useChunkBuffer()) {\r\n                chunk.getChunkBuffer().copyFrom(outputs[i].getBuffer());\r\n            }\r\n        }\r\n    }\r\n    long end = Time.monotonicNow();\r\n    dfsStripedInputStream.readStatistics.addErasureCodingDecodingTime(end - start);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "prepareErasedIndices",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int[] prepareErasedIndices()\n{\r\n    int[] decodeIndices = new int[parityBlkNum];\r\n    int pos = 0;\r\n    for (int i = 0; i < alignedStripe.chunks.length; i++) {\r\n        if (alignedStripe.chunks[i] != null && alignedStripe.chunks[i].state == StripingChunk.MISSING) {\r\n            decodeIndices[pos++] = i;\r\n        }\r\n    }\r\n    int[] erasedIndices = Arrays.copyOf(decodeIndices, pos);\r\n    return erasedIndices;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "clearFutures",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void clearFutures()\n{\r\n    for (Future future : futures.keySet()) {\r\n        future.cancel(false);\r\n    }\r\n    futures.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "useDirectBuffer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean useDirectBuffer()\n{\r\n    return decoder.preferDirectBuffer();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isSucceed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSucceed()\n{\r\n    return succeed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ErasureCodingPolicy getPolicy()\n{\r\n    return policy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getErrorMsg",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getErrorMsg()\n{\r\n    return errorMsg;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String toString()\n{\r\n    if (isSucceed()) {\r\n        return \"Add ErasureCodingPolicy \" + getPolicy().getName() + \" succeed.\";\r\n    } else {\r\n        return \"Add ErasureCodingPolicy \" + getPolicy().getName() + \" failed and \" + \"error message is \" + getErrorMsg();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (o instanceof AddErasureCodingPolicyResponse) {\r\n        AddErasureCodingPolicyResponse other = (AddErasureCodingPolicyResponse) o;\r\n        return new EqualsBuilder().append(policy, other.policy).append(succeed, other.succeed).append(errorMsg, other.errorMsg).isEquals();\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return new HashCodeBuilder(303855623, 582626729).append(policy).append(succeed).append(errorMsg).toHashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "unref",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void unref()\n{\r\n    cache.unref(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "isStale",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean isStale()\n{\r\n    if (slot != null) {\r\n        boolean stale = !slot.isValid();\r\n        LOG.trace(\"{}: checked shared memory segment.  isStale={}\", this, stale);\r\n        return stale;\r\n    } else {\r\n        long deltaMs = Time.monotonicNow() - creationTimeMs;\r\n        long staleThresholdMs = cache.getStaleThresholdMs();\r\n        if (deltaMs > staleThresholdMs) {\r\n            LOG.trace(\"{} is stale because it's {} ms old and staleThresholdMs={}\", this, deltaMs, staleThresholdMs);\r\n            return true;\r\n        } else {\r\n            LOG.trace(\"{} is not stale because it's only {} ms old \" + \"and staleThresholdMs={}\", this, deltaMs, staleThresholdMs);\r\n            return false;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "addNoChecksumAnchor",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean addNoChecksumAnchor()\n{\r\n    if (slot == null) {\r\n        return false;\r\n    }\r\n    boolean result = slot.addAnchor();\r\n    LOG.trace(\"{}: {} no-checksum anchor to slot {}\", this, result ? \"added\" : \"could not add\", slot);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "removeNoChecksumAnchor",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeNoChecksumAnchor()\n{\r\n    if (slot != null) {\r\n        slot.removeAnchor();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "hasMmap",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasMmap()\n{\r\n    return ((mmapData != null) && (mmapData instanceof MappedByteBuffer));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "munmap",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void munmap()\n{\r\n    MappedByteBuffer mmap = (MappedByteBuffer) mmapData;\r\n    NativeIO.POSIX.munmap(mmap);\r\n    mmapData = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void close()\n{\r\n    String suffix = \"\";\r\n    Preconditions.checkState(refCount == 0, \"tried to close replica with refCount %d: %s\", refCount, this);\r\n    refCount = -1;\r\n    Preconditions.checkState(purged, \"tried to close unpurged replica %s\", this);\r\n    if (hasMmap()) {\r\n        munmap();\r\n        if (LOG.isTraceEnabled()) {\r\n            suffix += \"  munmapped.\";\r\n        }\r\n    }\r\n    IOUtilsClient.cleanupWithLogger(LOG, dataStream, metaStream);\r\n    if (slot != null) {\r\n        cache.scheduleSlotReleaser(slot);\r\n        if (LOG.isTraceEnabled()) {\r\n            suffix += \"  scheduling \" + slot + \" for later release.\";\r\n        }\r\n    }\r\n    LOG.trace(\"closed {}{}\", this, suffix);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getDataStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileInputStream getDataStream()\n{\r\n    return dataStream;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getMetaStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileInputStream getMetaStream()\n{\r\n    return metaStream;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getMetaHeader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockMetadataHeader getMetaHeader()\n{\r\n    return metaHeader;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ExtendedBlockId getKey()\n{\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getOrCreateClientMmap",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ClientMmap getOrCreateClientMmap(boolean anchor)\n{\r\n    return cache.getOrCreateClientMmap(this, anchor);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "loadMmapInternal",
  "errType" : [ "IOException", "RuntimeException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "MappedByteBuffer loadMmapInternal()\n{\r\n    try {\r\n        FileChannel channel = dataStream.getChannel();\r\n        MappedByteBuffer mmap = channel.map(MapMode.READ_ONLY, 0, Math.min(Integer.MAX_VALUE, channel.size()));\r\n        LOG.trace(\"{}: created mmap of size {}\", this, channel.size());\r\n        return mmap;\r\n    } catch (IOException e) {\r\n        LOG.warn(this + \": mmap error\", e);\r\n        return null;\r\n    } catch (RuntimeException e) {\r\n        LOG.warn(this + \": mmap error\", e);\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getEvictableTimeNs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Long getEvictableTimeNs()\n{\r\n    return evictableTimeNs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "setEvictableTimeNs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setEvictableTimeNs(Long evictableTimeNs)\n{\r\n    this.evictableTimeNs = evictableTimeNs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getSlot",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Slot getSlot()\n{\r\n    return slot;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    return \"ShortCircuitReplica{\" + \"key=\" + key + \", metaHeader.version=\" + metaHeader.getVersion() + \", metaHeader.checksum=\" + metaHeader.getChecksum() + \", ident=\" + \"0x\" + Integer.toHexString(System.identityHashCode(this)) + \", creationTimeMs=\" + creationTimeMs + \"}\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "makeRequest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BatchedEntries<EncryptionZone> makeRequest(Long prevId) throws IOException\n{\r\n    try (TraceScope ignored = tracer.newScope(\"listEncryptionZones\")) {\r\n        return namenode.listEncryptionZones(prevId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "elementToPrevKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Long elementToPrevKey(EncryptionZone entry)\n{\r\n    return entry.getId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String validate(final String str)\n{\r\n    if (str == null || str.equals(DEFAULT)) {\r\n        return null;\r\n    }\r\n    if (!str.startsWith(Path.SEPARATOR)) {\r\n        throw new IllegalArgumentException(\"Invalid parameter value: \" + NAME + \" = \\\"\" + str + \"\\\" is not an absolute path.\");\r\n    }\r\n    return new Path(str).toUri().getPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getSnapshotRoot",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSnapshotRoot()\n{\r\n    return snapshotRoot;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFromSnapshot",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getFromSnapshot()\n{\r\n    return fromSnapshot;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLaterSnapshotName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLaterSnapshotName()\n{\r\n    return toSnapshot;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getStats",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DiffStats getStats()\n{\r\n    return this.diffStats;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getDiffList",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<DiffReportEntry> getDiffList()\n{\r\n    return diffList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder str = new StringBuilder();\r\n    String from = fromSnapshot == null || fromSnapshot.isEmpty() ? \"current directory\" : \"snapshot \" + fromSnapshot;\r\n    String to = toSnapshot == null || toSnapshot.isEmpty() ? \"current directory\" : \"snapshot \" + toSnapshot;\r\n    str.append(\"Difference between \").append(from).append(\" and \").append(to).append(\" under directory \").append(snapshotRoot).append(\":\").append(LINE_SEPARATOR);\r\n    for (DiffReportEntry entry : diffList) {\r\n        str.append(entry.toString()).append(LINE_SEPARATOR);\r\n    }\r\n    return str.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return WebHdfsConstants.WEBHDFS_SCHEME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getTransportScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTransportScheme()\n{\r\n    return \"http\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getTokenKind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getTokenKind()\n{\r\n    return WebHdfsConstants.WEBHDFS_TOKEN_KIND;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void initialize(URI uri, Configuration conf) throws IOException\n{\r\n    super.initialize(uri, conf);\r\n    setConf(conf);\r\n    UserParam.setUserPattern(conf.get(HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY, HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\r\n    AclPermissionParam.setAclPermissionPattern(conf.get(HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY, HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_DEFAULT));\r\n    int connectTimeout = (int) conf.getTimeDuration(HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_CONNECT_TIMEOUT_KEY, URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT, TimeUnit.MILLISECONDS);\r\n    int readTimeout = (int) conf.getTimeDuration(HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_READ_TIMEOUT_KEY, URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT, TimeUnit.MILLISECONDS);\r\n    boolean isOAuth = conf.getBoolean(HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY, HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\r\n    if (isOAuth) {\r\n        LOG.debug(\"Enabling OAuth2 in WebHDFS\");\r\n        connectionFactory = URLConnectionFactory.newOAuth2URLConnectionFactory(connectTimeout, readTimeout, conf);\r\n    } else {\r\n        LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\r\n        connectionFactory = URLConnectionFactory.newDefaultURLConnectionFactory(connectTimeout, readTimeout, conf);\r\n    }\r\n    this.isTLSKrb = \"HTTPS_ONLY\".equals(conf.get(DFS_HTTP_POLICY_KEY));\r\n    ugi = UserGroupInformation.getCurrentUser();\r\n    this.uri = URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\r\n    this.nnAddrs = resolveNNAddr();\r\n    boolean isHA = HAUtilClient.isClientFailoverConfigured(conf, this.uri);\r\n    boolean isLogicalUri = isHA && HAUtilClient.isLogicalUri(conf, this.uri);\r\n    this.tokenServiceName = isLogicalUri ? HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme()) : SecurityUtil.buildTokenService(getCanonicalUri());\r\n    if (!isHA) {\r\n        this.retryPolicy = RetryUtils.getDefaultRetryPolicy(conf, HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY, HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT, HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY, HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT, HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\r\n    } else {\r\n        int maxFailoverAttempts = conf.getInt(HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY, HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\r\n        int maxRetryAttempts = conf.getInt(HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY, HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\r\n        int failoverSleepBaseMillis = conf.getInt(HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY, HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\r\n        int failoverSleepMaxMillis = conf.getInt(HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY, HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\r\n        this.retryPolicy = RetryPolicies.failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL, maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis, failoverSleepMaxMillis);\r\n    }\r\n    this.workingDir = makeQualified(new Path(getHomeDirectoryString(ugi)));\r\n    this.canRefreshDelegationToken = UserGroupInformation.isSecurityEnabled();\r\n    this.isInsecureCluster = !this.canRefreshDelegationToken;\r\n    this.disallowFallbackToInsecureCluster = !conf.getBoolean(CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY, CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\r\n    this.initializeRestCsrf(conf);\r\n    this.delegationToken = null;\r\n    storageStatistics = (DFSOpsCountStatistics) GlobalStorageStatistics.INSTANCE.put(DFSOpsCountStatistics.NAME, new StorageStatisticsProvider() {\r\n\r\n        @Override\r\n        public StorageStatistics provide() {\r\n            return new DFSOpsCountStatistics();\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "initializeRestCsrf",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initializeRestCsrf(Configuration conf)\n{\r\n    if (conf.getBoolean(DFS_WEBHDFS_REST_CSRF_ENABLED_KEY, DFS_WEBHDFS_REST_CSRF_ENABLED_DEFAULT)) {\r\n        this.restCsrfCustomHeader = conf.getTrimmed(DFS_WEBHDFS_REST_CSRF_CUSTOM_HEADER_KEY, DFS_WEBHDFS_REST_CSRF_CUSTOM_HEADER_DEFAULT);\r\n        this.restCsrfMethodsToIgnore = new HashSet<>();\r\n        this.restCsrfMethodsToIgnore.addAll(getTrimmedStringList(conf, DFS_WEBHDFS_REST_CSRF_METHODS_TO_IGNORE_KEY, DFS_WEBHDFS_REST_CSRF_METHODS_TO_IGNORE_DEFAULT));\r\n    } else {\r\n        this.restCsrfCustomHeader = null;\r\n        this.restCsrfMethodsToIgnore = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getTrimmedStringList",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<String> getTrimmedStringList(Configuration conf, String name, String defaultValue)\n{\r\n    String valueString = conf.get(name, defaultValue);\r\n    if (valueString == null) {\r\n        return new ArrayList<>();\r\n    }\r\n    return new ArrayList<>(StringUtils.getTrimmedStringCollection(valueString));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getCanonicalUri",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI getCanonicalUri()\n{\r\n    return super.getCanonicalUri();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Token<?> getDelegationToken() throws IOException\n{\r\n    if (delegationToken == null) {\r\n        Token<?> token = tokenSelector.selectToken(new Text(getCanonicalServiceName()), ugi.getTokens());\r\n        if (token != null) {\r\n            LOG.debug(\"Using UGI token: {}\", token);\r\n            canRefreshDelegationToken = false;\r\n        } else {\r\n            if (canRefreshDelegationToken) {\r\n                token = getDelegationToken(null);\r\n                if (token != null) {\r\n                    LOG.debug(\"Fetched new token: {}\", token);\r\n                } else {\r\n                    canRefreshDelegationToken = false;\r\n                    isInsecureCluster = true;\r\n                }\r\n            }\r\n        }\r\n        setDelegationToken(token);\r\n    }\r\n    return delegationToken;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "replaceExpiredDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean replaceExpiredDelegationToken() throws IOException\n{\r\n    boolean replaced = false;\r\n    if (attemptReplaceDelegationTokenFromUGI()) {\r\n        return true;\r\n    }\r\n    if (canRefreshDelegationToken) {\r\n        Token<?> token = getDelegationToken(null);\r\n        LOG.debug(\"Replaced expired token: {}\", token);\r\n        setDelegationToken(token);\r\n        replaced = (token != null);\r\n    }\r\n    return replaced;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "attemptReplaceDelegationTokenFromUGI",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean attemptReplaceDelegationTokenFromUGI()\n{\r\n    Token<?> token = tokenSelector.selectToken(new Text(getCanonicalServiceName()), ugi.getTokens());\r\n    if (token != null && !token.equals(delegationToken)) {\r\n        LOG.debug(\"Replaced expired token with new UGI token: {}\", token);\r\n        setDelegationToken(token);\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getDefaultPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getDefaultPort()\n{\r\n    return HdfsClientConfigKeys.DFS_NAMENODE_HTTP_PORT_DEFAULT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI getUri()\n{\r\n    return this.uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "canonicalizeUri",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI canonicalizeUri(URI uri)\n{\r\n    return NetUtils.getCanonicalUri(uri, getDefaultPort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getHomeDirectoryString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getHomeDirectoryString(final UserGroupInformation ugi)\n{\r\n    return \"/user/\" + ugi.getShortUserName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getHomeDirectory",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path getHomeDirectory()\n{\r\n    if (cachedHomeDirectory == null) {\r\n        final HttpOpParam.Op op = GetOpParam.Op.GETHOMEDIRECTORY;\r\n        try {\r\n            String pathFromDelegatedFS = new FsPathResponseRunner<String>(op, null) {\r\n\r\n                @Override\r\n                String decodeResponse(Map<?, ?> json) throws IOException {\r\n                    return JsonUtilClient.getPath(json);\r\n                }\r\n            }.run();\r\n            cachedHomeDirectory = new Path(pathFromDelegatedFS).makeQualified(this.getUri(), null);\r\n        } catch (IOException e) {\r\n            LOG.error(\"Unable to get HomeDirectory from original File System\", e);\r\n            cachedHomeDirectory = new Path(\"/user/\" + ugi.getShortUserName()).makeQualified(this.getUri(), null);\r\n        }\r\n    }\r\n    return cachedHomeDirectory;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getWorkingDirectory()\n{\r\n    return workingDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "setWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setWorkingDirectory(final Path dir)\n{\r\n    Path absolutePath = makeAbsolute(dir);\r\n    String result = absolutePath.toUri().getPath();\r\n    if (!DFSUtilClient.isValidName(result)) {\r\n        throw new IllegalArgumentException(\"Invalid DFS directory name \" + result);\r\n    }\r\n    workingDir = absolutePath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "makeAbsolute",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path makeAbsolute(Path f)\n{\r\n    return f.isAbsolute() ? f : new Path(workingDir, f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "jsonParse",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Map<?, ?> jsonParse(final HttpURLConnection c, final boolean useErrorStream) throws IOException\n{\r\n    if (c.getContentLength() == 0) {\r\n        return null;\r\n    }\r\n    final InputStream in = useErrorStream ? c.getErrorStream() : c.getInputStream();\r\n    if (in == null) {\r\n        throw new IOException(\"The \" + (useErrorStream ? \"error\" : \"input\") + \" stream is null.\");\r\n    }\r\n    try {\r\n        final String contentType = c.getContentType();\r\n        if (contentType != null) {\r\n            final MediaType parsed = MediaType.valueOf(contentType);\r\n            if (!MediaType.APPLICATION_JSON_TYPE.isCompatible(parsed)) {\r\n                throw new IOException(\"Content-Type \\\"\" + contentType + \"\\\" is incompatible with \\\"\" + MediaType.APPLICATION_JSON + \"\\\" (parsed=\\\"\" + parsed + \"\\\")\");\r\n            }\r\n        }\r\n        return JsonSerialization.mapReader().readValue(in);\r\n    } finally {\r\n        in.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "validateResponse",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "Map<?, ?> validateResponse(final HttpOpParam.Op op, final HttpURLConnection conn, boolean unwrapException) throws IOException\n{\r\n    final int code = conn.getResponseCode();\r\n    if (code == HttpURLConnection.HTTP_UNAUTHORIZED) {\r\n        throw new AccessControlException(conn.getResponseMessage());\r\n    }\r\n    if (code != op.getExpectedHttpResponseCode()) {\r\n        final Map<?, ?> m;\r\n        try {\r\n            m = jsonParse(conn, true);\r\n        } catch (Exception e) {\r\n            throw new IOException(\"Unexpected HTTP response: code=\" + code + \" != \" + op.getExpectedHttpResponseCode() + \", \" + op.toQueryString() + \", message=\" + conn.getResponseMessage(), e);\r\n        }\r\n        if (m == null) {\r\n            throw new IOException(\"Unexpected HTTP response: code=\" + code + \" != \" + op.getExpectedHttpResponseCode() + \", \" + op.toQueryString() + \", message=\" + conn.getResponseMessage());\r\n        } else if (m.get(RemoteException.class.getSimpleName()) == null) {\r\n            return m;\r\n        }\r\n        IOException re = JsonUtilClient.toRemoteException(m);\r\n        if (re.getMessage() != null && re.getMessage().endsWith(StandbyException.class.getSimpleName())) {\r\n            LOG.trace(\"Detected StandbyException\", re);\r\n            throw new IOException(re);\r\n        }\r\n        if (re.getMessage() != null && re.getMessage().startsWith(SecurityUtil.FAILED_TO_GET_UGI_MSG_HEADER)) {\r\n            String[] parts = re.getMessage().split(\":\\\\s+\", 3);\r\n            re = new RemoteException(parts[1], parts[2]);\r\n            re = ((RemoteException) re).unwrapRemoteException(InvalidToken.class);\r\n        }\r\n        throw unwrapException ? toIOException(re) : re;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toIOException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOException toIOException(Exception e)\n{\r\n    if (!(e instanceof IOException)) {\r\n        return new IOException(e);\r\n    }\r\n    final IOException ioe = (IOException) e;\r\n    if (!(ioe instanceof RemoteException)) {\r\n        return ioe;\r\n    }\r\n    return ((RemoteException) ioe).unwrapRemoteException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getCurrentNNAddr",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InetSocketAddress getCurrentNNAddr()\n{\r\n    return nnAddrs[currentNNAddrIndex];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "resetStateToFailOver",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void resetStateToFailOver()\n{\r\n    currentNNAddrIndex = (currentNNAddrIndex + 1) % nnAddrs.length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getNamenodeURL",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "URL getNamenodeURL(String path, String query) throws IOException\n{\r\n    InetSocketAddress nnAddr = getCurrentNNAddr();\r\n    final URL url = new URL(getTransportScheme(), nnAddr.getHostName(), nnAddr.getPort(), path + '?' + query);\r\n    LOG.trace(\"url={}\", url);\r\n    return url;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getAuthParameters",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Param<?, ?>[] getAuthParameters(final HttpOpParam.Op op) throws IOException\n{\r\n    List<Param<?, ?>> authParams = Lists.newArrayList();\r\n    Token<?> token = null;\r\n    if (!op.getRequireAuth()) {\r\n        token = getDelegationToken();\r\n    }\r\n    if (token != null) {\r\n        authParams.add(new DelegationParam(token.encodeToUrlString()));\r\n    } else {\r\n        UserGroupInformation userUgi = ugi;\r\n        UserGroupInformation realUgi = userUgi.getRealUser();\r\n        if (realUgi != null) {\r\n            authParams.add(new DoAsParam(userUgi.getShortUserName()));\r\n            userUgi = realUgi;\r\n        }\r\n        UserParam userParam = new UserParam((userUgi.getShortUserName()));\r\n        if (isInsecureCluster) {\r\n            authParams.add(userParam);\r\n        }\r\n    }\r\n    return authParams.toArray(new Param<?, ?>[0]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toUrl",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "URL toUrl(final HttpOpParam.Op op, final Path fspath, final Param<?, ?>... parameters) throws IOException\n{\r\n    final String path = PATH_PREFIX + (fspath == null ? \"/\" : makeQualified(fspath).toUri().getRawPath());\r\n    final String query = op.toQueryString() + Param.toSortedString(\"&\", getAuthParameters(op)) + Param.toSortedString(\"&\", parameters);\r\n    final URL url = getNamenodeURL(path, query);\r\n    LOG.trace(\"url={}\", url);\r\n    return url;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "applyUMask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FsPermission applyUMask(FsPermission permission)\n{\r\n    if (permission == null) {\r\n        permission = FsPermission.getDefault();\r\n    }\r\n    return FsCreateModes.applyUMask(permission, FsPermission.getUMask(getConf()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getHdfsFileStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsFileStatus getHdfsFileStatus(Path f) throws IOException\n{\r\n    final HttpOpParam.Op op = GetOpParam.Op.GETFILESTATUS;\r\n    HdfsFileStatus status = new FsPathResponseRunner<HdfsFileStatus>(op, f) {\r\n\r\n        @Override\r\n        HdfsFileStatus decodeResponse(Map<?, ?> json) {\r\n            return JsonUtilClient.toFileStatus(json, true);\r\n        }\r\n    }.run();\r\n    if (status == null) {\r\n        throw new FileNotFoundException(\"File does not exist: \" + f);\r\n    }\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FileStatus getFileStatus(Path f) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_FILE_STATUS);\r\n    return getHdfsFileStatus(f).makeQualified(getUri(), f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getAclStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AclStatus getAclStatus(Path f) throws IOException\n{\r\n    final HttpOpParam.Op op = GetOpParam.Op.GETACLSTATUS;\r\n    AclStatus status = new FsPathResponseRunner<AclStatus>(op, f) {\r\n\r\n        @Override\r\n        AclStatus decodeResponse(Map<?, ?> json) {\r\n            return JsonUtilClient.toAclStatus(json);\r\n        }\r\n    }.run();\r\n    if (status == null) {\r\n        throw new FileNotFoundException(\"File does not exist: \" + f);\r\n    }\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean mkdirs(Path f, FsPermission permission) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.MKDIRS);\r\n    final HttpOpParam.Op op = PutOpParam.Op.MKDIRS;\r\n    final FsPermission modes = applyUMask(permission);\r\n    return new FsPathBooleanRunner(op, f, new PermissionParam(modes.getMasked()), new UnmaskedPermissionParam(modes.getUnmasked())).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "supportsSymlinks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean supportsSymlinks()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "createSymlink",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createSymlink(Path destination, Path f, boolean createParent) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.CREATE_SYM_LINK);\r\n    final HttpOpParam.Op op = PutOpParam.Op.CREATESYMLINK;\r\n    new FsPathRunner(op, f, new DestinationParam(makeQualified(destination).toUri().getPath()), new CreateParentParam(createParent)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "rename",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean rename(final Path src, final Path dst) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.RENAME);\r\n    final HttpOpParam.Op op = PutOpParam.Op.RENAME;\r\n    return new FsPathBooleanRunner(op, src, new DestinationParam(makeQualified(dst).toUri().getPath())).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "rename",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void rename(final Path src, final Path dst, final Options.Rename... options) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.RENAME);\r\n    final HttpOpParam.Op op = PutOpParam.Op.RENAME;\r\n    new FsPathRunner(op, src, new DestinationParam(makeQualified(dst).toUri().getPath()), new RenameOptionSetParam(options)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "setXAttr",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setXAttr(Path p, String name, byte[] value, EnumSet<XAttrSetFlag> flag) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_XATTR);\r\n    final HttpOpParam.Op op = PutOpParam.Op.SETXATTR;\r\n    if (value != null) {\r\n        new FsPathRunner(op, p, new XAttrNameParam(name), new XAttrValueParam(XAttrCodec.encodeValue(value, XAttrCodec.HEX)), new XAttrSetFlagParam(flag)).run();\r\n    } else {\r\n        new FsPathRunner(op, p, new XAttrNameParam(name), new XAttrSetFlagParam(flag)).run();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getXAttr",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "byte[] getXAttr(Path p, final String name) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_XATTR);\r\n    final HttpOpParam.Op op = GetOpParam.Op.GETXATTRS;\r\n    return new FsPathResponseRunner<byte[]>(op, p, new XAttrNameParam(name), new XAttrEncodingParam(XAttrCodec.HEX)) {\r\n\r\n        @Override\r\n        byte[] decodeResponse(Map<?, ?> json) throws IOException {\r\n            return JsonUtilClient.getXAttr(json);\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(Path p) throws IOException\n{\r\n    final HttpOpParam.Op op = GetOpParam.Op.GETXATTRS;\r\n    return new FsPathResponseRunner<Map<String, byte[]>>(op, p, new XAttrEncodingParam(XAttrCodec.HEX)) {\r\n\r\n        @Override\r\n        Map<String, byte[]> decodeResponse(Map<?, ?> json) throws IOException {\r\n            return JsonUtilClient.toXAttrs(json);\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(Path p, final List<String> names) throws IOException\n{\r\n    Preconditions.checkArgument(names != null && !names.isEmpty(), \"XAttr names cannot be null or empty.\");\r\n    Param<?, ?>[] parameters = new Param<?, ?>[names.size() + 1];\r\n    for (int i = 0; i < parameters.length - 1; i++) {\r\n        parameters[i] = new XAttrNameParam(names.get(i));\r\n    }\r\n    parameters[parameters.length - 1] = new XAttrEncodingParam(XAttrCodec.HEX);\r\n    final HttpOpParam.Op op = GetOpParam.Op.GETXATTRS;\r\n    return new FsPathResponseRunner<Map<String, byte[]>>(op, parameters, p) {\r\n\r\n        @Override\r\n        Map<String, byte[]> decodeResponse(Map<?, ?> json) throws IOException {\r\n            return JsonUtilClient.toXAttrs(json);\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "listXAttrs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> listXAttrs(Path p) throws IOException\n{\r\n    final HttpOpParam.Op op = GetOpParam.Op.LISTXATTRS;\r\n    return new FsPathResponseRunner<List<String>>(op, p) {\r\n\r\n        @Override\r\n        List<String> decodeResponse(Map<?, ?> json) throws IOException {\r\n            return JsonUtilClient.toXAttrNames(json);\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "removeXAttr",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeXAttr(Path p, String name) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.REMOVE_XATTR);\r\n    final HttpOpParam.Op op = PutOpParam.Op.REMOVEXATTR;\r\n    new FsPathRunner(op, p, new XAttrNameParam(name)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "setOwner",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setOwner(final Path p, final String owner, final String group) throws IOException\n{\r\n    if (owner == null && group == null) {\r\n        throw new IOException(\"owner == null && group == null\");\r\n    }\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_OWNER);\r\n    final HttpOpParam.Op op = PutOpParam.Op.SETOWNER;\r\n    new FsPathRunner(op, p, new OwnerParam(owner), new GroupParam(group)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "setPermission",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setPermission(final Path p, final FsPermission permission) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_PERMISSION);\r\n    final HttpOpParam.Op op = PutOpParam.Op.SETPERMISSION;\r\n    new FsPathRunner(op, p, new PermissionParam(permission)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "modifyAclEntries",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void modifyAclEntries(Path path, List<AclEntry> aclSpec) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.MODIFY_ACL_ENTRIES);\r\n    final HttpOpParam.Op op = PutOpParam.Op.MODIFYACLENTRIES;\r\n    new FsPathRunner(op, path, new AclPermissionParam(aclSpec)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "removeAclEntries",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeAclEntries(Path path, List<AclEntry> aclSpec) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.REMOVE_ACL_ENTRIES);\r\n    final HttpOpParam.Op op = PutOpParam.Op.REMOVEACLENTRIES;\r\n    new FsPathRunner(op, path, new AclPermissionParam(aclSpec)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "removeDefaultAcl",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeDefaultAcl(Path path) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.REMOVE_DEFAULT_ACL);\r\n    final HttpOpParam.Op op = PutOpParam.Op.REMOVEDEFAULTACL;\r\n    new FsPathRunner(op, path).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "removeAcl",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeAcl(Path path) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.REMOVE_ACL);\r\n    final HttpOpParam.Op op = PutOpParam.Op.REMOVEACL;\r\n    new FsPathRunner(op, path).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "setAcl",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setAcl(final Path p, final List<AclEntry> aclSpec) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_ACL);\r\n    final HttpOpParam.Op op = PutOpParam.Op.SETACL;\r\n    new FsPathRunner(op, p, new AclPermissionParam(aclSpec)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "allowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void allowSnapshot(final Path p) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.ALLOW_SNAPSHOT);\r\n    final HttpOpParam.Op op = PutOpParam.Op.ALLOWSNAPSHOT;\r\n    new FsPathRunner(op, p).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "satisfyStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void satisfyStoragePolicy(final Path p) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SATISFY_STORAGE_POLICY);\r\n    final HttpOpParam.Op op = PutOpParam.Op.SATISFYSTORAGEPOLICY;\r\n    new FsPathRunner(op, p).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "enableECPolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void enableECPolicy(String policyName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.ENABLE_EC_POLICY);\r\n    final HttpOpParam.Op op = PutOpParam.Op.ENABLEECPOLICY;\r\n    new FsPathRunner(op, null, new ECPolicyParam(policyName)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "disableECPolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void disableECPolicy(String policyName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.DISABLE_EC_POLICY);\r\n    final HttpOpParam.Op op = PutOpParam.Op.DISABLEECPOLICY;\r\n    new FsPathRunner(op, null, new ECPolicyParam(policyName)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "setErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setErasureCodingPolicy(Path p, String policyName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_EC_POLICY);\r\n    final HttpOpParam.Op op = PutOpParam.Op.SETECPOLICY;\r\n    new FsPathRunner(op, p, new ECPolicyParam(policyName)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "unsetErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void unsetErasureCodingPolicy(Path p) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.UNSET_EC_POLICY);\r\n    final HttpOpParam.Op op = PostOpParam.Op.UNSETECPOLICY;\r\n    new FsPathRunner(op, p).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ErasureCodingPolicy getErasureCodingPolicy(Path p) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_EC_POLICY);\r\n    final HttpOpParam.Op op = GetOpParam.Op.GETECPOLICY;\r\n    return new FsPathResponseRunner<ErasureCodingPolicy>(op, p) {\r\n\r\n        @Override\r\n        ErasureCodingPolicy decodeResponse(Map<?, ?> json) throws IOException {\r\n            return JsonUtilClient.toECPolicy((Map<?, ?>) json);\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "createSnapshot",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path createSnapshot(final Path path, final String snapshotName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.CREATE_SNAPSHOT);\r\n    final HttpOpParam.Op op = PutOpParam.Op.CREATESNAPSHOT;\r\n    return new FsPathResponseRunner<Path>(op, path, new SnapshotNameParam(snapshotName)) {\r\n\r\n        @Override\r\n        Path decodeResponse(Map<?, ?> json) {\r\n            return new Path((String) json.get(Path.class.getSimpleName()));\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "disallowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void disallowSnapshot(final Path p) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.DISALLOW_SNAPSHOT);\r\n    final HttpOpParam.Op op = PutOpParam.Op.DISALLOWSNAPSHOT;\r\n    new FsPathRunner(op, p).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "deleteSnapshot",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void deleteSnapshot(final Path path, final String snapshotName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.DELETE_SNAPSHOT);\r\n    final HttpOpParam.Op op = DeleteOpParam.Op.DELETESNAPSHOT;\r\n    new FsPathRunner(op, path, new SnapshotNameParam(snapshotName)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "renameSnapshot",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void renameSnapshot(final Path path, final String snapshotOldName, final String snapshotNewName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.RENAME_SNAPSHOT);\r\n    final HttpOpParam.Op op = PutOpParam.Op.RENAMESNAPSHOT;\r\n    new FsPathRunner(op, path, new OldSnapshotNameParam(snapshotOldName), new SnapshotNameParam(snapshotNewName)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getSnapshotDiffReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SnapshotDiffReport getSnapshotDiffReport(final String snapshotDir, final String fromSnapshot, final String toSnapshot) throws IOException\n{\r\n    return new FsPathResponseRunner<SnapshotDiffReport>(GetOpParam.Op.GETSNAPSHOTDIFF, new Path(snapshotDir), new OldSnapshotNameParam(fromSnapshot), new SnapshotNameParam(toSnapshot)) {\r\n\r\n        @Override\r\n        SnapshotDiffReport decodeResponse(Map<?, ?> json) {\r\n            return JsonUtilClient.toSnapshotDiffReport(json);\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getSnapshotDiffReportListing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SnapshotDiffReportListing getSnapshotDiffReportListing(String snapshotDir, final String fromSnapshot, final String toSnapshot, byte[] startPath, int index) throws IOException\n{\r\n    return new FsPathResponseRunner<SnapshotDiffReportListing>(GetOpParam.Op.GETSNAPSHOTDIFFLISTING, new Path(snapshotDir), new OldSnapshotNameParam(fromSnapshot), new SnapshotNameParam(toSnapshot), new SnapshotDiffStartPathParam(DFSUtilClient.bytes2String(startPath)), new SnapshotDiffIndexParam(index)) {\r\n\r\n        @Override\r\n        SnapshotDiffReportListing decodeResponse(Map<?, ?> json) {\r\n            return JsonUtilClient.toSnapshotDiffReportListing(json);\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getSnapshotDiffReport",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "SnapshotDiffReport getSnapshotDiffReport(final Path snapshotDir, final String fromSnapshot, final String toSnapshot) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_SNAPSHOT_DIFF);\r\n    return DFSUtilClient.getSnapshotDiffReport(snapshotDir.toUri().getPath(), fromSnapshot, toSnapshot, this::getSnapshotDiffReport, this::getSnapshotDiffReportListing);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getSnapshottableDirectoryList",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "SnapshottableDirectoryStatus[] getSnapshottableDirectoryList() throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_SNAPSHOTTABLE_DIRECTORY_LIST);\r\n    final HttpOpParam.Op op = GetOpParam.Op.GETSNAPSHOTTABLEDIRECTORYLIST;\r\n    return new FsPathResponseRunner<SnapshottableDirectoryStatus[]>(op, null) {\r\n\r\n        @Override\r\n        SnapshottableDirectoryStatus[] decodeResponse(Map<?, ?> json) {\r\n            return JsonUtilClient.toSnapshottableDirectoryList(json);\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getSnapshotListing",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SnapshotStatus[] getSnapshotListing(final Path snapshotDir) throws IOException\n{\r\n    storageStatistics.incrementOpCounter(OpType.GET_SNAPSHOT_LIST);\r\n    final HttpOpParam.Op op = GetOpParam.Op.GETSNAPSHOTLIST;\r\n    return new FsPathResponseRunner<SnapshotStatus[]>(op, snapshotDir) {\r\n\r\n        @Override\r\n        SnapshotStatus[] decodeResponse(Map<?, ?> json) {\r\n            return JsonUtilClient.toSnapshotList(json);\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "setReplication",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean setReplication(final Path p, final short replication) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_REPLICATION);\r\n    final HttpOpParam.Op op = PutOpParam.Op.SETREPLICATION;\r\n    return new FsPathBooleanRunner(op, p, new ReplicationParam(replication)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "setTimes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setTimes(final Path p, final long mtime, final long atime) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_TIMES);\r\n    final HttpOpParam.Op op = PutOpParam.Op.SETTIMES;\r\n    new FsPathRunner(op, p, new ModificationTimeParam(mtime), new AccessTimeParam(atime)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getDefaultBlockSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDefaultBlockSize()\n{\r\n    return getConf().getLongBytes(HdfsClientConfigKeys.DFS_BLOCK_SIZE_KEY, HdfsClientConfigKeys.DFS_BLOCK_SIZE_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getDefaultReplication",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "short getDefaultReplication()\n{\r\n    return (short) getConf().getInt(HdfsClientConfigKeys.DFS_REPLICATION_KEY, HdfsClientConfigKeys.DFS_REPLICATION_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "concat",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void concat(final Path trg, final Path[] srcs) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.CONCAT);\r\n    final HttpOpParam.Op op = PostOpParam.Op.CONCAT;\r\n    new FsPathRunner(op, trg, new ConcatSourcesParam(srcs)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FSDataOutputStream create(final Path f, final FsPermission permission, final boolean overwrite, final int bufferSize, final short replication, final long blockSize, final Progressable progress) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.CREATE);\r\n    final FsPermission modes = applyUMask(permission);\r\n    final HttpOpParam.Op op = PutOpParam.Op.CREATE;\r\n    return new FsPathOutputStreamRunner(op, f, bufferSize, new PermissionParam(modes.getMasked()), new UnmaskedPermissionParam(modes.getUnmasked()), new OverwriteParam(overwrite), new BufferSizeParam(bufferSize), new ReplicationParam(replication), new BlockSizeParam(blockSize)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "createNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FSDataOutputStream createNonRecursive(final Path f, final FsPermission permission, final EnumSet<CreateFlag> flag, final int bufferSize, final short replication, final long blockSize, final Progressable progress) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.CREATE_NON_RECURSIVE);\r\n    final FsPermission modes = applyUMask(permission);\r\n    final HttpOpParam.Op op = PutOpParam.Op.CREATE;\r\n    return new FsPathOutputStreamRunner(op, f, bufferSize, new PermissionParam(modes.getMasked()), new UnmaskedPermissionParam(modes.getUnmasked()), new CreateFlagParam(flag), new CreateParentParam(false), new BufferSizeParam(bufferSize), new ReplicationParam(replication), new BlockSizeParam(blockSize)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FSDataOutputStream append(final Path f, final int bufferSize, final Progressable progress) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.APPEND);\r\n    final HttpOpParam.Op op = PostOpParam.Op.APPEND;\r\n    return new FsPathOutputStreamRunner(op, f, bufferSize, new BufferSizeParam(bufferSize)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "truncate",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean truncate(Path f, long newLength) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.TRUNCATE);\r\n    final HttpOpParam.Op op = PostOpParam.Op.TRUNCATE;\r\n    return new FsPathBooleanRunner(op, f, new NewLengthParam(newLength)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean delete(Path f, boolean recursive) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.DELETE);\r\n    final HttpOpParam.Op op = DeleteOpParam.Op.DELETE;\r\n    return new FsPathBooleanRunner(op, f, new RecursiveParam(recursive)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FSDataInputStream open(final Path f, final int bufferSize) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.OPEN);\r\n    WebHdfsInputStream webfsInputStream = new WebHdfsInputStream(f, bufferSize);\r\n    if (webfsInputStream.getFileEncryptionInfo() == null) {\r\n        return new FSDataInputStream(webfsInputStream);\r\n    } else {\r\n        return new FSDataInputStream(webfsInputStream.createWrappedInputStream());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "close",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    try {\r\n        if (canRefreshDelegationToken && delegationToken != null) {\r\n            cancelDelegationToken(delegationToken);\r\n        }\r\n    } catch (IOException ioe) {\r\n        LOG.debug(\"Token cancel failed: \", ioe);\r\n    } finally {\r\n        if (connectionFactory != null) {\r\n            connectionFactory.destroy();\r\n        }\r\n        super.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "removeOffsetParam",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "URL removeOffsetParam(final URL url) throws MalformedURLException\n{\r\n    String query = url.getQuery();\r\n    if (query == null) {\r\n        return url;\r\n    }\r\n    final String lower = StringUtils.toLowerCase(query);\r\n    if (!lower.startsWith(OFFSET_PARAM_PREFIX) && !lower.contains(\"&\" + OFFSET_PARAM_PREFIX)) {\r\n        return url;\r\n    }\r\n    StringBuilder b = null;\r\n    for (final StringTokenizer st = new StringTokenizer(query, \"&\"); st.hasMoreTokens(); ) {\r\n        final String token = st.nextToken();\r\n        if (!StringUtils.toLowerCase(token).startsWith(OFFSET_PARAM_PREFIX)) {\r\n            if (b == null) {\r\n                b = new StringBuilder(\"?\").append(token);\r\n            } else {\r\n                b.append('&').append(token);\r\n            }\r\n        }\r\n    }\r\n    query = b == null ? \"\" : b.toString();\r\n    final String urlStr = url.toString();\r\n    return new URL(urlStr.substring(0, urlStr.indexOf('?')) + query);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "listStatus",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FileStatus[] listStatus(final Path f) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.LIST_STATUS);\r\n    final URI fsUri = getUri();\r\n    final HttpOpParam.Op op = GetOpParam.Op.LISTSTATUS;\r\n    return new FsPathResponseRunner<FileStatus[]>(op, f) {\r\n\r\n        @Override\r\n        FileStatus[] decodeResponse(Map<?, ?> json) {\r\n            HdfsFileStatus[] hdfsStatuses = JsonUtilClient.toHdfsFileStatusArray(json);\r\n            final FileStatus[] statuses = new FileStatus[hdfsStatuses.length];\r\n            for (int i = 0; i < hdfsStatuses.length; i++) {\r\n                statuses[i] = hdfsStatuses[i].makeQualified(fsUri, f);\r\n            }\r\n            return statuses;\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "listStatusBatch",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "DirectoryEntries listStatusBatch(Path f, byte[] token) throws FileNotFoundException, IOException\n{\r\n    byte[] prevKey = EMPTY_ARRAY;\r\n    if (token != null) {\r\n        prevKey = token;\r\n    }\r\n    DirectoryListing listing = new FsPathResponseRunner<DirectoryListing>(GetOpParam.Op.LISTSTATUS_BATCH, f, new StartAfterParam(new String(prevKey, Charsets.UTF_8))) {\r\n\r\n        @Override\r\n        DirectoryListing decodeResponse(Map<?, ?> json) throws IOException {\r\n            return JsonUtilClient.toDirectoryListing(json);\r\n        }\r\n    }.run();\r\n    final URI fsUri = getUri();\r\n    final HdfsFileStatus[] statuses = listing.getPartialListing();\r\n    FileStatus[] qualified = new FileStatus[statuses.length];\r\n    for (int i = 0; i < statuses.length; i++) {\r\n        qualified[i] = statuses[i].makeQualified(fsUri, f);\r\n    }\r\n    return new DirectoryEntries(qualified, listing.getLastName(), listing.hasMore());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> getDelegationToken(final String renewer) throws IOException\n{\r\n    final HttpOpParam.Op op = GetOpParam.Op.GETDELEGATIONTOKEN;\r\n    Token<DelegationTokenIdentifier> token = new FsPathResponseRunner<Token<DelegationTokenIdentifier>>(op, null, new RenewerParam(renewer)) {\r\n\r\n        @Override\r\n        Token<DelegationTokenIdentifier> decodeResponse(Map<?, ?> json) throws IOException {\r\n            return JsonUtilClient.toDelegationToken(json);\r\n        }\r\n    }.run();\r\n    if (token != null) {\r\n        token.setService(tokenServiceName);\r\n    } else {\r\n        if (disallowFallbackToInsecureCluster) {\r\n            throw new AccessControlException(CANT_FALLBACK_TO_INSECURE_MSG);\r\n        }\r\n    }\r\n    return token;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getAdditionalTokenIssuers",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DelegationTokenIssuer[] getAdditionalTokenIssuers() throws IOException\n{\r\n    KeyProvider keyProvider = getKeyProvider();\r\n    if (keyProvider instanceof DelegationTokenIssuer) {\r\n        return new DelegationTokenIssuer[] { (DelegationTokenIssuer) keyProvider };\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getRenewToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Token<?> getRenewToken()\n{\r\n    return delegationToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "setDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDelegationToken(final Token<T> token)\n{\r\n    synchronized (this) {\r\n        delegationToken = token;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "renewDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long renewDelegationToken(final Token<?> token) throws IOException\n{\r\n    final HttpOpParam.Op op = PutOpParam.Op.RENEWDELEGATIONTOKEN;\r\n    return new FsPathResponseRunner<Long>(op, null, new TokenArgumentParam(token.encodeToUrlString())) {\r\n\r\n        @Override\r\n        Long decodeResponse(Map<?, ?> json) throws IOException {\r\n            return ((Number) json.get(\"long\")).longValue();\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "cancelDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cancelDelegationToken(final Token<?> token) throws IOException\n{\r\n    final HttpOpParam.Op op = PutOpParam.Op.CANCELDELEGATIONTOKEN;\r\n    new FsPathRunner(op, null, new TokenArgumentParam(token.encodeToUrlString())).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getFileBlockLocations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockLocation[] getFileBlockLocations(final FileStatus status, final long offset, final long length) throws IOException\n{\r\n    if (status == null) {\r\n        return null;\r\n    }\r\n    return getFileBlockLocations(status.getPath(), offset, length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getFileBlockLocations",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "BlockLocation[] getFileBlockLocations(final Path p, final long offset, final long length) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_FILE_BLOCK_LOCATIONS);\r\n    final HttpOpParam.Op op = GetOpParam.Op.GET_BLOCK_LOCATIONS;\r\n    return new FsPathResponseRunner<BlockLocation[]>(op, p, new OffsetParam(offset), new LengthParam(length)) {\r\n\r\n        @Override\r\n        BlockLocation[] decodeResponse(Map<?, ?> json) throws IOException {\r\n            return DFSUtilClient.locatedBlocks2Locations(JsonUtilClient.toLocatedBlocks(json));\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getTrashRoot",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Path getTrashRoot(Path path)\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_TRASH_ROOT);\r\n    final HttpOpParam.Op op = GetOpParam.Op.GETTRASHROOT;\r\n    try {\r\n        String strTrashPath = new FsPathResponseRunner<String>(op, path) {\r\n\r\n            @Override\r\n            String decodeResponse(Map<?, ?> json) throws IOException {\r\n                return JsonUtilClient.getPath(json);\r\n            }\r\n        }.run();\r\n        return new Path(strTrashPath).makeQualified(getUri(), null);\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Cannot find trash root of \" + path, e);\r\n        return super.getTrashRoot(path).makeQualified(getUri(), null);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "access",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void access(final Path path, final FsAction mode) throws IOException\n{\r\n    final HttpOpParam.Op op = GetOpParam.Op.CHECKACCESS;\r\n    new FsPathRunner(op, path, new FsActionParam(mode)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getContentSummary",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ContentSummary getContentSummary(final Path p) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_CONTENT_SUMMARY);\r\n    final HttpOpParam.Op op = GetOpParam.Op.GETCONTENTSUMMARY;\r\n    return new FsPathResponseRunner<ContentSummary>(op, p) {\r\n\r\n        @Override\r\n        ContentSummary decodeResponse(Map<?, ?> json) {\r\n            return JsonUtilClient.toContentSummary(json);\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getQuotaUsage",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "QuotaUsage getQuotaUsage(final Path p) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_QUOTA_USAGE);\r\n    final HttpOpParam.Op op = GetOpParam.Op.GETQUOTAUSAGE;\r\n    return new FsPathResponseRunner<QuotaUsage>(op, p) {\r\n\r\n        @Override\r\n        QuotaUsage decodeResponse(Map<?, ?> json) {\r\n            return JsonUtilClient.toQuotaUsage(json);\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "setQuota",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setQuota(Path p, final long namespaceQuota, final long storagespaceQuota) throws IOException\n{\r\n    if ((namespaceQuota <= 0 && namespaceQuota != HdfsConstants.QUOTA_RESET) || (storagespaceQuota < 0 && storagespaceQuota != HdfsConstants.QUOTA_RESET)) {\r\n        throw new IllegalArgumentException(\"Invalid values for quota : \" + namespaceQuota + \" and \" + storagespaceQuota);\r\n    }\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_QUOTA_USAGE);\r\n    final HttpOpParam.Op op = PutOpParam.Op.SETQUOTA;\r\n    new FsPathRunner(op, p, new NameSpaceQuotaParam(namespaceQuota), new StorageSpaceQuotaParam(storagespaceQuota)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "setQuotaByStorageType",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setQuotaByStorageType(Path path, StorageType type, long quota) throws IOException\n{\r\n    if (quota <= 0 && quota != HdfsConstants.QUOTA_RESET) {\r\n        throw new IllegalArgumentException(\"Invalid values for quota :\" + quota);\r\n    }\r\n    if (type == null) {\r\n        throw new IllegalArgumentException(\"Invalid storage type (null)\");\r\n    }\r\n    if (!type.supportTypeQuota()) {\r\n        throw new IllegalArgumentException(\"Quota for storage type '\" + type.toString() + \"' is not supported\");\r\n    }\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_QUOTA_BYTSTORAGEYPE);\r\n    final HttpOpParam.Op op = PutOpParam.Op.SETQUOTABYSTORAGETYPE;\r\n    new FsPathRunner(op, path, new StorageTypeParam(type.name()), new StorageSpaceQuotaParam(quota)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getFileChecksum",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "MD5MD5CRC32FileChecksum getFileChecksum(final Path p) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_FILE_CHECKSUM);\r\n    final HttpOpParam.Op op = GetOpParam.Op.GETFILECHECKSUM;\r\n    return new FsPathResponseRunner<MD5MD5CRC32FileChecksum>(op, p) {\r\n\r\n        @Override\r\n        MD5MD5CRC32FileChecksum decodeResponse(Map<?, ?> json) throws IOException {\r\n            return JsonUtilClient.toMD5MD5CRC32FileChecksum(json);\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "resolveNNAddr",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "InetSocketAddress[] resolveNNAddr()\n{\r\n    Configuration conf = getConf();\r\n    final String scheme = uri.getScheme();\r\n    ArrayList<InetSocketAddress> ret = new ArrayList<>();\r\n    if (!HAUtilClient.isLogicalUri(conf, uri)) {\r\n        InetSocketAddress addr = NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\r\n        ret.add(addr);\r\n    } else {\r\n        Map<String, Map<String, InetSocketAddress>> addresses = DFSUtilClient.getHaNnWebHdfsAddresses(conf, scheme);\r\n        Map<String, InetSocketAddress> addrs = addresses.get(uri.getHost());\r\n        for (InetSocketAddress addr : addrs.values()) {\r\n            ret.add(addr);\r\n        }\r\n    }\r\n    InetSocketAddress[] r = new InetSocketAddress[ret.size()];\r\n    return ret.toArray(r);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getCanonicalServiceName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getCanonicalServiceName()\n{\r\n    return tokenServiceName == null ? super.getCanonicalServiceName() : tokenServiceName.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "setStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setStoragePolicy(Path p, String policyName) throws IOException\n{\r\n    if (policyName == null) {\r\n        throw new IOException(\"policyName == null\");\r\n    }\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_STORAGE_POLICY);\r\n    final HttpOpParam.Op op = PutOpParam.Op.SETSTORAGEPOLICY;\r\n    new FsPathRunner(op, p, new StoragePolicyParam(policyName)).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getAllStoragePolicies",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Collection<BlockStoragePolicy> getAllStoragePolicies() throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_STORAGE_POLICIES);\r\n    final HttpOpParam.Op op = GetOpParam.Op.GETALLSTORAGEPOLICY;\r\n    return new FsPathResponseRunner<Collection<BlockStoragePolicy>>(op, null) {\r\n\r\n        @Override\r\n        Collection<BlockStoragePolicy> decodeResponse(Map<?, ?> json) throws IOException {\r\n            return JsonUtilClient.getStoragePolicies(json);\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "BlockStoragePolicy getStoragePolicy(Path src) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_STORAGE_POLICY);\r\n    final HttpOpParam.Op op = GetOpParam.Op.GETSTORAGEPOLICY;\r\n    return new FsPathResponseRunner<BlockStoragePolicy>(op, src) {\r\n\r\n        @Override\r\n        BlockStoragePolicy decodeResponse(Map<?, ?> json) throws IOException {\r\n            return JsonUtilClient.toBlockStoragePolicy((Map<?, ?>) json.get(BlockStoragePolicy.class.getSimpleName()));\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "unsetStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void unsetStoragePolicy(Path src) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.UNSET_STORAGE_POLICY);\r\n    final HttpOpParam.Op op = PostOpParam.Op.UNSETSTORAGEPOLICY;\r\n    new FsPathRunner(op, src).run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FsServerDefaults getServerDefaults() throws IOException\n{\r\n    final HttpOpParam.Op op = GetOpParam.Op.GETSERVERDEFAULTS;\r\n    return new FsPathResponseRunner<FsServerDefaults>(op, null) {\r\n\r\n        @Override\r\n        FsServerDefaults decodeResponse(Map<?, ?> json) throws IOException {\r\n            return JsonUtilClient.toFsServerDefaults(json);\r\n        }\r\n    }.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getResolvedNNAddr",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InetSocketAddress[] getResolvedNNAddr()\n{\r\n    return nnAddrs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "setRetryPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRetryPolicy(RetryPolicy rp)\n{\r\n    this.retryPolicy = rp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getKeyProviderUri",
  "errType" : [ "UnsupportedOperationException", "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "URI getKeyProviderUri() throws IOException\n{\r\n    String keyProviderUri = null;\r\n    try {\r\n        keyProviderUri = getServerDefaults().getKeyProviderUri();\r\n    } catch (UnsupportedOperationException e) {\r\n    } catch (RemoteException e) {\r\n        if (e.getClassName() != null && e.getClassName().equals(\"java.lang.IllegalArgumentException\")) {\r\n        } else {\r\n            throw e;\r\n        }\r\n    }\r\n    return HdfsKMSUtil.getKeyProviderUri(ugi, getUri(), keyProviderUri, getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getKeyProvider",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "KeyProvider getKeyProvider() throws IOException\n{\r\n    if (testProvider != null) {\r\n        return testProvider;\r\n    }\r\n    URI keyProviderUri = getKeyProviderUri();\r\n    if (keyProviderUri == null) {\r\n        return null;\r\n    }\r\n    return KMSUtil.createKeyProviderFromUri(getConf(), keyProviderUri);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "setTestProvider",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTestProvider(KeyProvider kp)\n{\r\n    testProvider = kp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "hasPathCapability",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean hasPathCapability(final Path path, final String capability) throws IOException\n{\r\n    final Path p = makeQualified(path);\r\n    Optional<Boolean> cap = DfsPathCapabilities.hasPathCapability(p, capability);\r\n    if (cap.isPresent()) {\r\n        return cap.get();\r\n    }\r\n    return super.hasPathCapability(p, capability);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "createMultipartUploader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MultipartUploaderBuilder createMultipartUploader(final Path basePath) throws IOException\n{\r\n    return new FileSystemMultipartUploaderBuilder(this, basePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createKeyProvider",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "KeyProvider createKeyProvider(final Configuration conf) throws IOException\n{\r\n    return KMSUtil.createKeyProvider(conf, keyProviderUriKeyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getCryptoProtocolVersion",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "CryptoProtocolVersion getCryptoProtocolVersion(FileEncryptionInfo feInfo) throws IOException\n{\r\n    final CryptoProtocolVersion version = feInfo.getCryptoProtocolVersion();\r\n    if (!CryptoProtocolVersion.supports(version)) {\r\n        throw new IOException(\"Client does not support specified \" + \"CryptoProtocolVersion \" + version.getDescription() + \" version \" + \"number\" + version.getVersion());\r\n    }\r\n    return version;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getCryptoCodec",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "CryptoCodec getCryptoCodec(Configuration conf, FileEncryptionInfo feInfo) throws IOException\n{\r\n    final CipherSuite suite = feInfo.getCipherSuite();\r\n    if (suite.equals(CipherSuite.UNKNOWN)) {\r\n        throw new IOException(\"NameNode specified unknown CipherSuite with ID \" + suite.getUnknownValue() + \", cannot instantiate CryptoCodec.\");\r\n    }\r\n    final CryptoCodec codec = CryptoCodec.getInstance(conf, suite);\r\n    if (codec == null) {\r\n        throw new UnknownCipherSuiteException(\"No configuration found for the cipher suite \" + suite.getConfigSuffix() + \" prefixed with \" + HADOOP_SECURITY_CRYPTO_CODEC_CLASSES_KEY_PREFIX + \". Please see the example configuration \" + \"hadoop.security.crypto.codec.classes.EXAMPLECIPHERSUITE \" + \"at core-default.xml for details.\");\r\n    }\r\n    return codec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getKeyProviderUri",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "URI getKeyProviderUri(UserGroupInformation ugi, URI namenodeUri, String keyProviderUriStr, Configuration conf) throws IOException\n{\r\n    URI keyProviderUri = null;\r\n    Credentials credentials = ugi.getCredentials();\r\n    Text credsKey = getKeyProviderMapKey(namenodeUri);\r\n    byte[] keyProviderUriBytes = credentials.getSecretKey(credsKey);\r\n    if (keyProviderUriBytes != null) {\r\n        keyProviderUri = URI.create(DFSUtilClient.bytes2String(keyProviderUriBytes));\r\n    }\r\n    if (keyProviderUri == null) {\r\n        if (keyProviderUriStr != null && !conf.getBoolean(DFS_CLIENT_IGNORE_NAMENODE_DEFAULT_KMS_URI, DFS_CLIENT_IGNORE_NAMENODE_DEFAULT_KMS_URI_DEFAULT)) {\r\n            if (!keyProviderUriStr.isEmpty()) {\r\n                keyProviderUri = URI.create(keyProviderUriStr);\r\n            }\r\n        }\r\n        if (keyProviderUri == null) {\r\n            keyProviderUri = KMSUtil.getKeyProviderUri(conf, keyProviderUriKeyName);\r\n        }\r\n        if (keyProviderUri != null) {\r\n            credentials.addSecretKey(credsKey, DFSUtilClient.string2Bytes(keyProviderUri.toString()));\r\n        }\r\n    }\r\n    return keyProviderUri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getKeyProvider",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "KeyProvider getKeyProvider(KeyProviderTokenIssuer issuer, Configuration conf) throws IOException\n{\r\n    URI keyProviderUri = issuer.getKeyProviderUri();\r\n    if (keyProviderUri != null) {\r\n        return KMSUtil.createKeyProviderFromUri(conf, keyProviderUri);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getKeyProviderMapKey",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Text getKeyProviderMapKey(URI namenodeUri)\n{\r\n    return new Text(DFS_KMS_PREFIX + namenodeUri.getScheme() + \"://\" + namenodeUri.getAuthority());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createWrappedInputStream",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "CryptoInputStream createWrappedInputStream(InputStream is, KeyProvider keyProvider, FileEncryptionInfo fileEncryptionInfo, Configuration conf) throws IOException\n{\r\n    HdfsKMSUtil.getCryptoProtocolVersion(fileEncryptionInfo);\r\n    final CryptoCodec codec = HdfsKMSUtil.getCryptoCodec(conf, fileEncryptionInfo);\r\n    final KeyVersion decrypted = decryptEncryptedDataEncryptionKey(fileEncryptionInfo, keyProvider);\r\n    return new CryptoInputStream(is, codec, decrypted.getMaterial(), fileEncryptionInfo.getIV());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "decryptEncryptedDataEncryptionKey",
  "errType" : [ "GeneralSecurityException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "KeyVersion decryptEncryptedDataEncryptionKey(FileEncryptionInfo feInfo, KeyProvider keyProvider) throws IOException\n{\r\n    if (keyProvider == null) {\r\n        throw new IOException(\"No KeyProvider is configured, cannot access\" + \" an encrypted file\");\r\n    }\r\n    EncryptedKeyVersion ekv = EncryptedKeyVersion.createForDecryption(feInfo.getKeyName(), feInfo.getEzKeyVersionName(), feInfo.getIV(), feInfo.getEncryptedDataEncryptionKey());\r\n    try {\r\n        KeyProviderCryptoExtension cryptoProvider = KeyProviderCryptoExtension.createKeyProviderCryptoExtension(keyProvider);\r\n        return cryptoProvider.decryptEncryptedKey(ekv);\r\n    } catch (GeneralSecurityException e) {\r\n        throw new IOException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "useDirectBuffer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean useDirectBuffer()\n{\r\n    return decoder.preferDirectBuffer();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "resetCurStripeBuffer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void resetCurStripeBuffer(boolean shouldAllocateBuf)\n{\r\n    if (shouldAllocateBuf && curStripeBuf == null) {\r\n        curStripeBuf = BUFFER_POOL.getBuffer(useDirectBuffer(), cellSize * dataBlkNum);\r\n    }\r\n    if (curStripeBuf != null) {\r\n        curStripeBuf.clear();\r\n    }\r\n    curStripeRange = new StripeRange(0, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getParityBuffer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ByteBuffer getParityBuffer()\n{\r\n    if (parityBuf == null) {\r\n        parityBuf = BUFFER_POOL.getBuffer(useDirectBuffer(), cellSize * parityBlkNum);\r\n    }\r\n    parityBuf.clear();\r\n    return parityBuf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getCurStripeBuf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ByteBuffer getCurStripeBuf()\n{\r\n    return curStripeBuf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBufferPool",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ByteBufferPool getBufferPool()\n{\r\n    return BUFFER_POOL;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStripedReadsThreadPool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ThreadPoolExecutor getStripedReadsThreadPool()\n{\r\n    return dfsClient.getStripedReadsThreadPool();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "blockSeekTo",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void blockSeekTo(long target) throws IOException\n{\r\n    if (target >= getFileLength()) {\r\n        throw new IOException(\"Attempted to read past end of file\");\r\n    }\r\n    maybeRegisterBlockRefresh();\r\n    closeCurrentBlockReaders();\r\n    LocatedStripedBlock targetBlockGroup = getBlockGroupAt(target);\r\n    this.pos = target;\r\n    this.blockEnd = targetBlockGroup.getStartOffset() + targetBlockGroup.getBlockSize() - 1;\r\n    currentLocatedBlock = targetBlockGroup;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    try {\r\n        super.close();\r\n    } finally {\r\n        if (curStripeBuf != null) {\r\n            BUFFER_POOL.putBuffer(curStripeBuf);\r\n            curStripeBuf = null;\r\n        }\r\n        if (parityBuf != null) {\r\n            BUFFER_POOL.putBuffer(parityBuf);\r\n            parityBuf = null;\r\n        }\r\n        if (decoder != null) {\r\n            decoder.release();\r\n            decoder = null;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "closeCurrentBlockReaders",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void closeCurrentBlockReaders()\n{\r\n    resetCurStripeBuffer(false);\r\n    if (blockReaders == null || blockReaders.length == 0) {\r\n        return;\r\n    }\r\n    for (int i = 0; i < groupSize; i++) {\r\n        closeReader(blockReaders[i]);\r\n        blockReaders[i] = null;\r\n    }\r\n    blockEnd = -1;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "closeReader",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void closeReader(BlockReaderInfo readerInfo)\n{\r\n    if (readerInfo != null) {\r\n        if (readerInfo.reader != null) {\r\n            try {\r\n                readerInfo.reader.close();\r\n            } catch (Throwable ignored) {\r\n            }\r\n        }\r\n        readerInfo.skip();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getOffsetInBlockGroup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getOffsetInBlockGroup()\n{\r\n    return getOffsetInBlockGroup(pos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getOffsetInBlockGroup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getOffsetInBlockGroup(long pos)\n{\r\n    return pos - currentLocatedBlock.getStartOffset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createBlockReader",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "boolean createBlockReader(LocatedBlock block, long offsetInBlock, LocatedBlock[] targetBlocks, BlockReaderInfo[] readerInfos, int chunkIndex) throws IOException\n{\r\n    BlockReader reader = null;\r\n    final ReaderRetryPolicy retry = new ReaderRetryPolicy();\r\n    DFSInputStream.DNAddrPair dnInfo = new DFSInputStream.DNAddrPair(null, null, null, null);\r\n    while (true) {\r\n        try {\r\n            block = refreshLocatedBlock(block);\r\n            targetBlocks[chunkIndex] = block;\r\n            dnInfo = getBestNodeDNAddrPair(block, null);\r\n            if (dnInfo == null) {\r\n                break;\r\n            }\r\n            reader = getBlockReader(block, offsetInBlock, block.getBlockSize() - offsetInBlock, dnInfo.addr, dnInfo.storageType, dnInfo.info);\r\n        } catch (IOException e) {\r\n            if (e instanceof InvalidEncryptionKeyException && retry.shouldRefetchEncryptionKey()) {\r\n                DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" + \"encryption key was invalid when connecting to \" + dnInfo.addr + \" : \" + e);\r\n                dfsClient.clearDataEncryptionKey();\r\n                retry.refetchEncryptionKey();\r\n            } else if (retry.shouldRefetchToken() && tokenRefetchNeeded(e, dnInfo.addr)) {\r\n                fetchBlockAt(block.getStartOffset());\r\n                retry.refetchToken();\r\n            } else {\r\n                DFSClient.LOG.warn(\"Failed to connect to \" + dnInfo.addr + \" for \" + \"block\" + block.getBlock(), e);\r\n                fetchBlockAt(block.getStartOffset());\r\n                addToLocalDeadNodes(dnInfo.info);\r\n            }\r\n        }\r\n        if (reader != null) {\r\n            readerInfos[chunkIndex] = new BlockReaderInfo(reader, dnInfo.info, offsetInBlock);\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readOneStripe",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void readOneStripe(CorruptedBlocks corruptedBlocks) throws IOException\n{\r\n    resetCurStripeBuffer(true);\r\n    final long offsetInBlockGroup = getOffsetInBlockGroup();\r\n    final long stripeLen = cellSize * dataBlkNum;\r\n    final int stripeIndex = (int) (offsetInBlockGroup / stripeLen);\r\n    final int stripeBufOffset = (int) (offsetInBlockGroup % stripeLen);\r\n    final int stripeLimit = (int) Math.min(currentLocatedBlock.getBlockSize() - (stripeIndex * stripeLen), stripeLen);\r\n    StripeRange stripeRange = new StripeRange(offsetInBlockGroup, stripeLimit - stripeBufOffset);\r\n    LocatedStripedBlock blockGroup = (LocatedStripedBlock) currentLocatedBlock;\r\n    AlignedStripe[] stripes = StripedBlockUtil.divideOneStripe(ecPolicy, cellSize, blockGroup, offsetInBlockGroup, offsetInBlockGroup + stripeRange.getLength() - 1, curStripeBuf);\r\n    final LocatedBlock[] blks = StripedBlockUtil.parseStripedBlockGroup(blockGroup, cellSize, dataBlkNum, parityBlkNum);\r\n    for (AlignedStripe stripe : stripes) {\r\n        StripeReader sreader = new StatefulStripeReader(stripe, ecPolicy, blks, blockReaders, corruptedBlocks, decoder, this);\r\n        sreader.readStripe();\r\n    }\r\n    curStripeBuf.position(stripeBufOffset);\r\n    curStripeBuf.limit(stripeLimit);\r\n    curStripeRange = stripeRange;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "updateReadStats",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void updateReadStats(final StripedBlockUtil.BlockReadStats stats)\n{\r\n    if (stats == null) {\r\n        return;\r\n    }\r\n    updateReadStatistics(readStatistics, stats.getBytesRead(), stats.isShortCircuit(), stats.getNetworkDistance());\r\n    dfsClient.updateFileSystemReadStats(stats.getNetworkDistance(), stats.getBytesRead());\r\n    assert readStatistics.getBlockType() == BlockType.STRIPED;\r\n    dfsClient.updateFileSystemECReadStats(stats.getBytesRead());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "seek",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void seek(long targetPos) throws IOException\n{\r\n    if (targetPos > getFileLength()) {\r\n        throw new EOFException(\"Cannot seek after EOF\");\r\n    }\r\n    if (targetPos < 0) {\r\n        throw new EOFException(\"Cannot seek to negative offset\");\r\n    }\r\n    if (closed.get()) {\r\n        throw new IOException(\"Stream is closed!\");\r\n    }\r\n    if (targetPos <= blockEnd) {\r\n        final long targetOffsetInBlk = getOffsetInBlockGroup(targetPos);\r\n        if (curStripeRange.include(targetOffsetInBlk)) {\r\n            int bufOffset = getStripedBufOffset(targetOffsetInBlk);\r\n            curStripeBuf.position(bufOffset);\r\n            pos = targetPos;\r\n            return;\r\n        }\r\n    }\r\n    pos = targetPos;\r\n    blockEnd = -1;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStripedBufOffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getStripedBufOffset(long offsetInBlockGroup)\n{\r\n    final long stripeLen = cellSize * dataBlkNum;\r\n    return (int) (offsetInBlockGroup % stripeLen);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "seekToNewSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean seekToNewSource(long targetPos) throws IOException\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readWithStrategy",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "int readWithStrategy(ReaderStrategy strategy) throws IOException\n{\r\n    dfsClient.checkOpen();\r\n    if (closed.get()) {\r\n        throw new IOException(\"Stream closed\");\r\n    }\r\n    int len = strategy.getTargetLength();\r\n    CorruptedBlocks corruptedBlocks = new CorruptedBlocks();\r\n    if (pos < getFileLength()) {\r\n        try {\r\n            if (pos > blockEnd) {\r\n                blockSeekTo(pos);\r\n            }\r\n            int realLen = (int) Math.min(len, (blockEnd - pos + 1L));\r\n            synchronized (infoLock) {\r\n                if (locatedBlocks.isLastBlockComplete()) {\r\n                    realLen = (int) Math.min(realLen, locatedBlocks.getFileLength() - pos);\r\n                }\r\n            }\r\n            int result = 0;\r\n            while (result < realLen) {\r\n                if (!curStripeRange.include(getOffsetInBlockGroup())) {\r\n                    readOneStripe(corruptedBlocks);\r\n                }\r\n                int ret = copyToTargetBuf(strategy, realLen - result);\r\n                result += ret;\r\n                pos += ret;\r\n            }\r\n            return result;\r\n        } finally {\r\n            reportCheckSumFailure(corruptedBlocks, getCurrentBlockLocationsLength(), true);\r\n        }\r\n    }\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "copyToTargetBuf",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int copyToTargetBuf(ReaderStrategy strategy, int length)\n{\r\n    final long offsetInBlk = getOffsetInBlockGroup();\r\n    int bufOffset = getStripedBufOffset(offsetInBlk);\r\n    curStripeBuf.position(bufOffset);\r\n    return strategy.readFromBuffer(curStripeBuf, Math.min(length, curStripeBuf.remaining()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "refreshLocatedBlock",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "LocatedBlock refreshLocatedBlock(LocatedBlock block) throws IOException\n{\r\n    int idx = StripedBlockUtil.getBlockIndex(block.getBlock().getLocalBlock());\r\n    LocatedBlock lb = getBlockGroupAt(block.getStartOffset());\r\n    LocatedStripedBlock lsb = (LocatedStripedBlock) lb;\r\n    int i = 0;\r\n    for (; i < lsb.getBlockIndices().length; i++) {\r\n        if (lsb.getBlockIndices()[i] == idx) {\r\n            break;\r\n        }\r\n    }\r\n    DFSClient.LOG.debug(\"refreshLocatedBlock for striped blocks, offset={}.\" + \" Obtained block {}, idx={}\", block.getStartOffset(), lb, idx);\r\n    return StripedBlockUtil.constructInternalBlock(lsb, i, cellSize, dataBlkNum, idx);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBlockGroupAt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocatedStripedBlock getBlockGroupAt(long offset) throws IOException\n{\r\n    LocatedBlock lb = super.getBlockAt(offset);\r\n    assert lb instanceof LocatedStripedBlock : \"NameNode\" + \" should return a LocatedStripedBlock for a striped file\";\r\n    return (LocatedStripedBlock) lb;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "fetchBlockByteRange",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void fetchBlockByteRange(LocatedBlock block, long start, long end, ByteBuffer buf, CorruptedBlocks corruptedBlocks) throws IOException\n{\r\n    LocatedStripedBlock blockGroup = getBlockGroupAt(block.getStartOffset());\r\n    AlignedStripe[] stripes = StripedBlockUtil.divideByteRangeIntoStripes(ecPolicy, cellSize, blockGroup, start, end, buf);\r\n    final LocatedBlock[] blks = StripedBlockUtil.parseStripedBlockGroup(blockGroup, cellSize, dataBlkNum, parityBlkNum);\r\n    final BlockReaderInfo[] preaderInfos = new BlockReaderInfo[groupSize];\r\n    try {\r\n        for (AlignedStripe stripe : stripes) {\r\n            StripeReader preader = new PositionStripeReader(stripe, ecPolicy, blks, preaderInfos, corruptedBlocks, decoder, this);\r\n            try {\r\n                preader.readStripe();\r\n            } finally {\r\n                preader.close();\r\n            }\r\n        }\r\n        buf.position(buf.position() + (int) (end - start + 1));\r\n    } finally {\r\n        for (BlockReaderInfo preaderInfo : preaderInfos) {\r\n            closeReader(preaderInfo);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "reportLostBlock",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void reportLostBlock(LocatedBlock lostBlock, Collection<DatanodeInfo> ignoredNodes)\n{\r\n    DatanodeInfo[] nodes = lostBlock.getLocations();\r\n    if (nodes != null && nodes.length > 0) {\r\n        List<String> dnUUIDs = new ArrayList<>();\r\n        for (DatanodeInfo node : nodes) {\r\n            dnUUIDs.add(node.getDatanodeUuid());\r\n        }\r\n        if (!warnedNodes.containsAll(dnUUIDs)) {\r\n            DFSClient.LOG.warn(Arrays.toString(nodes) + \" are unavailable and \" + \"all striping blocks on them are lost. \" + \"IgnoredNodes = {}\", ignoredNodes);\r\n            warnedNodes.addAll(dnUUIDs);\r\n        }\r\n    } else {\r\n        super.reportLostBlock(lostBlock, ignoredNodes);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ByteBuffer read(ByteBufferPool bufferPool, int maxLength, EnumSet<ReadOption> opts) throws IOException, UnsupportedOperationException\n{\r\n    throw new UnsupportedOperationException(\"Not support enhanced byte buffer access.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "releaseBuffer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void releaseBuffer(ByteBuffer buffer)\n{\r\n    throw new UnsupportedOperationException(\"Not support enhanced byte buffer access.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "unbuffer",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void unbuffer()\n{\r\n    super.unbuffer();\r\n    if (curStripeBuf != null) {\r\n        BUFFER_POOL.putBuffer(curStripeBuf);\r\n        curStripeBuf = null;\r\n    }\r\n    if (parityBuf != null) {\r\n        BUFFER_POOL.putBuffer(parityBuf);\r\n        parityBuf = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setPathName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setPathName(String path)\n{\r\n    this.pathName = path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getMessage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getMessage()\n{\r\n    return super.getMessage();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createPacket",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DFSPacket createPacket(int packetSize, int chunksPerPkt, long offsetInBlock, long seqno, boolean lastPacketInBlock) throws InterruptedIOException\n{\r\n    final byte[] buf;\r\n    final int bufferSize = PacketHeader.PKT_MAX_HEADER_LEN + packetSize;\r\n    try {\r\n        buf = byteArrayManager.newByteArray(bufferSize);\r\n    } catch (InterruptedException ie) {\r\n        final InterruptedIOException iioe = new InterruptedIOException(\"seqno=\" + seqno);\r\n        iioe.initCause(ie);\r\n        throw iioe;\r\n    }\r\n    return new DFSPacket(buf, chunksPerPkt, offsetInBlock, seqno, getChecksumSize(), lastPacketInBlock);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkClosed",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkClosed() throws IOException\n{\r\n    if (isClosed()) {\r\n        getStreamer().getLastException().throwException4Close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getPipeline",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DatanodeInfo[] getPipeline()\n{\r\n    if (getStreamer().streamerClosed()) {\r\n        return null;\r\n    }\r\n    DatanodeInfo[] currentNodes = getStreamer().getNodes();\r\n    if (currentNodes == null) {\r\n        return null;\r\n    }\r\n    DatanodeInfo[] value = new DatanodeInfo[currentNodes.length];\r\n    System.arraycopy(currentNodes, 0, value, 0, currentNodes.length);\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getChecksum4Compute",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DataChecksum getChecksum4Compute(DataChecksum checksum, HdfsFileStatus stat)\n{\r\n    if (DataStreamer.isLazyPersist(stat) && stat.getReplication() == 1) {\r\n        return DataChecksum.newDataChecksum(Type.NULL, checksum.getBytesPerChecksum());\r\n    }\r\n    return checksum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "initWritePacketSize",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initWritePacketSize()\n{\r\n    writePacketSize = dfsClient.getConf().getWritePacketSize();\r\n    if (writePacketSize > PacketReceiver.MAX_PACKET_SIZE) {\r\n        LOG.warn(\"Configured write packet exceeds {} bytes as max,\" + \" using {} bytes.\", PacketReceiver.MAX_PACKET_SIZE, PacketReceiver.MAX_PACKET_SIZE);\r\n        writePacketSize = PacketReceiver.MAX_PACKET_SIZE;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "newStreamForCreate",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src, FsPermission masked, EnumSet<CreateFlag> flag, boolean createParent, short replication, long blockSize, Progressable progress, DataChecksum checksum, String[] favoredNodes, String ecPolicyName, String storagePolicy) throws IOException\n{\r\n    try (TraceScope ignored = dfsClient.newPathTraceScope(\"newStreamForCreate\", src)) {\r\n        HdfsFileStatus stat = null;\r\n        boolean shouldRetry = true;\r\n        int retryCount = CREATE_RETRY_COUNT;\r\n        while (shouldRetry) {\r\n            shouldRetry = false;\r\n            try {\r\n                stat = dfsClient.namenode.create(src, masked, dfsClient.clientName, new EnumSetWritable<>(flag), createParent, replication, blockSize, SUPPORTED_CRYPTO_VERSIONS, ecPolicyName, storagePolicy);\r\n                break;\r\n            } catch (RemoteException re) {\r\n                IOException e = re.unwrapRemoteException(AccessControlException.class, DSQuotaExceededException.class, QuotaByStorageTypeExceededException.class, FileAlreadyExistsException.class, FileNotFoundException.class, ParentNotDirectoryException.class, NSQuotaExceededException.class, RetryStartFileException.class, SafeModeException.class, UnresolvedPathException.class, SnapshotAccessControlException.class, UnknownCryptoProtocolVersionException.class);\r\n                if (e instanceof RetryStartFileException) {\r\n                    if (retryCount > 0) {\r\n                        shouldRetry = true;\r\n                        retryCount--;\r\n                    } else {\r\n                        throw new IOException(\"Too many retries because of encryption\" + \" zone operations\", e);\r\n                    }\r\n                } else {\r\n                    throw e;\r\n                }\r\n            }\r\n        }\r\n        Preconditions.checkNotNull(stat, \"HdfsFileStatus should not be null!\");\r\n        final DFSOutputStream out;\r\n        if (stat.getErasureCodingPolicy() != null) {\r\n            out = new DFSStripedOutputStream(dfsClient, src, stat, flag, progress, checksum, favoredNodes);\r\n        } else {\r\n            out = new DFSOutputStream(dfsClient, src, stat, flag, progress, checksum, favoredNodes, true);\r\n        }\r\n        out.start();\r\n        return out;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "adjustPacketChunkSize",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void adjustPacketChunkSize(HdfsFileStatus stat) throws IOException\n{\r\n    long usedInLastBlock = stat.getLen() % blockSize;\r\n    int freeInLastBlock = (int) (blockSize - usedInLastBlock);\r\n    int usedInCksum = (int) (stat.getLen() % bytesPerChecksum);\r\n    int freeInCksum = bytesPerChecksum - usedInCksum;\r\n    if (freeInLastBlock == blockSize) {\r\n        throw new IOException(\"The last block for file \" + src + \" is full.\");\r\n    }\r\n    if (usedInCksum > 0 && freeInCksum > 0) {\r\n        computePacketChunkSize(0, freeInCksum);\r\n        setChecksumBufSize(freeInCksum);\r\n        getStreamer().setAppendChunk(true);\r\n    } else {\r\n        computePacketChunkSize(Math.min(dfsClient.getConf().getWritePacketSize(), freeInLastBlock), bytesPerChecksum);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "newStreamForAppend",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DFSOutputStream newStreamForAppend(DFSClient dfsClient, String src, EnumSet<CreateFlag> flags, Progressable progress, LocatedBlock lastBlock, HdfsFileStatus stat, DataChecksum checksum, String[] favoredNodes) throws IOException\n{\r\n    try (TraceScope ignored = dfsClient.newPathTraceScope(\"newStreamForAppend\", src)) {\r\n        DFSOutputStream out;\r\n        if (stat.isErasureCoded()) {\r\n            out = new DFSStripedOutputStream(dfsClient, src, flags, progress, lastBlock, stat, checksum, favoredNodes);\r\n        } else {\r\n            out = new DFSOutputStream(dfsClient, src, flags, progress, lastBlock, stat, checksum, favoredNodes);\r\n        }\r\n        out.start();\r\n        return out;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "computePacketChunkSize",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void computePacketChunkSize(int psize, int csize)\n{\r\n    final int bodySize = psize - PacketHeader.PKT_MAX_HEADER_LEN;\r\n    final int chunkSize = csize + getChecksumSize();\r\n    chunksPerPacket = Math.max(bodySize / chunkSize, 1);\r\n    packetSize = chunkSize * chunksPerPacket;\r\n    DFSClient.LOG.debug(\"computePacketChunkSize: src={}, chunkSize={}, \" + \"chunksPerPacket={}, packetSize={}\", src, chunkSize, chunksPerPacket, packetSize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createWriteTraceScope",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TraceScope createWriteTraceScope()\n{\r\n    return dfsClient.newPathTraceScope(\"DFSOutputStream#write\", src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "writeChunk",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void writeChunk(byte[] b, int offset, int len, byte[] checksum, int ckoff, int cklen) throws IOException\n{\r\n    writeChunkPrepare(len, ckoff, cklen);\r\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\r\n    currentPacket.writeData(b, offset, len);\r\n    currentPacket.incNumChunks();\r\n    getStreamer().incBytesCurBlock(len);\r\n    if (currentPacket.getNumChunks() == currentPacket.getMaxChunks() || getStreamer().getBytesCurBlock() == blockSize) {\r\n        enqueueCurrentPacketFull();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "writeChunk",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void writeChunk(ByteBuffer buffer, int len, byte[] checksum, int ckoff, int cklen) throws IOException\n{\r\n    writeChunkPrepare(len, ckoff, cklen);\r\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\r\n    currentPacket.writeData(buffer, len);\r\n    currentPacket.incNumChunks();\r\n    getStreamer().incBytesCurBlock(len);\r\n    if (currentPacket.getNumChunks() == currentPacket.getMaxChunks() || getStreamer().getBytesCurBlock() == blockSize) {\r\n        enqueueCurrentPacketFull();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "writeChunkPrepare",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void writeChunkPrepare(int buflen, int ckoff, int cklen) throws IOException\n{\r\n    dfsClient.checkOpen();\r\n    checkClosed();\r\n    if (buflen > bytesPerChecksum) {\r\n        throw new IOException(\"writeChunk() buffer size is \" + buflen + \" is larger than supported  bytesPerChecksum \" + bytesPerChecksum);\r\n    }\r\n    if (cklen != 0 && cklen != getChecksumSize()) {\r\n        throw new IOException(\"writeChunk() checksum size is supposed to be \" + getChecksumSize() + \" but found to be \" + cklen);\r\n    }\r\n    if (currentPacket == null) {\r\n        currentPacket = createPacket(packetSize, chunksPerPacket, getStreamer().getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\r\n        DFSClient.LOG.debug(\"WriteChunk allocating new packet seqno={},\" + \" src={}, packetSize={}, chunksPerPacket={}, bytesCurBlock={},\" + \" output stream={}\", currentPacket.getSeqno(), src, packetSize, chunksPerPacket, getStreamer().getBytesCurBlock(), this);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "enqueueCurrentPacket",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void enqueueCurrentPacket() throws IOException\n{\r\n    getStreamer().waitAndQueuePacket(currentPacket);\r\n    currentPacket = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "enqueueCurrentPacketFull",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void enqueueCurrentPacketFull() throws IOException\n{\r\n    LOG.debug(\"enqueue full {}, src={}, bytesCurBlock={}, blockSize={},\" + \" appendChunk={}, {}\", currentPacket, src, getStreamer().getBytesCurBlock(), blockSize, getStreamer().getAppendChunk(), getStreamer());\r\n    enqueueCurrentPacket();\r\n    adjustChunkBoundary();\r\n    endBlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setCurrentPacketToEmpty",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setCurrentPacketToEmpty() throws InterruptedIOException\n{\r\n    currentPacket = createPacket(0, 0, getStreamer().getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), true);\r\n    currentPacket.setSyncBlock(shouldSyncBlock);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "adjustChunkBoundary",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void adjustChunkBoundary()\n{\r\n    if (getStreamer().getAppendChunk() && getStreamer().getBytesCurBlock() % bytesPerChecksum == 0) {\r\n        getStreamer().setAppendChunk(false);\r\n        resetChecksumBufSize();\r\n    }\r\n    if (!getStreamer().getAppendChunk()) {\r\n        final int psize = (int) Math.min(blockSize - getStreamer().getBytesCurBlock(), writePacketSize);\r\n        computePacketChunkSize(psize, bytesPerChecksum);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setAppendChunk",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setAppendChunk(final boolean appendChunk)\n{\r\n    getStreamer().setAppendChunk(appendChunk);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setBytesCurBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setBytesCurBlock(final long bytesCurBlock)\n{\r\n    getStreamer().setBytesCurBlock(bytesCurBlock);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "endBlock",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void endBlock() throws IOException\n{\r\n    if (getStreamer().getBytesCurBlock() == blockSize) {\r\n        setCurrentPacketToEmpty();\r\n        enqueueCurrentPacket();\r\n        getStreamer().setBytesCurBlock(0);\r\n        lastFlushOffset = 0;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "hasCapability",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasCapability(String capability)\n{\r\n    return StoreImplementationUtils.isProbeForSyncable(capability);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "hflush",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void hflush() throws IOException\n{\r\n    try (TraceScope ignored = dfsClient.newPathTraceScope(\"hflush\", src)) {\r\n        flushOrSync(false, EnumSet.noneOf(SyncFlag.class));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "hsync",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void hsync() throws IOException\n{\r\n    try (TraceScope ignored = dfsClient.newPathTraceScope(\"hsync\", src)) {\r\n        flushOrSync(true, EnumSet.noneOf(SyncFlag.class));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "hsync",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void hsync(EnumSet<SyncFlag> syncFlags) throws IOException\n{\r\n    try (TraceScope ignored = dfsClient.newPathTraceScope(\"hsync\", src)) {\r\n        flushOrSync(true, syncFlags);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "flushOrSync",
  "errType" : [ "InterruptedIOException", "IOException", "IOException" ],
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void flushOrSync(boolean isSync, EnumSet<SyncFlag> syncFlags) throws IOException\n{\r\n    dfsClient.checkOpen();\r\n    checkClosed();\r\n    try {\r\n        long toWaitFor;\r\n        long lastBlockLength = -1L;\r\n        boolean updateLength = syncFlags.contains(SyncFlag.UPDATE_LENGTH);\r\n        boolean endBlock = syncFlags.contains(SyncFlag.END_BLOCK);\r\n        synchronized (this) {\r\n            int numKept = flushBuffer(!endBlock, true);\r\n            DFSClient.LOG.debug(\"DFSClient flush():  bytesCurBlock={}, \" + \"lastFlushOffset={}, createNewBlock={}\", getStreamer().getBytesCurBlock(), lastFlushOffset, endBlock);\r\n            if (lastFlushOffset != getStreamer().getBytesCurBlock()) {\r\n                assert getStreamer().getBytesCurBlock() > lastFlushOffset;\r\n                lastFlushOffset = getStreamer().getBytesCurBlock();\r\n                if (isSync && currentPacket == null && !endBlock) {\r\n                    currentPacket = createPacket(packetSize, chunksPerPacket, getStreamer().getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\r\n                }\r\n            } else {\r\n                if (isSync && getStreamer().getBytesCurBlock() > 0 && !endBlock) {\r\n                    currentPacket = createPacket(packetSize, chunksPerPacket, getStreamer().getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), false);\r\n                } else if (currentPacket != null) {\r\n                    currentPacket.releaseBuffer(byteArrayManager);\r\n                    currentPacket = null;\r\n                }\r\n            }\r\n            if (currentPacket != null) {\r\n                currentPacket.setSyncBlock(isSync);\r\n                enqueueCurrentPacket();\r\n            }\r\n            if (endBlock && getStreamer().getBytesCurBlock() > 0) {\r\n                currentPacket = createPacket(0, 0, getStreamer().getBytesCurBlock(), getStreamer().getAndIncCurrentSeqno(), true);\r\n                currentPacket.setSyncBlock(shouldSyncBlock || isSync);\r\n                enqueueCurrentPacket();\r\n                getStreamer().setBytesCurBlock(0);\r\n                lastFlushOffset = 0;\r\n            } else {\r\n                getStreamer().setBytesCurBlock(getStreamer().getBytesCurBlock() - numKept);\r\n            }\r\n            toWaitFor = getStreamer().getLastQueuedSeqno();\r\n        }\r\n        getStreamer().waitForAckedSeqno(toWaitFor);\r\n        if (updateLength || getStreamer().getPersistBlocks().get()) {\r\n            synchronized (this) {\r\n                if (!getStreamer().streamerClosed() && getStreamer().getBlock() != null) {\r\n                    lastBlockLength = getStreamer().getBlock().getNumBytes();\r\n                }\r\n            }\r\n        }\r\n        if (getStreamer().getPersistBlocks().getAndSet(false) || updateLength) {\r\n            try {\r\n                dfsClient.namenode.fsync(src, fileId, dfsClient.clientName, lastBlockLength);\r\n            } catch (IOException ioe) {\r\n                DFSClient.LOG.warn(\"Unable to persist blocks in hflush for \" + src, ioe);\r\n                checkClosed();\r\n                throw ioe;\r\n            }\r\n        }\r\n        synchronized (this) {\r\n            if (!getStreamer().streamerClosed()) {\r\n                getStreamer().setHflush();\r\n            }\r\n        }\r\n    } catch (InterruptedIOException interrupt) {\r\n        throw interrupt;\r\n    } catch (IOException e) {\r\n        DFSClient.LOG.warn(\"Error while syncing\", e);\r\n        synchronized (this) {\r\n            if (!isClosed()) {\r\n                getStreamer().getLastException().set(e);\r\n                closeThreads(true);\r\n            }\r\n        }\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getNumCurrentReplicas",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumCurrentReplicas() throws IOException\n{\r\n    return getCurrentBlockReplication();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getCurrentBlockReplication",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int getCurrentBlockReplication() throws IOException\n{\r\n    dfsClient.checkOpen();\r\n    checkClosed();\r\n    if (getStreamer().streamerClosed()) {\r\n        return blockReplication;\r\n    }\r\n    DatanodeInfo[] currentNodes = getStreamer().getNodes();\r\n    if (currentNodes == null) {\r\n        return blockReplication;\r\n    }\r\n    return currentNodes.length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "flushInternal",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void flushInternal() throws IOException\n{\r\n    long toWaitFor = flushInternalWithoutWaitingAck();\r\n    getStreamer().waitForAckedSeqno(toWaitFor);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "start",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void start()\n{\r\n    getStreamer().start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "abort",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void abort() throws IOException\n{\r\n    final MultipleIOException.Builder b = new MultipleIOException.Builder();\r\n    synchronized (this) {\r\n        if (isClosed()) {\r\n            return;\r\n        }\r\n        getStreamer().getLastException().set(new IOException(\"Lease timeout of \" + (dfsClient.getConf().getHdfsTimeout() / 1000) + \" seconds expired.\"));\r\n        try {\r\n            closeThreads(true);\r\n        } catch (IOException e) {\r\n            b.add(e);\r\n        }\r\n    }\r\n    final IOException ioe = b.build();\r\n    if (ioe != null) {\r\n        throw ioe;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isClosed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isClosed()\n{\r\n    return closed || getStreamer().streamerClosed();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setClosed",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setClosed()\n{\r\n    closed = true;\r\n    dfsClient.endFileLease(fileId);\r\n    getStreamer().release();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "closeThreads",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void closeThreads(boolean force) throws IOException\n{\r\n    try {\r\n        getStreamer().close(force);\r\n        getStreamer().join();\r\n        getStreamer().closeSocket();\r\n    } catch (InterruptedException e) {\r\n        throw new IOException(\"Failed to shutdown streamer\");\r\n    } finally {\r\n        getStreamer().setSocketToNull();\r\n        setClosed();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "close",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    final MultipleIOException.Builder b = new MultipleIOException.Builder();\r\n    synchronized (this) {\r\n        try (TraceScope ignored = dfsClient.newPathTraceScope(\"DFSOutputStream#close\", src)) {\r\n            closeImpl();\r\n        } catch (IOException e) {\r\n            b.add(e);\r\n        }\r\n    }\r\n    final IOException ioe = b.build();\r\n    if (ioe != null) {\r\n        throw ioe;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "closeImpl",
  "errType" : [ "IOException", "ClosedChannelException", "IOException", "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void closeImpl() throws IOException\n{\r\n    boolean recoverLeaseOnCloseException = dfsClient.getConfiguration().getBoolean(RECOVER_LEASE_ON_CLOSE_EXCEPTION_KEY, RECOVER_LEASE_ON_CLOSE_EXCEPTION_DEFAULT);\r\n    if (isClosed()) {\r\n        if (!leaseRecovered) {\r\n            recoverLease(recoverLeaseOnCloseException);\r\n        }\r\n        LOG.debug(\"Closing an already closed stream. [Stream:{}, streamer:{}]\", closed, getStreamer().streamerClosed());\r\n        try {\r\n            getStreamer().getLastException().check(true);\r\n        } catch (IOException ioe) {\r\n            cleanupAndRethrowIOException(ioe);\r\n        } finally {\r\n            if (!closed) {\r\n                closeThreads(true);\r\n            }\r\n        }\r\n        return;\r\n    }\r\n    try {\r\n        flushBuffer();\r\n        if (currentPacket != null) {\r\n            enqueueCurrentPacket();\r\n        }\r\n        if (getStreamer().getBytesCurBlock() != 0) {\r\n            setCurrentPacketToEmpty();\r\n        }\r\n        try {\r\n            flushInternal();\r\n        } catch (IOException ioe) {\r\n            cleanupAndRethrowIOException(ioe);\r\n        }\r\n        completeFile();\r\n    } catch (ClosedChannelException ignored) {\r\n    } catch (IOException ioe) {\r\n        recoverLease(recoverLeaseOnCloseException);\r\n        throw ioe;\r\n    } finally {\r\n        closeThreads(true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "recoverLease",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void recoverLease(boolean recoverLeaseOnCloseException)\n{\r\n    if (recoverLeaseOnCloseException) {\r\n        try {\r\n            dfsClient.endFileLease(fileId);\r\n            dfsClient.recoverLease(src);\r\n            leaseRecovered = true;\r\n        } catch (Exception e) {\r\n            LOG.warn(\"Fail to recover lease for {}\", src, e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "completeFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void completeFile() throws IOException\n{\r\n    ExtendedBlock lastBlock = getStreamer().getBlock();\r\n    try (TraceScope ignored = dfsClient.getTracer().newScope(\"DFSOutputStream#completeFile\")) {\r\n        completeFile(lastBlock);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "exceptionNeedsCleanup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean exceptionNeedsCleanup(IOException ioe)\n{\r\n    return ioe instanceof DSQuotaExceededException || ioe instanceof QuotaByStorageTypeExceededException;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "cleanupAndRethrowIOException",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void cleanupAndRethrowIOException(IOException ioe) throws IOException\n{\r\n    if (exceptionNeedsCleanup(ioe)) {\r\n        final MultipleIOException.Builder b = new MultipleIOException.Builder();\r\n        b.add(ioe);\r\n        try {\r\n            completeFile();\r\n        } catch (IOException e) {\r\n            b.add(e);\r\n            throw b.build();\r\n        }\r\n    }\r\n    throw ioe;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "completeFile",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void completeFile(ExtendedBlock last) throws IOException\n{\r\n    long localstart = Time.monotonicNow();\r\n    final DfsClientConf conf = dfsClient.getConf();\r\n    long sleeptime = conf.getBlockWriteLocateFollowingInitialDelayMs();\r\n    long maxSleepTime = conf.getBlockWriteLocateFollowingMaxDelayMs();\r\n    boolean fileComplete = false;\r\n    int retries = conf.getNumBlockWriteLocateFollowingRetry();\r\n    while (!fileComplete) {\r\n        fileComplete = dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);\r\n        if (!fileComplete) {\r\n            final int hdfsTimeout = conf.getHdfsTimeout();\r\n            if (!dfsClient.clientRunning || (hdfsTimeout > 0 && localstart + hdfsTimeout < Time.monotonicNow())) {\r\n                String msg = \"Unable to close file because dfsclient \" + \" was unable to contact the HDFS servers. clientRunning \" + dfsClient.clientRunning + \" hdfsTimeout \" + hdfsTimeout;\r\n                DFSClient.LOG.info(msg);\r\n                throw new IOException(msg);\r\n            }\r\n            try (TraceScope scope = dfsClient.getTracer().newScope(\"DFSOutputStream#completeFile: Retry\")) {\r\n                scope.addKVAnnotation(\"retries left\", retries);\r\n                scope.addKVAnnotation(\"sleeptime (sleeping for)\", sleeptime);\r\n                if (retries == 0) {\r\n                    throw new IOException(\"Unable to close file because the last block \" + last + \" does not have enough number of replicas.\");\r\n                }\r\n                retries--;\r\n                Thread.sleep(sleeptime);\r\n                sleeptime = calculateDelayForNextRetry(sleeptime, maxSleepTime);\r\n                if (Time.monotonicNow() - localstart > 5000) {\r\n                    DFSClient.LOG.info(\"Could not complete \" + src + \" retrying...\");\r\n                }\r\n            } catch (InterruptedException ie) {\r\n                DFSClient.LOG.warn(\"Caught exception \", ie);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setArtificialSlowdown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setArtificialSlowdown(long period)\n{\r\n    getStreamer().setArtificialSlowdown(period);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setChunksPerPacket",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setChunksPerPacket(int value)\n{\r\n    chunksPerPacket = Math.min(chunksPerPacket, value);\r\n    packetSize = (bytesPerChecksum + getChecksumSize()) * chunksPerPacket;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getInitialLen",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getInitialLen()\n{\r\n    return initialFileSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAddBlockFlags",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EnumSet<AddBlockFlag> getAddBlockFlags()\n{\r\n    return addBlockFlags;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileEncryptionInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileEncryptionInfo getFileEncryptionInfo()\n{\r\n    return fileEncryptionInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBlockToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<BlockTokenIdentifier> getBlockToken()\n{\r\n    return getStreamer().getBlockToken();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "flushInternalWithoutWaitingAck",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long flushInternalWithoutWaitingAck() throws IOException\n{\r\n    long toWaitFor;\r\n    synchronized (this) {\r\n        dfsClient.checkOpen();\r\n        checkClosed();\r\n        getStreamer().queuePacket(currentPacket);\r\n        currentPacket = null;\r\n        toWaitFor = getStreamer().getLastQueuedSeqno();\r\n    }\r\n    return toWaitFor;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setDropBehind",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setDropBehind(Boolean dropBehind) throws IOException\n{\r\n    CachingStrategy prevStrategy, nextStrategy;\r\n    do {\r\n        prevStrategy = this.cachingStrategy.get();\r\n        nextStrategy = new CachingStrategy.Builder(prevStrategy).setDropBehind(dropBehind).build();\r\n    } while (!this.cachingStrategy.compareAndSet(prevStrategy, nextStrategy));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExtendedBlock getBlock()\n{\r\n    return getStreamer().getBlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFileId()\n{\r\n    return fileId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSrc",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSrc()\n{\r\n    return src;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStreamer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DataStreamer getStreamer()\n{\r\n    return streamer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return getClass().getSimpleName() + \":\" + streamer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isLeaseRecovered",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isLeaseRecovered()\n{\r\n    return leaseRecovered;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addBlock",
  "errType" : [ "RemoteException", "InterruptedException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "LocatedBlock addBlock(DatanodeInfo[] excludedNodes, DFSClient dfsClient, String src, ExtendedBlock prevBlock, long fileId, String[] favoredNodes, EnumSet<AddBlockFlag> allocFlags) throws IOException\n{\r\n    final DfsClientConf conf = dfsClient.getConf();\r\n    int retries = conf.getNumBlockWriteLocateFollowingRetry();\r\n    long sleeptime = conf.getBlockWriteLocateFollowingInitialDelayMs();\r\n    long maxSleepTime = conf.getBlockWriteLocateFollowingMaxDelayMs();\r\n    long localstart = Time.monotonicNow();\r\n    while (true) {\r\n        try {\r\n            return dfsClient.namenode.addBlock(src, dfsClient.clientName, prevBlock, excludedNodes, fileId, favoredNodes, allocFlags);\r\n        } catch (RemoteException e) {\r\n            IOException ue = e.unwrapRemoteException(FileNotFoundException.class, AccessControlException.class, NSQuotaExceededException.class, DSQuotaExceededException.class, QuotaByStorageTypeExceededException.class, UnresolvedPathException.class);\r\n            if (ue != e) {\r\n                throw ue;\r\n            }\r\n            if (NotReplicatedYetException.class.getName().equals(e.getClassName())) {\r\n                if (retries == 0) {\r\n                    throw e;\r\n                } else {\r\n                    --retries;\r\n                    LOG.info(\"Exception while adding a block\", e);\r\n                    long elapsed = Time.monotonicNow() - localstart;\r\n                    if (elapsed > 5000) {\r\n                        LOG.info(\"Waiting for replication for \" + (elapsed / 1000) + \" seconds\");\r\n                    }\r\n                    try {\r\n                        LOG.warn(\"NotReplicatedYetException sleeping \" + src + \" retries left \" + retries);\r\n                        Thread.sleep(sleeptime);\r\n                        sleeptime = calculateDelayForNextRetry(sleeptime, maxSleepTime);\r\n                    } catch (InterruptedException ie) {\r\n                        LOG.warn(\"Caught exception\", ie);\r\n                    }\r\n                }\r\n            } else {\r\n                throw e;\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "calculateDelayForNextRetry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long calculateDelayForNextRetry(long previousDelay, long maxDelay)\n{\r\n    return Math.min(previousDelay * 2, maxDelay);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "buildXAttr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "XAttr buildXAttr(String name)\n{\r\n    return buildXAttr(name, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "buildXAttr",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "XAttr buildXAttr(String name, byte[] value)\n{\r\n    Preconditions.checkNotNull(name, \"XAttr name cannot be null.\");\r\n    final int prefixIndex = name.indexOf(\".\");\r\n    if (prefixIndex < 3) {\r\n        throw new HadoopIllegalArgumentException(\"An XAttr name must be \" + \"prefixed with user/trusted/security/system/raw, followed by a '.'\");\r\n    } else if (prefixIndex == name.length() - 1) {\r\n        throw new HadoopIllegalArgumentException(\"XAttr name cannot be empty.\");\r\n    }\r\n    NameSpace ns;\r\n    final String prefix = name.substring(0, prefixIndex);\r\n    if (StringUtils.equalsIgnoreCase(prefix, NameSpace.USER.toString())) {\r\n        ns = NameSpace.USER;\r\n    } else if (StringUtils.equalsIgnoreCase(prefix, NameSpace.TRUSTED.toString())) {\r\n        ns = NameSpace.TRUSTED;\r\n    } else if (StringUtils.equalsIgnoreCase(prefix, NameSpace.SYSTEM.toString())) {\r\n        ns = NameSpace.SYSTEM;\r\n    } else if (StringUtils.equalsIgnoreCase(prefix, NameSpace.SECURITY.toString())) {\r\n        ns = NameSpace.SECURITY;\r\n    } else if (StringUtils.equalsIgnoreCase(prefix, NameSpace.RAW.toString())) {\r\n        ns = NameSpace.RAW;\r\n    } else {\r\n        throw new HadoopIllegalArgumentException(\"An XAttr name must be \" + \"prefixed with user/trusted/security/system/raw, followed by a '.'\");\r\n    }\r\n    return (new XAttr.Builder()).setNameSpace(ns).setName(name.substring(prefixIndex + 1)).setValue(value).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "buildXAttrAsList",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<XAttr> buildXAttrAsList(String name)\n{\r\n    XAttr xAttr = buildXAttr(name);\r\n    List<XAttr> xAttrs = Lists.newArrayListWithCapacity(1);\r\n    xAttrs.add(xAttr);\r\n    return xAttrs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFirstXAttrValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "byte[] getFirstXAttrValue(List<XAttr> xAttrs)\n{\r\n    byte[] value = null;\r\n    XAttr xAttr = getFirstXAttr(xAttrs);\r\n    if (xAttr != null) {\r\n        value = xAttr.getValue();\r\n        if (value == null) {\r\n            value = new byte[0];\r\n        }\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFirstXAttr",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "XAttr getFirstXAttr(List<XAttr> xAttrs)\n{\r\n    if (xAttrs != null && !xAttrs.isEmpty()) {\r\n        return xAttrs.get(0);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "buildXAttrMap",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<String, byte[]> buildXAttrMap(List<XAttr> xAttrs)\n{\r\n    if (xAttrs == null) {\r\n        return null;\r\n    }\r\n    Map<String, byte[]> xAttrMap = Maps.newHashMap();\r\n    for (XAttr xAttr : xAttrs) {\r\n        String name = getPrefixedName(xAttr);\r\n        byte[] value = xAttr.getValue();\r\n        if (value == null) {\r\n            value = new byte[0];\r\n        }\r\n        xAttrMap.put(name, value);\r\n    }\r\n    return xAttrMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getPrefixedName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getPrefixedName(XAttr xAttr)\n{\r\n    if (xAttr == null) {\r\n        return null;\r\n    }\r\n    return getPrefixedName(xAttr.getNameSpace(), xAttr.getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getPrefixedName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getPrefixedName(XAttr.NameSpace ns, String name)\n{\r\n    return StringUtils.toLowerCase(ns.toString()) + \".\" + name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "buildXAttrs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<XAttr> buildXAttrs(List<String> names)\n{\r\n    if (names == null || names.isEmpty()) {\r\n        throw new HadoopIllegalArgumentException(\"XAttr names can not be \" + \"null or empty.\");\r\n    }\r\n    List<XAttr> xAttrs = Lists.newArrayListWithCapacity(names.size());\r\n    for (String name : names) {\r\n        xAttrs.add(buildXAttr(name, null));\r\n    }\r\n    return xAttrs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getInputStreamChannel",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReadableByteChannel getInputStreamChannel()\n{\r\n    return channel;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "setReadTimeout",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setReadTimeout(int timeoutMs) throws IOException\n{\r\n    enclosedPeer.setReadTimeout(timeoutMs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getReceiveBufferSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getReceiveBufferSize() throws IOException\n{\r\n    return enclosedPeer.getReceiveBufferSize();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getTcpNoDelay",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getTcpNoDelay() throws IOException\n{\r\n    return enclosedPeer.getTcpNoDelay();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "setWriteTimeout",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setWriteTimeout(int timeoutMs) throws IOException\n{\r\n    enclosedPeer.setWriteTimeout(timeoutMs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "isClosed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isClosed()\n{\r\n    return enclosedPeer.isClosed();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    try {\r\n        in.close();\r\n    } finally {\r\n        try {\r\n            out.close();\r\n        } finally {\r\n            enclosedPeer.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getRemoteAddressString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRemoteAddressString()\n{\r\n    return enclosedPeer.getRemoteAddressString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getLocalAddressString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getLocalAddressString()\n{\r\n    return enclosedPeer.getLocalAddressString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getInputStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InputStream getInputStream() throws IOException\n{\r\n    return in;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getOutputStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OutputStream getOutputStream() throws IOException\n{\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "isLocal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isLocal()\n{\r\n    return enclosedPeer.isLocal();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"EncryptedPeer(\" + enclosedPeer + \")\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getDomainSocket",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DomainSocket getDomainSocket()\n{\r\n    return enclosedPeer.getDomainSocket();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "hasSecureChannel",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasSecureChannel()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isBlockFilename",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isBlockFilename(File f)\n{\r\n    String name = f.getName();\r\n    return blockFilePattern.matcher(name).matches();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "filename2id",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long filename2id(String name)\n{\r\n    Matcher m = blockFilePattern.matcher(name);\r\n    return m.matches() ? Long.parseLong(m.group(1)) : 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isMetaFilename",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isMetaFilename(String name)\n{\r\n    return metaFilePattern.matcher(name).matches();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "metaToBlockFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "File metaToBlockFile(File metaFile)\n{\r\n    return new File(metaFile.getParent(), metaFile.getName().substring(0, metaFile.getName().lastIndexOf('_')));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getGenerationStamp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getGenerationStamp(String metaFile)\n{\r\n    Matcher m = metaFilePattern.matcher(metaFile);\r\n    return m.matches() ? Long.parseLong(m.group(2)) : HdfsConstants.GRANDFATHER_GENERATION_STAMP;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getBlockId(String metaOrBlockFile)\n{\r\n    Matcher m = metaOrBlockFilePattern.matcher(metaOrBlockFile);\r\n    return m.matches() ? Long.parseLong(m.group(1)) : 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "set",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void set(long blkid, long len, long genStamp)\n{\r\n    this.blockId = blkid;\r\n    this.numBytes = len;\r\n    this.generationStamp = genStamp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBlockId()\n{\r\n    return blockId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setBlockId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setBlockId(long bid)\n{\r\n    blockId = bid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockName()\n{\r\n    return BLOCK_FILE_PREFIX + blockId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getNumBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumBytes()\n{\r\n    return numBytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setNumBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumBytes(long len)\n{\r\n    this.numBytes = len;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getGenerationStamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getGenerationStamp()\n{\r\n    return generationStamp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setGenerationStamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setGenerationStamp(long stamp)\n{\r\n    generationStamp = stamp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString(final Block b)\n{\r\n    return new StringBuilder(BLOCK_FILE_PREFIX).append(b.blockId).append('_').append(b.generationStamp).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return Block.toString(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "appendStringTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void appendStringTo(StringBuilder sb)\n{\r\n    sb.append(BLOCK_FILE_PREFIX).append(blockId).append('_').append(getGenerationStamp());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    writeHelper(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    readHelper(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "writeHelper",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writeHelper(DataOutput out) throws IOException\n{\r\n    out.writeLong(blockId);\r\n    out.writeLong(numBytes);\r\n    out.writeLong(generationStamp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "readHelper",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void readHelper(DataInput in) throws IOException\n{\r\n    this.blockId = in.readLong();\r\n    this.numBytes = in.readLong();\r\n    this.generationStamp = in.readLong();\r\n    if (numBytes < 0) {\r\n        throw new IOException(\"Unexpected block size: \" + numBytes);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "writeId",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writeId(DataOutput out) throws IOException\n{\r\n    out.writeLong(blockId);\r\n    out.writeLong(generationStamp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "readId",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readId(DataInput in) throws IOException\n{\r\n    this.blockId = in.readLong();\r\n    this.generationStamp = in.readLong();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int compareTo(@Nonnull Block b)\n{\r\n    return Long.compare(blockId, b.blockId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (this == obj) {\r\n        return true;\r\n    }\r\n    if (!(obj instanceof Block)) {\r\n        return false;\r\n    }\r\n    Block other = (Block) obj;\r\n    return (blockId == other.blockId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return Long.hashCode(blockId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "matchingIdAndGenStamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean matchingIdAndGenStamp(Block a, Block b)\n{\r\n    if (a == b) {\r\n        return true;\r\n    } else if (a == null || b == null) {\r\n        return false;\r\n    } else {\r\n        return a.blockId == b.blockId && a.generationStamp == b.generationStamp;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "logDebugMessage",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void logDebugMessage()\n{\r\n    final StringBuilder b = DEBUG_MESSAGE.get();\r\n    LOG.debug(b.toString());\r\n    b.setLength(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "leastPowerOfTwo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int leastPowerOfTwo(final int n)\n{\r\n    if (n <= 0) {\r\n        throw new HadoopIllegalArgumentException(\"n = \" + n + \" <= 0\");\r\n    }\r\n    final int highestOne = Integer.highestOneBit(n);\r\n    if (highestOne == n) {\r\n        return n;\r\n    }\r\n    final int roundUp = highestOne << 1;\r\n    if (roundUp < 0) {\r\n        final long overflow = ((long) highestOne) << 1;\r\n        throw new ArithmeticException(\"Overflow: for n = \" + n + \", the least power of two (the least\" + \" integer x with x >= n and x a power of two) = \" + overflow + \" > Integer.MAX_VALUE = \" + Integer.MAX_VALUE);\r\n    }\r\n    return roundUp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "newByteArray",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] newByteArray(int size) throws InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "release",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int release(byte[] array)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ByteArrayManager newInstance(Conf conf)\n{\r\n    return conf == null ? new NewByteArrayWithoutLimit() : new Impl(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getByteString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ByteString getByteString(String str)\n{\r\n    if (str != null) {\r\n        return ByteString.copyFromUtf8(str);\r\n    }\r\n    return ByteString.EMPTY;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setIpAddr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setIpAddr(String ipAddr)\n{\r\n    setIpAndXferPort(ipAddr, getByteString(ipAddr), xferPort);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setIpAndXferPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setIpAndXferPort(String ipAddr, ByteString ipAddrBytes, int xferPort)\n{\r\n    this.ipAddr = ipAddr;\r\n    this.ipAddrBytes = ipAddrBytes;\r\n    this.xferPort = xferPort;\r\n    this.xferAddr = ipAddr + \":\" + xferPort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setPeerHostName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setPeerHostName(String peerHostName)\n{\r\n    this.peerHostName = peerHostName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getDatanodeUuid",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDatanodeUuid()\n{\r\n    return datanodeUuid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getDatanodeUuidBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ByteString getDatanodeUuidBytes()\n{\r\n    return datanodeUuidBytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "checkDatanodeUuid",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String checkDatanodeUuid(String uuid)\n{\r\n    if (uuid == null || uuid.isEmpty()) {\r\n        return null;\r\n    } else {\r\n        return uuid;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getIpAddr",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getIpAddr()\n{\r\n    return ipAddr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getIpAddrBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ByteString getIpAddrBytes()\n{\r\n    return ipAddrBytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getHostName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHostName()\n{\r\n    return hostName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getHostNameBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ByteString getHostNameBytes()\n{\r\n    return hostNameBytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getPeerHostName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPeerHostName()\n{\r\n    return peerHostName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getXferAddr",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getXferAddr()\n{\r\n    return xferAddr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getIpcAddr",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getIpcAddr()\n{\r\n    return ipAddr + \":\" + ipcPort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getInfoAddr",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getInfoAddr()\n{\r\n    return ipAddr + \":\" + infoPort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getInfoSecureAddr",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getInfoSecureAddr()\n{\r\n    return ipAddr + \":\" + infoSecurePort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getXferAddrWithHostname",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getXferAddrWithHostname()\n{\r\n    return hostName + \":\" + xferPort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getIpcAddrWithHostname",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getIpcAddrWithHostname()\n{\r\n    return hostName + \":\" + ipcPort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getXferAddr",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getXferAddr(boolean useHostname)\n{\r\n    return useHostname ? getXferAddrWithHostname() : getXferAddr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getIpcAddr",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getIpcAddr(boolean useHostname)\n{\r\n    return useHostname ? getIpcAddrWithHostname() : getIpcAddr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getXferPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getXferPort()\n{\r\n    return xferPort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getInfoPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getInfoPort()\n{\r\n    return infoPort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getInfoSecurePort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getInfoSecurePort()\n{\r\n    return infoSecurePort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getIpcPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getIpcPort()\n{\r\n    return ipcPort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean equals(Object to)\n{\r\n    return this == to || (to instanceof DatanodeID && getXferAddr().equals(((DatanodeID) to).getXferAddr()) && datanodeUuid.equals(((DatanodeID) to).getDatanodeUuid()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return datanodeUuid.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return getXferAddr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "updateRegInfo",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void updateRegInfo(DatanodeID nodeReg)\n{\r\n    setIpAndXferPort(nodeReg.getIpAddr(), nodeReg.getIpAddrBytes(), nodeReg.getXferPort());\r\n    hostName = nodeReg.getHostName();\r\n    peerHostName = nodeReg.getPeerHostName();\r\n    infoPort = nodeReg.getInfoPort();\r\n    infoSecurePort = nodeReg.getInfoSecurePort();\r\n    ipcPort = nodeReg.getIpcPort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int compareTo(DatanodeID that)\n{\r\n    return getXferAddr().compareTo(that.getXferAddr());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getResolvedAddress",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "InetSocketAddress getResolvedAddress()\n{\r\n    return new InetSocketAddress(this.getIpAddr(), this.getXferPort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getId()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFilePath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getFilePath()\n{\r\n    return filePath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getClientMachine",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getClientMachine()\n{\r\n    return clientMachine;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getClientName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getClientName()\n{\r\n    return clientName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLowRedundancyBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLowRedundancyBlocks()\n{\r\n    return lowRedundancyBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getCorruptBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCorruptBlocks()\n{\r\n    return corruptBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getMissingReplicaBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMissingReplicaBlocks()\n{\r\n    return missingBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getMissingReplicationOneBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMissingReplicationOneBlocks()\n{\r\n    return missingReplicationOneBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBytesInFutureBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesInFutureBlocks()\n{\r\n    return bytesInFutureBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getPendingDeletionBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPendingDeletionBlocks()\n{\r\n    return pendingDeletionBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hasHighestPriorityLowRedundancyBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasHighestPriorityLowRedundancyBlocks()\n{\r\n    return getHighestPriorityLowRedundancyBlocks() != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getHighestPriorityLowRedundancyBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Long getHighestPriorityLowRedundancyBlocks()\n{\r\n    return highestPriorityLowRedundancyBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder statsBuilder = new StringBuilder();\r\n    statsBuilder.append(\"ReplicatedBlockStats=[\").append(\"LowRedundancyBlocks=\").append(getLowRedundancyBlocks()).append(\", CorruptBlocks=\").append(getCorruptBlocks()).append(\", MissingReplicaBlocks=\").append(getMissingReplicaBlocks()).append(\", MissingReplicationOneBlocks=\").append(getMissingReplicationOneBlocks()).append(\", BytesInFutureBlocks=\").append(getBytesInFutureBlocks()).append(\", PendingDeletionBlocks=\").append(getPendingDeletionBlocks());\r\n    if (hasHighestPriorityLowRedundancyBlocks()) {\r\n        statsBuilder.append(\", HighestPriorityLowRedundancyBlocks=\").append(getHighestPriorityLowRedundancyBlocks());\r\n    }\r\n    statsBuilder.append(\"]\");\r\n    return statsBuilder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "merge",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ReplicatedBlockStats merge(Collection<ReplicatedBlockStats> stats)\n{\r\n    long lowRedundancyBlocks = 0;\r\n    long corruptBlocks = 0;\r\n    long missingBlocks = 0;\r\n    long missingReplicationOneBlocks = 0;\r\n    long bytesInFutureBlocks = 0;\r\n    long pendingDeletionBlocks = 0;\r\n    long highestPriorityLowRedundancyBlocks = 0;\r\n    boolean hasHighestPriorityLowRedundancyBlocks = false;\r\n    for (ReplicatedBlockStats stat : stats) {\r\n        lowRedundancyBlocks += stat.getLowRedundancyBlocks();\r\n        corruptBlocks += stat.getCorruptBlocks();\r\n        missingBlocks += stat.getMissingReplicaBlocks();\r\n        missingReplicationOneBlocks += stat.getMissingReplicationOneBlocks();\r\n        bytesInFutureBlocks += stat.getBytesInFutureBlocks();\r\n        pendingDeletionBlocks += stat.getPendingDeletionBlocks();\r\n        if (stat.hasHighestPriorityLowRedundancyBlocks()) {\r\n            hasHighestPriorityLowRedundancyBlocks = true;\r\n            highestPriorityLowRedundancyBlocks += stat.getHighestPriorityLowRedundancyBlocks();\r\n        }\r\n    }\r\n    if (hasHighestPriorityLowRedundancyBlocks) {\r\n        return new ReplicatedBlockStats(lowRedundancyBlocks, corruptBlocks, missingBlocks, missingReplicationOneBlocks, bytesInFutureBlocks, pendingDeletionBlocks, highestPriorityLowRedundancyBlocks);\r\n    }\r\n    return new ReplicatedBlockStats(lowRedundancyBlocks, corruptBlocks, missingBlocks, missingReplicationOneBlocks, bytesInFutureBlocks, pendingDeletionBlocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getModifyList",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<DiffReportListingEntry> getModifyList()\n{\r\n    return modifyList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getCreateList",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<DiffReportListingEntry> getCreateList()\n{\r\n    return createList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getDeleteList",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<DiffReportListingEntry> getDeleteList()\n{\r\n    return deleteList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLastPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getLastPath()\n{\r\n    return lastPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLastIndex",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getLastIndex()\n{\r\n    return lastIndex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getIsFromEarlier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getIsFromEarlier()\n{\r\n    return isFromEarlier;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getUserPatternDomain",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Domain getUserPatternDomain()\n{\r\n    return domain;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "setUserPatternDomain",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setUserPatternDomain(Domain dm)\n{\r\n    domain = dm;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "setUserPattern",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUserPattern(String pattern)\n{\r\n    domain = new Domain(NAME, Pattern.compile(pattern));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "validateLength",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String validateLength(String str)\n{\r\n    if (str == null) {\r\n        throw new IllegalArgumentException(MessageFormat.format(\"Parameter [{0}], cannot be NULL\", NAME));\r\n    }\r\n    int len = str.length();\r\n    if (len < 1) {\r\n        throw new IllegalArgumentException(MessageFormat.format(\"Parameter [{0}], it's length must be at least 1\", NAME));\r\n    }\r\n    return str;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getCurrentBlockReplication",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getCurrentBlockReplication() throws IOException\n{\r\n    OutputStream wrappedStream = getWrappedStream();\r\n    if (wrappedStream instanceof CryptoOutputStream) {\r\n        wrappedStream = ((CryptoOutputStream) wrappedStream).getWrappedStream();\r\n    }\r\n    return ((DFSOutputStream) wrappedStream).getCurrentBlockReplication();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "hsync",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void hsync(EnumSet<SyncFlag> syncFlags) throws IOException\n{\r\n    OutputStream wrappedStream = getWrappedStream();\r\n    if (wrappedStream instanceof CryptoOutputStream) {\r\n        wrappedStream.flush();\r\n        wrappedStream = ((CryptoOutputStream) wrappedStream).getWrappedStream();\r\n    }\r\n    ((DFSOutputStream) wrappedStream).hsync(syncFlags);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "paths2String",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String paths2String(Path[] paths)\n{\r\n    if (paths == null || paths.length == 0) {\r\n        return \"\";\r\n    }\r\n    final StringBuilder b = new StringBuilder(paths[0].toUri().getPath());\r\n    for (int i = 1; i < paths.length; i++) {\r\n        b.append(',').append(paths[i].toUri().getPath());\r\n    }\r\n    return b.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getAbsolutePaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getAbsolutePaths()\n{\r\n    return getValue().split(\",\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CachePoolInfo getInfo()\n{\r\n    return info;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getStats",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CachePoolStats getStats()\n{\r\n    return stats;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DatanodeInfoWithStorage[] convert(DatanodeInfo[] infos, String[] storageIDs, StorageType[] storageTypes)\n{\r\n    if (null == infos) {\r\n        return EMPTY_LOCS;\r\n    }\r\n    DatanodeInfoWithStorage[] ret = new DatanodeInfoWithStorage[infos.length];\r\n    for (int i = 0; i < infos.length; i++) {\r\n        ret[i] = new DatanodeInfoWithStorage(infos[i], storageIDs != null ? storageIDs[i] : null, storageTypes != null ? storageTypes[i] : null);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Token<BlockTokenIdentifier> getBlockToken()\n{\r\n    return blockToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setBlockToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setBlockToken(Token<BlockTokenIdentifier> token)\n{\r\n    this.blockToken = token;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ExtendedBlock getBlock()\n{\r\n    return b;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLocations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DatanodeInfoWithStorage[] getLocations()\n{\r\n    return locs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getStorageTypes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StorageType[] getStorageTypes()\n{\r\n    return storageTypes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getStorageIDs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] getStorageIDs()\n{\r\n    return storageIDs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "updateCachedStorageInfo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateCachedStorageInfo()\n{\r\n    if (storageIDs != null) {\r\n        for (int i = 0; i < locs.length; i++) {\r\n            storageIDs[i] = locs[i].getStorageID();\r\n        }\r\n    }\r\n    if (storageTypes != null) {\r\n        for (int i = 0; i < locs.length; i++) {\r\n            storageTypes[i] = locs[i].getStorageType();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "moveProvidedToEnd",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void moveProvidedToEnd(int activeLen)\n{\r\n    if (activeLen <= 0) {\r\n        return;\r\n    }\r\n    Arrays.sort(locs, 0, (activeLen < locs.length) ? activeLen : locs.length, providedLastComparator);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getStartOffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartOffset()\n{\r\n    return offset;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBlockSize()\n{\r\n    return b.getNumBytes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setStartOffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStartOffset(long value)\n{\r\n    this.offset = value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setCorrupt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCorrupt(boolean corrupt)\n{\r\n    this.corrupt = corrupt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isCorrupt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isCorrupt()\n{\r\n    return this.corrupt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "addCachedLoc",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void addCachedLoc(DatanodeInfo loc)\n{\r\n    List<DatanodeInfo> cachedList = Lists.newArrayList(cachedLocs);\r\n    if (cachedList.contains(loc)) {\r\n        return;\r\n    }\r\n    for (DatanodeInfoWithStorage di : locs) {\r\n        if (loc.equals(di)) {\r\n            cachedList.add(di);\r\n            cachedLocs = cachedList.toArray(cachedLocs);\r\n            return;\r\n        }\r\n    }\r\n    cachedList.add(loc);\r\n    Preconditions.checkArgument(cachedLocs != EMPTY_LOCS, \"Cached locations should only be added when having a backing\" + \" disk replica!\", loc, locs.length, Arrays.toString(locs));\r\n    cachedLocs = cachedList.toArray(cachedLocs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getCachedLocations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DatanodeInfo[] getCachedLocations()\n{\r\n    return cachedLocs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String toString()\n{\r\n    return getClass().getSimpleName() + \"{\" + b + \"; getBlockSize()=\" + getBlockSize() + \"; corrupt=\" + corrupt + \"; offset=\" + offset + \"; locs=\" + Arrays.asList(locs) + \"; cachedLocs=\" + Arrays.asList(cachedLocs) + \"}\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isStriped",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isStriped()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockType getBlockType()\n{\r\n    return BlockType.CONTIGUOUS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getOp",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Op getOp(String str)\n{\r\n    try {\r\n        return DOMAIN.parse(str);\r\n    } catch (IllegalArgumentException e) {\r\n        throw new IllegalArgumentException(str + \" is not a valid \" + Type.PUT + \" operation.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getStorage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DatanodeStorage getStorage()\n{\r\n    return storage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "isFailed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isFailed()\n{\r\n    return failed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getCapacity",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCapacity()\n{\r\n    return capacity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getDfsUsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDfsUsed()\n{\r\n    return dfsUsed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getNonDfsUsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNonDfsUsed()\n{\r\n    return nonDfsUsed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getRemaining",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getRemaining()\n{\r\n    return remaining;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getBlockPoolUsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBlockPoolUsed()\n{\r\n    return blockPoolUsed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getMount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getMount()\n{\r\n    return mount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getBlockPoolUsagePercent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getBlockPoolUsagePercent()\n{\r\n    return blockPoolUsagePercent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getResolvedUrl",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URL getResolvedUrl(final HttpURLConnection connection) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getInputStream",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "InputStream getInputStream() throws IOException\n{\r\n    switch(status) {\r\n        case NORMAL:\r\n            break;\r\n        case SEEK:\r\n            if (in != null) {\r\n                in.close();\r\n            }\r\n            InputStreamAndFileLength fin = openInputStream(startPos);\r\n            in = fin.in;\r\n            fileLength = fin.length;\r\n            status = StreamStatus.NORMAL;\r\n            break;\r\n        case CLOSED:\r\n            throw new IOException(\"Stream closed\");\r\n    }\r\n    return in;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "openInputStream",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "InputStreamAndFileLength openInputStream(long startOffset) throws IOException\n{\r\n    if (startOffset < 0) {\r\n        throw new EOFException(\"Negative Position\");\r\n    }\r\n    final boolean resolved = resolvedURL.getURL() != null;\r\n    final URLOpener opener = resolved ? resolvedURL : originalURL;\r\n    final HttpURLConnection connection = opener.connect(startOffset, resolved);\r\n    resolvedURL.setURL(getResolvedUrl(connection));\r\n    InputStream in = connection.getInputStream();\r\n    final Long length;\r\n    final Map<String, List<String>> headers = connection.getHeaderFields();\r\n    if (isChunkedTransferEncoding(headers)) {\r\n        length = null;\r\n    } else {\r\n        final String cl = connection.getHeaderField(HttpHeaders.CONTENT_LENGTH);\r\n        if (cl == null) {\r\n            throw new IOException(HttpHeaders.CONTENT_LENGTH + \" is missing: \" + headers);\r\n        }\r\n        final long streamlength = Long.parseLong(cl);\r\n        length = startOffset + streamlength;\r\n        in = new BoundedInputStream(in, streamlength);\r\n    }\r\n    return new InputStreamAndFileLength(length, in);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "isChunkedTransferEncoding",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isChunkedTransferEncoding(final Map<String, List<String>> headers)\n{\r\n    return contains(headers, HttpHeaders.TRANSFER_ENCODING, \"chunked\") || contains(headers, HttpHeaders.TE, \"chunked\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "contains",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean contains(final Map<String, List<String>> headers, final String key, final String value)\n{\r\n    final List<String> values = headers.get(key);\r\n    if (values != null) {\r\n        for (String v : values) {\r\n            for (final StringTokenizer t = new StringTokenizer(v, \",\"); t.hasMoreTokens(); ) {\r\n                if (value.equalsIgnoreCase(t.nextToken())) {\r\n                    return true;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "update",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int update(final int n) throws IOException\n{\r\n    if (n != -1) {\r\n        currentPos += n;\r\n    } else if (fileLength != null && currentPos < fileLength) {\r\n        throw new IOException(\"Got EOF but currentPos = \" + currentPos + \" < filelength = \" + fileLength);\r\n    }\r\n    return n;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int read() throws IOException\n{\r\n    final int b = getInputStream().read();\r\n    update((b == -1) ? -1 : 1);\r\n    return b;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int read(@Nonnull byte[] b, int off, int len) throws IOException\n{\r\n    return update(getInputStream().read(b, off, len));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "seek",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void seek(long pos) throws IOException\n{\r\n    if (pos != currentPos) {\r\n        startPos = pos;\r\n        currentPos = pos;\r\n        if (status != StreamStatus.CLOSED) {\r\n            status = StreamStatus.SEEK;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int read(long position, byte[] buffer, int offset, int length) throws IOException\n{\r\n    validatePositionedReadArgs(position, buffer, offset, length);\r\n    if (length == 0) {\r\n        return 0;\r\n    }\r\n    try (InputStream in = openInputStream(position).in) {\r\n        return in.read(buffer, offset, length);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "readFully",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void readFully(long position, byte[] buffer, int offset, int length) throws IOException\n{\r\n    validatePositionedReadArgs(position, buffer, offset, length);\r\n    if (length == 0) {\r\n        return;\r\n    }\r\n    final InputStreamAndFileLength fin = openInputStream(position);\r\n    try {\r\n        if (fin.length != null && length + position > fin.length) {\r\n            throw new EOFException(\"The length to read \" + length + \" exceeds the file length \" + fin.length);\r\n        }\r\n        int nread = 0;\r\n        while (nread < length) {\r\n            int nbytes = fin.in.read(buffer, offset + nread, length - nread);\r\n            if (nbytes < 0) {\r\n                throw new EOFException(FSExceptionMessages.EOF_IN_READ_FULLY);\r\n            }\r\n            nread += nbytes;\r\n        }\r\n    } finally {\r\n        fin.in.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPos() throws IOException\n{\r\n    return currentPos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "seekToNewSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean seekToNewSource(long targetPos) throws IOException\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (in != null) {\r\n        in.close();\r\n        in = null;\r\n    }\r\n    status = StreamStatus.CLOSED;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "available",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int available() throws IOException\n{\r\n    getInputStream();\r\n    if (fileLength != null) {\r\n        long remaining = fileLength - currentPos;\r\n        return remaining <= Integer.MAX_VALUE ? (int) remaining : Integer.MAX_VALUE;\r\n    } else {\r\n        return Integer.MAX_VALUE;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getDFSInputStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DFSInputStream getDFSInputStream()\n{\r\n    if (in instanceof CryptoInputStream) {\r\n        return (DFSInputStream) ((CryptoInputStream) in).getWrappedStream();\r\n    }\r\n    return (DFSInputStream) in;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getWrappedStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InputStream getWrappedStream()\n{\r\n    return in;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getCurrentDatanode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DatanodeInfo getCurrentDatanode()\n{\r\n    return getDFSInputStream().getCurrentDatanode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getCurrentBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExtendedBlock getCurrentBlock()\n{\r\n    return getDFSInputStream().getCurrentBlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getAllBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<LocatedBlock> getAllBlocks() throws IOException\n{\r\n    return getDFSInputStream().getAllBlocks();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getVisibleLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getVisibleLength()\n{\r\n    return getDFSInputStream().getFileLength();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getReadStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ReadStatistics getReadStatistics()\n{\r\n    return getDFSInputStream().getReadStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "clearReadStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearReadStatistics()\n{\r\n    getDFSInputStream().clearReadStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "poll",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "EventBatch poll() throws IOException, MissingEventsException\n{\r\n    try (TraceScope ignored = tracer.newScope(\"inotifyPoll\")) {\r\n        if (lastReadTxid == -1) {\r\n            LOG.debug(\"poll(): lastReadTxid is -1, reading current txid from NN\");\r\n            lastReadTxid = namenode.getCurrentEditLogTxid();\r\n            return null;\r\n        }\r\n        if (!it.hasNext()) {\r\n            EventBatchList el = namenode.getEditsFromTxid(lastReadTxid + 1);\r\n            if (el.getLastTxid() != -1) {\r\n                syncTxid = el.getSyncTxid();\r\n                it = el.getBatches().iterator();\r\n                long formerLastReadTxid = lastReadTxid;\r\n                lastReadTxid = el.getLastTxid();\r\n                if (el.getFirstTxid() != formerLastReadTxid + 1) {\r\n                    throw new MissingEventsException(formerLastReadTxid + 1, el.getFirstTxid());\r\n                }\r\n            } else {\r\n                LOG.debug(\"poll(): read no edits from the NN when requesting edits \" + \"after txid {}\", lastReadTxid);\r\n                return null;\r\n            }\r\n        }\r\n        if (it.hasNext()) {\r\n            return it.next();\r\n        } else {\r\n            return null;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getTxidsBehindEstimate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTxidsBehindEstimate()\n{\r\n    if (syncTxid == 0) {\r\n        return -1;\r\n    } else {\r\n        assert syncTxid >= lastReadTxid;\r\n        return syncTxid - lastReadTxid;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "poll",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "EventBatch poll(long time, TimeUnit tu) throws IOException, InterruptedException, MissingEventsException\n{\r\n    EventBatch next;\r\n    try (TraceScope ignored = tracer.newScope(\"inotifyPollWithTimeout\")) {\r\n        long initialTime = Time.monotonicNow();\r\n        long totalWait = TimeUnit.MILLISECONDS.convert(time, tu);\r\n        long nextWait = INITIAL_WAIT_MS;\r\n        while ((next = poll()) == null) {\r\n            long timeLeft = totalWait - (Time.monotonicNow() - initialTime);\r\n            if (timeLeft <= 0) {\r\n                LOG.debug(\"timed poll(): timed out\");\r\n                break;\r\n            } else if (timeLeft < nextWait * 2) {\r\n                nextWait = timeLeft;\r\n            } else {\r\n                nextWait *= 2;\r\n            }\r\n            LOG.debug(\"timed poll(): poll() returned null, sleeping for {} ms\", nextWait);\r\n            Thread.sleep(nextWait);\r\n        }\r\n    }\r\n    return next;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "take",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "EventBatch take() throws IOException, InterruptedException, MissingEventsException\n{\r\n    EventBatch next;\r\n    try (TraceScope ignored = tracer.newScope(\"inotifyTake\")) {\r\n        int nextWaitMin = INITIAL_WAIT_MS;\r\n        while ((next = poll()) == null) {\r\n            int sleepTime = nextWaitMin + rng.nextInt(nextWaitMin);\r\n            LOG.debug(\"take(): poll() returned null, sleeping for {} ms\", sleepTime);\r\n            Thread.sleep(sleepTime);\r\n            nextWaitMin = Math.min(60000, nextWaitMin * 2);\r\n        }\r\n    }\r\n    return next;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getFallBackAuthenticator",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Authenticator getFallBackAuthenticator()\n{\r\n    return new PseudoAuthenticator() {\r\n\r\n        @Override\r\n        protected String getUserName() {\r\n            try {\r\n                return UserGroupInformation.getLoginUser().getUserName();\r\n            } catch (IOException e) {\r\n                throw new SecurityException(\"Failed to obtain current username\", e);\r\n            }\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "incHedgedReadOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incHedgedReadOps()\n{\r\n    hedgedReadOps.increment();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "incHedgedReadOpsInCurThread",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incHedgedReadOpsInCurThread()\n{\r\n    hedgedReadOpsInCurThread.increment();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "incHedgedReadWins",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incHedgedReadWins()\n{\r\n    hedgedReadOpsWin.increment();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getHedgedReadOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getHedgedReadOps()\n{\r\n    return hedgedReadOps.longValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getHedgedReadOpsInCurThread",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getHedgedReadOpsInCurThread()\n{\r\n    return hedgedReadOpsInCurThread.longValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getHedgedReadWins",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getHedgedReadWins()\n{\r\n    return hedgedReadOpsWin.longValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "safetyDance",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Unsafe safetyDance()\n{\r\n    try {\r\n        Field f = Unsafe.class.getDeclaredField(\"theUnsafe\");\r\n        f.setAccessible(true);\r\n        return (Unsafe) f.get(null);\r\n    } catch (Throwable e) {\r\n        LOG.error(\"failed to load misc.Unsafe\", e);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getUsableLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getUsableLength(FileInputStream stream) throws IOException\n{\r\n    int intSize = Ints.checkedCast(stream.getChannel().size());\r\n    int slots = intSize / BYTES_PER_SLOT;\r\n    if (slots == 0) {\r\n        throw new IOException(\"size of shared memory segment was \" + intSize + \", but that is not enough to hold even one slot.\");\r\n    }\r\n    return slots * BYTES_PER_SLOT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getShmId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ShmId getShmId()\n{\r\n    return shmId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "isEmpty",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isEmpty()\n{\r\n    return allocatedSlots.nextSetBit(0) == -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "isFull",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isFull()\n{\r\n    return allocatedSlots.nextClearBit(0) >= slots.length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "calculateSlotAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long calculateSlotAddress(int slotIdx)\n{\r\n    long offset = slotIdx;\r\n    offset *= BYTES_PER_SLOT;\r\n    return this.baseAddress + offset;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "allocAndRegisterSlot",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Slot allocAndRegisterSlot(ExtendedBlockId blockId)\n{\r\n    int idx = allocatedSlots.nextClearBit(0);\r\n    if (idx >= slots.length) {\r\n        throw new RuntimeException(this + \": no more slots are available.\");\r\n    }\r\n    allocatedSlots.set(idx, true);\r\n    Slot slot = new Slot(calculateSlotAddress(idx), blockId);\r\n    slot.clear();\r\n    slot.makeValid();\r\n    slots[idx] = slot;\r\n    if (LOG.isTraceEnabled()) {\r\n        LOG.trace(this + \": allocAndRegisterSlot \" + idx + \": allocatedSlots=\" + allocatedSlots + StringUtils.getStackTrace(Thread.currentThread()));\r\n    }\r\n    return slot;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getSlot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Slot getSlot(int slotIdx) throws InvalidRequestException\n{\r\n    if (!allocatedSlots.get(slotIdx)) {\r\n        throw new InvalidRequestException(this + \": slot \" + slotIdx + \" does not exist.\");\r\n    }\r\n    return slots[slotIdx];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "registerSlot",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Slot registerSlot(int slotIdx, ExtendedBlockId blockId) throws InvalidRequestException\n{\r\n    if (slotIdx < 0) {\r\n        throw new InvalidRequestException(this + \": invalid negative slot \" + \"index \" + slotIdx);\r\n    }\r\n    if (slotIdx >= slots.length) {\r\n        throw new InvalidRequestException(this + \": invalid slot \" + \"index \" + slotIdx);\r\n    }\r\n    if (allocatedSlots.get(slotIdx)) {\r\n        throw new InvalidRequestException(this + \": slot \" + slotIdx + \" is already in use.\");\r\n    }\r\n    Slot slot = new Slot(calculateSlotAddress(slotIdx), blockId);\r\n    if (!slot.isValid()) {\r\n        throw new InvalidRequestException(this + \": slot \" + slotIdx + \" is not marked as valid.\");\r\n    }\r\n    slots[slotIdx] = slot;\r\n    allocatedSlots.set(slotIdx, true);\r\n    if (LOG.isTraceEnabled()) {\r\n        LOG.trace(this + \": registerSlot \" + slotIdx + \": allocatedSlots=\" + allocatedSlots + StringUtils.getStackTrace(Thread.currentThread()));\r\n    }\r\n    return slot;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "unregisterSlot",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void unregisterSlot(int slotIdx)\n{\r\n    Preconditions.checkState(allocatedSlots.get(slotIdx), \"tried to unregister slot \" + slotIdx + \", which was not registered.\");\r\n    allocatedSlots.set(slotIdx, false);\r\n    slots[slotIdx] = null;\r\n    LOG.trace(\"{}: unregisterSlot {}\", this, slotIdx);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "slotIterator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SlotIterator slotIterator()\n{\r\n    return new SlotIterator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "free",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void free()\n{\r\n    try {\r\n        POSIX.munmap(baseAddress, mmappedLength);\r\n    } catch (IOException e) {\r\n        LOG.warn(this + \": failed to munmap\", e);\r\n    }\r\n    LOG.trace(this + \": freed\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return this.getClass().getSimpleName() + \"(\" + shmId + \")\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "allocSlot",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Slot allocSlot(DatanodeInfo datanode, DomainPeer peer, MutableBoolean usedPeer, ExtendedBlockId blockId, String clientName) throws IOException\n{\r\n    lock.lock();\r\n    try {\r\n        if (closed) {\r\n            LOG.trace(this + \": the DfsClientShmManager isclosed.\");\r\n            return null;\r\n        }\r\n        EndpointShmManager shmManager = datanodes.get(datanode);\r\n        if (shmManager == null) {\r\n            shmManager = new EndpointShmManager(datanode);\r\n            datanodes.put(datanode, shmManager);\r\n        }\r\n        return shmManager.allocSlot(peer, usedPeer, clientName, blockId);\r\n    } finally {\r\n        lock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "freeSlot",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void freeSlot(Slot slot)\n{\r\n    lock.lock();\r\n    try {\r\n        DfsClientShm shm = (DfsClientShm) slot.getShm();\r\n        shm.getEndpointShmManager().freeSlot(slot);\r\n    } finally {\r\n        lock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "visit",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void visit(Visitor visitor) throws IOException\n{\r\n    lock.lock();\r\n    try {\r\n        HashMap<DatanodeInfo, PerDatanodeVisitorInfo> info = new HashMap<>();\r\n        for (Entry<DatanodeInfo, EndpointShmManager> entry : datanodes.entrySet()) {\r\n            info.put(entry.getKey(), entry.getValue().getVisitorInfo());\r\n        }\r\n        visitor.visit(info);\r\n    } finally {\r\n        lock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "close",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    lock.lock();\r\n    try {\r\n        if (closed)\r\n            return;\r\n        closed = true;\r\n    } finally {\r\n        lock.unlock();\r\n    }\r\n    try {\r\n        domainSocketWatcher.close();\r\n    } catch (Throwable e) {\r\n        LOG.debug(\"Exception in closing \" + domainSocketWatcher, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return String.format(\"ShortCircuitShmManager(%08x)\", System.identityHashCode(this));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getDomainSocketWatcher",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DomainSocketWatcher getDomainSocketWatcher()\n{\r\n    return domainSocketWatcher;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getShmNum",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getShmNum()\n{\r\n    int segments = 0;\r\n    for (EndpointShmManager endpointShmManager : datanodes.values()) {\r\n        segments += endpointShmManager.notFull.size() + endpointShmManager.full.size();\r\n    }\r\n    return segments;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBytesNeeded",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesNeeded()\n{\r\n    return bytesNeeded;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBytesCached",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesCached()\n{\r\n    return bytesCached;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFilesNeeded",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFilesNeeded()\n{\r\n    return filesNeeded;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFilesCached",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFilesCached()\n{\r\n    return filesCached;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hasExpired",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasExpired()\n{\r\n    return hasExpired;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"{\" + \"bytesNeeded: \" + bytesNeeded + \", \" + \"bytesCached: \" + bytesCached + \", \" + \"filesNeeded: \" + filesNeeded + \", \" + \"filesCached: \" + filesCached + \", \" + \"hasExpired: \" + hasExpired + \"}\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getHostName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHostName()\n{\r\n    return hostName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setHostName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHostName(final String hostName)\n{\r\n    this.hostName = hostName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getPort()\n{\r\n    return port;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setPort(final int port)\n{\r\n    this.port = port;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getUpgradeDomain",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUpgradeDomain()\n{\r\n    return upgradeDomain;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setUpgradeDomain",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setUpgradeDomain(final String upgradeDomain)\n{\r\n    this.upgradeDomain = upgradeDomain;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getAdminState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AdminStates getAdminState()\n{\r\n    return adminState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setAdminState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAdminState(final AdminStates adminState)\n{\r\n    this.adminState = adminState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getMaintenanceExpireTimeInMS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMaintenanceExpireTimeInMS()\n{\r\n    return this.maintenanceExpireTimeInMS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setMaintenanceExpireTimeInMS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMaintenanceExpireTimeInMS(final long maintenanceExpireTimeInMS)\n{\r\n    this.maintenanceExpireTimeInMS = maintenanceExpireTimeInMS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "selectToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<BlockTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens)\n{\r\n    if (service == null) {\r\n        return null;\r\n    }\r\n    for (Token<? extends TokenIdentifier> token : tokens) {\r\n        if (BlockTokenIdentifier.KIND_NAME.equals(token.getKind())) {\r\n            return (Token<BlockTokenIdentifier>) token;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int read(long pos, byte[] buf, int off, int len) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int read(long pos, ByteBuffer buf) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isLocal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isLocal()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isShortCircuit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isShortCircuit()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getNetworkDistance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNetworkDistance()\n{\r\n    return isLocal() ? 0 : Integer.MAX_VALUE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SlowDiskReports create(@Nullable Map<String, Map<DiskOp, Double>> slowDisks)\n{\r\n    if (slowDisks == null || slowDisks.isEmpty()) {\r\n        return EMPTY_REPORT;\r\n    }\r\n    return new SlowDiskReports(slowDisks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getSlowDisks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, Map<DiskOp, Double>> getSlowDisks()\n{\r\n    return slowDisks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "haveSlowDisks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean haveSlowDisks()\n{\r\n    return slowDisks.size() > 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (this == o) {\r\n        return true;\r\n    }\r\n    if (!(o instanceof SlowDiskReports)) {\r\n        return false;\r\n    }\r\n    SlowDiskReports that = (SlowDiskReports) o;\r\n    if (this.slowDisks.size() != that.slowDisks.size()) {\r\n        return false;\r\n    }\r\n    if (!this.slowDisks.keySet().containsAll(that.slowDisks.keySet()) || !that.slowDisks.keySet().containsAll(this.slowDisks.keySet())) {\r\n        return false;\r\n    }\r\n    boolean areEqual;\r\n    for (Map.Entry<String, Map<DiskOp, Double>> entry : this.slowDisks.entrySet()) {\r\n        if (!entry.getValue().equals(that.slowDisks.get(entry.getKey()))) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return slowDisks.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "fromProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockConstructionStage fromProto(OpWriteBlockProto.BlockConstructionStage stage)\n{\r\n    return BlockConstructionStage.valueOf(stage.name());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "toProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OpWriteBlockProto.BlockConstructionStage toProto(BlockConstructionStage stage)\n{\r\n    return OpWriteBlockProto.BlockConstructionStage.valueOf(stage.name());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "toProto",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ChecksumProto toProto(DataChecksum checksum)\n{\r\n    ChecksumTypeProto type = PBHelperClient.convert(checksum.getChecksumType());\r\n    return ChecksumProto.newBuilder().setBytesPerChecksum(checksum.getBytesPerChecksum()).setType(type).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "fromProto",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DataChecksum fromProto(ChecksumProto proto)\n{\r\n    if (proto == null) {\r\n        return null;\r\n    }\r\n    int bytesPerChecksum = proto.getBytesPerChecksum();\r\n    DataChecksum.Type type = PBHelperClient.convert(proto.getType());\r\n    return DataChecksum.newDataChecksum(type, bytesPerChecksum);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "buildClientHeader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ClientOperationHeaderProto buildClientHeader(ExtendedBlock blk, String client, Token<BlockTokenIdentifier> blockToken)\n{\r\n    return ClientOperationHeaderProto.newBuilder().setBaseHeader(buildBaseHeader(blk, blockToken)).setClientName(client).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "buildBaseHeader",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "BaseHeaderProto buildBaseHeader(ExtendedBlock blk, Token<BlockTokenIdentifier> blockToken)\n{\r\n    BaseHeaderProto.Builder builder = BaseHeaderProto.newBuilder().setBlock(PBHelperClient.convert(blk)).setToken(PBHelperClient.convert(blockToken));\r\n    Span span = Tracer.getCurrentSpan();\r\n    if (span != null) {\r\n        DataTransferTraceInfoProto.Builder traceInfoProtoBuilder = DataTransferTraceInfoProto.newBuilder().setSpanContext(TraceUtils.spanContextToByteString(span.getContext()));\r\n        builder.setTraceInfo(traceInfoProtoBuilder);\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "checkBlockOpStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkBlockOpStatus(BlockOpResponseProto response, String logInfo) throws IOException\n{\r\n    checkBlockOpStatus(response, logInfo, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "checkBlockOpStatus",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void checkBlockOpStatus(BlockOpResponseProto response, String logInfo, boolean checkBlockPinningErr) throws IOException\n{\r\n    if (response.getStatus() != Status.SUCCESS) {\r\n        if (response.getStatus() == Status.ERROR_ACCESS_TOKEN) {\r\n            throw new InvalidBlockTokenException(\"Got access token error\" + \", status message \" + response.getMessage() + \", \" + logInfo);\r\n        } else if (checkBlockPinningErr && response.getStatus() == Status.ERROR_BLOCK_PINNED) {\r\n            throw new BlockPinningException(\"Got error\" + \", status=\" + response.getStatus().name() + \", status message \" + response.getMessage() + \", \" + logInfo);\r\n        } else {\r\n            throw new IOException(\"Got error\" + \", status=\" + response.getStatus().name() + \", status message \" + response.getMessage() + \", \" + logInfo);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "checkRange",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkRange(final Long min, final Long max)\n{\r\n    if (value == null) {\r\n        return;\r\n    }\r\n    if (min != null && value < min) {\r\n        throw new IllegalArgumentException(\"Invalid parameter range: \" + getName() + \" = \" + domain.toString(value) + \" < \" + domain.toString(min));\r\n    }\r\n    if (max != null && value > max) {\r\n        throw new IllegalArgumentException(\"Invalid parameter range: \" + getName() + \" = \" + domain.toString(value) + \" > \" + domain.toString(max));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return getName() + \"=\" + domain.toString(getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getValueString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getValueString()\n{\r\n    return domain.toString(getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getSeqno",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getSeqno()\n{\r\n    return proto.getSeqno();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getNumOfReplies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "short getNumOfReplies()\n{\r\n    return (short) proto.getReplyCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getHeaderFlag",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getHeaderFlag(int i)\n{\r\n    if (proto.getFlagCount() > 0) {\r\n        return proto.getFlag(i);\r\n    } else {\r\n        return combineHeader(ECN.DISABLED, proto.getReply(i), SLOW.DISABLED);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getFlag",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getFlag(int i)\n{\r\n    return proto.getFlag(i);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getDownstreamAckTimeNanos",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDownstreamAckTimeNanos()\n{\r\n    return proto.getDownstreamAckTimeNanos();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "isSuccess",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSuccess()\n{\r\n    for (Status s : proto.getReplyList()) {\r\n        if (s != Status.SUCCESS) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getOOBStatus",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Status getOOBStatus()\n{\r\n    if (getSeqno() != UNKOWN_SEQNO) {\r\n        return null;\r\n    }\r\n    for (Status s : proto.getReplyList()) {\r\n        if (s.getNumber() >= OOB_START && s.getNumber() <= OOB_END) {\r\n            return s;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getRestartOOBStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Status getRestartOOBStatus()\n{\r\n    return Status.OOB_RESTART;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "isRestartOOBStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isRestartOOBStatus(Status st)\n{\r\n    return st.equals(Status.OOB_RESTART);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readFields(InputStream in) throws IOException\n{\r\n    proto = PipelineAckProto.parseFrom(vintPrefixed(in));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(OutputStream out) throws IOException\n{\r\n    proto.writeDelimitedTo(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return TextFormat.shortDebugString(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getStatusFromHeader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Status getStatusFromHeader(int header)\n{\r\n    return StatusFormat.getStatus(header);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getECNFromHeader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ECN getECNFromHeader(int header)\n{\r\n    return StatusFormat.getECN(header);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getSLOWFromHeader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SLOW getSLOWFromHeader(int header)\n{\r\n    return StatusFormat.getSLOW(header);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "setStatusForHeader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int setStatusForHeader(int old, Status status)\n{\r\n    return StatusFormat.setStatus(old, status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "setSLOWForHeader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int setSLOWForHeader(int old, SLOW slow)\n{\r\n    return StatusFormat.setSLOW(old, slow);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "combineHeader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int combineHeader(ECN ecn, Status status)\n{\r\n    return combineHeader(ecn, status, SLOW.DISABLED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "combineHeader",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int combineHeader(ECN ecn, Status status, SLOW slow)\n{\r\n    int header = 0;\r\n    header = StatusFormat.setStatus(header, status);\r\n    header = StatusFormat.setECN(header, ecn);\r\n    header = StatusFormat.setSLOW(header, slow);\r\n    return header;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getXAttrName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getXAttrName()\n{\r\n    return getValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLastSeenStateId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLastSeenStateId()\n{\r\n    return lastSeenStateId.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isCoordinatedCall",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isCoordinatedCall(String protocolName, String method)\n{\r\n    throw new UnsupportedOperationException(\"Client should not be checking uncoordinated call\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "updateResponseState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void updateResponseState(RpcResponseHeaderProto.Builder header)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "receiveResponseState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void receiveResponseState(RpcResponseHeaderProto header)\n{\r\n    lastSeenStateId.accumulate(header.getStateId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "updateRequestState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateRequestState(RpcRequestHeaderProto.Builder header)\n{\r\n    header.setStateId(lastSeenStateId.longValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "receiveRequestState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long receiveRequestState(RpcRequestHeaderProto header, long threshold) throws IOException\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getXAttrValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "byte[] getXAttrValue() throws IOException\n{\r\n    final String v = getValue();\r\n    return XAttrCodec.decodeValue(v);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getAlignmentContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AlignmentContext getAlignmentContext()\n{\r\n    return alignmentContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getProxy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ProxyInfo<T> getProxy()\n{\r\n    return combinedProxy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "performFailover",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void performFailover(T currentProxy)\n{\r\n    failoverProxy.performFailover(currentProxy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "isRead",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isRead(Method method)\n{\r\n    if (!method.isAnnotationPresent(ReadOnly.class)) {\r\n        return false;\r\n    }\r\n    return !method.getAnnotationsByType(ReadOnly.class)[0].activeOnly();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "setObserverReadEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setObserverReadEnabled(boolean flag)\n{\r\n    this.observerReadEnabled = flag;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getLastProxy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ProxyInfo<T> getLastProxy()\n{\r\n    return lastProxy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getCurrentProxy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NNProxyInfo<T> getCurrentProxy()\n{\r\n    return changeProxy(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "changeProxy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "NNProxyInfo<T> changeProxy(NNProxyInfo<T> initial)\n{\r\n    if (currentProxy != initial) {\r\n        return currentProxy;\r\n    }\r\n    currentIndex = (currentIndex + 1) % nameNodeProxies.size();\r\n    currentProxy = createProxyIfNeeded(nameNodeProxies.get(currentIndex));\r\n    currentProxy.setCachedState(getHAServiceState(currentProxy));\r\n    LOG.debug(\"Changed current proxy from {} to {}\", initial == null ? \"none\" : initial.proxyInfo, currentProxy.proxyInfo);\r\n    return currentProxy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getHAServiceState",
  "errType" : [ "RemoteException", "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "HAServiceState getHAServiceState(NNProxyInfo<T> proxyInfo)\n{\r\n    IOException ioe;\r\n    try {\r\n        return getProxyAsClientProtocol(proxyInfo.proxy).getHAServiceState();\r\n    } catch (RemoteException re) {\r\n        if (re.unwrapRemoteException() instanceof StandbyException) {\r\n            LOG.debug(\"NameNode {} threw StandbyException when fetching HAState\", proxyInfo.getAddress());\r\n            return HAServiceState.STANDBY;\r\n        }\r\n        ioe = re;\r\n    } catch (IOException e) {\r\n        ioe = e;\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Failed to connect to {} while fetching HAServiceState\", proxyInfo.getAddress(), ioe);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getProxyAsClientProtocol",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ClientProtocol getProxyAsClientProtocol(T proxy)\n{\r\n    assert proxy instanceof ClientProtocol : \"BUG: Attempted to use proxy \" + \"of class \" + proxy.getClass() + \" as if it was a ClientProtocol.\";\r\n    return (ClientProtocol) proxy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "initializeMsync",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initializeMsync() throws IOException\n{\r\n    if (msynced) {\r\n        return;\r\n    }\r\n    getProxyAsClientProtocol(failoverProxy.getProxy().proxy).msync();\r\n    msynced = true;\r\n    lastMsyncTimeMs = Time.monotonicNow();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "shouldFindObserver",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldFindObserver()\n{\r\n    if (lastObserverProbeTime > 0) {\r\n        return Time.monotonicNow() - lastObserverProbeTime >= observerProbeRetryPeriodMs;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "autoMsyncIfNecessary",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void autoMsyncIfNecessary() throws IOException\n{\r\n    if (autoMsyncPeriodMs == 0) {\r\n        getProxyAsClientProtocol(failoverProxy.getProxy().proxy).msync();\r\n    } else if (autoMsyncPeriodMs > 0) {\r\n        if (Time.monotonicNow() - lastMsyncTimeMs > autoMsyncPeriodMs) {\r\n            synchronized (this) {\r\n                if (Time.monotonicNow() - lastMsyncTimeMs > autoMsyncPeriodMs) {\r\n                    getProxyAsClientProtocol(failoverProxy.getProxy().proxy).msync();\r\n                    lastMsyncTimeMs = Time.monotonicNow();\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    for (ProxyInfo<T> pi : nameNodeProxies) {\r\n        if (pi.proxy != null) {\r\n            if (pi.proxy instanceof Closeable) {\r\n                ((Closeable) pi.proxy).close();\r\n            } else {\r\n                RPC.stopProxy(pi.proxy);\r\n            }\r\n            pi.proxy = null;\r\n        }\r\n    }\r\n    failoverProxy.close();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "useLogicalURI",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean useLogicalURI()\n{\r\n    return failoverProxy.useLogicalURI();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "hasPathCapability",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Optional<Boolean> hasPathCapability(final Path path, final String capability)\n{\r\n    switch(validatePathCapabilityArgs(path, capability)) {\r\n        case CommonPathCapabilities.FS_ACLS:\r\n        case CommonPathCapabilities.FS_APPEND:\r\n        case CommonPathCapabilities.FS_CHECKSUMS:\r\n        case CommonPathCapabilities.FS_CONCAT:\r\n        case CommonPathCapabilities.FS_LIST_CORRUPT_FILE_BLOCKS:\r\n        case CommonPathCapabilities.FS_MULTIPART_UPLOADER:\r\n        case CommonPathCapabilities.FS_PATHHANDLES:\r\n        case CommonPathCapabilities.FS_PERMISSIONS:\r\n        case CommonPathCapabilities.FS_SNAPSHOTS:\r\n        case CommonPathCapabilities.FS_STORAGEPOLICY:\r\n        case CommonPathCapabilities.FS_XATTRS:\r\n            return Optional.of(true);\r\n        case CommonPathCapabilities.FS_SYMLINKS:\r\n            return Optional.of(FileSystem.areSymlinksEnabled());\r\n        default:\r\n            return Optional.empty();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLastBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LocatedBlock getLastBlock()\n{\r\n    return lastBlock;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HdfsFileStatus getFileStatus()\n{\r\n    return fileStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getProxy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ProxyInfo<T> getProxy()\n{\r\n    NNProxyInfo<T> current = proxies.get(currentProxyIndex);\r\n    return createProxyIfNeeded(current);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "performFailover",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void performFailover(T currentProxy)\n{\r\n    incrementProxyIndex();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "incrementProxyIndex",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementProxyIndex()\n{\r\n    currentProxyIndex = (currentProxyIndex + 1) % proxies.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    for (ProxyInfo<T> proxy : proxies) {\r\n        if (proxy.proxy != null) {\r\n            if (proxy.proxy instanceof Closeable) {\r\n                ((Closeable) proxy.proxy).close();\r\n            } else {\r\n                RPC.stopProxy(proxy.proxy);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "useLogicalURI",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean useLogicalURI()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFiles",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] getFiles()\n{\r\n    return files;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getCookie",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCookie()\n{\r\n    return cookie;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (this == obj) {\r\n        return true;\r\n    }\r\n    if (!(obj instanceof CorruptFileBlocks)) {\r\n        return false;\r\n    }\r\n    CorruptFileBlocks other = (CorruptFileBlocks) obj;\r\n    return cookie.equals(other.cookie) && Arrays.equals(files, other.files);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int hashCode()\n{\r\n    int result = cookie.hashCode();\r\n    for (String file : files) {\r\n        result = PRIME * result + file.hashCode();\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "makeRequest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BatchedEntries<OpenFileEntry> makeRequest(Long prevId) throws IOException\n{\r\n    try (TraceScope ignored = tracer.newScope(\"listOpenFiles\")) {\r\n        return namenode.listOpenFiles(prevId, types, path);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "elementToPrevKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Long elementToPrevKey(OpenFileEntry entry)\n{\r\n    return entry.getId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "checkRange",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkRange(final Short min, final Short max)\n{\r\n    if (value == null) {\r\n        return;\r\n    }\r\n    if (min != null && value < min) {\r\n        throw new IllegalArgumentException(\"Invalid parameter range: \" + getName() + \" = \" + domain.toString(value) + \" < \" + domain.toString(min));\r\n    }\r\n    if (max != null && value > max) {\r\n        throw new IllegalArgumentException(\"Invalid parameter range: \" + getName() + \" = \" + domain.toString(value) + \" > \" + domain.toString(max));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return getName() + \"=\" + domain.toString(getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getValueString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getValueString()\n{\r\n    return domain.toString(getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isLogicalUri",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isLogicalUri(Configuration conf, URI nameNodeUri)\n{\r\n    String host = nameNodeUri.getHost();\r\n    return DFSUtilClient.getNameServiceIds(conf).contains(host);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isClientFailoverConfigured",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isClientFailoverConfigured(Configuration conf, URI nameNodeUri)\n{\r\n    String host = nameNodeUri.getHost();\r\n    String configKey = HdfsClientConfigKeys.Failover.PROXY_PROVIDER_KEY_PREFIX + \".\" + host;\r\n    return conf.get(configKey) != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "buildTokenServiceForLogicalUri",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Text buildTokenServiceForLogicalUri(final URI uri, final String scheme)\n{\r\n    return new Text(buildTokenServicePrefixForLogicalUri(scheme) + uri.getHost());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "buildTokenServicePrefixForLogicalUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String buildTokenServicePrefixForLogicalUri(String scheme)\n{\r\n    return HA_DT_SERVICE_PREFIX + scheme + \":\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getServiceUriFromToken",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "URI getServiceUriFromToken(final String scheme, Token<?> token)\n{\r\n    String tokStr = token.getService().toString();\r\n    final String prefix = buildTokenServicePrefixForLogicalUri(scheme);\r\n    if (tokStr.startsWith(prefix)) {\r\n        tokStr = tokStr.replaceFirst(prefix, \"\");\r\n    }\r\n    return URI.create(scheme + \"://\" + tokStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isTokenForLogicalUri",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isTokenForLogicalUri(Token<?> token)\n{\r\n    return token.getService().toString().startsWith(HA_DT_SERVICE_PREFIX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "cloneDelegationTokenForLogicalUri",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void cloneDelegationTokenForLogicalUri(UserGroupInformation ugi, URI haUri, Collection<InetSocketAddress> nnAddrs)\n{\r\n    Text haService = HAUtilClient.buildTokenServiceForLogicalUri(haUri, HdfsConstants.HDFS_URI_SCHEME);\r\n    Token<DelegationTokenIdentifier> haToken = tokenSelector.selectToken(haService, ugi.getTokens());\r\n    if (haToken != null) {\r\n        for (InetSocketAddress singleNNAddr : nnAddrs) {\r\n            Token<DelegationTokenIdentifier> specificToken = haToken.privateClone(buildTokenService(singleNNAddr));\r\n            Text alias = new Text(HAUtilClient.buildTokenServicePrefixForLogicalUri(HdfsConstants.HDFS_URI_SCHEME) + \"//\" + specificToken.getService());\r\n            ugi.addToken(alias, specificToken);\r\n            if (LOG.isDebugEnabled()) {\r\n                LOG.debug(\"Mapped HA service delegation token for logical URI \" + haUri + \" to namenode \" + singleNNAddr);\r\n            }\r\n        }\r\n    } else {\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"No HA service delegation token found for logical URI \" + haUri);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setParentFullPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setParentFullPath(byte[] path)\n{\r\n    parentFullPath = path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getSnapshotID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getSnapshotID()\n{\r\n    return snapshotID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isDeleted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isDeleted()\n{\r\n    return isDeleted;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getDirStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HdfsFileStatus getDirStatus()\n{\r\n    return dirStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getParentFullPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getParentFullPath()\n{\r\n    return parentFullPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFullPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getFullPath()\n{\r\n    String parentFullPathStr = (parentFullPath == null || parentFullPath.length == 0) ? \"/\" : DFSUtilClient.bytes2String(parentFullPath);\r\n    return new Path(getSnapshotPath(parentFullPathStr, dirStatus.getLocalName()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "print",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void print(SnapshotStatus[] stats, PrintStream out)\n{\r\n    if (stats == null || stats.length == 0) {\r\n        out.println();\r\n        return;\r\n    }\r\n    int maxRepl = 0, maxLen = 0, maxOwner = 0, maxGroup = 0;\r\n    int maxSnapshotID = 0;\r\n    for (SnapshotStatus status : stats) {\r\n        maxRepl = maxLength(maxRepl, status.dirStatus.getReplication());\r\n        maxLen = maxLength(maxLen, status.dirStatus.getLen());\r\n        maxOwner = maxLength(maxOwner, status.dirStatus.getOwner());\r\n        maxGroup = maxLength(maxGroup, status.dirStatus.getGroup());\r\n        maxSnapshotID = maxLength(maxSnapshotID, status.snapshotID);\r\n    }\r\n    String lineFormat = \"%s%s \" + \"%\" + maxRepl + \"s \" + (maxOwner > 0 ? \"%-\" + maxOwner + \"s \" : \"%s\") + (maxGroup > 0 ? \"%-\" + maxGroup + \"s \" : \"%s\") + \"%\" + maxLen + \"s \" + \"%s \" + \"%\" + maxSnapshotID + \"s \" + \"%s \" + \"%s\";\r\n    SimpleDateFormat dateFormat = new SimpleDateFormat(\"yyyy-MM-dd HH:mm\");\r\n    for (SnapshotStatus status : stats) {\r\n        String line = String.format(lineFormat, \"d\", status.dirStatus.getPermission(), status.dirStatus.getReplication(), status.dirStatus.getOwner(), status.dirStatus.getGroup(), String.valueOf(status.dirStatus.getLen()), dateFormat.format(new Date(status.dirStatus.getModificationTime())), status.snapshotID, status.isDeleted ? \"DELETED\" : \"ACTIVE\", getSnapshotPath(DFSUtilClient.bytes2String(status.parentFullPath), status.dirStatus.getLocalName()));\r\n        out.println(line);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "maxLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int maxLength(int n, Object value)\n{\r\n    return Math.max(n, String.valueOf(value).length());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getSnapshotPath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getSnapshotPath(String snapshottableDir, String snapshotRelativePath)\n{\r\n    String parentFullPathStr = snapshottableDir == null || snapshottableDir.isEmpty() ? \"/\" : snapshottableDir;\r\n    final StringBuilder b = new StringBuilder(parentFullPathStr);\r\n    if (b.charAt(b.length() - 1) != Path.SEPARATOR_CHAR) {\r\n        b.append(Path.SEPARATOR);\r\n    }\r\n    return b.append(HdfsConstants.DOT_SNAPSHOT_DIR).append(Path.SEPARATOR).append(snapshotRelativePath).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getParentPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getParentPath(String snapshotPath)\n{\r\n    int index = snapshotPath.indexOf(HdfsConstants.DOT_SNAPSHOT_DIR);\r\n    return index == -1 ? snapshotPath : snapshotPath.substring(0, index - 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createProxyWithClientProtocol",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ProxyAndInfo<ClientProtocol> createProxyWithClientProtocol(Configuration conf, URI nameNodeUri, AtomicBoolean fallbackToSimpleAuth) throws IOException\n{\r\n    AbstractNNFailoverProxyProvider<ClientProtocol> failoverProxyProvider = createFailoverProxyProvider(conf, nameNodeUri, ClientProtocol.class, true, fallbackToSimpleAuth);\r\n    if (failoverProxyProvider == null) {\r\n        InetSocketAddress nnAddr = DFSUtilClient.getNNAddress(nameNodeUri);\r\n        Text dtService = SecurityUtil.buildTokenService(nnAddr);\r\n        ClientProtocol proxy = createNonHAProxyWithClientProtocol(nnAddr, conf, UserGroupInformation.getCurrentUser(), true, fallbackToSimpleAuth);\r\n        return new ProxyAndInfo<>(proxy, dtService, nnAddr);\r\n    } else {\r\n        return createHAProxy(conf, nameNodeUri, ClientProtocol.class, failoverProxyProvider);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createProxyWithLossyRetryHandler",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "ProxyAndInfo<T> createProxyWithLossyRetryHandler(Configuration config, URI nameNodeUri, Class<T> xface, int numResponseToDrop, AtomicBoolean fallbackToSimpleAuth) throws IOException\n{\r\n    Preconditions.checkArgument(numResponseToDrop > 0);\r\n    AbstractNNFailoverProxyProvider<T> failoverProxyProvider = createFailoverProxyProvider(config, nameNodeUri, xface, true, fallbackToSimpleAuth);\r\n    if (failoverProxyProvider != null) {\r\n        int delay = config.getInt(HdfsClientConfigKeys.Failover.SLEEPTIME_BASE_KEY, HdfsClientConfigKeys.Failover.SLEEPTIME_BASE_DEFAULT);\r\n        int maxCap = config.getInt(HdfsClientConfigKeys.Failover.SLEEPTIME_MAX_KEY, HdfsClientConfigKeys.Failover.SLEEPTIME_MAX_DEFAULT);\r\n        int maxFailoverAttempts = config.getInt(HdfsClientConfigKeys.Failover.MAX_ATTEMPTS_KEY, HdfsClientConfigKeys.Failover.MAX_ATTEMPTS_DEFAULT);\r\n        int maxRetryAttempts = config.getInt(HdfsClientConfigKeys.Retry.MAX_ATTEMPTS_KEY, HdfsClientConfigKeys.Retry.MAX_ATTEMPTS_DEFAULT);\r\n        InvocationHandler dummyHandler = new LossyRetryInvocationHandler<>(numResponseToDrop, failoverProxyProvider, RetryPolicies.failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL, maxFailoverAttempts, Math.max(numResponseToDrop + 1, maxRetryAttempts), delay, maxCap));\r\n        @SuppressWarnings(\"unchecked\")\r\n        T proxy = (T) Proxy.newProxyInstance(failoverProxyProvider.getInterface().getClassLoader(), new Class[] { xface }, dummyHandler);\r\n        Text dtService;\r\n        if (failoverProxyProvider.useLogicalURI()) {\r\n            dtService = HAUtilClient.buildTokenServiceForLogicalUri(nameNodeUri, HdfsConstants.HDFS_URI_SCHEME);\r\n        } else {\r\n            dtService = SecurityUtil.buildTokenService(DFSUtilClient.getNNAddress(nameNodeUri));\r\n        }\r\n        return new ProxyAndInfo<>(proxy, dtService, DFSUtilClient.getNNAddress(nameNodeUri));\r\n    } else {\r\n        LOG.warn(\"Currently creating proxy using \" + \"LossyRetryInvocationHandler requires NN HA setup\");\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createFailoverProxyProvider",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractNNFailoverProxyProvider<T> createFailoverProxyProvider(Configuration conf, URI nameNodeUri, Class<T> xface, boolean checkPort, AtomicBoolean fallbackToSimpleAuth) throws IOException\n{\r\n    return createFailoverProxyProvider(conf, nameNodeUri, xface, checkPort, fallbackToSimpleAuth, new ClientHAProxyFactory<T>());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createFailoverProxyProvider",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "AbstractNNFailoverProxyProvider<T> createFailoverProxyProvider(Configuration conf, URI nameNodeUri, Class<T> xface, boolean checkPort, AtomicBoolean fallbackToSimpleAuth, HAProxyFactory<T> proxyFactory) throws IOException\n{\r\n    Class<FailoverProxyProvider<T>> failoverProxyProviderClass = null;\r\n    AbstractNNFailoverProxyProvider<T> providerNN;\r\n    try {\r\n        failoverProxyProviderClass = getFailoverProxyProviderClass(conf, nameNodeUri);\r\n        if (failoverProxyProviderClass == null) {\r\n            return null;\r\n        }\r\n        Constructor<FailoverProxyProvider<T>> ctor = failoverProxyProviderClass.getConstructor(Configuration.class, URI.class, Class.class, HAProxyFactory.class);\r\n        FailoverProxyProvider<T> provider = ctor.newInstance(conf, nameNodeUri, xface, proxyFactory);\r\n        if (!(provider instanceof AbstractNNFailoverProxyProvider)) {\r\n            providerNN = new WrappedFailoverProxyProvider<>(provider);\r\n        } else {\r\n            providerNN = (AbstractNNFailoverProxyProvider<T>) provider;\r\n        }\r\n    } catch (Exception e) {\r\n        final String message = \"Couldn't create proxy provider \" + failoverProxyProviderClass;\r\n        LOG.debug(message, e);\r\n        if (e.getCause() instanceof IOException) {\r\n            throw (IOException) e.getCause();\r\n        } else {\r\n            throw new IOException(message, e);\r\n        }\r\n    }\r\n    if (checkPort && providerNN.useLogicalURI()) {\r\n        int port = nameNodeUri.getPort();\r\n        if (port > 0 && port != HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT) {\r\n            throw new IOException(\"Port \" + port + \" specified in URI \" + nameNodeUri + \" but host '\" + nameNodeUri.getHost() + \"' is a logical (HA) namenode\" + \" and does not use port information.\");\r\n        }\r\n    }\r\n    providerNN.setFallbackToSimpleAuth(fallbackToSimpleAuth);\r\n    return providerNN;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFailoverProxyProviderClass",
  "errType" : [ "RuntimeException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Class<FailoverProxyProvider<T>> getFailoverProxyProviderClass(Configuration conf, URI nameNodeUri) throws IOException\n{\r\n    if (nameNodeUri == null) {\r\n        return null;\r\n    }\r\n    String host = nameNodeUri.getHost();\r\n    String configKey = HdfsClientConfigKeys.Failover.PROXY_PROVIDER_KEY_PREFIX + \".\" + host;\r\n    try {\r\n        @SuppressWarnings(\"unchecked\")\r\n        Class<FailoverProxyProvider<T>> ret = (Class<FailoverProxyProvider<T>>) conf.getClass(configKey, null, FailoverProxyProvider.class);\r\n        return ret;\r\n    } catch (RuntimeException e) {\r\n        if (e.getCause() instanceof ClassNotFoundException) {\r\n            throw new IOException(\"Could not load failover proxy provider class \" + conf.get(configKey) + \" which is configured for authority \" + nameNodeUri, e);\r\n        } else {\r\n            throw e;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createHAProxy",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ProxyAndInfo<T> createHAProxy(Configuration conf, URI nameNodeUri, Class<T> xface, AbstractNNFailoverProxyProvider<T> failoverProxyProvider)\n{\r\n    Preconditions.checkNotNull(failoverProxyProvider);\r\n    DfsClientConf config = new DfsClientConf(conf);\r\n    T proxy = (T) RetryProxy.create(xface, failoverProxyProvider, RetryPolicies.failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL, config.getMaxFailoverAttempts(), config.getMaxRetryAttempts(), config.getFailoverSleepBaseMillis(), config.getFailoverSleepMaxMillis()));\r\n    Text dtService;\r\n    if (failoverProxyProvider.useLogicalURI()) {\r\n        dtService = HAUtilClient.buildTokenServiceForLogicalUri(nameNodeUri, HdfsConstants.HDFS_URI_SCHEME);\r\n    } else {\r\n        dtService = SecurityUtil.buildTokenService(DFSUtilClient.getNNAddress(nameNodeUri));\r\n    }\r\n    return new ProxyAndInfo<>(proxy, dtService, DFSUtilClient.getNNAddressCheckLogical(conf, nameNodeUri));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createNonHAProxyWithClientProtocol",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ClientProtocol createNonHAProxyWithClientProtocol(InetSocketAddress address, Configuration conf, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth) throws IOException\n{\r\n    return createProxyWithAlignmentContext(address, conf, ugi, withRetries, fallbackToSimpleAuth, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createProxyWithAlignmentContext",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ClientProtocol createProxyWithAlignmentContext(InetSocketAddress address, Configuration conf, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth, AlignmentContext alignmentContext) throws IOException\n{\r\n    RPC.setProtocolEngine(conf, ClientNamenodeProtocolPB.class, ProtobufRpcEngine2.class);\r\n    final RetryPolicy defaultPolicy = RetryUtils.getDefaultRetryPolicy(conf, HdfsClientConfigKeys.Retry.POLICY_ENABLED_KEY, HdfsClientConfigKeys.Retry.POLICY_ENABLED_DEFAULT, HdfsClientConfigKeys.Retry.POLICY_SPEC_KEY, HdfsClientConfigKeys.Retry.POLICY_SPEC_DEFAULT, SafeModeException.class.getName());\r\n    final long version = RPC.getProtocolVersion(ClientNamenodeProtocolPB.class);\r\n    ClientNamenodeProtocolPB proxy = RPC.getProtocolProxy(ClientNamenodeProtocolPB.class, version, address, ugi, conf, NetUtils.getDefaultSocketFactory(conf), org.apache.hadoop.ipc.Client.getTimeout(conf), defaultPolicy, fallbackToSimpleAuth, alignmentContext).getProxy();\r\n    if (withRetries) {\r\n        Map<String, RetryPolicy> methodNameToPolicyMap = new HashMap<>();\r\n        ClientProtocol translatorProxy = new ClientNamenodeProtocolTranslatorPB(proxy);\r\n        return (ClientProtocol) RetryProxy.create(ClientProtocol.class, new DefaultFailoverProxyProvider<>(ClientProtocol.class, translatorProxy), methodNameToPolicyMap, defaultPolicy);\r\n    } else {\r\n        return new ClientNamenodeProtocolTranslatorPB(proxy);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getListings",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HdfsPartialListing[] getListings()\n{\r\n    return listings;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hasMore",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasMore()\n{\r\n    return hasMore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getStartAfter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getStartAfter()\n{\r\n    return startAfter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return new ToStringBuilder(this).append(\"listings\", listings).append(\"hasMore\", hasMore).append(\"startAfter\", startAfter).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "createReconfigurationProtocolProxy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ReconfigurationProtocolPB createReconfigurationProtocolProxy(InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int socketTimeout) throws IOException\n{\r\n    RPC.setProtocolEngine(conf, ReconfigurationProtocolPB.class, ProtobufRpcEngine2.class);\r\n    return RPC.getProxy(ReconfigurationProtocolPB.class, RPC.getProtocolVersion(ReconfigurationProtocolPB.class), addr, ticket, conf, factory, socketTimeout);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    RPC.stopProxy(rpcProxy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getUnderlyingProxyObject",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getUnderlyingProxyObject()\n{\r\n    return rpcProxy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "startReconfiguration",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void startReconfiguration() throws IOException\n{\r\n    try {\r\n        rpcProxy.startReconfiguration(NULL_CONTROLLER, VOID_START_RECONFIG);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getReconfigurationStatus",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ReconfigurationTaskStatus getReconfigurationStatus() throws IOException\n{\r\n    try {\r\n        return ReconfigurationProtocolUtils.getReconfigurationStatus(rpcProxy.getReconfigurationStatus(NULL_CONTROLLER, VOID_GET_RECONFIG_STATUS));\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "listReconfigurableProperties",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<String> listReconfigurableProperties() throws IOException\n{\r\n    ListReconfigurablePropertiesResponseProto response;\r\n    try {\r\n        response = rpcProxy.listReconfigurableProperties(NULL_CONTROLLER, VOID_LIST_RECONFIGURABLE_PROPERTIES);\r\n        return response.getNameList();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "isMethodSupported",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isMethodSupported(String methodName) throws IOException\n{\r\n    return RpcClientUtil.isMethodSupported(rpcProxy, ReconfigurationProtocolPB.class, RPC.RpcKind.RPC_PROTOCOL_BUFFER, RPC.getProtocolVersion(ReconfigurationProtocolPB.class), methodName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setFileName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReplicaAccessorBuilder setFileName(String fileName)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReplicaAccessorBuilder setBlock(long blockId, String blockPoolId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setGenerationStamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReplicaAccessorBuilder setGenerationStamp(long genstamp)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setVerifyChecksum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReplicaAccessorBuilder setVerifyChecksum(boolean verifyChecksum)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setClientName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReplicaAccessorBuilder setClientName(String clientName)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setAllowShortCircuitReads",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReplicaAccessorBuilder setAllowShortCircuitReads(boolean allowShortCircuit)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setVisibleLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReplicaAccessorBuilder setVisibleLength(long visibleLength)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setConfiguration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReplicaAccessorBuilder setConfiguration(Configuration conf)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setBlockAccessToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReplicaAccessorBuilder setBlockAccessToken(byte[] token)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "build",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReplicaAccessor build()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getExtendedReadBuffers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IdentityHashStore<ByteBuffer, Object> getExtendedReadBuffers()\n{\r\n    if (extendedReadBuffers == null) {\r\n        extendedReadBuffers = new IdentityHashStore<>(0);\r\n    }\r\n    return extendedReadBuffers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addToLocalDeadNodes",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addToLocalDeadNodes(DatanodeInfo dnInfo)\n{\r\n    DFSClient.LOG.debug(\"Add {} to local dead nodes, previously was {}.\", dnInfo, deadNodes);\r\n    deadNodes.put(dnInfo, dnInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeFromLocalDeadNodes",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void removeFromLocalDeadNodes(DatanodeInfo dnInfo)\n{\r\n    DFSClient.LOG.debug(\"Remove {} from local dead nodes.\", dnInfo);\r\n    deadNodes.remove(dnInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLocalDeadNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ConcurrentHashMap<DatanodeInfo, DatanodeInfo> getLocalDeadNodes()\n{\r\n    return deadNodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "clearLocalDeadNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearLocalDeadNodes()\n{\r\n    deadNodes.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDFSClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DFSClient getDFSClient()\n{\r\n    return dfsClient;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getlastBlockBeingWrittenLengthForTesting",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getlastBlockBeingWrittenLengthForTesting()\n{\r\n    return lastBlockBeingWrittenLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "deadNodesContain",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean deadNodesContain(DatanodeInfo nodeInfo)\n{\r\n    return deadNodes.containsKey(nodeInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "openInfo",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void openInfo(boolean refreshLocatedBlocks) throws IOException\n{\r\n    final DfsClientConf conf = dfsClient.getConf();\r\n    synchronized (infoLock) {\r\n        int retriesForLastBlockLength = conf.getRetryTimesForGetLastBlockLength();\r\n        while (true) {\r\n            LocatedBlocks newLocatedBlocks;\r\n            if (locatedBlocks == null || refreshLocatedBlocks) {\r\n                newLocatedBlocks = fetchAndCheckLocatedBlocks(locatedBlocks);\r\n            } else {\r\n                newLocatedBlocks = locatedBlocks;\r\n            }\r\n            long lastBlockLength = getLastBlockLength(newLocatedBlocks);\r\n            if (lastBlockLength != -1) {\r\n                setLocatedBlocksFields(newLocatedBlocks, lastBlockLength);\r\n                return;\r\n            }\r\n            if (retriesForLastBlockLength-- <= 0) {\r\n                throw new IOException(\"Could not obtain the last block locations.\");\r\n            }\r\n            DFSClient.LOG.warn(\"Last block locations not available. \" + \"Datanodes might not have reported blocks completely.\" + \" Will retry for \" + retriesForLastBlockLength + \" times\");\r\n            waitFor(conf.getRetryIntervalForGetLastBlockLength());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setLocatedBlocksFields",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setLocatedBlocksFields(LocatedBlocks locatedBlocksToSet, long lastBlockLength)\n{\r\n    locatedBlocks = locatedBlocksToSet;\r\n    lastBlockBeingWrittenLength = lastBlockLength;\r\n    fileEncryptionInfo = locatedBlocks.getFileEncryptionInfo();\r\n    setLastRefreshedBlocksAt();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "waitFor",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void waitFor(int waitTime) throws IOException\n{\r\n    try {\r\n        Thread.sleep(waitTime);\r\n    } catch (InterruptedException e) {\r\n        Thread.currentThread().interrupt();\r\n        throw new InterruptedIOException(\"Interrupted while getting the last block length.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "fetchAndCheckLocatedBlocks",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "LocatedBlocks fetchAndCheckLocatedBlocks(LocatedBlocks existing) throws IOException\n{\r\n    LocatedBlocks newInfo = dfsClient.getLocatedBlocks(src, 0);\r\n    DFSClient.LOG.debug(\"newInfo = {}\", newInfo);\r\n    if (newInfo == null) {\r\n        throw new IOException(\"Cannot open filename \" + src);\r\n    }\r\n    if (existing != null) {\r\n        Iterator<LocatedBlock> oldIter = existing.getLocatedBlocks().iterator();\r\n        Iterator<LocatedBlock> newIter = newInfo.getLocatedBlocks().iterator();\r\n        while (oldIter.hasNext() && newIter.hasNext()) {\r\n            if (!oldIter.next().getBlock().equals(newIter.next().getBlock())) {\r\n                throw new IOException(\"Blocklist for \" + src + \" has changed!\");\r\n            }\r\n        }\r\n    }\r\n    return newInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLastBlockLength",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "long getLastBlockLength(LocatedBlocks blocks) throws IOException\n{\r\n    long lastBlockBeingWrittenLength = 0;\r\n    if (!blocks.isLastBlockComplete()) {\r\n        final LocatedBlock last = blocks.getLastLocatedBlock();\r\n        if (last != null) {\r\n            if (last.getLocations().length == 0) {\r\n                if (last.getBlockSize() == 0) {\r\n                    return 0;\r\n                }\r\n                return -1;\r\n            }\r\n            final long len = readBlockLength(last);\r\n            last.getBlock().setNumBytes(len);\r\n            lastBlockBeingWrittenLength = len;\r\n        }\r\n    }\r\n    return lastBlockBeingWrittenLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readBlockLength",
  "errType" : [ "IOException", "InterruptedException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "long readBlockLength(LocatedBlock locatedblock) throws IOException\n{\r\n    assert locatedblock != null : \"LocatedBlock cannot be null\";\r\n    int replicaNotFoundCount = locatedblock.getLocations().length;\r\n    final DfsClientConf conf = dfsClient.getConf();\r\n    final int timeout = conf.getSocketTimeout();\r\n    LinkedList<DatanodeInfo> nodeList = new LinkedList<DatanodeInfo>(Arrays.asList(locatedblock.getLocations()));\r\n    LinkedList<DatanodeInfo> retryList = new LinkedList<DatanodeInfo>();\r\n    boolean isRetry = false;\r\n    StopWatch sw = new StopWatch();\r\n    while (nodeList.size() > 0) {\r\n        DatanodeInfo datanode = nodeList.pop();\r\n        ClientDatanodeProtocol cdp = null;\r\n        try {\r\n            cdp = DFSUtilClient.createClientDatanodeProtocolProxy(datanode, dfsClient.getConfiguration(), timeout, conf.isConnectToDnViaHostname(), locatedblock);\r\n            final long n = cdp.getReplicaVisibleLength(locatedblock.getBlock());\r\n            if (n >= 0) {\r\n                return n;\r\n            }\r\n        } catch (IOException ioe) {\r\n            checkInterrupted(ioe);\r\n            if (ioe instanceof RemoteException) {\r\n                if (((RemoteException) ioe).unwrapRemoteException() instanceof ReplicaNotFoundException) {\r\n                    replicaNotFoundCount--;\r\n                } else if (((RemoteException) ioe).unwrapRemoteException() instanceof RetriableException) {\r\n                    retryList.add(datanode);\r\n                }\r\n            }\r\n            DFSClient.LOG.debug(\"Failed to getReplicaVisibleLength from datanode {}\" + \" for block {}\", datanode, locatedblock.getBlock(), ioe);\r\n        } finally {\r\n            if (cdp != null) {\r\n                RPC.stopProxy(cdp);\r\n            }\r\n        }\r\n        if (nodeList.size() == 0 && retryList.size() > 0) {\r\n            nodeList.addAll(retryList);\r\n            retryList.clear();\r\n            isRetry = true;\r\n        }\r\n        if (isRetry) {\r\n            if (!sw.isRunning()) {\r\n                sw.start();\r\n            }\r\n            try {\r\n                Thread.sleep(500);\r\n            } catch (InterruptedException e) {\r\n                Thread.currentThread().interrupt();\r\n                throw new InterruptedIOException(\"Interrupted while getting the length.\");\r\n            }\r\n        }\r\n        if (sw.isRunning() && sw.now(TimeUnit.MILLISECONDS) > timeout) {\r\n            break;\r\n        }\r\n    }\r\n    if (replicaNotFoundCount == 0) {\r\n        return 0;\r\n    }\r\n    throw new CannotObtainBlockLengthException(locatedblock, src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getFileLength()\n{\r\n    synchronized (infoLock) {\r\n        return locatedBlocks == null ? 0 : locatedBlocks.getFileLength() + lastBlockBeingWrittenLength;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "shortCircuitForbidden",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shortCircuitForbidden()\n{\r\n    synchronized (infoLock) {\r\n        return locatedBlocks.isUnderConstruction();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getCurrentDatanode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DatanodeInfo getCurrentDatanode()\n{\r\n    return currentNode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getCurrentBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExtendedBlock getCurrentBlock()\n{\r\n    if (currentLocatedBlock == null) {\r\n        return null;\r\n    }\r\n    return currentLocatedBlock.getBlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAllBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<LocatedBlock> getAllBlocks() throws IOException\n{\r\n    return getBlockRange(0, getFileLength());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSrc",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSrc()\n{\r\n    return src;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLocatedBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LocatedBlocks getLocatedBlocks()\n{\r\n    return locatedBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBlockAt",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "LocatedBlock getBlockAt(long offset) throws IOException\n{\r\n    synchronized (infoLock) {\r\n        assert (locatedBlocks != null) : \"locatedBlocks is null\";\r\n        final LocatedBlock blk;\r\n        if (offset < 0 || offset >= getFileLength()) {\r\n            throw new IOException(\"offset < 0 || offset >= getFileLength(), offset=\" + offset + \", locatedBlocks=\" + locatedBlocks);\r\n        } else if (offset >= locatedBlocks.getFileLength()) {\r\n            blk = locatedBlocks.getLastLocatedBlock();\r\n        } else {\r\n            blk = fetchBlockAt(offset, 0, true);\r\n        }\r\n        return blk;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "fetchBlockAt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocatedBlock fetchBlockAt(long offset) throws IOException\n{\r\n    return fetchBlockAt(offset, 0, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "fetchBlockAt",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "LocatedBlock fetchBlockAt(long offset, long length, boolean useCache) throws IOException\n{\r\n    maybeRegisterBlockRefresh();\r\n    synchronized (infoLock) {\r\n        int targetBlockIdx = locatedBlocks.findBlock(offset);\r\n        if (targetBlockIdx < 0) {\r\n            targetBlockIdx = LocatedBlocks.getInsertIndex(targetBlockIdx);\r\n            useCache = false;\r\n        }\r\n        if (!useCache) {\r\n            final LocatedBlocks newBlocks = (length == 0) ? dfsClient.getLocatedBlocks(src, offset) : dfsClient.getLocatedBlocks(src, offset, length);\r\n            if (newBlocks == null || newBlocks.locatedBlockCount() == 0) {\r\n                throw new EOFException(\"Could not find target position \" + offset);\r\n            }\r\n            if (offset >= locatedBlocks.getFileLength()) {\r\n                setLocatedBlocksFields(newBlocks, getLastBlockLength(newBlocks));\r\n            } else {\r\n                locatedBlocks.insertRange(targetBlockIdx, newBlocks.getLocatedBlocks());\r\n            }\r\n        }\r\n        return locatedBlocks.get(targetBlockIdx);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBlockRange",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<LocatedBlock> getBlockRange(long offset, long length) throws IOException\n{\r\n    if (offset >= getFileLength()) {\r\n        throw new IOException(\"Offset: \" + offset + \" exceeds file length: \" + getFileLength());\r\n    }\r\n    synchronized (infoLock) {\r\n        final List<LocatedBlock> blocks;\r\n        final long lengthOfCompleteBlk = locatedBlocks.getFileLength();\r\n        final boolean readOffsetWithinCompleteBlk = offset < lengthOfCompleteBlk;\r\n        final boolean readLengthPastCompleteBlk = offset + length > lengthOfCompleteBlk;\r\n        if (readOffsetWithinCompleteBlk) {\r\n            blocks = getFinalizedBlockRange(offset, Math.min(length, lengthOfCompleteBlk - offset));\r\n        } else {\r\n            blocks = new ArrayList<>(1);\r\n        }\r\n        if (readLengthPastCompleteBlk) {\r\n            blocks.add(locatedBlocks.getLastLocatedBlock());\r\n        }\r\n        return blocks;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFinalizedBlockRange",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<LocatedBlock> getFinalizedBlockRange(long offset, long length) throws IOException\n{\r\n    synchronized (infoLock) {\r\n        assert (locatedBlocks != null) : \"locatedBlocks is null\";\r\n        List<LocatedBlock> blockRange = new ArrayList<>();\r\n        long remaining = length;\r\n        long curOff = offset;\r\n        while (remaining > 0) {\r\n            LocatedBlock blk = fetchBlockAt(curOff, remaining, true);\r\n            assert curOff >= blk.getStartOffset() : \"Block not found\";\r\n            blockRange.add(blk);\r\n            long bytesRead = blk.getStartOffset() + blk.getBlockSize() - curOff;\r\n            remaining -= bytesRead;\r\n            curOff += bytesRead;\r\n        }\r\n        return blockRange;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "blockSeekTo",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "DatanodeInfo blockSeekTo(long target) throws IOException\n{\r\n    if (target >= getFileLength()) {\r\n        throw new IOException(\"Attempted to read past end of file\");\r\n    }\r\n    maybeRegisterBlockRefresh();\r\n    closeCurrentBlockReaders();\r\n    DatanodeInfo chosenNode;\r\n    int refetchToken = 1;\r\n    int refetchEncryptionKey = 1;\r\n    boolean connectFailedOnce = false;\r\n    while (true) {\r\n        LocatedBlock targetBlock = getBlockAt(target);\r\n        this.pos = target;\r\n        this.blockEnd = targetBlock.getStartOffset() + targetBlock.getBlockSize() - 1;\r\n        this.currentLocatedBlock = targetBlock;\r\n        long offsetIntoBlock = target - targetBlock.getStartOffset();\r\n        DNAddrPair retval = chooseDataNode(targetBlock, null);\r\n        chosenNode = retval.info;\r\n        InetSocketAddress targetAddr = retval.addr;\r\n        StorageType storageType = retval.storageType;\r\n        targetBlock = retval.block;\r\n        try {\r\n            blockReader = getBlockReader(targetBlock, offsetIntoBlock, targetBlock.getBlockSize() - offsetIntoBlock, targetAddr, storageType, chosenNode);\r\n            if (connectFailedOnce) {\r\n                DFSClient.LOG.info(\"Successfully connected to \" + targetAddr + \" for \" + targetBlock.getBlock());\r\n            }\r\n            return chosenNode;\r\n        } catch (IOException ex) {\r\n            checkInterrupted(ex);\r\n            if (ex instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0) {\r\n                DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" + \"encryption key was invalid when connecting to \" + targetAddr + \" : \" + ex);\r\n                refetchEncryptionKey--;\r\n                dfsClient.clearDataEncryptionKey();\r\n            } else if (refetchToken > 0 && tokenRefetchNeeded(ex, targetAddr)) {\r\n                refetchToken--;\r\n                fetchBlockAt(target);\r\n            } else {\r\n                connectFailedOnce = true;\r\n                DFSClient.LOG.warn(\"Failed to connect to {} for file {} for block \" + \"{}, add to deadNodes and continue. \", targetAddr, src, targetBlock.getBlock(), ex);\r\n                addToLocalDeadNodes(chosenNode);\r\n                dfsClient.addNodeToDeadNodeDetector(this, chosenNode);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkInterrupted",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkInterrupted(IOException e) throws IOException\n{\r\n    if (Thread.currentThread().isInterrupted() && (e instanceof ClosedByInterruptException || e instanceof InterruptedIOException)) {\r\n        DFSClient.LOG.debug(\"The reading thread has been interrupted.\", e);\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBlockReader",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "BlockReader getBlockReader(LocatedBlock targetBlock, long offsetInBlock, long length, InetSocketAddress targetAddr, StorageType storageType, DatanodeInfo datanode) throws IOException\n{\r\n    ExtendedBlock blk = targetBlock.getBlock();\r\n    Token<BlockTokenIdentifier> accessToken = targetBlock.getBlockToken();\r\n    CachingStrategy curCachingStrategy;\r\n    boolean shortCircuitForbidden;\r\n    synchronized (infoLock) {\r\n        curCachingStrategy = cachingStrategy;\r\n        shortCircuitForbidden = shortCircuitForbidden();\r\n    }\r\n    return new BlockReaderFactory(dfsClient.getConf()).setInetSocketAddress(targetAddr).setRemotePeerFactory(dfsClient).setDatanodeInfo(datanode).setStorageType(storageType).setFileName(src).setBlock(blk).setBlockToken(accessToken).setStartOffset(offsetInBlock).setVerifyChecksum(verifyChecksum).setClientName(dfsClient.clientName).setLength(length).setCachingStrategy(curCachingStrategy).setAllowShortCircuitLocalReads(!shortCircuitForbidden).setClientCacheContext(dfsClient.getClientContext()).setUserGroupInformation(dfsClient.ugi).setConfiguration(dfsClient.getConfiguration()).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    try {\r\n        if (!closed.compareAndSet(false, true)) {\r\n            DFSClient.LOG.debug(\"DFSInputStream has been closed already\");\r\n            return;\r\n        }\r\n        dfsClient.checkOpen();\r\n        if ((extendedReadBuffers != null) && (!extendedReadBuffers.isEmpty())) {\r\n            final StringBuilder builder = new StringBuilder();\r\n            extendedReadBuffers.visitAll(new IdentityHashStore.Visitor<ByteBuffer, Object>() {\r\n\r\n                private String prefix = \"\";\r\n\r\n                @Override\r\n                public void accept(ByteBuffer k, Object v) {\r\n                    builder.append(prefix).append(k);\r\n                    prefix = \", \";\r\n                }\r\n            });\r\n            DFSClient.LOG.warn(\"closing file \" + src + \", but there are still \" + \"unreleased ByteBuffers allocated by read().  \" + \"Please release \" + builder.toString() + \".\");\r\n        }\r\n        closeCurrentBlockReaders();\r\n        super.close();\r\n    } finally {\r\n        dfsClient.removeNodeFromDeadNodeDetector(this, locatedBlocks);\r\n        maybeDeRegisterBlockRefresh();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int read() throws IOException\n{\r\n    if (oneByteBuf == null) {\r\n        oneByteBuf = new byte[1];\r\n    }\r\n    int ret = read(oneByteBuf, 0, 1);\r\n    return (ret <= 0) ? -1 : (oneByteBuf[0] & 0xff);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readBuffer",
  "errType" : [ "ChecksumException", "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "int readBuffer(ReaderStrategy reader, int len, CorruptedBlocks corruptedBlocks) throws IOException\n{\r\n    IOException ioe;\r\n    boolean retryCurrentNode = true;\r\n    while (true) {\r\n        try {\r\n            return reader.readFromBlock(blockReader, len);\r\n        } catch (ChecksumException ce) {\r\n            DFSClient.LOG.warn(\"Found Checksum error for \" + getCurrentBlock() + \" from \" + currentNode + \" at \" + ce.getPos());\r\n            ioe = ce;\r\n            retryCurrentNode = false;\r\n            corruptedBlocks.addCorruptedBlock(getCurrentBlock(), currentNode);\r\n        } catch (IOException e) {\r\n            if (!retryCurrentNode) {\r\n                DFSClient.LOG.warn(\"Exception while reading from \" + getCurrentBlock() + \" of \" + src + \" from \" + currentNode, e);\r\n            }\r\n            ioe = e;\r\n        }\r\n        boolean sourceFound;\r\n        if (retryCurrentNode) {\r\n            sourceFound = seekToBlockSource(pos);\r\n        } else {\r\n            addToLocalDeadNodes(currentNode);\r\n            dfsClient.addNodeToDeadNodeDetector(this, currentNode);\r\n            sourceFound = seekToNewSource(pos);\r\n        }\r\n        if (!sourceFound) {\r\n            throw ioe;\r\n        }\r\n        retryCurrentNode = false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readWithStrategy",
  "errType" : [ "ChecksumException", "IOException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "int readWithStrategy(ReaderStrategy strategy) throws IOException\n{\r\n    dfsClient.checkOpen();\r\n    if (closed.get()) {\r\n        throw new IOException(\"Stream closed\");\r\n    }\r\n    int len = strategy.getTargetLength();\r\n    CorruptedBlocks corruptedBlocks = new CorruptedBlocks();\r\n    failures = 0;\r\n    maybeRegisterBlockRefresh();\r\n    if (pos < getFileLength()) {\r\n        int retries = 2;\r\n        while (retries > 0) {\r\n            try {\r\n                if (pos > blockEnd || currentNode == null) {\r\n                    currentNode = blockSeekTo(pos);\r\n                }\r\n                int realLen = (int) Math.min(len, (blockEnd - pos + 1L));\r\n                synchronized (infoLock) {\r\n                    if (locatedBlocks.isLastBlockComplete()) {\r\n                        realLen = (int) Math.min(realLen, locatedBlocks.getFileLength() - pos);\r\n                    }\r\n                }\r\n                int result = readBuffer(strategy, realLen, corruptedBlocks);\r\n                if (result >= 0) {\r\n                    pos += result;\r\n                } else {\r\n                    throw new IOException(\"Unexpected EOS from the reader\");\r\n                }\r\n                updateReadStatistics(readStatistics, result, blockReader);\r\n                dfsClient.updateFileSystemReadStats(blockReader.getNetworkDistance(), result);\r\n                if (readStatistics.getBlockType() == BlockType.STRIPED) {\r\n                    dfsClient.updateFileSystemECReadStats(result);\r\n                }\r\n                return result;\r\n            } catch (ChecksumException ce) {\r\n                throw ce;\r\n            } catch (IOException e) {\r\n                checkInterrupted(e);\r\n                if (retries == 1) {\r\n                    DFSClient.LOG.warn(\"DFS Read\", e);\r\n                }\r\n                blockEnd = -1;\r\n                if (currentNode != null) {\r\n                    addToLocalDeadNodes(currentNode);\r\n                    dfsClient.addNodeToDeadNodeDetector(this, currentNode);\r\n                }\r\n                if (--retries == 0) {\r\n                    throw e;\r\n                }\r\n            } finally {\r\n                reportCheckSumFailure(corruptedBlocks, getCurrentBlockLocationsLength(), false);\r\n            }\r\n        }\r\n    }\r\n    return -1;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getCurrentBlockLocationsLength",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getCurrentBlockLocationsLength()\n{\r\n    int len = 0;\r\n    if (currentLocatedBlock == null) {\r\n        DFSClient.LOG.info(\"Found null currentLocatedBlock. pos={}, \" + \"blockEnd={}, fileLength={}\", pos, blockEnd, getFileLength());\r\n    } else {\r\n        len = currentLocatedBlock.getLocations().length;\r\n    }\r\n    return len;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int read(@Nonnull final byte[] buf, int off, int len) throws IOException\n{\r\n    validatePositionedReadArgs(pos, buf, off, len);\r\n    if (len == 0) {\r\n        return 0;\r\n    }\r\n    ReaderStrategy byteArrayReader = new ByteArrayStrategy(buf, off, len, readStatistics, dfsClient);\r\n    return readWithStrategy(byteArrayReader);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int read(final ByteBuffer buf) throws IOException\n{\r\n    ReaderStrategy byteBufferReader = new ByteBufferStrategy(buf, readStatistics, dfsClient);\r\n    return readWithStrategy(byteBufferReader);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "chooseDataNode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DNAddrPair chooseDataNode(LocatedBlock block, Collection<DatanodeInfo> ignoredNodes) throws IOException\n{\r\n    return chooseDataNode(block, ignoredNodes, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "chooseDataNode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DNAddrPair chooseDataNode(LocatedBlock block, Collection<DatanodeInfo> ignoredNodes, boolean refetchIfRequired) throws IOException\n{\r\n    while (true) {\r\n        DNAddrPair result = getBestNodeDNAddrPair(block, ignoredNodes);\r\n        if (result != null) {\r\n            return result;\r\n        } else if (refetchIfRequired) {\r\n            block = refetchLocations(block, ignoredNodes);\r\n        } else {\r\n            return null;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "refetchLocations",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "LocatedBlock refetchLocations(LocatedBlock block, Collection<DatanodeInfo> ignoredNodes) throws IOException\n{\r\n    String errMsg = getBestNodeDNAddrPairErrorString(block.getLocations(), dfsClient.getDeadNodes(this), ignoredNodes);\r\n    String blockInfo = block.getBlock() + \" file=\" + src;\r\n    if (failures >= dfsClient.getConf().getMaxBlockAcquireFailures()) {\r\n        String description = \"Could not obtain block: \" + blockInfo;\r\n        DFSClient.LOG.warn(description + errMsg + \". Throwing a BlockMissingException\");\r\n        throw new BlockMissingException(src, description + errMsg, block.getStartOffset());\r\n    }\r\n    DatanodeInfo[] nodes = block.getLocations();\r\n    if (nodes == null || nodes.length == 0) {\r\n        DFSClient.LOG.info(\"No node available for \" + blockInfo);\r\n    }\r\n    DFSClient.LOG.info(\"Could not obtain \" + block.getBlock() + \" from any node: \" + errMsg + \". Will get new block locations from namenode and retry...\");\r\n    try {\r\n        final int timeWindow = dfsClient.getConf().getTimeWindow();\r\n        double waitTime = timeWindow * failures + timeWindow * (failures + 1) * ThreadLocalRandom.current().nextDouble();\r\n        DFSClient.LOG.warn(\"DFS chooseDataNode: got # \" + (failures + 1) + \" IOException, will wait for \" + waitTime + \" msec.\");\r\n        Thread.sleep((long) waitTime);\r\n    } catch (InterruptedException e) {\r\n        Thread.currentThread().interrupt();\r\n        throw new InterruptedIOException(\"Interrupted while choosing DataNode for read.\");\r\n    }\r\n    clearLocalDeadNodes();\r\n    openInfo(true);\r\n    block = refreshLocatedBlock(block);\r\n    failures++;\r\n    return block;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBestNodeDNAddrPair",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "DNAddrPair getBestNodeDNAddrPair(LocatedBlock block, Collection<DatanodeInfo> ignoredNodes)\n{\r\n    DatanodeInfo[] nodes = block.getLocations();\r\n    StorageType[] storageTypes = block.getStorageTypes();\r\n    DatanodeInfo chosenNode = null;\r\n    StorageType storageType = null;\r\n    if (dfsClient.getConf().isReadUseCachePriority()) {\r\n        DatanodeInfo[] cachedLocs = block.getCachedLocations();\r\n        if (cachedLocs != null) {\r\n            for (int i = 0; i < cachedLocs.length; i++) {\r\n                if (isValidNode(cachedLocs[i], ignoredNodes)) {\r\n                    chosenNode = cachedLocs[i];\r\n                    break;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    if (chosenNode == null && nodes != null) {\r\n        for (int i = 0; i < nodes.length; i++) {\r\n            if (isValidNode(nodes[i], ignoredNodes)) {\r\n                chosenNode = nodes[i];\r\n                if (storageTypes != null && i < storageTypes.length) {\r\n                    storageType = storageTypes[i];\r\n                }\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    if (chosenNode == null) {\r\n        reportLostBlock(block, ignoredNodes);\r\n        return null;\r\n    }\r\n    final String dnAddr = chosenNode.getXferAddr(dfsClient.getConf().isConnectToDnViaHostname());\r\n    DFSClient.LOG.debug(\"Connecting to datanode {}\", dnAddr);\r\n    boolean uriCacheEnabled = dfsClient.getConf().isUriCacheEnabled();\r\n    InetSocketAddress targetAddr = NetUtils.createSocketAddr(dnAddr, -1, null, uriCacheEnabled);\r\n    return new DNAddrPair(chosenNode, targetAddr, storageType, block);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "reportLostBlock",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void reportLostBlock(LocatedBlock lostBlock, Collection<DatanodeInfo> ignoredNodes)\n{\r\n    DatanodeInfo[] nodes = lostBlock.getLocations();\r\n    DFSClient.LOG.warn(\"No live nodes contain block \" + lostBlock.getBlock() + \" after checking nodes = \" + Arrays.toString(nodes) + \", ignoredNodes = \" + ignoredNodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isValidNode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isValidNode(DatanodeInfo node, Collection<DatanodeInfo> ignoredNodes)\n{\r\n    if (!dfsClient.getDeadNodes(this).containsKey(node) && (ignoredNodes == null || !ignoredNodes.contains(node))) {\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBestNodeDNAddrPairErrorString",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String getBestNodeDNAddrPairErrorString(DatanodeInfo[] nodes, AbstractMap<DatanodeInfo, DatanodeInfo> deadNodes, Collection<DatanodeInfo> ignoredNodes)\n{\r\n    StringBuilder errMsgr = new StringBuilder(\" No live nodes contain current block \");\r\n    errMsgr.append(\"Block locations:\");\r\n    for (DatanodeInfo datanode : nodes) {\r\n        errMsgr.append(\" \").append(datanode.toString());\r\n    }\r\n    errMsgr.append(\" Dead nodes: \");\r\n    for (DatanodeInfo datanode : deadNodes.keySet()) {\r\n        errMsgr.append(\" \").append(datanode.toString());\r\n    }\r\n    if (ignoredNodes != null) {\r\n        errMsgr.append(\" Ignored nodes: \");\r\n        for (DatanodeInfo datanode : ignoredNodes) {\r\n            errMsgr.append(\" \").append(datanode.toString());\r\n        }\r\n    }\r\n    return errMsgr.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "fetchBlockByteRange",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void fetchBlockByteRange(LocatedBlock block, long start, long end, ByteBuffer buf, CorruptedBlocks corruptedBlocks) throws IOException\n{\r\n    while (true) {\r\n        DNAddrPair addressPair = chooseDataNode(block, null);\r\n        block = addressPair.block;\r\n        try {\r\n            actualGetFromOneDataNode(addressPair, start, end, buf, corruptedBlocks);\r\n            return;\r\n        } catch (IOException e) {\r\n            checkInterrupted(e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFromOneDataNode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Callable<ByteBuffer> getFromOneDataNode(final DNAddrPair datanode, final LocatedBlock block, final long start, final long end, final ByteBuffer bb, final CorruptedBlocks corruptedBlocks, final int hedgedReadId)\n{\r\n    return new Callable<ByteBuffer>() {\r\n\r\n        @Override\r\n        public ByteBuffer call() throws Exception {\r\n            DFSClientFaultInjector.get().sleepBeforeHedgedGet();\r\n            actualGetFromOneDataNode(datanode, start, end, bb, corruptedBlocks);\r\n            return bb;\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "actualGetFromOneDataNode",
  "errType" : [ "ChecksumException", "IOException", "IOException" ],
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void actualGetFromOneDataNode(final DNAddrPair datanode, final long startInBlk, final long endInBlk, ByteBuffer buf, CorruptedBlocks corruptedBlocks) throws IOException\n{\r\n    DFSClientFaultInjector.get().startFetchFromDatanode();\r\n    int refetchToken = 1;\r\n    int refetchEncryptionKey = 1;\r\n    final int len = (int) (endInBlk - startInBlk + 1);\r\n    LocatedBlock block = datanode.block;\r\n    while (true) {\r\n        BlockReader reader = null;\r\n        try {\r\n            DFSClientFaultInjector.get().fetchFromDatanodeException();\r\n            reader = getBlockReader(block, startInBlk, len, datanode.addr, datanode.storageType, datanode.info);\r\n            ByteBuffer tmp = buf.duplicate();\r\n            tmp.limit(tmp.position() + len);\r\n            tmp = tmp.slice();\r\n            int nread = 0;\r\n            int ret;\r\n            while (true) {\r\n                ret = reader.read(tmp);\r\n                if (ret <= 0) {\r\n                    break;\r\n                }\r\n                nread += ret;\r\n            }\r\n            buf.position(buf.position() + nread);\r\n            IOUtilsClient.updateReadStatistics(readStatistics, nread, reader);\r\n            dfsClient.updateFileSystemReadStats(reader.getNetworkDistance(), nread);\r\n            if (readStatistics.getBlockType() == BlockType.STRIPED) {\r\n                dfsClient.updateFileSystemECReadStats(nread);\r\n            }\r\n            if (nread != len) {\r\n                throw new IOException(\"truncated return from reader.read(): \" + \"excpected \" + len + \", got \" + nread);\r\n            }\r\n            DFSClientFaultInjector.get().readFromDatanodeDelay();\r\n            return;\r\n        } catch (ChecksumException e) {\r\n            String msg = \"fetchBlockByteRange(). Got a checksum exception for \" + src + \" at \" + block.getBlock() + \":\" + e.getPos() + \" from \" + datanode.info;\r\n            DFSClient.LOG.warn(msg);\r\n            corruptedBlocks.addCorruptedBlock(block.getBlock(), datanode.info);\r\n            addToLocalDeadNodes(datanode.info);\r\n            throw new IOException(msg);\r\n        } catch (IOException e) {\r\n            checkInterrupted(e);\r\n            if (e instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0) {\r\n                DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" + \"encryption key was invalid when connecting to \" + datanode.addr + \" : \" + e);\r\n                refetchEncryptionKey--;\r\n                dfsClient.clearDataEncryptionKey();\r\n            } else if (refetchToken > 0 && tokenRefetchNeeded(e, datanode.addr)) {\r\n                refetchToken--;\r\n                try {\r\n                    fetchBlockAt(block.getStartOffset());\r\n                } catch (IOException fbae) {\r\n                }\r\n            } else {\r\n                String msg = \"Failed to connect to \" + datanode.addr + \" for file \" + src + \" for block \" + block.getBlock() + \":\" + e;\r\n                DFSClient.LOG.warn(\"Connection failure: \" + msg, e);\r\n                addToLocalDeadNodes(datanode.info);\r\n                dfsClient.addNodeToDeadNodeDetector(this, datanode.info);\r\n                throw new IOException(msg);\r\n            }\r\n            block = refreshLocatedBlock(block);\r\n        } finally {\r\n            if (reader != null) {\r\n                reader.close();\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "refreshLocatedBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocatedBlock refreshLocatedBlock(LocatedBlock block) throws IOException\n{\r\n    return getBlockAt(block.getStartOffset());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "hedgedFetchBlockByteRange",
  "errType" : [ "IOException", "InterruptedException", "ExecutionException", "InterruptedException" ],
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void hedgedFetchBlockByteRange(LocatedBlock block, long start, long end, ByteBuffer buf, CorruptedBlocks corruptedBlocks) throws IOException\n{\r\n    final DfsClientConf conf = dfsClient.getConf();\r\n    ArrayList<Future<ByteBuffer>> futures = new ArrayList<>();\r\n    CompletionService<ByteBuffer> hedgedService = new ExecutorCompletionService<>(dfsClient.getHedgedReadsThreadPool());\r\n    ArrayList<DatanodeInfo> ignored = new ArrayList<>();\r\n    ByteBuffer bb;\r\n    int len = (int) (end - start + 1);\r\n    int hedgedReadId = 0;\r\n    while (true) {\r\n        hedgedReadOpsLoopNumForTesting++;\r\n        DNAddrPair chosenNode = null;\r\n        if (futures.isEmpty()) {\r\n            chosenNode = chooseDataNode(block, ignored);\r\n            block = chosenNode.block;\r\n            bb = ByteBuffer.allocate(len);\r\n            Callable<ByteBuffer> getFromDataNodeCallable = getFromOneDataNode(chosenNode, block, start, end, bb, corruptedBlocks, hedgedReadId++);\r\n            Future<ByteBuffer> firstRequest = hedgedService.submit(getFromDataNodeCallable);\r\n            futures.add(firstRequest);\r\n            Future<ByteBuffer> future = null;\r\n            try {\r\n                future = hedgedService.poll(conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\r\n                if (future != null) {\r\n                    ByteBuffer result = future.get();\r\n                    result.flip();\r\n                    buf.put(result);\r\n                    return;\r\n                }\r\n                DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \" + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\r\n                dfsClient.getHedgedReadMetrics().incHedgedReadOps();\r\n            } catch (ExecutionException e) {\r\n                futures.remove(future);\r\n            } catch (InterruptedException e) {\r\n                throw new InterruptedIOException(\"Interrupted while waiting for reading task\");\r\n            }\r\n            ignored.add(chosenNode.info);\r\n        } else {\r\n            boolean refetch = false;\r\n            try {\r\n                chosenNode = chooseDataNode(block, ignored, false);\r\n                if (chosenNode != null) {\r\n                    block = chosenNode.block;\r\n                    bb = ByteBuffer.allocate(len);\r\n                    Callable<ByteBuffer> getFromDataNodeCallable = getFromOneDataNode(chosenNode, block, start, end, bb, corruptedBlocks, hedgedReadId++);\r\n                    Future<ByteBuffer> oneMoreRequest = hedgedService.submit(getFromDataNodeCallable);\r\n                    futures.add(oneMoreRequest);\r\n                } else {\r\n                    refetch = true;\r\n                }\r\n            } catch (IOException ioe) {\r\n                DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\", ioe.getMessage());\r\n            }\r\n            try {\r\n                ByteBuffer result = getFirstToComplete(hedgedService, futures);\r\n                cancelAll(futures);\r\n                dfsClient.getHedgedReadMetrics().incHedgedReadWins();\r\n                result.flip();\r\n                buf.put(result);\r\n                return;\r\n            } catch (InterruptedException ie) {\r\n            }\r\n            if (refetch) {\r\n                refetchLocations(block, ignored);\r\n            }\r\n            if (chosenNode != null && chosenNode.info != null) {\r\n                ignored.add(chosenNode.info);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getHedgedReadOpsLoopNumForTesting",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getHedgedReadOpsLoopNumForTesting()\n{\r\n    return hedgedReadOpsLoopNumForTesting;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFirstToComplete",
  "errType" : [ "ExecutionException|CancellationException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ByteBuffer getFirstToComplete(CompletionService<ByteBuffer> hedgedService, ArrayList<Future<ByteBuffer>> futures) throws InterruptedException\n{\r\n    if (futures.isEmpty()) {\r\n        throw new InterruptedException(\"let's retry\");\r\n    }\r\n    Future<ByteBuffer> future = null;\r\n    try {\r\n        future = hedgedService.take();\r\n        ByteBuffer bb = future.get();\r\n        futures.remove(future);\r\n        return bb;\r\n    } catch (ExecutionException | CancellationException e) {\r\n        futures.remove(future);\r\n    }\r\n    throw new InterruptedException(\"let's retry\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "cancelAll",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cancelAll(List<Future<ByteBuffer>> futures)\n{\r\n    for (Future<ByteBuffer> future : futures) {\r\n        future.cancel(false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "tokenRefetchNeeded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean tokenRefetchNeeded(IOException ex, InetSocketAddress targetAddr)\n{\r\n    if (ex instanceof InvalidBlockTokenException || ex instanceof InvalidToken) {\r\n        DFSClient.LOG.debug(\"Access token was invalid when connecting to {}: {}\", targetAddr, ex);\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int read(long position, byte[] buffer, int offset, int length) throws IOException\n{\r\n    validatePositionedReadArgs(position, buffer, offset, length);\r\n    if (length == 0) {\r\n        return 0;\r\n    }\r\n    ByteBuffer bb = ByteBuffer.wrap(buffer, offset, length);\r\n    return pread(position, bb);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "pread",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "int pread(long position, ByteBuffer buffer) throws IOException\n{\r\n    dfsClient.checkOpen();\r\n    if (closed.get()) {\r\n        throw new IOException(\"Stream closed\");\r\n    }\r\n    failures = 0;\r\n    long filelen = getFileLength();\r\n    if ((position < 0) || (position >= filelen)) {\r\n        return -1;\r\n    }\r\n    int length = buffer.remaining();\r\n    int realLen = length;\r\n    if ((position + length) > filelen) {\r\n        realLen = (int) (filelen - position);\r\n    }\r\n    List<LocatedBlock> blockRange = getBlockRange(position, realLen);\r\n    int remaining = realLen;\r\n    CorruptedBlocks corruptedBlocks = new CorruptedBlocks();\r\n    for (LocatedBlock blk : blockRange) {\r\n        long targetStart = position - blk.getStartOffset();\r\n        int bytesToRead = (int) Math.min(remaining, blk.getBlockSize() - targetStart);\r\n        long targetEnd = targetStart + bytesToRead - 1;\r\n        try {\r\n            if (dfsClient.isHedgedReadsEnabled() && !blk.isStriped()) {\r\n                hedgedFetchBlockByteRange(blk, targetStart, targetEnd, buffer, corruptedBlocks);\r\n            } else {\r\n                fetchBlockByteRange(blk, targetStart, targetEnd, buffer, corruptedBlocks);\r\n            }\r\n        } finally {\r\n            reportCheckSumFailure(corruptedBlocks, blk.getLocations().length, false);\r\n        }\r\n        remaining -= bytesToRead;\r\n        position += bytesToRead;\r\n    }\r\n    assert remaining == 0 : \"Wrong number of bytes read.\";\r\n    return realLen;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "reportCheckSumFailure",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void reportCheckSumFailure(CorruptedBlocks corruptedBlocks, int dataNodeCount, boolean isStriped)\n{\r\n    Map<ExtendedBlock, Set<DatanodeInfo>> corruptedBlockMap = corruptedBlocks.getCorruptionMap();\r\n    if (corruptedBlockMap == null) {\r\n        return;\r\n    }\r\n    List<LocatedBlock> reportList = new ArrayList<>(corruptedBlockMap.size());\r\n    for (Map.Entry<ExtendedBlock, Set<DatanodeInfo>> entry : corruptedBlockMap.entrySet()) {\r\n        ExtendedBlock blk = entry.getKey();\r\n        Set<DatanodeInfo> dnSet = entry.getValue();\r\n        if (isStriped || ((dnSet.size() < dataNodeCount) && (dnSet.size() > 0)) || ((dataNodeCount == 1) && (dnSet.size() == dataNodeCount))) {\r\n            DatanodeInfo[] locs = new DatanodeInfo[dnSet.size()];\r\n            int i = 0;\r\n            for (DatanodeInfo dn : dnSet) {\r\n                locs[i++] = dn;\r\n            }\r\n            reportList.add(new LocatedBlock(blk, locs));\r\n        }\r\n    }\r\n    if (reportList.size() > 0) {\r\n        dfsClient.reportChecksumFailure(src, reportList.toArray(new LocatedBlock[reportList.size()]));\r\n    }\r\n    corruptedBlockMap.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "skip",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long skip(long n) throws IOException\n{\r\n    if (n > 0) {\r\n        long curPos = getPos();\r\n        long fileLen = getFileLength();\r\n        if (n + curPos > fileLen) {\r\n            n = fileLen - curPos;\r\n        }\r\n        seek(curPos + n);\r\n        return n;\r\n    }\r\n    return n < 0 ? -1 : 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "seek",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void seek(long targetPos) throws IOException\n{\r\n    if (targetPos > getFileLength()) {\r\n        throw new EOFException(\"Cannot seek after EOF\");\r\n    }\r\n    if (targetPos < 0) {\r\n        throw new EOFException(\"Cannot seek to negative offset\");\r\n    }\r\n    if (closed.get()) {\r\n        throw new IOException(\"Stream is closed!\");\r\n    }\r\n    boolean done = false;\r\n    if (pos <= targetPos && targetPos <= blockEnd) {\r\n        int diff = (int) (targetPos - pos);\r\n        if (diff <= blockReader.available()) {\r\n            try {\r\n                pos += blockReader.skip(diff);\r\n                if (pos == targetPos) {\r\n                    done = true;\r\n                } else {\r\n                    String errMsg = \"BlockReader failed to seek to \" + targetPos + \". Instead, it seeked to \" + pos + \".\";\r\n                    DFSClient.LOG.warn(errMsg);\r\n                    throw new IOException(errMsg);\r\n                }\r\n            } catch (IOException e) {\r\n                DFSClient.LOG.debug(\"Exception while seek to {} from {} of {} from \" + \"{}\", targetPos, getCurrentBlock(), src, currentNode, e);\r\n                checkInterrupted(e);\r\n            }\r\n        }\r\n    }\r\n    if (!done) {\r\n        pos = targetPos;\r\n        blockEnd = -1;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "seekToBlockSource",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean seekToBlockSource(long targetPos) throws IOException\n{\r\n    currentNode = blockSeekTo(targetPos);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "seekToNewSource",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean seekToNewSource(long targetPos) throws IOException\n{\r\n    if (currentNode == null) {\r\n        return seekToBlockSource(targetPos);\r\n    }\r\n    boolean markedDead = dfsClient.isDeadNode(this, currentNode);\r\n    addToLocalDeadNodes(currentNode);\r\n    DatanodeInfo oldNode = currentNode;\r\n    DatanodeInfo newNode = blockSeekTo(targetPos);\r\n    if (!markedDead) {\r\n        removeFromLocalDeadNodes(oldNode);\r\n    }\r\n    if (!oldNode.getDatanodeUuid().equals(newNode.getDatanodeUuid())) {\r\n        currentNode = newNode;\r\n        return true;\r\n    } else {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPos()\n{\r\n    return pos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "available",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int available() throws IOException\n{\r\n    if (closed.get()) {\r\n        throw new IOException(\"Stream closed\");\r\n    }\r\n    final long remaining = getFileLength() - pos;\r\n    return remaining <= Integer.MAX_VALUE ? (int) remaining : Integer.MAX_VALUE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "markSupported",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean markSupported()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "mark",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void mark(int readLimit)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reset() throws IOException\n{\r\n    throw new IOException(\"Mark/reset not supported\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int read(long position, final ByteBuffer buf) throws IOException\n{\r\n    if (!buf.hasRemaining()) {\r\n        return 0;\r\n    }\r\n    return pread(position, buf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readFully",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readFully(long position, final ByteBuffer buf) throws IOException\n{\r\n    int nread = 0;\r\n    while (buf.hasRemaining()) {\r\n        int nbytes = read(position + nread, buf);\r\n        if (nbytes < 0) {\r\n            throw new EOFException(FSExceptionMessages.EOF_IN_READ_FULLY);\r\n        }\r\n        nread += nbytes;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getReadStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReadStatistics getReadStatistics()\n{\r\n    return readStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "clearReadStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearReadStatistics()\n{\r\n    readStatistics.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileEncryptionInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileEncryptionInfo getFileEncryptionInfo()\n{\r\n    synchronized (infoLock) {\r\n        return fileEncryptionInfo;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "closeCurrentBlockReaders",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void closeCurrentBlockReaders()\n{\r\n    if (blockReader == null)\r\n        return;\r\n    try {\r\n        blockReader.close();\r\n    } catch (IOException e) {\r\n        DFSClient.LOG.error(\"error closing blockReader\", e);\r\n    }\r\n    blockReader = null;\r\n    blockEnd = -1;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setReadahead",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setReadahead(Long readahead) throws IOException\n{\r\n    synchronized (infoLock) {\r\n        this.cachingStrategy = new CachingStrategy.Builder(this.cachingStrategy).setReadahead(readahead).build();\r\n    }\r\n    closeCurrentBlockReaders();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setDropBehind",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setDropBehind(Boolean dropBehind) throws IOException\n{\r\n    synchronized (infoLock) {\r\n        this.cachingStrategy = new CachingStrategy.Builder(this.cachingStrategy).setDropBehind(dropBehind).build();\r\n    }\r\n    closeCurrentBlockReaders();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ByteBuffer read(ByteBufferPool bufferPool, int maxLength, EnumSet<ReadOption> opts) throws IOException, UnsupportedOperationException\n{\r\n    if (maxLength == 0) {\r\n        return EMPTY_BUFFER;\r\n    } else if (maxLength < 0) {\r\n        throw new IllegalArgumentException(\"can't read a negative \" + \"number of bytes.\");\r\n    }\r\n    if ((blockReader == null) || (blockEnd == -1)) {\r\n        if (pos >= getFileLength()) {\r\n            return null;\r\n        }\r\n        if ((!seekToBlockSource(pos)) || (blockReader == null)) {\r\n            throw new IOException(\"failed to allocate new BlockReader \" + \"at position \" + pos);\r\n        }\r\n    }\r\n    ByteBuffer buffer = null;\r\n    if (dfsClient.getConf().getShortCircuitConf().isShortCircuitMmapEnabled()) {\r\n        buffer = tryReadZeroCopy(maxLength, opts);\r\n    }\r\n    if (buffer != null) {\r\n        return buffer;\r\n    }\r\n    buffer = ByteBufferUtil.fallbackRead(this, bufferPool, maxLength);\r\n    if (buffer != null) {\r\n        getExtendedReadBuffers().put(buffer, bufferPool);\r\n    }\r\n    return buffer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "tryReadZeroCopy",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "ByteBuffer tryReadZeroCopy(int maxLength, EnumSet<ReadOption> opts) throws IOException\n{\r\n    final long curPos = pos;\r\n    final long curEnd = blockEnd;\r\n    final long blockStartInFile = currentLocatedBlock.getStartOffset();\r\n    final long blockPos = curPos - blockStartInFile;\r\n    long length63;\r\n    if ((curPos + maxLength) <= (curEnd + 1)) {\r\n        length63 = maxLength;\r\n    } else {\r\n        length63 = 1 + curEnd - curPos;\r\n        if (length63 <= 0) {\r\n            DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {}\" + \" of {}; {} bytes left in block. blockPos={}; curPos={};\" + \"curEnd={}\", curPos, src, length63, blockPos, curPos, curEnd);\r\n            return null;\r\n        }\r\n        DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid going \" + \"more than one byte past the end of the block.  blockPos={}; \" + \" curPos={}; curEnd={}\", maxLength, length63, blockPos, curPos, curEnd);\r\n    }\r\n    int length;\r\n    if (blockPos + length63 <= Integer.MAX_VALUE) {\r\n        length = (int) length63;\r\n    } else {\r\n        long length31 = Integer.MAX_VALUE - blockPos;\r\n        if (length31 <= 0) {\r\n            DFSClient.LOG.debug(\"Unable to perform a zero-copy read from offset {} \" + \" of {}; 31-bit MappedByteBuffer limit exceeded.  blockPos={}, \" + \"curEnd={}\", curPos, src, blockPos, curEnd);\r\n            return null;\r\n        }\r\n        length = (int) length31;\r\n        DFSClient.LOG.debug(\"Reducing read length from {} to {} to avoid 31-bit \" + \"limit.  blockPos={}; curPos={}; curEnd={}\", maxLength, length, blockPos, curPos, curEnd);\r\n    }\r\n    final ClientMmap clientMmap = blockReader.getClientMmap(opts);\r\n    if (clientMmap == null) {\r\n        DFSClient.LOG.debug(\"unable to perform a zero-copy read from offset {} of\" + \" {}; BlockReader#getClientMmap returned null.\", curPos, src);\r\n        return null;\r\n    }\r\n    boolean success = false;\r\n    ByteBuffer buffer;\r\n    try {\r\n        seek(curPos + length);\r\n        buffer = clientMmap.getMappedByteBuffer().asReadOnlyBuffer();\r\n        buffer.position((int) blockPos);\r\n        buffer.limit((int) (blockPos + length));\r\n        getExtendedReadBuffers().put(buffer, clientMmap);\r\n        readStatistics.addZeroCopyBytes(length);\r\n        DFSClient.LOG.debug(\"readZeroCopy read {} bytes from offset {} via the \" + \"zero-copy read path.  blockEnd = {}\", length, curPos, blockEnd);\r\n        success = true;\r\n    } finally {\r\n        if (!success) {\r\n            IOUtils.closeStream(clientMmap);\r\n        }\r\n    }\r\n    return buffer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "releaseBuffer",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void releaseBuffer(ByteBuffer buffer)\n{\r\n    if (buffer == EMPTY_BUFFER)\r\n        return;\r\n    Object val = getExtendedReadBuffers().remove(buffer);\r\n    if (val == null) {\r\n        throw new IllegalArgumentException(\"tried to release a buffer \" + \"that was not created by this stream, \" + buffer);\r\n    }\r\n    if (val instanceof ClientMmap) {\r\n        IOUtils.closeStream((ClientMmap) val);\r\n    } else if (val instanceof ByteBufferPool) {\r\n        ((ByteBufferPool) val).putBuffer(buffer);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "unbuffer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void unbuffer()\n{\r\n    closeCurrentBlockReaders();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "hasCapability",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasCapability(String capability)\n{\r\n    switch(StringUtils.toLowerCase(capability)) {\r\n        case StreamCapabilities.READAHEAD:\r\n        case StreamCapabilities.DROPBEHIND:\r\n        case StreamCapabilities.UNBUFFER:\r\n        case StreamCapabilities.READBYTEBUFFER:\r\n        case StreamCapabilities.PREADBYTEBUFFER:\r\n            return true;\r\n        default:\r\n            return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "maybeRegisterBlockRefresh",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void maybeRegisterBlockRefresh()\n{\r\n    if (!dfsClient.getConf().isRefreshReadBlockLocationsAutomatically() || !dfsClient.getConf().isLocatedBlocksRefresherEnabled()) {\r\n        return;\r\n    }\r\n    if (refreshingBlockLocations.get()) {\r\n        return;\r\n    }\r\n    long timeSinceLastRefresh = Time.monotonicNow() - lastRefreshedBlocksAt;\r\n    if (timeSinceLastRefresh < dfsClient.getConf().getLocatedBlocksRefresherInterval()) {\r\n        return;\r\n    }\r\n    if (!refreshingBlockLocations.getAndSet(true)) {\r\n        dfsClient.addLocatedBlocksRefresh(this);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "maybeDeRegisterBlockRefresh",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void maybeDeRegisterBlockRefresh()\n{\r\n    if (refreshingBlockLocations.get()) {\r\n        dfsClient.removeLocatedBlocksRefresh(this);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "refreshBlockLocations",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "boolean refreshBlockLocations(Map<String, InetSocketAddress> addressCache)\n{\r\n    LocatedBlocks blocks;\r\n    synchronized (infoLock) {\r\n        blocks = getLocatedBlocks();\r\n    }\r\n    if (getLocalDeadNodes().isEmpty() && allBlocksLocal(blocks, addressCache)) {\r\n        return false;\r\n    }\r\n    try {\r\n        DFSClient.LOG.debug(\"Refreshing {} for path {}\", this, getSrc());\r\n        LocatedBlocks newLocatedBlocks = fetchAndCheckLocatedBlocks(blocks);\r\n        long lastBlockLength = getLastBlockLength(newLocatedBlocks);\r\n        if (lastBlockLength == -1) {\r\n            DFSClient.LOG.debug(\"Discarding refreshed blocks for path {} because lastBlockLength was -1\", getSrc());\r\n            return true;\r\n        }\r\n        setRefreshedValues(newLocatedBlocks, lastBlockLength);\r\n    } catch (IOException e) {\r\n        DFSClient.LOG.debug(\"Failed to refresh DFSInputStream for path {}\", getSrc(), e);\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setRefreshedValues",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setRefreshedValues(LocatedBlocks blocks, long lastBlockLength) throws IOException\n{\r\n    synchronized (infoLock) {\r\n        setLocatedBlocksFields(blocks, lastBlockLength);\r\n    }\r\n    getLocalDeadNodes().clear();\r\n    if (currentNode != null) {\r\n        currentNode = blockSeekTo(pos);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "allBlocksLocal",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean allBlocksLocal(LocatedBlocks blocks, Map<String, InetSocketAddress> addressCache)\n{\r\n    if (addressCache == null) {\r\n        addressCache = new HashMap<>();\r\n    }\r\n    for (LocatedBlock lb : blocks.getLocatedBlocks()) {\r\n        if (lb.getLocations().length == 0) {\r\n            return false;\r\n        }\r\n        DatanodeInfoWithStorage location = lb.getLocations()[0];\r\n        if (location == null) {\r\n            return false;\r\n        }\r\n        InetSocketAddress targetAddr = addressCache.computeIfAbsent(location.getDatanodeUuid(), unused -> {\r\n            String dnAddr = location.getXferAddr(dfsClient.getConf().isConnectToDnViaHostname());\r\n            return NetUtils.createSocketAddr(dnAddr, -1, null, dfsClient.getConf().isUriCacheEnabled());\r\n        });\r\n        if (!isResolveableAndLocal(targetAddr)) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isResolveableAndLocal",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isResolveableAndLocal(InetSocketAddress targetAddr)\n{\r\n    try {\r\n        return DFSUtilClient.isLocalAddress(targetAddr);\r\n    } catch (IOException e) {\r\n        DFSClient.LOG.debug(\"Got an error checking if {} is local\", targetAddr, e);\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setLastRefreshedBlocksAtForTesting",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLastRefreshedBlocksAtForTesting(long timestamp)\n{\r\n    lastRefreshedBlocksAt = timestamp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLastRefreshedBlocksAtForTesting",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLastRefreshedBlocksAtForTesting()\n{\r\n    return lastRefreshedBlocksAt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setLastRefreshedBlocksAt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setLastRefreshedBlocksAt()\n{\r\n    lastRefreshedBlocksAt = Time.monotonicNow();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "createWebHdfsFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "WebHdfsFileSystem createWebHdfsFileSystem(Configuration conf)\n{\r\n    WebHdfsFileSystem fs = new WebHdfsFileSystem();\r\n    fs.setConf(conf);\r\n    return fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getStorageID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStorageID()\n{\r\n    return storageID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getStorageType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StorageType getStorageType()\n{\r\n    return storageType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    return super.equals(o);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return super.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"DatanodeInfoWithStorage[\" + super.toString() + \",\" + storageID + \",\" + storageType + \"]\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "createClientDatanodeProtocolProxy",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "ClientDatanodeProtocolPB createClientDatanodeProtocolProxy(DatanodeID datanodeid, Configuration conf, int socketTimeout, boolean connectToDnViaHostname, LocatedBlock locatedBlock) throws IOException\n{\r\n    final String dnAddr = datanodeid.getIpcAddr(connectToDnViaHostname);\r\n    InetSocketAddress addr = NetUtils.createSocketAddr(dnAddr);\r\n    LOG.debug(\"Connecting to datanode {} addr={}\", dnAddr, addr);\r\n    Configuration confWithNoIpcIdle = new Configuration(conf);\r\n    confWithNoIpcIdle.setInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECTION_MAXIDLETIME_KEY, 0);\r\n    UserGroupInformation ticket = UserGroupInformation.createRemoteUser(locatedBlock.getBlock().getLocalBlock().toString());\r\n    ticket.addToken(locatedBlock.getBlockToken());\r\n    return createClientDatanodeProtocolProxy(addr, ticket, confWithNoIpcIdle, NetUtils.getDefaultSocketFactory(conf), socketTimeout);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "createClientDatanodeProtocolProxy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ClientDatanodeProtocolPB createClientDatanodeProtocolProxy(InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int socketTimeout) throws IOException\n{\r\n    RPC.setProtocolEngine(conf, ClientDatanodeProtocolPB.class, ProtobufRpcEngine2.class);\r\n    return RPC.getProxy(ClientDatanodeProtocolPB.class, RPC.getProtocolVersion(ClientDatanodeProtocolPB.class), addr, ticket, conf, factory, socketTimeout);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close()\n{\r\n    RPC.stopProxy(rpcProxy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getReplicaVisibleLength",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getReplicaVisibleLength(ExtendedBlock b) throws IOException\n{\r\n    GetReplicaVisibleLengthRequestProto req = GetReplicaVisibleLengthRequestProto.newBuilder().setBlock(PBHelperClient.convert(b)).build();\r\n    try {\r\n        return rpcProxy.getReplicaVisibleLength(NULL_CONTROLLER, req).getLength();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "refreshNamenodes",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void refreshNamenodes() throws IOException\n{\r\n    try {\r\n        rpcProxy.refreshNamenodes(NULL_CONTROLLER, VOID_REFRESH_NAMENODES);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "deleteBlockPool",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void deleteBlockPool(String bpid, boolean force) throws IOException\n{\r\n    DeleteBlockPoolRequestProto req = DeleteBlockPoolRequestProto.newBuilder().setBlockPool(bpid).setForce(force).build();\r\n    try {\r\n        rpcProxy.deleteBlockPool(NULL_CONTROLLER, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getBlockLocalPathInfo",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "BlockLocalPathInfo getBlockLocalPathInfo(ExtendedBlock block, Token<BlockTokenIdentifier> token) throws IOException\n{\r\n    GetBlockLocalPathInfoRequestProto req = GetBlockLocalPathInfoRequestProto.newBuilder().setBlock(PBHelperClient.convert(block)).setToken(PBHelperClient.convert(token)).build();\r\n    GetBlockLocalPathInfoResponseProto resp;\r\n    try {\r\n        resp = rpcProxy.getBlockLocalPathInfo(NULL_CONTROLLER, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n    return new BlockLocalPathInfo(PBHelperClient.convert(resp.getBlock()), resp.getLocalPath(), resp.getLocalMetaPath());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "isMethodSupported",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isMethodSupported(String methodName) throws IOException\n{\r\n    return RpcClientUtil.isMethodSupported(rpcProxy, ClientDatanodeProtocolPB.class, RPC.RpcKind.RPC_PROTOCOL_BUFFER, RPC.getProtocolVersion(ClientDatanodeProtocolPB.class), methodName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getUnderlyingProxyObject",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getUnderlyingProxyObject()\n{\r\n    return rpcProxy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "shutdownDatanode",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void shutdownDatanode(boolean forUpgrade) throws IOException\n{\r\n    ShutdownDatanodeRequestProto request = ShutdownDatanodeRequestProto.newBuilder().setForUpgrade(forUpgrade).build();\r\n    try {\r\n        rpcProxy.shutdownDatanode(NULL_CONTROLLER, request);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "evictWriters",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void evictWriters() throws IOException\n{\r\n    try {\r\n        rpcProxy.evictWriters(NULL_CONTROLLER, VOID_EVICT_WRITERS);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getDatanodeInfo",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DatanodeLocalInfo getDatanodeInfo() throws IOException\n{\r\n    GetDatanodeInfoResponseProto response;\r\n    try {\r\n        response = rpcProxy.getDatanodeInfo(NULL_CONTROLLER, VOID_GET_DATANODE_INFO);\r\n        return PBHelperClient.convert(response.getLocalInfo());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "startReconfiguration",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void startReconfiguration() throws IOException\n{\r\n    try {\r\n        rpcProxy.startReconfiguration(NULL_CONTROLLER, VOID_START_RECONFIG);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getReconfigurationStatus",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ReconfigurationTaskStatus getReconfigurationStatus() throws IOException\n{\r\n    try {\r\n        return ReconfigurationProtocolUtils.getReconfigurationStatus(rpcProxy.getReconfigurationStatus(NULL_CONTROLLER, VOID_GET_RECONFIG_STATUS));\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "listReconfigurableProperties",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<String> listReconfigurableProperties() throws IOException\n{\r\n    ListReconfigurablePropertiesResponseProto response;\r\n    try {\r\n        response = rpcProxy.listReconfigurableProperties(NULL_CONTROLLER, VOID_LIST_RECONFIGURABLE_PROPERTIES);\r\n        return response.getNameList();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "triggerBlockReport",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void triggerBlockReport(BlockReportOptions options) throws IOException\n{\r\n    try {\r\n        TriggerBlockReportRequestProto.Builder builder = TriggerBlockReportRequestProto.newBuilder().setIncremental(options.isIncremental());\r\n        if (options.getNamenodeAddr() != null) {\r\n            builder.setNnAddress(NetUtils.getHostPortString(options.getNamenodeAddr()));\r\n        }\r\n        rpcProxy.triggerBlockReport(NULL_CONTROLLER, builder.build());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getBalancerBandwidth",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getBalancerBandwidth() throws IOException\n{\r\n    GetBalancerBandwidthResponseProto response;\r\n    try {\r\n        response = rpcProxy.getBalancerBandwidth(NULL_CONTROLLER, VOID_GET_BALANCER_BANDWIDTH);\r\n        return response.getBandwidth();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "submitDiskBalancerPlan",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void submitDiskBalancerPlan(String planID, long planVersion, String planFile, String planData, boolean skipDateCheck) throws IOException\n{\r\n    try {\r\n        SubmitDiskBalancerPlanRequestProto request = SubmitDiskBalancerPlanRequestProto.newBuilder().setPlanID(planID).setPlanVersion(planVersion).setPlanFile(planFile).setPlan(planData).setIgnoreDateCheck(skipDateCheck).build();\r\n        rpcProxy.submitDiskBalancerPlan(NULL_CONTROLLER, request);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "cancelDiskBalancePlan",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cancelDiskBalancePlan(String planID) throws IOException\n{\r\n    try {\r\n        CancelPlanRequestProto request = CancelPlanRequestProto.newBuilder().setPlanID(planID).build();\r\n        rpcProxy.cancelDiskBalancerPlan(NULL_CONTROLLER, request);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "queryDiskBalancerPlan",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "DiskBalancerWorkStatus queryDiskBalancerPlan() throws IOException\n{\r\n    try {\r\n        QueryPlanStatusRequestProto request = QueryPlanStatusRequestProto.newBuilder().build();\r\n        QueryPlanStatusResponseProto response = rpcProxy.queryDiskBalancerPlan(NULL_CONTROLLER, request);\r\n        DiskBalancerWorkStatus.Result result = Result.NO_PLAN;\r\n        if (response.hasResult()) {\r\n            result = DiskBalancerWorkStatus.Result.values()[response.getResult()];\r\n        }\r\n        return new DiskBalancerWorkStatus(result, response.hasPlanID() ? response.getPlanID() : null, response.hasPlanFile() ? response.getPlanFile() : null, response.hasCurrentStatus() ? response.getCurrentStatus() : null);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getDiskBalancerSetting",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getDiskBalancerSetting(String key) throws IOException\n{\r\n    try {\r\n        DiskBalancerSettingRequestProto request = DiskBalancerSettingRequestProto.newBuilder().setKey(key).build();\r\n        DiskBalancerSettingResponseProto response = rpcProxy.getDiskBalancerSetting(NULL_CONTROLLER, request);\r\n        return response.hasValue() ? response.getValue() : null;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getVolumeReport",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<DatanodeVolumeInfo> getVolumeReport() throws IOException\n{\r\n    try {\r\n        List<DatanodeVolumeInfo> volumeInfoList = new ArrayList<>();\r\n        GetVolumeReportResponseProto volumeReport = rpcProxy.getVolumeReport(NULL_CONTROLLER, VOID_GET_DATANODE_STORAGE_INFO);\r\n        List<DatanodeVolumeInfoProto> volumeProtoList = volumeReport.getVolumeInfoList();\r\n        for (DatanodeVolumeInfoProto proto : volumeProtoList) {\r\n            volumeInfoList.add(new DatanodeVolumeInfo(proto.getPath(), proto.getUsedSpace(), proto.getFreeSpace(), proto.getReservedSpace(), proto.getReservedSpaceForReplicas(), proto.getNumBlocks(), PBHelperClient.convertStorageType(proto.getStorageType())));\r\n        }\r\n        return volumeInfoList;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getPathInfo",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "PathInfo getPathInfo(InetSocketAddress addr, ShortCircuitConf conf) throws IOException\n{\r\n    if (conf.getDomainSocketPath().isEmpty())\r\n        return PathInfo.NOT_CONFIGURED;\r\n    if (!conf.isDomainSocketDataTraffic() && (!conf.isShortCircuitLocalReads() || conf.isUseLegacyBlockReaderLocal())) {\r\n        return PathInfo.NOT_CONFIGURED;\r\n    }\r\n    if (DomainSocket.getLoadingFailureReason() != null) {\r\n        return PathInfo.NOT_CONFIGURED;\r\n    }\r\n    if (!DFSUtilClient.isLocalAddress(addr))\r\n        return PathInfo.NOT_CONFIGURED;\r\n    String escapedPath = DomainSocket.getEffectivePath(conf.getDomainSocketPath(), addr.getPort());\r\n    PathState status = pathMap.getIfPresent(escapedPath);\r\n    if (status == null) {\r\n        return new PathInfo(escapedPath, PathState.VALID);\r\n    } else {\r\n        return new PathInfo(escapedPath, status);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "createSocket",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "DomainSocket createSocket(PathInfo info, int socketTimeout)\n{\r\n    Preconditions.checkArgument(info.getPathState() != PathState.UNUSABLE);\r\n    boolean success = false;\r\n    DomainSocket sock = null;\r\n    try {\r\n        sock = DomainSocket.connect(info.getPath());\r\n        sock.setAttribute(DomainSocket.RECEIVE_TIMEOUT, socketTimeout);\r\n        success = true;\r\n    } catch (IOException e) {\r\n        LOG.warn(\"error creating DomainSocket\", e);\r\n    } finally {\r\n        if (!success) {\r\n            if (sock != null) {\r\n                IOUtils.closeStream(sock);\r\n            }\r\n            pathMap.put(info.getPath(), PathState.UNUSABLE);\r\n            sock = null;\r\n        }\r\n    }\r\n    return sock;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "disableShortCircuitForPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void disableShortCircuitForPath(String path)\n{\r\n    pathMap.put(path, PathState.SHORT_CIRCUIT_DISABLED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "disableDomainSocketPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void disableDomainSocketPath(String path)\n{\r\n    pathMap.put(path, PathState.UNUSABLE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "clearPathMap",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearPathMap()\n{\r\n    pathMap.invalidateAll();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getPathExpireSeconds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPathExpireSeconds()\n{\r\n    return pathExpireSeconds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    super.setConf(conf);\r\n    credential = notNull(conf, OAUTH_CREDENTIAL_KEY);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "getCredential",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCredential()\n{\r\n    if (credential == null) {\r\n        throw new IllegalArgumentException(\"Credential has not been \" + \"provided in configuration\");\r\n    }\r\n    return credential;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "ensureTokenInitialized",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void ensureTokenInitialized() throws IOException\n{\r\n    if (!hasInitedToken || (action != null && !action.isValid())) {\r\n        Token<?> token = fs.getDelegationToken(null);\r\n        if (token != null) {\r\n            fs.setDelegationToken(token);\r\n            addRenewAction(fs);\r\n            LOG.debug(\"Created new DT for {}\", token.getService());\r\n        }\r\n        hasInitedToken = true;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reset()\n{\r\n    hasInitedToken = false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "initDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initDelegationToken(UserGroupInformation ugi)\n{\r\n    Token<?> token = selectDelegationToken(ugi);\r\n    if (token != null) {\r\n        LOG.debug(\"Found existing DT for {}\", token.getService());\r\n        fs.setDelegationToken(token);\r\n        hasInitedToken = true;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "removeRenewAction",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeRenewAction() throws IOException\n{\r\n    if (dtRenewer != null) {\r\n        dtRenewer.removeRenewAction(fs);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "selectDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> selectDelegationToken(UserGroupInformation ugi)\n{\r\n    return dtSelector.selectToken(serviceName, ugi.getTokens());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "addRenewAction",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addRenewAction(final T webhdfs)\n{\r\n    if (dtRenewer == null) {\r\n        dtRenewer = DelegationTokenRenewer.getInstance();\r\n    }\r\n    action = dtRenewer.addRenewAction(webhdfs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getUriDefaultPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getUriDefaultPort()\n{\r\n    return HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "createInternal",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "HdfsDataOutputStream createInternal(Path f, EnumSet<CreateFlag> createFlag, FsPermission absolutePermission, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt, boolean createParent) throws IOException\n{\r\n    final DFSOutputStream dfsos = dfs.primitiveCreate(getUriPath(f), absolutePermission, createFlag, createParent, replication, blockSize, progress, bufferSize, checksumOpt);\r\n    return dfs.createWrappedOutputStream(dfsos, statistics, dfsos.getInitialLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean delete(Path f, boolean recursive) throws IOException, UnresolvedLinkException\n{\r\n    return dfs.delete(getUriPath(f), recursive);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getFileBlockLocations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockLocation[] getFileBlockLocations(Path p, long start, long len) throws IOException, UnresolvedLinkException\n{\r\n    return dfs.getBlockLocations(getUriPath(p), start, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getFileChecksum",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileChecksum getFileChecksum(Path f) throws IOException, UnresolvedLinkException\n{\r\n    return dfs.getFileChecksumWithCombineMode(getUriPath(f), Long.MAX_VALUE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FileStatus getFileStatus(Path f) throws IOException, UnresolvedLinkException\n{\r\n    HdfsFileStatus fi = dfs.getFileInfo(getUriPath(f));\r\n    if (fi != null) {\r\n        return fi.makeQualified(getUri(), f);\r\n    } else {\r\n        throw new FileNotFoundException(\"File does not exist: \" + f.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "msync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void msync() throws IOException\n{\r\n    dfs.msync();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getFileLinkStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileStatus getFileLinkStatus(Path f) throws IOException, UnresolvedLinkException\n{\r\n    HdfsFileStatus fi = dfs.getFileLinkInfo(getUriPath(f));\r\n    if (fi != null) {\r\n        return fi.makeQualified(getUri(), f);\r\n    } else {\r\n        throw new FileNotFoundException(\"File does not exist: \" + f);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getFsStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FsStatus getFsStatus() throws IOException\n{\r\n    return dfs.getDiskStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FsServerDefaults getServerDefaults() throws IOException\n{\r\n    return dfs.getServerDefaults();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FsServerDefaults getServerDefaults(final Path f) throws IOException\n{\r\n    return dfs.getServerDefaults();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "listLocatedStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<LocatedFileStatus> listLocatedStatus(final Path p) throws FileNotFoundException, IOException\n{\r\n    return new DirListingIterator<LocatedFileStatus>(p, true) {\r\n\r\n        @Override\r\n        public LocatedFileStatus next() throws IOException {\r\n            return ((HdfsLocatedFileStatus) getNext()).makeQualifiedLocated(getUri(), p);\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "listStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<FileStatus> listStatusIterator(final Path f) throws AccessControlException, FileNotFoundException, UnresolvedLinkException, IOException\n{\r\n    return new DirListingIterator<FileStatus>(f, false) {\r\n\r\n        @Override\r\n        public FileStatus next() throws IOException {\r\n            return getNext().makeQualified(getUri(), f);\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "listStatus",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "FileStatus[] listStatus(Path f) throws IOException, UnresolvedLinkException\n{\r\n    String src = getUriPath(f);\r\n    DirectoryListing thisListing = dfs.listPaths(src, HdfsFileStatus.EMPTY_NAME);\r\n    if (thisListing == null) {\r\n        throw new FileNotFoundException(\"File \" + f + \" does not exist.\");\r\n    }\r\n    HdfsFileStatus[] partialListing = thisListing.getPartialListing();\r\n    if (!thisListing.hasMore()) {\r\n        FileStatus[] stats = new FileStatus[partialListing.length];\r\n        for (int i = 0; i < partialListing.length; i++) {\r\n            stats[i] = partialListing[i].makeQualified(getUri(), f);\r\n        }\r\n        return stats;\r\n    }\r\n    int totalNumEntries = partialListing.length + thisListing.getRemainingEntries();\r\n    ArrayList<FileStatus> listing = new ArrayList<FileStatus>(totalNumEntries);\r\n    for (HdfsFileStatus fileStatus : partialListing) {\r\n        listing.add(fileStatus.makeQualified(getUri(), f));\r\n    }\r\n    do {\r\n        thisListing = dfs.listPaths(src, thisListing.getLastName());\r\n        if (thisListing == null) {\r\n            throw new FileNotFoundException(\"File \" + f + \" does not exist.\");\r\n        }\r\n        partialListing = thisListing.getPartialListing();\r\n        for (HdfsFileStatus fileStatus : partialListing) {\r\n            listing.add(fileStatus.makeQualified(getUri(), f));\r\n        }\r\n    } while (thisListing.hasMore());\r\n    return listing.toArray(new FileStatus[listing.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "listCorruptFileBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RemoteIterator<Path> listCorruptFileBlocks(Path path) throws IOException\n{\r\n    return new CorruptFileBlockIterator(dfs, path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "mkdir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void mkdir(Path dir, FsPermission permission, boolean createParent) throws IOException, UnresolvedLinkException\n{\r\n    dfs.primitiveMkdir(getUriPath(dir), permission, createParent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "HdfsDataInputStream open(Path f, int bufferSize) throws IOException, UnresolvedLinkException\n{\r\n    final DFSInputStream dfsis = dfs.open(getUriPath(f), bufferSize, verifyChecksum);\r\n    return dfs.createWrappedInputStream(dfsis);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "truncate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean truncate(Path f, long newLength) throws IOException, UnresolvedLinkException\n{\r\n    return dfs.truncate(getUriPath(f), newLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "renameInternal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void renameInternal(Path src, Path dst) throws IOException, UnresolvedLinkException\n{\r\n    dfs.rename(getUriPath(src), getUriPath(dst), Options.Rename.NONE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "renameInternal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void renameInternal(Path src, Path dst, boolean overwrite) throws IOException, UnresolvedLinkException\n{\r\n    dfs.rename(getUriPath(src), getUriPath(dst), overwrite ? Options.Rename.OVERWRITE : Options.Rename.NONE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "setOwner",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOwner(Path f, String username, String groupname) throws IOException, UnresolvedLinkException\n{\r\n    dfs.setOwner(getUriPath(f), username, groupname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "setPermission",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setPermission(Path f, FsPermission permission) throws IOException, UnresolvedLinkException\n{\r\n    dfs.setPermission(getUriPath(f), permission);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "setReplication",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean setReplication(Path f, short replication) throws IOException, UnresolvedLinkException\n{\r\n    return dfs.setReplication(getUriPath(f), replication);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "setTimes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTimes(Path f, long mtime, long atime) throws IOException, UnresolvedLinkException\n{\r\n    dfs.setTimes(getUriPath(f), mtime, atime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "setVerifyChecksum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setVerifyChecksum(boolean verifyChecksum) throws IOException\n{\r\n    this.verifyChecksum = verifyChecksum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "supportsSymlinks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean supportsSymlinks()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "createSymlink",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createSymlink(Path target, Path link, boolean createParent) throws IOException, UnresolvedLinkException\n{\r\n    dfs.createSymlink(target.toString(), getUriPath(link), createParent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getLinkTarget",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getLinkTarget(Path p) throws IOException\n{\r\n    return new Path(dfs.getLinkTarget(getUriPath(p)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getCanonicalServiceName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getCanonicalServiceName()\n{\r\n    return dfs.getCanonicalServiceName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getDelegationTokens",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<Token<?>> getDelegationTokens(String renewer) throws IOException\n{\r\n    Token<DelegationTokenIdentifier> result = dfs.getDelegationToken(renewer == null ? null : new Text(renewer));\r\n    List<Token<?>> tokenList = new ArrayList<Token<?>>();\r\n    tokenList.add(result);\r\n    return tokenList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "modifyAclEntries",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void modifyAclEntries(Path path, List<AclEntry> aclSpec) throws IOException\n{\r\n    dfs.modifyAclEntries(getUriPath(path), aclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "removeAclEntries",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeAclEntries(Path path, List<AclEntry> aclSpec) throws IOException\n{\r\n    dfs.removeAclEntries(getUriPath(path), aclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "removeDefaultAcl",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeDefaultAcl(Path path) throws IOException\n{\r\n    dfs.removeDefaultAcl(getUriPath(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "removeAcl",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeAcl(Path path) throws IOException\n{\r\n    dfs.removeAcl(getUriPath(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "setAcl",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setAcl(Path path, List<AclEntry> aclSpec) throws IOException\n{\r\n    dfs.setAcl(getUriPath(path), aclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getAclStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AclStatus getAclStatus(Path path) throws IOException\n{\r\n    return dfs.getAclStatus(getUriPath(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "setXAttr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setXAttr(Path path, String name, byte[] value, EnumSet<XAttrSetFlag> flag) throws IOException\n{\r\n    dfs.setXAttr(getUriPath(path), name, value, flag);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getXAttr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getXAttr(Path path, String name) throws IOException\n{\r\n    return dfs.getXAttr(getUriPath(path), name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(Path path) throws IOException\n{\r\n    return dfs.getXAttrs(getUriPath(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(Path path, List<String> names) throws IOException\n{\r\n    return dfs.getXAttrs(getUriPath(path), names);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "listXAttrs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> listXAttrs(Path path) throws IOException\n{\r\n    return dfs.listXAttrs(getUriPath(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "removeXAttr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeXAttr(Path path, String name) throws IOException\n{\r\n    dfs.removeXAttr(getUriPath(path), name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "access",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void access(Path path, final FsAction mode) throws IOException\n{\r\n    dfs.checkAccess(getUriPath(path), mode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "satisfyStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void satisfyStoragePolicy(Path path) throws IOException\n{\r\n    dfs.satisfyStoragePolicy(getUriPath(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "setStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStoragePolicy(Path path, String policyName) throws IOException\n{\r\n    dfs.setStoragePolicy(getUriPath(path), policyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "unsetStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void unsetStoragePolicy(final Path src) throws IOException\n{\r\n    dfs.unsetStoragePolicy(getUriPath(src));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockStoragePolicySpi getStoragePolicy(Path src) throws IOException\n{\r\n    return dfs.getStoragePolicy(getUriPath(src));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getAllStoragePolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<? extends BlockStoragePolicySpi> getAllStoragePolicies() throws IOException\n{\r\n    return Arrays.asList(dfs.getStoragePolicies());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "renewDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long renewDelegationToken(Token<? extends AbstractDelegationTokenIdentifier> token) throws InvalidToken, IOException\n{\r\n    return dfs.renewDelegationToken((Token<DelegationTokenIdentifier>) token);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "cancelDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cancelDelegationToken(Token<? extends AbstractDelegationTokenIdentifier> token) throws InvalidToken, IOException\n{\r\n    dfs.cancelDelegationToken((Token<DelegationTokenIdentifier>) token);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "createSnapshot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path createSnapshot(final Path path, final String snapshotName) throws IOException\n{\r\n    return new Path(dfs.createSnapshot(getUriPath(path), snapshotName));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "renameSnapshot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void renameSnapshot(final Path path, final String snapshotOldName, final String snapshotNewName) throws IOException\n{\r\n    dfs.renameSnapshot(getUriPath(path), snapshotOldName, snapshotNewName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "deleteSnapshot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deleteSnapshot(final Path snapshotDir, final String snapshotName) throws IOException\n{\r\n    dfs.deleteSnapshot(getUriPath(snapshotDir), snapshotName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "fromConf",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "ShortCircuitCache fromConf(ShortCircuitConf conf)\n{\r\n    return new ShortCircuitCache(conf.getShortCircuitStreamsCacheSize(), conf.getShortCircuitStreamsCacheExpiryMs(), conf.getShortCircuitMmapCacheSize(), conf.getShortCircuitMmapCacheExpiryMs(), conf.getShortCircuitMmapCacheRetryTimeout(), conf.getShortCircuitCacheStaleThresholdMs(), conf.getShortCircuitSharedMemoryWatcherInterruptCheckMs());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getStaleThresholdMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStaleThresholdMs()\n{\r\n    return staleThresholdMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "setMaxTotalSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMaxTotalSize(int maxTotalSize)\n{\r\n    this.maxTotalSize = maxTotalSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "ref",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void ref(ShortCircuitReplica replica)\n{\r\n    lock.lock();\r\n    try {\r\n        Preconditions.checkArgument(replica.refCount > 0, \"can't ref %s because its refCount reached %d\", replica, replica.refCount);\r\n        Long evictableTimeNs = replica.getEvictableTimeNs();\r\n        replica.refCount++;\r\n        if (evictableTimeNs != null) {\r\n            String removedFrom = removeEvictable(replica);\r\n            if (LOG.isTraceEnabled()) {\r\n                LOG.trace(this + \": \" + removedFrom + \" no longer contains \" + replica + \".  refCount \" + (replica.refCount - 1) + \" -> \" + replica.refCount + StringUtils.getStackTrace(Thread.currentThread()));\r\n            }\r\n        } else if (LOG.isTraceEnabled()) {\r\n            LOG.trace(this + \": replica  refCount \" + (replica.refCount - 1) + \" -> \" + replica.refCount + StringUtils.getStackTrace(Thread.currentThread()));\r\n        }\r\n    } finally {\r\n        lock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "unref",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void unref(ShortCircuitReplica replica)\n{\r\n    lock.lock();\r\n    try {\r\n        if (!replica.purged) {\r\n            String purgeReason = null;\r\n            if (!replica.getDataStream().getChannel().isOpen()) {\r\n                purgeReason = \"purging replica because its data channel is closed.\";\r\n            } else if (!replica.getMetaStream().getChannel().isOpen()) {\r\n                purgeReason = \"purging replica because its meta channel is closed.\";\r\n            } else if (replica.isStale()) {\r\n                purgeReason = \"purging replica because it is stale.\";\r\n            }\r\n            if (purgeReason != null) {\r\n                LOG.debug(\"{}: {}\", this, purgeReason);\r\n                purge(replica);\r\n            }\r\n        }\r\n        String addedString = \"\";\r\n        boolean shouldTrimEvictionMaps = false;\r\n        int newRefCount = --replica.refCount;\r\n        if (newRefCount == 0) {\r\n            Preconditions.checkArgument(replica.purged, \"Replica %s reached a refCount of 0 without being purged\", replica);\r\n            replica.close();\r\n        } else if (newRefCount == 1) {\r\n            Preconditions.checkState(null == replica.getEvictableTimeNs(), \"Replica %s had a refCount higher than 1, \" + \"but was still evictable (evictableTimeNs = %d)\", replica, replica.getEvictableTimeNs());\r\n            if (!replica.purged) {\r\n                if (replica.hasMmap()) {\r\n                    insertEvictable(System.nanoTime(), replica, evictableMmapped);\r\n                    addedString = \"added to evictableMmapped, \";\r\n                } else {\r\n                    insertEvictable(System.nanoTime(), replica, evictable);\r\n                    addedString = \"added to evictable, \";\r\n                }\r\n                shouldTrimEvictionMaps = true;\r\n            }\r\n        } else {\r\n            Preconditions.checkArgument(replica.refCount >= 0, \"replica's refCount went negative (refCount = %d\" + \" for %s)\", replica.refCount, replica);\r\n        }\r\n        if (LOG.isTraceEnabled()) {\r\n            LOG.trace(this + \": unref replica \" + replica + \": \" + addedString + \" refCount \" + (newRefCount + 1) + \" -> \" + newRefCount + StringUtils.getStackTrace(Thread.currentThread()));\r\n        }\r\n        if (shouldTrimEvictionMaps) {\r\n            trimEvictionMaps();\r\n        }\r\n    } finally {\r\n        lock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "demoteOldEvictableMmaped",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "int demoteOldEvictableMmaped(long now)\n{\r\n    int numDemoted = 0;\r\n    boolean needMoreSpace = false;\r\n    Long evictionTimeNs;\r\n    while (!evictableMmapped.isEmpty()) {\r\n        Object eldestKey = evictableMmapped.firstKey();\r\n        evictionTimeNs = (Long) eldestKey;\r\n        long evictionTimeMs = TimeUnit.MILLISECONDS.convert(evictionTimeNs, TimeUnit.NANOSECONDS);\r\n        if (evictionTimeMs + maxEvictableMmapedLifespanMs >= now) {\r\n            if (evictableMmapped.size() < maxEvictableMmapedSize) {\r\n                break;\r\n            }\r\n            needMoreSpace = true;\r\n        }\r\n        ShortCircuitReplica replica = (ShortCircuitReplica) evictableMmapped.get(eldestKey);\r\n        if (LOG.isTraceEnabled()) {\r\n            String rationale = needMoreSpace ? \"because we need more space\" : \"because it's too old\";\r\n            LOG.trace(\"demoteOldEvictable: demoting \" + replica + \": \" + rationale + \": \" + StringUtils.getStackTrace(Thread.currentThread()));\r\n        }\r\n        removeEvictable(replica, evictableMmapped);\r\n        munmap(replica);\r\n        insertEvictable(evictionTimeNs, replica, evictable);\r\n        numDemoted++;\r\n    }\r\n    return numDemoted;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "trimEvictionMaps",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void trimEvictionMaps()\n{\r\n    long now = Time.monotonicNow();\r\n    demoteOldEvictableMmaped(now);\r\n    while (evictable.size() + evictableMmapped.size() > maxTotalSize) {\r\n        ShortCircuitReplica replica;\r\n        if (evictable.isEmpty()) {\r\n            replica = (ShortCircuitReplica) evictableMmapped.get(evictableMmapped.firstKey());\r\n        } else {\r\n            replica = (ShortCircuitReplica) evictable.get(evictable.firstKey());\r\n        }\r\n        if (LOG.isTraceEnabled()) {\r\n            LOG.trace(this + \": trimEvictionMaps is purging \" + replica + StringUtils.getStackTrace(Thread.currentThread()));\r\n        }\r\n        purge(replica);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "munmap",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void munmap(ShortCircuitReplica replica)\n{\r\n    replica.munmap();\r\n    outstandingMmapCount--;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "removeEvictable",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String removeEvictable(ShortCircuitReplica replica)\n{\r\n    if (replica.hasMmap()) {\r\n        removeEvictable(replica, evictableMmapped);\r\n        return \"evictableMmapped\";\r\n    } else {\r\n        removeEvictable(replica, evictable);\r\n        return \"evictable\";\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "removeEvictable",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void removeEvictable(ShortCircuitReplica replica, LinkedMap map)\n{\r\n    Long evictableTimeNs = replica.getEvictableTimeNs();\r\n    Preconditions.checkNotNull(evictableTimeNs);\r\n    ShortCircuitReplica removed = (ShortCircuitReplica) map.remove(evictableTimeNs);\r\n    Preconditions.checkState(removed == replica, \"failed to make %s unevictable\", replica);\r\n    replica.setEvictableTimeNs(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "insertEvictable",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void insertEvictable(Long evictionTimeNs, ShortCircuitReplica replica, LinkedMap map)\n{\r\n    while (map.containsKey(evictionTimeNs)) {\r\n        evictionTimeNs++;\r\n    }\r\n    Preconditions.checkState(null == replica.getEvictableTimeNs());\r\n    replica.setEvictableTimeNs(evictionTimeNs);\r\n    map.put(evictionTimeNs, replica);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "purge",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void purge(ShortCircuitReplica replica)\n{\r\n    boolean removedFromInfoMap = false;\r\n    String evictionMapName = null;\r\n    Preconditions.checkArgument(!replica.purged);\r\n    replica.purged = true;\r\n    Waitable<ShortCircuitReplicaInfo> val = replicaInfoMap.get(replica.key);\r\n    if (val != null) {\r\n        ShortCircuitReplicaInfo info = val.getVal();\r\n        if ((info != null) && (info.getReplica() == replica)) {\r\n            replicaInfoMap.remove(replica.key);\r\n            removedFromInfoMap = true;\r\n        }\r\n    }\r\n    Long evictableTimeNs = replica.getEvictableTimeNs();\r\n    if (evictableTimeNs != null) {\r\n        evictionMapName = removeEvictable(replica);\r\n    }\r\n    if (LOG.isTraceEnabled()) {\r\n        StringBuilder builder = new StringBuilder();\r\n        builder.append(this).append(\": \").append(\": purged \").append(replica).append(\" from the cache.\");\r\n        if (removedFromInfoMap) {\r\n            builder.append(\"  Removed from the replicaInfoMap.\");\r\n        }\r\n        if (evictionMapName != null) {\r\n            builder.append(\"  Removed from \").append(evictionMapName);\r\n        }\r\n        LOG.trace(builder.toString());\r\n    }\r\n    unref(replica);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "fetchOrCreate",
  "errType" : [ "RetriableException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "ShortCircuitReplicaInfo fetchOrCreate(ExtendedBlockId key, ShortCircuitReplicaCreator creator)\n{\r\n    Waitable<ShortCircuitReplicaInfo> newWaitable;\r\n    lock.lock();\r\n    try {\r\n        ShortCircuitReplicaInfo info = null;\r\n        for (int i = 0; i < FETCH_OR_CREATE_RETRY_TIMES; i++) {\r\n            if (closed) {\r\n                LOG.trace(\"{}: can't fethchOrCreate {} because the cache is closed.\", this, key);\r\n                return null;\r\n            }\r\n            Waitable<ShortCircuitReplicaInfo> waitable = replicaInfoMap.get(key);\r\n            if (waitable != null) {\r\n                try {\r\n                    info = fetch(key, waitable);\r\n                    break;\r\n                } catch (RetriableException e) {\r\n                    LOG.debug(\"{}: retrying {}\", this, e.getMessage());\r\n                }\r\n            }\r\n        }\r\n        if (info != null)\r\n            return info;\r\n        newWaitable = new Waitable<>(lock.newCondition());\r\n        replicaInfoMap.put(key, newWaitable);\r\n    } finally {\r\n        lock.unlock();\r\n    }\r\n    return create(key, creator, newWaitable);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "fetch",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "ShortCircuitReplicaInfo fetch(ExtendedBlockId key, Waitable<ShortCircuitReplicaInfo> waitable) throws RetriableException\n{\r\n    ShortCircuitReplicaInfo info;\r\n    try {\r\n        LOG.trace(\"{}: found waitable for {}\", this, key);\r\n        info = waitable.await();\r\n    } catch (InterruptedException e) {\r\n        LOG.info(this + \": interrupted while waiting for \" + key);\r\n        Thread.currentThread().interrupt();\r\n        throw new RetriableException(\"interrupted\");\r\n    }\r\n    if (info.getInvalidTokenException() != null) {\r\n        LOG.info(this + \": could not get \" + key + \" due to InvalidToken \" + \"exception.\", info.getInvalidTokenException());\r\n        return info;\r\n    }\r\n    ShortCircuitReplica replica = info.getReplica();\r\n    if (replica == null) {\r\n        LOG.warn(this + \": failed to get \" + key);\r\n        return info;\r\n    }\r\n    if (replica.purged) {\r\n        throw new RetriableException(\"Ignoring purged replica \" + replica + \".  Retrying.\");\r\n    }\r\n    if (replica.isStale()) {\r\n        LOG.info(this + \": got stale replica \" + replica + \".  Removing \" + \"this replica from the replicaInfoMap and retrying.\");\r\n        purge(replica);\r\n        throw new RetriableException(\"ignoring stale replica \" + replica);\r\n    }\r\n    ref(replica);\r\n    return info;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "create",
  "errType" : [ "RuntimeException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "ShortCircuitReplicaInfo create(ExtendedBlockId key, ShortCircuitReplicaCreator creator, Waitable<ShortCircuitReplicaInfo> newWaitable)\n{\r\n    ShortCircuitReplicaInfo info = null;\r\n    try {\r\n        LOG.trace(\"{}: loading {}\", this, key);\r\n        info = creator.createShortCircuitReplicaInfo();\r\n    } catch (RuntimeException e) {\r\n        LOG.warn(this + \": failed to load \" + key, e);\r\n    }\r\n    if (info == null)\r\n        info = new ShortCircuitReplicaInfo();\r\n    lock.lock();\r\n    try {\r\n        if (info.getReplica() != null) {\r\n            LOG.trace(\"{}: successfully loaded {}\", this, info.getReplica());\r\n            startCacheCleanerThreadIfNeeded();\r\n        } else {\r\n            Waitable<ShortCircuitReplicaInfo> waitableInMap = replicaInfoMap.get(key);\r\n            if (waitableInMap == newWaitable)\r\n                replicaInfoMap.remove(key);\r\n            if (info.getInvalidTokenException() != null) {\r\n                LOG.info(this + \": could not load \" + key + \" due to InvalidToken \" + \"exception.\", info.getInvalidTokenException());\r\n            } else {\r\n                LOG.warn(this + \": failed to load \" + key);\r\n            }\r\n        }\r\n        newWaitable.provide(info);\r\n    } finally {\r\n        lock.unlock();\r\n    }\r\n    return info;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "startCacheCleanerThreadIfNeeded",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void startCacheCleanerThreadIfNeeded()\n{\r\n    if (cacheCleaner == null) {\r\n        cacheCleaner = new CacheCleaner();\r\n        long rateMs = cacheCleaner.getRateInMs();\r\n        ScheduledFuture<?> future = cleanerExecutor.scheduleAtFixedRate(cacheCleaner, rateMs, rateMs, TimeUnit.MILLISECONDS);\r\n        cacheCleaner.setFuture(future);\r\n        LOG.debug(\"{}: starting cache cleaner thread which will run every {} ms\", this, rateMs);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getOrCreateClientMmap",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "ClientMmap getOrCreateClientMmap(ShortCircuitReplica replica, boolean anchored)\n{\r\n    Condition newCond;\r\n    lock.lock();\r\n    try {\r\n        while (replica.mmapData != null) {\r\n            if (replica.mmapData instanceof MappedByteBuffer) {\r\n                ref(replica);\r\n                MappedByteBuffer mmap = (MappedByteBuffer) replica.mmapData;\r\n                return new ClientMmap(replica, mmap, anchored);\r\n            } else if (replica.mmapData instanceof Long) {\r\n                long lastAttemptTimeMs = (Long) replica.mmapData;\r\n                long delta = Time.monotonicNow() - lastAttemptTimeMs;\r\n                if (delta < mmapRetryTimeoutMs) {\r\n                    LOG.trace(\"{}: can't create client mmap for {} because we failed to\" + \" create one just {}ms ago.\", this, replica, delta);\r\n                    return null;\r\n                }\r\n                LOG.trace(\"{}: retrying client mmap for {}, {} ms after the previous \" + \"failure.\", this, replica, delta);\r\n            } else if (replica.mmapData instanceof Condition) {\r\n                Condition cond = (Condition) replica.mmapData;\r\n                cond.awaitUninterruptibly();\r\n            } else {\r\n                Preconditions.checkState(false, \"invalid mmapData type %s\", replica.mmapData.getClass().getName());\r\n            }\r\n        }\r\n        newCond = lock.newCondition();\r\n        replica.mmapData = newCond;\r\n    } finally {\r\n        lock.unlock();\r\n    }\r\n    MappedByteBuffer map = replica.loadMmapInternal();\r\n    lock.lock();\r\n    try {\r\n        if (map == null) {\r\n            replica.mmapData = Time.monotonicNow();\r\n            newCond.signalAll();\r\n            return null;\r\n        } else {\r\n            outstandingMmapCount++;\r\n            replica.mmapData = map;\r\n            ref(replica);\r\n            newCond.signalAll();\r\n            return new ClientMmap(replica, map, anchored);\r\n        }\r\n    } finally {\r\n        lock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "close",
  "errType" : [ "InterruptedException", "InterruptedException" ],
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void close()\n{\r\n    try {\r\n        lock.lock();\r\n        if (closed)\r\n            return;\r\n        closed = true;\r\n        LOG.info(this + \": closing\");\r\n        maxNonMmappedEvictableLifespanMs = 0;\r\n        maxEvictableMmapedSize = 0;\r\n        IOUtilsClient.cleanupWithLogger(LOG, cacheCleaner);\r\n        while (!evictable.isEmpty()) {\r\n            Object eldestKey = evictable.firstKey();\r\n            purge((ShortCircuitReplica) evictable.get(eldestKey));\r\n        }\r\n        while (!evictableMmapped.isEmpty()) {\r\n            Object eldestKey = evictableMmapped.firstKey();\r\n            purge((ShortCircuitReplica) evictableMmapped.get(eldestKey));\r\n        }\r\n    } finally {\r\n        lock.unlock();\r\n    }\r\n    releaserExecutor.shutdown();\r\n    cleanerExecutor.shutdown();\r\n    try {\r\n        if (!releaserExecutor.awaitTermination(30, TimeUnit.SECONDS)) {\r\n            LOG.error(\"Forcing SlotReleaserThreadPool to shutdown!\");\r\n            releaserExecutor.shutdownNow();\r\n        }\r\n    } catch (InterruptedException e) {\r\n        releaserExecutor.shutdownNow();\r\n        Thread.currentThread().interrupt();\r\n        LOG.error(\"Interrupted while waiting for SlotReleaserThreadPool \" + \"to terminate\", e);\r\n    }\r\n    try {\r\n        if (!cleanerExecutor.awaitTermination(30, TimeUnit.SECONDS)) {\r\n            LOG.error(\"Forcing CleanerThreadPool to shutdown!\");\r\n            cleanerExecutor.shutdownNow();\r\n        }\r\n    } catch (InterruptedException e) {\r\n        cleanerExecutor.shutdownNow();\r\n        Thread.currentThread().interrupt();\r\n        LOG.error(\"Interrupted while waiting for CleanerThreadPool \" + \"to terminate\", e);\r\n    }\r\n    IOUtilsClient.cleanupWithLogger(LOG, shmManager);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "accept",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void accept(CacheVisitor visitor)\n{\r\n    lock.lock();\r\n    try {\r\n        Map<ExtendedBlockId, ShortCircuitReplica> replicas = new HashMap<>();\r\n        Map<ExtendedBlockId, InvalidToken> failedLoads = new HashMap<>();\r\n        for (Entry<ExtendedBlockId, Waitable<ShortCircuitReplicaInfo>> entry : replicaInfoMap.entrySet()) {\r\n            Waitable<ShortCircuitReplicaInfo> waitable = entry.getValue();\r\n            if (waitable.hasVal()) {\r\n                if (waitable.getVal().getReplica() != null) {\r\n                    replicas.put(entry.getKey(), waitable.getVal().getReplica());\r\n                } else {\r\n                    failedLoads.put(entry.getKey(), waitable.getVal().getInvalidTokenException());\r\n                }\r\n            }\r\n        }\r\n        LOG.debug(\"visiting {} with outstandingMmapCount={}, replicas={}, \" + \"failedLoads={}, evictable={}, evictableMmapped={}\", visitor.getClass().getName(), outstandingMmapCount, replicas, failedLoads, evictable, evictableMmapped);\r\n        visitor.visit(outstandingMmapCount, replicas, failedLoads, evictable, evictableMmapped);\r\n    } finally {\r\n        lock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"ShortCircuitCache(0x\" + Integer.toHexString(System.identityHashCode(this)) + \")\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "allocShmSlot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Slot allocShmSlot(DatanodeInfo datanode, DomainPeer peer, MutableBoolean usedPeer, ExtendedBlockId blockId, String clientName) throws IOException\n{\r\n    if (shmManager != null) {\r\n        return shmManager.allocSlot(datanode, peer, usedPeer, blockId, clientName);\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "freeSlot",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void freeSlot(Slot slot)\n{\r\n    Preconditions.checkState(shmManager != null);\r\n    slot.makeInvalid();\r\n    shmManager.freeSlot(slot);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "scheduleSlotReleaser",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void scheduleSlotReleaser(Slot slot)\n{\r\n    if (slot == null) {\r\n        return;\r\n    }\r\n    Preconditions.checkState(shmManager != null);\r\n    releaserExecutor.execute(new SlotReleaser(slot));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getDfsClientShmManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DfsClientShmManager getDfsClientShmManager()\n{\r\n    return shmManager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getReplicaInfoMapSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getReplicaInfoMapSize()\n{\r\n    return replicaInfoMap.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toRemoteException",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "RemoteException toRemoteException(final Map<?, ?> json)\n{\r\n    final Map<?, ?> m = (Map<?, ?>) json.get(RemoteException.class.getSimpleName());\r\n    final String message = (String) m.get(\"message\");\r\n    final String javaClassName = (String) m.get(\"javaClassName\");\r\n    if (UNSUPPPORTED_EXCEPTION_STR.equals(javaClassName)) {\r\n        throw new UnsupportedOperationException(message);\r\n    }\r\n    return new RemoteException(javaClassName, message);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<? extends TokenIdentifier> toToken(final Map<?, ?> m) throws IOException\n{\r\n    if (m == null) {\r\n        return null;\r\n    }\r\n    final Token<DelegationTokenIdentifier> token = new Token<>();\r\n    token.decodeFromUrlString((String) m.get(\"urlString\"));\r\n    return token;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toBlockToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<BlockTokenIdentifier> toBlockToken(final Map<?, ?> m) throws IOException\n{\r\n    return (Token<BlockTokenIdentifier>) toToken(m);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toFsPermission",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FsPermission toFsPermission(final String s)\n{\r\n    return null == s ? null : new FsPermission(Short.parseShort(s, 8));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toFileStatus",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "HdfsFileStatus toFileStatus(final Map<?, ?> json, boolean includesType)\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    final Map<?, ?> m = includesType ? (Map<?, ?>) json.get(FileStatus.class.getSimpleName()) : json;\r\n    final String localName = (String) m.get(\"pathSuffix\");\r\n    final WebHdfsConstants.PathType type = WebHdfsConstants.PathType.valueOf((String) m.get(\"type\"));\r\n    final byte[] symlink = type != WebHdfsConstants.PathType.SYMLINK ? null : DFSUtilClient.string2Bytes((String) m.get(\"symlink\"));\r\n    final long len = ((Number) m.get(\"length\")).longValue();\r\n    final String owner = (String) m.get(\"owner\");\r\n    final String group = (String) m.get(\"group\");\r\n    final FsPermission permission = toFsPermission((String) m.get(\"permission\"));\r\n    Boolean aclBit = (Boolean) m.get(\"aclBit\");\r\n    Boolean encBit = (Boolean) m.get(\"encBit\");\r\n    Boolean erasureBit = (Boolean) m.get(\"ecBit\");\r\n    Boolean snapshotEnabledBit = (Boolean) m.get(\"snapshotEnabled\");\r\n    EnumSet<HdfsFileStatus.Flags> f = EnumSet.noneOf(HdfsFileStatus.Flags.class);\r\n    if (aclBit != null && aclBit) {\r\n        f.add(HdfsFileStatus.Flags.HAS_ACL);\r\n    }\r\n    if (encBit != null && encBit) {\r\n        f.add(HdfsFileStatus.Flags.HAS_CRYPT);\r\n    }\r\n    if (erasureBit != null && erasureBit) {\r\n        f.add(HdfsFileStatus.Flags.HAS_EC);\r\n    }\r\n    if (snapshotEnabledBit != null && snapshotEnabledBit) {\r\n        f.add(HdfsFileStatus.Flags.SNAPSHOT_ENABLED);\r\n    }\r\n    Map<String, Object> ecPolicyObj = (Map) m.get(\"ecPolicyObj\");\r\n    ErasureCodingPolicy ecPolicy = null;\r\n    if (ecPolicyObj != null) {\r\n        Map<String, String> extraOptions = (Map) ecPolicyObj.get(\"extraOptions\");\r\n        ECSchema ecSchema = new ECSchema((String) ecPolicyObj.get(\"codecName\"), (int) ((Number) ecPolicyObj.get(\"numDataUnits\")).longValue(), (int) ((Number) ecPolicyObj.get(\"numParityUnits\")).longValue(), extraOptions);\r\n        ecPolicy = new ErasureCodingPolicy((String) ecPolicyObj.get(\"name\"), ecSchema, (int) ((Number) ecPolicyObj.get(\"cellSize\")).longValue(), (byte) (int) ((Number) ecPolicyObj.get(\"id\")).longValue());\r\n    }\r\n    final long aTime = ((Number) m.get(\"accessTime\")).longValue();\r\n    final long mTime = ((Number) m.get(\"modificationTime\")).longValue();\r\n    final long blockSize = ((Number) m.get(\"blockSize\")).longValue();\r\n    final short replication = ((Number) m.get(\"replication\")).shortValue();\r\n    final long fileId = m.containsKey(\"fileId\") ? ((Number) m.get(\"fileId\")).longValue() : HdfsConstants.GRANDFATHER_INODE_ID;\r\n    final int childrenNum = getInt(m, \"childrenNum\", -1);\r\n    final byte storagePolicy = m.containsKey(\"storagePolicy\") ? (byte) ((Number) m.get(\"storagePolicy\")).longValue() : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\r\n    return new HdfsFileStatus.Builder().length(len).isdir(type == WebHdfsConstants.PathType.DIRECTORY).replication(replication).blocksize(blockSize).mtime(mTime).atime(aTime).perm(permission).flags(f).owner(owner).group(group).symlink(symlink).path(DFSUtilClient.string2Bytes(localName)).fileId(fileId).children(childrenNum).storagePolicy(storagePolicy).ecPolicy(ecPolicy).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toHdfsFileStatusArray",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "HdfsFileStatus[] toHdfsFileStatusArray(final Map<?, ?> json)\n{\r\n    Preconditions.checkNotNull(json);\r\n    final Map<?, ?> rootmap = (Map<?, ?>) json.get(FileStatus.class.getSimpleName() + \"es\");\r\n    final List<?> array = JsonUtilClient.getList(rootmap, FileStatus.class.getSimpleName());\r\n    Preconditions.checkNotNull(array);\r\n    final HdfsFileStatus[] statuses = new HdfsFileStatus[array.size()];\r\n    int i = 0;\r\n    for (Object object : array) {\r\n        final Map<?, ?> m = (Map<?, ?>) object;\r\n        statuses[i++] = JsonUtilClient.toFileStatus(m, false);\r\n    }\r\n    return statuses;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toDirectoryListing",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "DirectoryListing toDirectoryListing(final Map<?, ?> json)\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    final Map<?, ?> listing = getMap(json, \"DirectoryListing\");\r\n    final Map<?, ?> partialListing = getMap(listing, \"partialListing\");\r\n    HdfsFileStatus[] fileStatuses = toHdfsFileStatusArray(partialListing);\r\n    int remainingEntries = getInt(listing, \"remainingEntries\", -1);\r\n    Preconditions.checkState(remainingEntries != -1, \"remainingEntries was not set\");\r\n    return new DirectoryListing(fileStatuses, remainingEntries);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toExtendedBlock",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ExtendedBlock toExtendedBlock(final Map<?, ?> m)\n{\r\n    if (m == null) {\r\n        return null;\r\n    }\r\n    final String blockPoolId = (String) m.get(\"blockPoolId\");\r\n    final long blockId = ((Number) m.get(\"blockId\")).longValue();\r\n    final long numBytes = ((Number) m.get(\"numBytes\")).longValue();\r\n    final long generationStamp = ((Number) m.get(\"generationStamp\")).longValue();\r\n    return new ExtendedBlock(blockPoolId, blockId, numBytes, generationStamp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getBoolean",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean getBoolean(Map<?, ?> m, String key, final boolean defaultValue)\n{\r\n    Object value = m.get(key);\r\n    if (value == null) {\r\n        return defaultValue;\r\n    }\r\n    return ((Boolean) value).booleanValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getInt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getInt(Map<?, ?> m, String key, final int defaultValue)\n{\r\n    Object value = m.get(key);\r\n    if (value == null) {\r\n        return defaultValue;\r\n    }\r\n    return ((Number) value).intValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getLong",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getLong(Map<?, ?> m, String key, final long defaultValue)\n{\r\n    Object value = m.get(key);\r\n    if (value == null) {\r\n        return defaultValue;\r\n    }\r\n    return ((Number) value).longValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getString(Map<?, ?> m, String key, final String defaultValue)\n{\r\n    Object value = m.get(key);\r\n    if (value == null) {\r\n        return defaultValue;\r\n    }\r\n    return (String) value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getList",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<?> getList(Map<?, ?> m, String key)\n{\r\n    Object list = m.get(key);\r\n    if (list instanceof List<?>) {\r\n        return (List<?>) list;\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getMap",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<?, ?> getMap(Map<?, ?> m, String key)\n{\r\n    Object map = m.get(key);\r\n    if (map instanceof Map<?, ?>) {\r\n        return (Map<?, ?>) map;\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toDatanodeInfo",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "DatanodeInfo toDatanodeInfo(final Map<?, ?> m) throws IOException\n{\r\n    if (m == null) {\r\n        return null;\r\n    }\r\n    String ipAddr = getString(m, \"ipAddr\", null);\r\n    int xferPort = getInt(m, \"xferPort\", -1);\r\n    if (ipAddr == null) {\r\n        String name = getString(m, \"name\", null);\r\n        if (name != null) {\r\n            int colonIdx = name.indexOf(':');\r\n            if (colonIdx > 0) {\r\n                ipAddr = name.substring(0, colonIdx);\r\n                xferPort = Integer.parseInt(name.substring(colonIdx + 1));\r\n            } else {\r\n                throw new IOException(\"Invalid value in server response: name=[\" + name + \"]\");\r\n            }\r\n        } else {\r\n            throw new IOException(\"Missing both 'ipAddr' and 'name' in server response.\");\r\n        }\r\n    }\r\n    if (xferPort == -1) {\r\n        throw new IOException(\"Invalid or missing 'xferPort' in server response.\");\r\n    }\r\n    return new DatanodeInfoBuilder().setIpAddr(ipAddr).setHostName((String) m.get(\"hostName\")).setDatanodeUuid((String) m.get(\"storageID\")).setXferPort(xferPort).setInfoPort(((Number) m.get(\"infoPort\")).intValue()).setInfoSecurePort(getInt(m, \"infoSecurePort\", 0)).setIpcPort(((Number) m.get(\"ipcPort\")).intValue()).setCapacity(getLong(m, \"capacity\", 0L)).setDfsUsed(getLong(m, \"dfsUsed\", 0L)).setRemaining(getLong(m, \"remaining\", 0L)).setBlockPoolUsed(getLong(m, \"blockPoolUsed\", 0L)).setCacheCapacity(getLong(m, \"cacheCapacity\", 0L)).setCacheUsed(getLong(m, \"cacheUsed\", 0L)).setLastUpdate(getLong(m, \"lastUpdate\", 0L)).setLastUpdateMonotonic(getLong(m, \"lastUpdateMonotonic\", 0L)).setXceiverCount(getInt(m, \"xceiverCount\", 0)).setNetworkLocation(getString(m, \"networkLocation\", \"\")).setAdminState(DatanodeInfo.AdminStates.valueOf(getString(m, \"adminState\", \"NORMAL\"))).setUpgradeDomain(getString(m, \"upgradeDomain\", \"\")).setLastBlockReportTime(getLong(m, \"lastBlockReportTime\", 0L)).setLastBlockReportMonotonic(getLong(m, \"lastBlockReportMonotonic\", 0L)).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toDatanodeInfoArray",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DatanodeInfo[] toDatanodeInfoArray(final List<?> objects) throws IOException\n{\r\n    if (objects == null) {\r\n        return null;\r\n    } else if (objects.isEmpty()) {\r\n        return EMPTY_DATANODE_INFO_ARRAY;\r\n    } else {\r\n        final DatanodeInfo[] array = new DatanodeInfo[objects.size()];\r\n        int i = 0;\r\n        for (Object object : objects) {\r\n            array[i++] = toDatanodeInfo((Map<?, ?>) object);\r\n        }\r\n        return array;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toStorageTypeArray",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "StorageType[] toStorageTypeArray(final List<?> objects) throws IOException\n{\r\n    if (objects == null) {\r\n        return null;\r\n    } else if (objects.isEmpty()) {\r\n        return StorageType.EMPTY_ARRAY;\r\n    } else {\r\n        final StorageType[] array = new StorageType[objects.size()];\r\n        int i = 0;\r\n        for (Object object : objects) {\r\n            array[i++] = StorageType.parseStorageType(object.toString());\r\n        }\r\n        return array;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toLocatedBlock",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "LocatedBlock toLocatedBlock(final Map<?, ?> m) throws IOException\n{\r\n    if (m == null) {\r\n        return null;\r\n    }\r\n    final ExtendedBlock b = toExtendedBlock((Map<?, ?>) m.get(\"block\"));\r\n    final DatanodeInfo[] locations = toDatanodeInfoArray(getList(m, \"locations\"));\r\n    final long startOffset = ((Number) m.get(\"startOffset\")).longValue();\r\n    final boolean isCorrupt = (Boolean) m.get(\"isCorrupt\");\r\n    final DatanodeInfo[] cachedLocations = toDatanodeInfoArray(getList(m, \"cachedLocations\"));\r\n    final StorageType[] storageTypes = toStorageTypeArray(getList(m, \"storageTypes\"));\r\n    final LocatedBlock locatedblock = new LocatedBlock(b, locations, null, storageTypes, startOffset, isCorrupt, cachedLocations);\r\n    locatedblock.setBlockToken(toBlockToken((Map<?, ?>) m.get(\"blockToken\")));\r\n    return locatedblock;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toLocatedBlockList",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<LocatedBlock> toLocatedBlockList(final List<?> objects) throws IOException\n{\r\n    if (objects == null) {\r\n        return null;\r\n    } else if (objects.isEmpty()) {\r\n        return Collections.emptyList();\r\n    } else {\r\n        final List<LocatedBlock> list = new ArrayList<>(objects.size());\r\n        for (Object object : objects) {\r\n            list.add(toLocatedBlock((Map<?, ?>) object));\r\n        }\r\n        return list;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toContentSummary",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "ContentSummary toContentSummary(final Map<?, ?> json)\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    final Map<?, ?> m = (Map<?, ?>) json.get(ContentSummary.class.getSimpleName());\r\n    final long length = ((Number) m.get(\"length\")).longValue();\r\n    final long fileCount = ((Number) m.get(\"fileCount\")).longValue();\r\n    final long directoryCount = ((Number) m.get(\"directoryCount\")).longValue();\r\n    final String ecPolicy = ((String) m.get(\"ecPolicy\"));\r\n    ContentSummary.Builder builder = new ContentSummary.Builder().length(length).fileCount(fileCount).directoryCount(directoryCount).erasureCodingPolicy(ecPolicy);\r\n    builder = buildQuotaUsage(builder, m, ContentSummary.Builder.class);\r\n    if (m.get(\"snapshotLength\") != null) {\r\n        long snapshotLength = ((Number) m.get(\"snapshotLength\")).longValue();\r\n        builder.snapshotLength(snapshotLength);\r\n    }\r\n    if (m.get(\"snapshotFileCount\") != null) {\r\n        long snapshotFileCount = ((Number) m.get(\"snapshotFileCount\")).longValue();\r\n        builder.snapshotFileCount(snapshotFileCount);\r\n    }\r\n    if (m.get(\"snapshotDirectoryCount\") != null) {\r\n        long snapshotDirectoryCount = ((Number) m.get(\"snapshotDirectoryCount\")).longValue();\r\n        builder.snapshotDirectoryCount(snapshotDirectoryCount);\r\n    }\r\n    if (m.get(\"snapshotSpaceConsumed\") != null) {\r\n        long snapshotSpaceConsumed = ((Number) m.get(\"snapshotSpaceConsumed\")).longValue();\r\n        builder.snapshotSpaceConsumed(snapshotSpaceConsumed);\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toQuotaUsage",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "QuotaUsage toQuotaUsage(final Map<?, ?> json)\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    final Map<?, ?> m = (Map<?, ?>) json.get(QuotaUsage.class.getSimpleName());\r\n    QuotaUsage.Builder builder = new QuotaUsage.Builder();\r\n    builder = buildQuotaUsage(builder, m, QuotaUsage.Builder.class);\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "buildQuotaUsage",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "T buildQuotaUsage(T builder, Map<?, ?> m, Class<T> type)\n{\r\n    final long quota = ((Number) m.get(\"quota\")).longValue();\r\n    final long spaceConsumed = ((Number) m.get(\"spaceConsumed\")).longValue();\r\n    final long spaceQuota = ((Number) m.get(\"spaceQuota\")).longValue();\r\n    final Map<?, ?> typem = (Map<?, ?>) m.get(\"typeQuota\");\r\n    T result = type.cast(builder.quota(quota).spaceConsumed(spaceConsumed).spaceQuota(spaceQuota));\r\n    if (m.get(\"fileAndDirectoryCount\") != null) {\r\n        final long fileAndDirectoryCount = ((Number) m.get(\"fileAndDirectoryCount\")).longValue();\r\n        result = type.cast(result.fileAndDirectoryCount(fileAndDirectoryCount));\r\n    }\r\n    if (typem != null) {\r\n        for (StorageType t : StorageType.getTypesSupportingQuota()) {\r\n            Map<?, ?> typeQuota = (Map<?, ?>) typem.get(t.toString());\r\n            if (typeQuota != null) {\r\n                result = type.cast(result.typeQuota(t, ((Number) typeQuota.get(\"quota\")).longValue()).typeConsumed(t, ((Number) typeQuota.get(\"consumed\")).longValue()));\r\n            }\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toMD5MD5CRC32FileChecksum",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "MD5MD5CRC32FileChecksum toMD5MD5CRC32FileChecksum(final Map<?, ?> json) throws IOException\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    final Map<?, ?> m = (Map<?, ?>) json.get(FileChecksum.class.getSimpleName());\r\n    final String algorithm = (String) m.get(\"algorithm\");\r\n    final int length = ((Number) m.get(\"length\")).intValue();\r\n    final byte[] bytes = StringUtils.hexStringToByte((String) m.get(\"bytes\"));\r\n    final DataInputStream in = new DataInputStream(new ByteArrayInputStream(bytes));\r\n    final DataChecksum.Type crcType = MD5MD5CRC32FileChecksum.getCrcTypeFromAlgorithmName(algorithm);\r\n    final MD5MD5CRC32FileChecksum checksum;\r\n    switch(crcType) {\r\n        case CRC32:\r\n            checksum = new MD5MD5CRC32GzipFileChecksum();\r\n            break;\r\n        case CRC32C:\r\n            checksum = new MD5MD5CRC32CastagnoliFileChecksum();\r\n            break;\r\n        default:\r\n            throw new IOException(\"Unknown algorithm: \" + algorithm);\r\n    }\r\n    checksum.readFields(in);\r\n    if (!checksum.getAlgorithmName().equals(algorithm)) {\r\n        throw new IOException(\"Algorithm not matched. Expected \" + algorithm + \", Received \" + checksum.getAlgorithmName());\r\n    }\r\n    if (length != checksum.getLength()) {\r\n        throw new IOException(\"Length not matched: length=\" + length + \", checksum.getLength()=\" + checksum.getLength());\r\n    }\r\n    return checksum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toAclStatus",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "AclStatus toAclStatus(final Map<?, ?> json)\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    final Map<?, ?> m = (Map<?, ?>) json.get(AclStatus.class.getSimpleName());\r\n    AclStatus.Builder aclStatusBuilder = new AclStatus.Builder();\r\n    aclStatusBuilder.owner((String) m.get(\"owner\"));\r\n    aclStatusBuilder.group((String) m.get(\"group\"));\r\n    aclStatusBuilder.stickyBit((Boolean) m.get(\"stickyBit\"));\r\n    String permString = (String) m.get(\"permission\");\r\n    if (permString != null) {\r\n        final FsPermission permission = toFsPermission(permString);\r\n        aclStatusBuilder.setPermission(permission);\r\n    }\r\n    final List<?> entries = (List<?>) m.get(\"entries\");\r\n    List<AclEntry> aclEntryList = new ArrayList<>();\r\n    for (Object entry : entries) {\r\n        AclEntry aclEntry = AclEntry.parseAclEntry((String) entry, true);\r\n        aclEntryList.add(aclEntry);\r\n    }\r\n    aclStatusBuilder.addEntries(aclEntryList);\r\n    return aclStatusBuilder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getPath(final Map<?, ?> json)\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    return (String) json.get(\"Path\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getXAttr",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "byte[] getXAttr(final Map<?, ?> json, final String name) throws IOException\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    Map<String, byte[]> xAttrs = toXAttrs(json);\r\n    if (xAttrs != null) {\r\n        return xAttrs.get(name);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getXAttr",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "byte[] getXAttr(final Map<?, ?> json) throws IOException\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    Map<String, byte[]> xAttrs = toXAttrs(json);\r\n    if (xAttrs != null && !xAttrs.values().isEmpty()) {\r\n        return xAttrs.values().iterator().next();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toXAttrs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, byte[]> toXAttrs(final Map<?, ?> json) throws IOException\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    return toXAttrMap(getList(json, \"XAttrs\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toXAttrNames",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<String> toXAttrNames(final Map<?, ?> json) throws IOException\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    final String namesInJson = (String) json.get(\"XAttrNames\");\r\n    ObjectReader reader = new ObjectMapper().readerFor(List.class);\r\n    final List<Object> xattrs = reader.readValue(namesInJson);\r\n    final List<String> names = Lists.newArrayListWithCapacity(json.keySet().size());\r\n    for (Object xattr : xattrs) {\r\n        names.add((String) xattr);\r\n    }\r\n    return names;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toXAttrMap",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Map<String, byte[]> toXAttrMap(final List<?> objects) throws IOException\n{\r\n    if (objects == null) {\r\n        return null;\r\n    } else if (objects.isEmpty()) {\r\n        return Maps.newHashMap();\r\n    } else {\r\n        final Map<String, byte[]> xAttrs = Maps.newHashMap();\r\n        for (Object object : objects) {\r\n            Map<?, ?> m = (Map<?, ?>) object;\r\n            String name = (String) m.get(\"name\");\r\n            String value = (String) m.get(\"value\");\r\n            xAttrs.put(name, decodeXAttrValue(value));\r\n        }\r\n        return xAttrs;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "decodeXAttrValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] decodeXAttrValue(String value) throws IOException\n{\r\n    if (value != null) {\r\n        return XAttrCodec.decodeValue(value);\r\n    } else {\r\n        return new byte[0];\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> toDelegationToken(final Map<?, ?> json) throws IOException\n{\r\n    final Map<?, ?> m = (Map<?, ?>) json.get(Token.class.getSimpleName());\r\n    return (Token<DelegationTokenIdentifier>) toToken(m);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toLocatedBlocks",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "LocatedBlocks toLocatedBlocks(final Map<?, ?> json) throws IOException\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    final Map<?, ?> m = (Map<?, ?>) json.get(LocatedBlocks.class.getSimpleName());\r\n    final long fileLength = ((Number) m.get(\"fileLength\")).longValue();\r\n    final boolean isUnderConstruction = (Boolean) m.get(\"isUnderConstruction\");\r\n    final List<LocatedBlock> locatedBlocks = toLocatedBlockList(getList(m, \"locatedBlocks\"));\r\n    final LocatedBlock lastLocatedBlock = toLocatedBlock((Map<?, ?>) m.get(\"lastLocatedBlock\"));\r\n    final boolean isLastBlockComplete = (Boolean) m.get(\"isLastBlockComplete\");\r\n    return new LocatedBlocks(fileLength, isUnderConstruction, locatedBlocks, lastLocatedBlock, isLastBlockComplete, null, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getStoragePolicies",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Collection<BlockStoragePolicy> getStoragePolicies(Map<?, ?> json)\n{\r\n    Map<?, ?> policiesJson = (Map<?, ?>) json.get(\"BlockStoragePolicies\");\r\n    if (policiesJson != null) {\r\n        List<?> objs = (List<?>) policiesJson.get(BlockStoragePolicy.class.getSimpleName());\r\n        if (objs != null) {\r\n            BlockStoragePolicy[] storagePolicies = new BlockStoragePolicy[objs.size()];\r\n            for (int i = 0; i < objs.size(); i++) {\r\n                final Map<?, ?> m = (Map<?, ?>) objs.get(i);\r\n                BlockStoragePolicy blockStoragePolicy = toBlockStoragePolicy(m);\r\n                storagePolicies[i] = blockStoragePolicy;\r\n            }\r\n            return Arrays.asList(storagePolicies);\r\n        }\r\n    }\r\n    return new ArrayList<BlockStoragePolicy>(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toBlockStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "BlockStoragePolicy toBlockStoragePolicy(Map<?, ?> m)\n{\r\n    byte id = ((Number) m.get(\"id\")).byteValue();\r\n    String name = (String) m.get(\"name\");\r\n    StorageType[] storageTypes = toStorageTypes((List<?>) m.get(\"storageTypes\"));\r\n    StorageType[] creationFallbacks = toStorageTypes((List<?>) m.get(\"creationFallbacks\"));\r\n    StorageType[] replicationFallbacks = toStorageTypes((List<?>) m.get(\"replicationFallbacks\"));\r\n    Boolean copyOnCreateFile = (Boolean) m.get(\"copyOnCreateFile\");\r\n    return new BlockStoragePolicy(id, name, storageTypes, creationFallbacks, replicationFallbacks, copyOnCreateFile.booleanValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toECPolicy",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ErasureCodingPolicy toECPolicy(Map<?, ?> m)\n{\r\n    if (m == null) {\r\n        return null;\r\n    }\r\n    byte id = ((Number) m.get(\"id\")).byteValue();\r\n    String name = (String) m.get(\"name\");\r\n    String codec = (String) m.get(\"codecName\");\r\n    int cellsize = ((Number) m.get(\"cellSize\")).intValue();\r\n    int dataunits = ((Number) m.get(\"numDataUnits\")).intValue();\r\n    int parityunits = ((Number) m.get(\"numParityUnits\")).intValue();\r\n    ECSchema ecs = new ECSchema(codec, dataunits, parityunits);\r\n    return new ErasureCodingPolicy(name, ecs, cellsize, id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toStorageTypes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "StorageType[] toStorageTypes(List<?> list)\n{\r\n    if (list == null) {\r\n        return null;\r\n    } else {\r\n        StorageType[] storageTypes = new StorageType[list.size()];\r\n        for (int i = 0; i < list.size(); i++) {\r\n            storageTypes[i] = StorageType.parseStorageType((String) list.get(i));\r\n        }\r\n        return storageTypes;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toFsServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "FsServerDefaults toFsServerDefaults(final Map<?, ?> json)\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    Map<?, ?> m = (Map<?, ?>) json.get(FsServerDefaults.class.getSimpleName());\r\n    long blockSize = getLong(m, \"blockSize\", -1);\r\n    int bytesPerChecksum = getInt(m, \"bytesPerChecksum\", -1);\r\n    int writePacketSize = getInt(m, \"writePacketSize\", -1);\r\n    short replication = (short) getInt(m, \"replication\", -1);\r\n    int fileBufferSize = getInt(m, \"fileBufferSize\", -1);\r\n    boolean encryptDataTransfer = m.containsKey(\"encryptDataTransfer\") ? (Boolean) m.get(\"encryptDataTransfer\") : false;\r\n    long trashInterval = getLong(m, \"trashInterval\", 0);\r\n    DataChecksum.Type type = DataChecksum.Type.valueOf(getInt(m, \"checksumType\", 1));\r\n    String keyProviderUri = (String) m.get(\"keyProviderUri\");\r\n    byte storagepolicyId = m.containsKey(\"defaultStoragePolicyId\") ? ((Number) m.get(\"defaultStoragePolicyId\")).byteValue() : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\r\n    return new FsServerDefaults(blockSize, bytesPerChecksum, writePacketSize, replication, fileBufferSize, encryptDataTransfer, trashInterval, type, keyProviderUri, storagepolicyId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toSnapshotDiffReport",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "SnapshotDiffReport toSnapshotDiffReport(final Map<?, ?> json)\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    Map<?, ?> m = (Map<?, ?>) json.get(SnapshotDiffReport.class.getSimpleName());\r\n    String snapshotRoot = (String) m.get(\"snapshotRoot\");\r\n    String fromSnapshot = (String) m.get(\"fromSnapshot\");\r\n    String toSnapshot = (String) m.get(\"toSnapshot\");\r\n    List<SnapshotDiffReport.DiffReportEntry> diffList = toDiffList(getList(m, \"diffList\"));\r\n    return new SnapshotDiffReport(snapshotRoot, fromSnapshot, toSnapshot, diffList);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toDiffList",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<SnapshotDiffReport.DiffReportEntry> toDiffList(List<?> objs)\n{\r\n    if (objs == null) {\r\n        return null;\r\n    }\r\n    List<SnapshotDiffReport.DiffReportEntry> diffList = new ChunkedArrayList<>();\r\n    for (int i = 0; i < objs.size(); i++) {\r\n        diffList.add(toDiffReportEntry((Map<?, ?>) objs.get(i)));\r\n    }\r\n    return diffList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toDiffReportEntry",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "SnapshotDiffReport.DiffReportEntry toDiffReportEntry(Map<?, ?> json)\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    SnapshotDiffReport.DiffType type = SnapshotDiffReport.DiffType.parseDiffType((String) json.get(\"type\"));\r\n    byte[] sourcePath = toByteArray((String) json.get(\"sourcePath\"));\r\n    byte[] targetPath = toByteArray((String) json.get(\"targetPath\"));\r\n    return new SnapshotDiffReport.DiffReportEntry(type, sourcePath, targetPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toSnapshotDiffReportListing",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "SnapshotDiffReportListing toSnapshotDiffReportListing(final Map<?, ?> json)\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    Map<?, ?> m = (Map<?, ?>) json.get(SnapshotDiffReportListing.class.getSimpleName());\r\n    byte[] lastPath = DFSUtilClient.string2Bytes(getString(m, \"lastPath\", \"\"));\r\n    int lastIndex = getInt(m, \"lastIndex\", -1);\r\n    boolean isFromEarlier = getBoolean(m, \"isFromEarlier\", false);\r\n    List<DiffReportListingEntry> modifyList = toDiffListingList(getList(m, \"modifyList\"));\r\n    List<DiffReportListingEntry> createList = toDiffListingList(getList(m, \"createList\"));\r\n    List<DiffReportListingEntry> deleteList = toDiffListingList(getList(m, \"deleteList\"));\r\n    return new SnapshotDiffReportListing(lastPath, modifyList, createList, deleteList, lastIndex, isFromEarlier);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toDiffListingList",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<DiffReportListingEntry> toDiffListingList(List<?> objs)\n{\r\n    if (objs == null) {\r\n        return null;\r\n    }\r\n    List<DiffReportListingEntry> diffList = new ChunkedArrayList<>();\r\n    for (int i = 0; i < objs.size(); i++) {\r\n        diffList.add(toDiffReportListingEntry((Map<?, ?>) objs.get(i)));\r\n    }\r\n    return diffList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toDiffReportListingEntry",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "DiffReportListingEntry toDiffReportListingEntry(Map<?, ?> json)\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    long dirId = getLong(json, \"dirId\", 0);\r\n    long fileId = getLong(json, \"fileId\", 0);\r\n    byte[] sourcePath = toByteArray(getString(json, \"sourcePath\", null));\r\n    byte[] targetPath = toByteArray(getString(json, \"targetPath\", null));\r\n    boolean isReference = getBoolean(json, \"isReference\", false);\r\n    return new DiffReportListingEntry(dirId, fileId, sourcePath, isReference, targetPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toByteArray",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] toByteArray(String str)\n{\r\n    if (str == null) {\r\n        return null;\r\n    }\r\n    return DFSUtilClient.string2Bytes(str);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toSnapshottableDirectoryList",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "SnapshottableDirectoryStatus[] toSnapshottableDirectoryList(final Map<?, ?> json)\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    List<?> list = (List<?>) json.get(\"SnapshottableDirectoryList\");\r\n    if (list == null) {\r\n        return null;\r\n    }\r\n    SnapshottableDirectoryStatus[] statuses = new SnapshottableDirectoryStatus[list.size()];\r\n    for (int i = 0; i < list.size(); i++) {\r\n        statuses[i] = toSnapshottableDirectoryStatus((Map<?, ?>) list.get(i));\r\n    }\r\n    return statuses;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toSnapshottableDirectoryStatus",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "SnapshottableDirectoryStatus toSnapshottableDirectoryStatus(Map<?, ?> json)\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    int snapshotNumber = getInt(json, \"snapshotNumber\", 0);\r\n    int snapshotQuota = getInt(json, \"snapshotQuota\", 0);\r\n    byte[] parentFullPath = toByteArray((String) json.get(\"parentFullPath\"));\r\n    HdfsFileStatus dirStatus = toFileStatus((Map<?, ?>) json.get(\"dirStatus\"), false);\r\n    SnapshottableDirectoryStatus snapshottableDirectoryStatus = new SnapshottableDirectoryStatus(dirStatus, snapshotNumber, snapshotQuota, parentFullPath);\r\n    return snapshottableDirectoryStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toSnapshotList",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "SnapshotStatus[] toSnapshotList(final Map<?, ?> json)\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    List<?> list = (List<?>) json.get(\"SnapshotList\");\r\n    if (list == null) {\r\n        return null;\r\n    }\r\n    SnapshotStatus[] statuses = new SnapshotStatus[list.size()];\r\n    for (int i = 0; i < list.size(); i++) {\r\n        statuses[i] = toSnapshotStatus((Map<?, ?>) list.get(i));\r\n    }\r\n    return statuses;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "toSnapshotStatus",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "SnapshotStatus toSnapshotStatus(Map<?, ?> json)\n{\r\n    if (json == null) {\r\n        return null;\r\n    }\r\n    int snapshotID = getInt(json, \"snapshotID\", 0);\r\n    boolean isDeleted = \"DELETED\".equalsIgnoreCase((String) json.get(\"deletionStatus\"));\r\n    String fullPath = ((String) json.get(\"fullPath\"));\r\n    HdfsFileStatus dirStatus = toFileStatus((Map<?, ?>) json.get(\"dirStatus\"), false);\r\n    SnapshotStatus snapshotStatus = new SnapshotStatus(dirStatus, snapshotID, isDeleted, DFSUtilClient.string2Bytes(SnapshotStatus.getParentPath(fullPath)));\r\n    return snapshotStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\inotify",
  "methodName" : "getTxid",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTxid()\n{\r\n    return txid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\inotify",
  "methodName" : "getEvents",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Event[] getEvents()\n{\r\n    return events;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"bytesWrittenPerSec:\" + bytesWrittenPerSec + \" \" + \" bytesReadPerSec:\" + bytesReadPerSec + \" writeTime:\" + writeTime + \" readTime:\" + readTime + \" blocksWrittenPerSec:\" + blocksWrittenPerSec + \" blocksReadPerSec:\" + blocksReadPerSec + \" timestamp:\" + timestamp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int hashCode()\n{\r\n    return (int) (timestamp + bytesWrittenPerSec + bytesReadPerSec + writeTime + readTime + blocksWrittenPerSec + blocksReadPerSec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (o == this) {\r\n        return true;\r\n    }\r\n    if (!(o instanceof DataNodeUsageReport)) {\r\n        return false;\r\n    }\r\n    DataNodeUsageReport c = (DataNodeUsageReport) o;\r\n    return this.timestamp == c.timestamp && this.readTime == c.readTime && this.writeTime == c.writeTime && this.bytesWrittenPerSec == c.bytesWrittenPerSec && this.bytesReadPerSec == c.bytesReadPerSec && this.blocksWrittenPerSec == c.blocksWrittenPerSec && this.blocksReadPerSec == c.blocksReadPerSec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getBytesWrittenPerSec",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesWrittenPerSec()\n{\r\n    return bytesWrittenPerSec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getBytesReadPerSec",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesReadPerSec()\n{\r\n    return bytesReadPerSec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getWriteTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getWriteTime()\n{\r\n    return writeTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getReadTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getReadTime()\n{\r\n    return readTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getBlocksWrittenPerSec",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBlocksWrittenPerSec()\n{\r\n    return blocksWrittenPerSec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getBlocksReadPerSec",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBlocksReadPerSec()\n{\r\n    return blocksReadPerSec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getTimestamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTimestamp()\n{\r\n    return timestamp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\inotify",
  "methodName" : "getBatches",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<EventBatch> getBatches()\n{\r\n    return batches;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\inotify",
  "methodName" : "getFirstTxid",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFirstTxid()\n{\r\n    return firstTxid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\inotify",
  "methodName" : "getLastTxid",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLastTxid()\n{\r\n    return lastTxid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\inotify",
  "methodName" : "getSyncTxid",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSyncTxid()\n{\r\n    return syncTxid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "setAlignmentContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAlignmentContext(AlignmentContext alignmentContext)\n{\r\n    this.alignmentContext = alignmentContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "createProxy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T createProxy(Configuration conf, InetSocketAddress nnAddr, Class<T> xface, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth) throws IOException\n{\r\n    if (alignmentContext != null) {\r\n        return (T) NameNodeProxiesClient.createProxyWithAlignmentContext(nnAddr, conf, ugi, false, fallbackToSimpleAuth, alignmentContext);\r\n    }\r\n    return (T) NameNodeProxiesClient.createNonHAProxyWithClientProtocol(nnAddr, conf, ugi, false, fallbackToSimpleAuth);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "createProxy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T createProxy(Configuration conf, InetSocketAddress nnAddr, Class<T> xface, UserGroupInformation ugi, boolean withRetries) throws IOException\n{\r\n    return createProxy(conf, nnAddr, xface, ugi, withRetries, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getUsedSpace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getUsedSpace()\n{\r\n    return usedSpace;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFreeSpace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFreeSpace()\n{\r\n    return freeSpace;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getReservedSpace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getReservedSpace()\n{\r\n    return reservedSpace;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getReservedSpaceForReplicas",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getReservedSpaceForReplicas()\n{\r\n    return reservedSpaceForReplicas;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getNumBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumBlocks()\n{\r\n    return numBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getStorageType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StorageType getStorageType()\n{\r\n    return storageType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPath()\n{\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getDatanodeVolumeReport",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getDatanodeVolumeReport()\n{\r\n    StringBuilder report = new StringBuilder();\r\n    report.append(\"Directory: \" + path).append(\"\\nStorageType: \" + storageType).append(\"\\nCapacity Used: \" + usedSpace + \"(\" + StringUtils.byteDesc(usedSpace) + \")\").append(\"\\nCapacity Left: \" + freeSpace + \"(\" + StringUtils.byteDesc(freeSpace) + \")\").append(\"\\nCapacity Reserved: \" + reservedSpace + \"(\" + StringUtils.byteDesc(reservedSpace) + \")\").append(\"\\nReserved Space for Replicas: \" + reservedSpaceForReplicas + \"(\" + StringUtils.byteDesc(reservedSpaceForReplicas) + \")\").append(\"\\nBlocks: \" + numBlocks);\r\n    return report.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getMessage",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getMessage()\n{\r\n    String msg = super.getMessage();\r\n    if (msg == null) {\r\n        return \"The DiskSpace quota\" + (pathName == null ? \"\" : \" of \" + pathName) + \" is exceeded: quota = \" + quota + \" B = \" + long2String(quota, \"B\", 2) + \" but diskspace consumed = \" + count + \" B = \" + long2String(count, \"B\", 2);\r\n    } else {\r\n        return msg;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "HttpURLConnection configure(HttpURLConnection conn) throws IOException\n{\r\n    if (sslConfigurator != null) {\r\n        sslConfigurator.configure(conn);\r\n    }\r\n    String accessToken = accessTokenProvider.getAccessToken();\r\n    conn.setRequestProperty(\"AUTHORIZATION\", HEADER + accessToken);\r\n    return conn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "initTopologyResolution",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void initTopologyResolution(Configuration config)\n{\r\n    topologyResolutionEnabled = config.getBoolean(FS_CLIENT_TOPOLOGY_RESOLUTION_ENABLED, FS_CLIENT_TOPOLOGY_RESOLUTION_ENABLED_DEFAULT);\r\n    if (!topologyResolutionEnabled) {\r\n        return;\r\n    }\r\n    DNSToSwitchMapping dnsToSwitchMapping = ReflectionUtils.newInstance(config.getClass(CommonConfigurationKeys.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, ScriptBasedMapping.class, DNSToSwitchMapping.class), config);\r\n    String clientHostName = NetUtils.getLocalHostname();\r\n    List<String> nodes = new ArrayList<>();\r\n    nodes.add(clientHostName);\r\n    List<String> resolvedHosts = dnsToSwitchMapping.resolve(nodes);\r\n    if (resolvedHosts != null && !resolvedHosts.isEmpty() && !resolvedHosts.get(0).equals(NetworkTopology.DEFAULT_RACK)) {\r\n        this.clientNode = new NodeBase(clientHostName, resolvedHosts.get(0));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ClientContext get(String name, DfsClientConf conf, Configuration config)\n{\r\n    ClientContext context;\r\n    synchronized (ClientContext.class) {\r\n        context = CACHES.get(name);\r\n        if (context == null) {\r\n            context = new ClientContext(name, conf, config);\r\n            CACHES.put(name, context);\r\n        } else {\r\n            context.printConfWarningIfNeeded(conf);\r\n        }\r\n    }\r\n    context.reference();\r\n    return context;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ClientContext get(String name, Configuration config)\n{\r\n    return get(name, new DfsClientConf(config), config);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFromConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ClientContext getFromConf(Configuration conf)\n{\r\n    return get(conf.get(HdfsClientConfigKeys.DFS_CLIENT_CONTEXT, HdfsClientConfigKeys.DFS_CLIENT_CONTEXT_DEFAULT), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "printConfWarningIfNeeded",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void printConfWarningIfNeeded(DfsClientConf conf)\n{\r\n    String existing = this.getConfString();\r\n    String requested = conf.getShortCircuitConf().confAsString();\r\n    if (!existing.equals(requested)) {\r\n        if (!printedConfWarning) {\r\n            printedConfWarning = true;\r\n            LOG.warn(\"Existing client context '\" + name + \"' does not match \" + \"requested configuration.  Existing: \" + existing + \", Requested: \" + requested);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getConfString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getConfString()\n{\r\n    return confString;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getShortCircuitCache",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ShortCircuitCache getShortCircuitCache()\n{\r\n    return shortCircuitCache[0];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getShortCircuitCache",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ShortCircuitCache getShortCircuitCache(long idx)\n{\r\n    return shortCircuitCache[(int) (idx % clientShortCircuitNum)];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getPeerCache",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "PeerCache getPeerCache()\n{\r\n    return peerCache;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getKeyProviderCache",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "KeyProviderCache getKeyProviderCache()\n{\r\n    return keyProviderCache;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getUseLegacyBlockReaderLocal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getUseLegacyBlockReaderLocal()\n{\r\n    return useLegacyBlockReaderLocal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDisableLegacyBlockReaderLocal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getDisableLegacyBlockReaderLocal()\n{\r\n    return disableLegacyBlockReaderLocal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setDisableLegacyBlockReaderLocal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDisableLegacyBlockReaderLocal()\n{\r\n    disableLegacyBlockReaderLocal = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDomainSocketFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DomainSocketFactory getDomainSocketFactory()\n{\r\n    return domainSocketFactory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getByteArrayManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ByteArrayManager getByteArrayManager()\n{\r\n    return byteArrayManager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getNetworkDistance",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int getNetworkDistance(DatanodeInfo datanodeInfo) throws IOException\n{\r\n    if (clientNode == null) {\r\n        return DFSUtilClient.isLocalAddress(NetUtils.createSocketAddr(datanodeInfo.getXferAddr())) ? 0 : Integer.MAX_VALUE;\r\n    }\r\n    NodeBase node = new NodeBase(datanodeInfo.getHostName(), datanodeInfo.getNetworkLocation());\r\n    return NetworkTopology.getDistanceByPath(clientNode, node);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isDeadNodeDetectionEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isDeadNodeDetectionEnabled()\n{\r\n    return deadNodeDetectionEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDeadNodeDetector",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DeadNodeDetector getDeadNodeDetector()\n{\r\n    return deadNodeDetector;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isLocatedBlocksRefresherEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isLocatedBlocksRefresherEnabled()\n{\r\n    return locatedBlocksRefresherEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLocatedBlocksRefresher",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LocatedBlocksRefresher getLocatedBlocksRefresher()\n{\r\n    return locatedBlocksRefresher;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "reference",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void reference()\n{\r\n    counter++;\r\n    if (deadNodeDetectionEnabled && deadNodeDetector == null) {\r\n        deadNodeDetector = new DeadNodeDetector(name, configuration);\r\n        deadNodeDetector.start();\r\n    }\r\n    if (locatedBlocksRefresherEnabled && locatedBlocksRefresher == null) {\r\n        locatedBlocksRefresher = new LocatedBlocksRefresher(name, configuration, dfsClientConf);\r\n        locatedBlocksRefresher.start();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "unreference",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void unreference()\n{\r\n    Preconditions.checkState(counter > 0);\r\n    counter--;\r\n    if (counter == 0 && deadNodeDetectionEnabled && deadNodeDetector != null) {\r\n        deadNodeDetector.shutdown();\r\n        deadNodeDetector = null;\r\n    }\r\n    if (counter == 0 && locatedBlocksRefresherEnabled && locatedBlocksRefresher != null) {\r\n        locatedBlocksRefresher.shutdown();\r\n        locatedBlocksRefresher = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\inotify",
  "methodName" : "getExpectedTxid",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getExpectedTxid()\n{\r\n    return expectedTxid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\inotify",
  "methodName" : "getActualTxid",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getActualTxid()\n{\r\n    return actualTxid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\inotify",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"We expected the next batch of events to start with transaction ID \" + expectedTxid + \", but it instead started with transaction ID \" + actualTxid + \". Most likely the intervening transactions were cleaned \" + \"up as part of checkpointing.\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getResultMessage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getResultMessage()\n{\r\n    return resultMessage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isSupported",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSupported()\n{\r\n    return isSupported;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "initializeSaslServerFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initializeSaslServerFactory()\n{\r\n    if (saslServerFactory == null) {\r\n        saslServerFactory = new FastSaslServerFactory(null);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "initializeSaslClientFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initializeSaslClientFactory()\n{\r\n    if (saslClientFactory == null) {\r\n        saslClientFactory = new FastSaslClientFactory(null);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "createServerSaslParticipant",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SaslParticipant createServerSaslParticipant(Map<String, String> saslProps, CallbackHandler callbackHandler) throws SaslException\n{\r\n    initializeSaslServerFactory();\r\n    return new SaslParticipant(saslServerFactory.createSaslServer(MECHANISM, PROTOCOL, SERVER_NAME, saslProps, callbackHandler));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "createClientSaslParticipant",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SaslParticipant createClientSaslParticipant(String userName, Map<String, String> saslProps, CallbackHandler callbackHandler) throws SaslException\n{\r\n    initializeSaslClientFactory();\r\n    return new SaslParticipant(saslClientFactory.createSaslClient(new String[] { MECHANISM }, userName, PROTOCOL, SERVER_NAME, saslProps, callbackHandler));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "evaluateChallengeOrResponse",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "byte[] evaluateChallengeOrResponse(byte[] challengeOrResponse) throws SaslException\n{\r\n    if (saslClient != null) {\r\n        return saslClient.evaluateChallenge(challengeOrResponse);\r\n    } else {\r\n        return saslServer.evaluateResponse(challengeOrResponse);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "getNegotiatedQop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getNegotiatedQop()\n{\r\n    if (saslClient != null) {\r\n        return (String) saslClient.getNegotiatedProperty(Sasl.QOP);\r\n    } else {\r\n        return (String) saslServer.getNegotiatedProperty(Sasl.QOP);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "isNegotiatedQopPrivacy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isNegotiatedQopPrivacy()\n{\r\n    String qop = getNegotiatedQop();\r\n    return qop != null && \"auth-conf\".equalsIgnoreCase(qop);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "wrap",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "byte[] wrap(byte[] bytes, int off, int len) throws SaslException\n{\r\n    if (saslClient != null) {\r\n        return saslClient.wrap(bytes, off, len);\r\n    } else {\r\n        return saslServer.wrap(bytes, off, len);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "unwrap",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "byte[] unwrap(byte[] bytes, int off, int len) throws SaslException\n{\r\n    if (saslClient != null) {\r\n        return saslClient.unwrap(bytes, off, len);\r\n    } else {\r\n        return saslServer.unwrap(bytes, off, len);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "isComplete",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isComplete()\n{\r\n    if (saslClient != null) {\r\n        return saslClient.isComplete();\r\n    } else {\r\n        return saslServer.isComplete();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "createStreamPair",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStreamPair createStreamPair(DataOutputStream out, DataInputStream in)\n{\r\n    if (saslClient != null) {\r\n        return new IOStreamPair(new SaslInputStream(in, saslClient), new SaslOutputStream(out, saslClient));\r\n    } else {\r\n        return new IOStreamPair(new SaslInputStream(in, saslServer), new SaslOutputStream(out, saslServer));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\delegation",
  "methodName" : "selectToken",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> selectToken(final URI nnUri, Collection<Token<?>> tokens, final Configuration conf)\n{\r\n    Text serviceName = SecurityUtil.buildTokenService(nnUri);\r\n    final String nnServiceName = conf.get(SERVICE_NAME_KEY + serviceName);\r\n    int nnRpcPort = HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT;\r\n    if (nnServiceName != null) {\r\n        nnRpcPort = NetUtils.createSocketAddr(nnServiceName, nnRpcPort).getPort();\r\n    }\r\n    serviceName = SecurityUtil.buildTokenService(NetUtils.createSocketAddrForHost(nnUri.getHost(), nnRpcPort));\r\n    return selectToken(serviceName, tokens);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CacheDirectiveInfo getInfo()\n{\r\n    return info;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getStats",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CacheDirectiveStats getStats()\n{\r\n    return stats;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Long getId()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getPath()\n{\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getReplication",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Short getReplication()\n{\r\n    return replication;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getPool",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPool()\n{\r\n    return pool;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getExpiration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Expiration getExpiration()\n{\r\n    return expiration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (o == null) {\r\n        return false;\r\n    }\r\n    if (getClass() != o.getClass()) {\r\n        return false;\r\n    }\r\n    CacheDirectiveInfo other = (CacheDirectiveInfo) o;\r\n    return new EqualsBuilder().append(getId(), other.getId()).append(getPath(), other.getPath()).append(getReplication(), other.getReplication()).append(getPool(), other.getPool()).append(getExpiration(), other.getExpiration()).isEquals();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return new HashCodeBuilder().append(id).append(path).append(replication).append(pool).append(expiration).hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder builder = new StringBuilder();\r\n    builder.append(\"{\");\r\n    String prefix = \"\";\r\n    if (id != null) {\r\n        builder.append(prefix).append(\"id: \").append(id);\r\n        prefix = \", \";\r\n    }\r\n    if (path != null) {\r\n        builder.append(prefix).append(\"path: \").append(path);\r\n        prefix = \", \";\r\n    }\r\n    if (replication != null) {\r\n        builder.append(prefix).append(\"replication: \").append(replication);\r\n        prefix = \", \";\r\n    }\r\n    if (pool != null) {\r\n        builder.append(prefix).append(\"pool: \").append(pool);\r\n        prefix = \", \";\r\n    }\r\n    if (expiration != null) {\r\n        builder.append(prefix).append(\"expiration: \").append(expiration);\r\n    }\r\n    builder.append(\"}\");\r\n    return builder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createSocketForPipeline",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "Socket createSocketForPipeline(final DatanodeInfo first, final int length, final DFSClient client) throws IOException\n{\r\n    final DfsClientConf conf = client.getConf();\r\n    final String dnAddr = first.getXferAddr(conf.isConnectToDnViaHostname());\r\n    LOG.debug(\"Connecting to datanode {}\", dnAddr);\r\n    final InetSocketAddress isa = NetUtils.createSocketAddr(dnAddr);\r\n    final Socket sock = client.socketFactory.createSocket();\r\n    final int timeout = client.getDatanodeReadTimeout(length);\r\n    NetUtils.connect(sock, isa, client.getRandomLocalInterfaceAddr(), conf.getSocketTimeout());\r\n    sock.setTcpNoDelay(conf.getDataTransferTcpNoDelay());\r\n    sock.setSoTimeout(timeout);\r\n    sock.setKeepAlive(true);\r\n    if (conf.getSocketSendBufferSize() > 0) {\r\n        sock.setSendBufferSize(conf.getSocketSendBufferSize());\r\n    }\r\n    LOG.debug(\"Send buf size {}\", sock.getSendBufferSize());\r\n    return sock;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isLazyPersist",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isLazyPersist(HdfsFileStatus stat)\n{\r\n    return stat.getStoragePolicy() == HdfsConstants.MEMORY_STORAGE_POLICY_ID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "releaseBuffer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void releaseBuffer(List<DFSPacket> packets, ByteArrayManager bam)\n{\r\n    for (DFSPacket p : packets) {\r\n        p.releaseBuffer(bam);\r\n    }\r\n    packets.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setPipelineInConstruction",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setPipelineInConstruction(LocatedBlock lastBlock) throws IOException\n{\r\n    setPipeline(lastBlock);\r\n    if (nodes.length < 1) {\r\n        throw new IOException(\"Unable to retrieve blocks locations \" + \" for last block \" + block + \" of file \" + src);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setAccessToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAccessToken(Token<BlockTokenIdentifier> t)\n{\r\n    this.accessToken = t;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setPipeline",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setPipeline(LocatedBlock lb)\n{\r\n    setPipeline(lb.getLocations(), lb.getStorageTypes(), lb.getStorageIDs());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setPipeline",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setPipeline(DatanodeInfo[] nodes, StorageType[] storageTypes, String[] storageIDs)\n{\r\n    this.nodes = nodes;\r\n    this.storageTypes = storageTypes;\r\n    this.storageIDs = storageIDs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "initDataStreaming",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void initDataStreaming()\n{\r\n    this.setName(\"DataStreamer for file \" + src + \" block \" + block);\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"nodes {} storageTypes {} storageIDs {}\", Arrays.toString(nodes), Arrays.toString(storageTypes), Arrays.toString(storageIDs));\r\n    }\r\n    response = new ResponseProcessor(nodes);\r\n    response.start();\r\n    stage = BlockConstructionStage.DATA_STREAMING;\r\n    lastPacket = Time.monotonicNow();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "endBlock",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void endBlock()\n{\r\n    LOG.debug(\"Closing old block {}\", block);\r\n    this.setName(\"DataStreamer for file \" + src);\r\n    closeResponder();\r\n    closeStream();\r\n    setPipeline(null, null, null);\r\n    stage = BlockConstructionStage.PIPELINE_SETUP_CREATE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "shouldStop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldStop()\n{\r\n    return streamerClosed || errorState.hasError() || !dfsClient.clientRunning;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "run",
  "errType" : [ "Throwable", "InterruptedException", "InterruptedException", "IOException", "IOException" ],
  "containingMethodsNum" : 56,
  "sourceCodeText" : "void run()\n{\r\n    TraceScope scope = null;\r\n    while (!streamerClosed && dfsClient.clientRunning) {\r\n        if (errorState.hasError()) {\r\n            closeResponder();\r\n        }\r\n        DFSPacket one;\r\n        try {\r\n            boolean doSleep = processDatanodeOrExternalError();\r\n            synchronized (dataQueue) {\r\n                while ((!shouldStop() && dataQueue.isEmpty()) || doSleep) {\r\n                    long timeout = 1000;\r\n                    if (stage == BlockConstructionStage.DATA_STREAMING) {\r\n                        timeout = sendHeartbeat();\r\n                    }\r\n                    try {\r\n                        dataQueue.wait(timeout);\r\n                    } catch (InterruptedException e) {\r\n                        LOG.debug(\"Thread interrupted\", e);\r\n                    }\r\n                    doSleep = false;\r\n                }\r\n                if (shouldStop()) {\r\n                    continue;\r\n                }\r\n                one = dataQueue.getFirst();\r\n                SpanContext[] parents = one.getTraceParents();\r\n                if (parents != null && parents.length > 0) {\r\n                    scope = dfsClient.getTracer().newScope(\"dataStreamer\", parents[0], false);\r\n                }\r\n            }\r\n            try {\r\n                backOffIfNecessary();\r\n            } catch (InterruptedException e) {\r\n                LOG.debug(\"Thread interrupted\", e);\r\n            }\r\n            LOG.debug(\"stage={}, {}\", stage, this);\r\n            if (stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) {\r\n                LOG.debug(\"Allocating new block: {}\", this);\r\n                setPipeline(nextBlockOutputStream());\r\n                initDataStreaming();\r\n            } else if (stage == BlockConstructionStage.PIPELINE_SETUP_APPEND) {\r\n                LOG.debug(\"Append to block {}\", block);\r\n                setupPipelineForAppendOrRecovery();\r\n                if (streamerClosed) {\r\n                    continue;\r\n                }\r\n                initDataStreaming();\r\n            }\r\n            long lastByteOffsetInBlock = one.getLastByteOffsetBlock();\r\n            if (lastByteOffsetInBlock > stat.getBlockSize()) {\r\n                throw new IOException(\"BlockSize \" + stat.getBlockSize() + \" < lastByteOffsetInBlock, \" + this + \", \" + one);\r\n            }\r\n            if (one.isLastPacketInBlock()) {\r\n                waitForAllAcks();\r\n                if (shouldStop()) {\r\n                    continue;\r\n                }\r\n                stage = BlockConstructionStage.PIPELINE_CLOSE;\r\n            }\r\n            SpanContext spanContext = null;\r\n            synchronized (dataQueue) {\r\n                if (!one.isHeartbeatPacket()) {\r\n                    if (scope != null) {\r\n                        one.setSpan(scope.span());\r\n                        spanContext = scope.span().getContext();\r\n                        scope.close();\r\n                    }\r\n                    scope = null;\r\n                    dataQueue.removeFirst();\r\n                    ackQueue.addLast(one);\r\n                    packetSendTime.put(one.getSeqno(), Time.monotonicNow());\r\n                    dataQueue.notifyAll();\r\n                }\r\n            }\r\n            LOG.debug(\"{} sending {}\", this, one);\r\n            try (TraceScope ignored = dfsClient.getTracer().newScope(\"DataStreamer#writeTo\", spanContext)) {\r\n                sendPacket(one);\r\n            } catch (IOException e) {\r\n                errorState.markFirstNodeIfNotMarked();\r\n                throw e;\r\n            }\r\n            long tmpBytesSent = one.getLastByteOffsetBlock();\r\n            if (bytesSent < tmpBytesSent) {\r\n                bytesSent = tmpBytesSent;\r\n            }\r\n            if (shouldStop()) {\r\n                continue;\r\n            }\r\n            if (one.isLastPacketInBlock()) {\r\n                try {\r\n                    waitForAllAcks();\r\n                } catch (IOException ioe) {\r\n                    synchronized (dataQueue) {\r\n                        if (!ackQueue.isEmpty()) {\r\n                            throw ioe;\r\n                        }\r\n                    }\r\n                }\r\n                if (shouldStop()) {\r\n                    continue;\r\n                }\r\n                endBlock();\r\n            }\r\n            if (progress != null) {\r\n                progress.progress();\r\n            }\r\n            if (artificialSlowdown != 0 && dfsClient.clientRunning) {\r\n                Thread.sleep(artificialSlowdown);\r\n            }\r\n        } catch (Throwable e) {\r\n            if (!errorState.isRestartingNode()) {\r\n                if (e instanceof QuotaExceededException) {\r\n                    LOG.debug(\"DataStreamer Quota Exception\", e);\r\n                } else {\r\n                    LOG.warn(\"DataStreamer Exception\", e);\r\n                }\r\n            }\r\n            lastException.set(e);\r\n            assert !(e instanceof NullPointerException);\r\n            errorState.setInternalError();\r\n            if (!errorState.isNodeMarked()) {\r\n                streamerClosed = true;\r\n            }\r\n        } finally {\r\n            if (scope != null) {\r\n                scope.close();\r\n                scope = null;\r\n            }\r\n        }\r\n    }\r\n    closeInternal();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 5,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "waitForAllAcks",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void waitForAllAcks() throws IOException\n{\r\n    synchronized (dataQueue) {\r\n        while (!shouldStop() && !ackQueue.isEmpty()) {\r\n            try {\r\n                dataQueue.wait(sendHeartbeat());\r\n            } catch (InterruptedException e) {\r\n                LOG.debug(\"Thread interrupted \", e);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "sendPacket",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void sendPacket(DFSPacket packet) throws IOException\n{\r\n    try {\r\n        packet.writeTo(blockStream);\r\n        blockStream.flush();\r\n    } catch (IOException e) {\r\n        errorState.markFirstNodeIfNotMarked();\r\n        throw e;\r\n    }\r\n    lastPacket = Time.monotonicNow();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "sendHeartbeat",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long sendHeartbeat() throws IOException\n{\r\n    final long heartbeatInterval = dfsClient.getConf().getSocketTimeout() / 2;\r\n    long timeout = heartbeatInterval - (Time.monotonicNow() - lastPacket);\r\n    if (timeout <= 0) {\r\n        sendPacket(createHeartbeatPacket());\r\n        timeout = heartbeatInterval;\r\n    }\r\n    return timeout;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "closeInternal",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void closeInternal()\n{\r\n    closeResponder();\r\n    closeStream();\r\n    streamerClosed = true;\r\n    release();\r\n    synchronized (dataQueue) {\r\n        dataQueue.notifyAll();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "release",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void release()\n{\r\n    synchronized (dataQueue) {\r\n        releaseBuffer(dataQueue, byteArrayManager);\r\n        releaseBuffer(ackQueue, byteArrayManager);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "waitForAckedSeqno",
  "errType" : [ "ClosedChannelException", "InterruptedException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void waitForAckedSeqno(long seqno) throws IOException\n{\r\n    try (TraceScope ignored = dfsClient.getTracer().newScope(\"waitForAckedSeqno\")) {\r\n        LOG.debug(\"{} waiting for ack for: {}\", this, seqno);\r\n        int dnodes = nodes != null ? nodes.length : 3;\r\n        int writeTimeout = dfsClient.getDatanodeWriteTimeout(dnodes);\r\n        long begin = Time.monotonicNow();\r\n        try {\r\n            synchronized (dataQueue) {\r\n                while (!streamerClosed) {\r\n                    checkClosed();\r\n                    if (lastAckedSeqno >= seqno) {\r\n                        break;\r\n                    }\r\n                    try {\r\n                        dataQueue.wait(1000);\r\n                        long duration = Time.monotonicNow() - begin;\r\n                        if (duration > writeTimeout) {\r\n                            LOG.error(\"No ack received, took {}ms (threshold={}ms). \" + \"File being written: {}, block: {}, \" + \"Write pipeline datanodes: {}.\", duration, writeTimeout, src, block, nodes);\r\n                            throw new InterruptedIOException(\"No ack received after \" + duration / 1000 + \"s and a timeout of \" + writeTimeout / 1000 + \"s\");\r\n                        }\r\n                    } catch (InterruptedException ie) {\r\n                        throw new InterruptedIOException(\"Interrupted while waiting for data to be acknowledged by pipeline\");\r\n                    }\r\n                }\r\n            }\r\n            checkClosed();\r\n        } catch (ClosedChannelException cce) {\r\n            LOG.debug(\"Closed channel exception\", cce);\r\n        }\r\n        long duration = Time.monotonicNow() - begin;\r\n        if (duration > dfsclientSlowLogThresholdMs) {\r\n            LOG.warn(\"Slow waitForAckedSeqno took {}ms (threshold={}ms). File being\" + \" written: {}, block: {}, Write pipeline datanodes: {}.\", duration, dfsclientSlowLogThresholdMs, src, block, nodes);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "waitAndQueuePacket",
  "errType" : [ "ClosedChannelException", "InterruptedException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void waitAndQueuePacket(DFSPacket packet) throws IOException\n{\r\n    synchronized (dataQueue) {\r\n        try {\r\n            boolean firstWait = true;\r\n            try {\r\n                while (!streamerClosed && dataQueue.size() + ackQueue.size() > dfsClient.getConf().getWriteMaxPackets()) {\r\n                    if (firstWait) {\r\n                        Span span = Tracer.getCurrentSpan();\r\n                        if (span != null) {\r\n                            span.addTimelineAnnotation(\"dataQueue.wait\");\r\n                        }\r\n                        firstWait = false;\r\n                    }\r\n                    try {\r\n                        dataQueue.wait();\r\n                    } catch (InterruptedException e) {\r\n                        Thread.currentThread().interrupt();\r\n                        break;\r\n                    }\r\n                }\r\n            } finally {\r\n                Span span = Tracer.getCurrentSpan();\r\n                if ((span != null) && (!firstWait)) {\r\n                    span.addTimelineAnnotation(\"end.wait\");\r\n                }\r\n            }\r\n            checkClosed();\r\n            queuePacket(packet);\r\n        } catch (ClosedChannelException cce) {\r\n            LOG.debug(\"Closed channel exception\", cce);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close(boolean force)\n{\r\n    streamerClosed = true;\r\n    synchronized (dataQueue) {\r\n        dataQueue.notifyAll();\r\n    }\r\n    if (force) {\r\n        this.interrupt();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setStreamerAsClosed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStreamerAsClosed()\n{\r\n    streamerClosed = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkClosed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkClosed() throws IOException\n{\r\n    if (streamerClosed) {\r\n        lastException.throwException4Close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "closeResponder",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void closeResponder()\n{\r\n    if (response != null) {\r\n        try {\r\n            response.close();\r\n            response.join();\r\n        } catch (InterruptedException e) {\r\n            LOG.debug(\"Thread interrupted\", e);\r\n            Thread.currentThread().interrupt();\r\n        } finally {\r\n            response = null;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "closeStream",
  "errType" : [ "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void closeStream()\n{\r\n    final MultipleIOException.Builder b = new MultipleIOException.Builder();\r\n    if (blockStream != null) {\r\n        try {\r\n            blockStream.close();\r\n        } catch (IOException e) {\r\n            b.add(e);\r\n        } finally {\r\n            blockStream = null;\r\n        }\r\n    }\r\n    if (blockReplyStream != null) {\r\n        try {\r\n            blockReplyStream.close();\r\n        } catch (IOException e) {\r\n            b.add(e);\r\n        } finally {\r\n            blockReplyStream = null;\r\n        }\r\n    }\r\n    if (null != s) {\r\n        try {\r\n            s.close();\r\n        } catch (IOException e) {\r\n            b.add(e);\r\n        } finally {\r\n            s = null;\r\n        }\r\n    }\r\n    final IOException ioe = b.build();\r\n    if (ioe != null) {\r\n        lastException.set(ioe);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "shouldWaitForRestart",
  "errType" : [ "java.net.UnknownHostException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean shouldWaitForRestart(int index)\n{\r\n    if (nodes.length == 1) {\r\n        return true;\r\n    }\r\n    if (DFSClientFaultInjector.get().skipRollingRestartWait()) {\r\n        return false;\r\n    }\r\n    InetAddress addr = null;\r\n    try {\r\n        addr = InetAddress.getByName(nodes[index].getIpAddr());\r\n    } catch (java.net.UnknownHostException e) {\r\n        assert false;\r\n    }\r\n    return addr != null && NetUtils.isLocalAddress(addr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "shouldHandleExternalError",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldHandleExternalError()\n{\r\n    return errorState.hasExternalError() && blockStream != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "processDatanodeOrExternalError",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "boolean processDatanodeOrExternalError() throws IOException\n{\r\n    if (!errorState.hasDatanodeError() && !shouldHandleExternalError()) {\r\n        return false;\r\n    }\r\n    LOG.debug(\"start process datanode/external error, {}\", this);\r\n    if (response != null) {\r\n        LOG.info(\"Error Recovery for \" + block + \" waiting for responder to exit. \");\r\n        return true;\r\n    }\r\n    closeStream();\r\n    synchronized (dataQueue) {\r\n        dataQueue.addAll(0, ackQueue);\r\n        ackQueue.clear();\r\n        packetSendTime.clear();\r\n    }\r\n    if (!errorState.isRestartingNode() && ++pipelineRecoveryCount > maxPipelineRecoveryRetries) {\r\n        LOG.warn(\"Error recovering pipeline for writing \" + block + \". Already retried \" + maxPipelineRecoveryRetries + \" times for the same packet.\");\r\n        lastException.set(new IOException(\"Failing write. Tried pipeline \" + \"recovery \" + maxPipelineRecoveryRetries + \" times without success.\"));\r\n        streamerClosed = true;\r\n        return false;\r\n    }\r\n    setupPipelineForAppendOrRecovery();\r\n    if (!streamerClosed && dfsClient.clientRunning) {\r\n        if (stage == BlockConstructionStage.PIPELINE_CLOSE) {\r\n            synchronized (dataQueue) {\r\n                DFSPacket endOfBlockPacket = dataQueue.remove();\r\n                Span span = endOfBlockPacket.getSpan();\r\n                if (span != null) {\r\n                    span.finish();\r\n                    endOfBlockPacket.setSpan(null);\r\n                }\r\n                assert endOfBlockPacket.isLastPacketInBlock();\r\n                assert lastAckedSeqno == endOfBlockPacket.getSeqno() - 1;\r\n                lastAckedSeqno = endOfBlockPacket.getSeqno();\r\n                pipelineRecoveryCount = 0;\r\n                dataQueue.notifyAll();\r\n            }\r\n            endBlock();\r\n        } else {\r\n            initDataStreaming();\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setHflush",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHflush()\n{\r\n    isHflushed = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "findNewDatanode",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int findNewDatanode(final DatanodeInfo[] original) throws IOException\n{\r\n    if (nodes.length != original.length + 1) {\r\n        throw new IOException(\"Failed to replace a bad datanode on the existing pipeline \" + \"due to no more good datanodes being available to try. \" + \"(Nodes: current=\" + Arrays.asList(nodes) + \", original=\" + Arrays.asList(original) + \"). \" + \"The current failed datanode replacement policy is \" + dfsClient.dtpReplaceDatanodeOnFailure + \", and a client may configure this via '\" + BlockWrite.ReplaceDatanodeOnFailure.POLICY_KEY + \"' in its configuration.\");\r\n    }\r\n    for (int i = 0; i < nodes.length; i++) {\r\n        int j = 0;\r\n        for (; j < original.length && !nodes[i].equals(original[j]); j++) ;\r\n        if (j == original.length) {\r\n            return i;\r\n        }\r\n    }\r\n    throw new IOException(\"Failed: new datanode not found: nodes=\" + Arrays.asList(nodes) + \", original=\" + Arrays.asList(original));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addDatanode2ExistingPipeline",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void addDatanode2ExistingPipeline() throws IOException\n{\r\n    DataTransferProtocol.LOG.debug(\"lastAckedSeqno = {}\", lastAckedSeqno);\r\n    if (!isAppend && lastAckedSeqno < 0 && stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) {\r\n        return;\r\n    }\r\n    int tried = 0;\r\n    final DatanodeInfo[] original = nodes;\r\n    final StorageType[] originalTypes = storageTypes;\r\n    final String[] originalIDs = storageIDs;\r\n    IOException caughtException = null;\r\n    ArrayList<DatanodeInfo> exclude = new ArrayList<>(failed);\r\n    while (tried < 3) {\r\n        LocatedBlock lb;\r\n        lb = dfsClient.namenode.getAdditionalDatanode(src, stat.getFileId(), block.getCurrentBlock(), nodes, storageIDs, exclude.toArray(new DatanodeInfo[exclude.size()]), 1, dfsClient.clientName);\r\n        setPipeline(lb);\r\n        final int d;\r\n        try {\r\n            d = findNewDatanode(original);\r\n        } catch (IOException ioe) {\r\n            if (dfsClient.dtpReplaceDatanodeOnFailureReplication > 0 && nodes.length >= dfsClient.dtpReplaceDatanodeOnFailureReplication) {\r\n                DFSClient.LOG.warn(\"Failed to find a new datanode to add to the write pipeline,\" + \" continue to write to the pipeline with \" + nodes.length + \" nodes since it's no less than minimum replication: \" + dfsClient.dtpReplaceDatanodeOnFailureReplication + \" configured by \" + BlockWrite.ReplaceDatanodeOnFailure.MIN_REPLICATION + \".\", ioe);\r\n                return;\r\n            }\r\n            throw ioe;\r\n        }\r\n        final DatanodeInfo src = original[tried % original.length];\r\n        final DatanodeInfo[] targets = { nodes[d] };\r\n        final StorageType[] targetStorageTypes = { storageTypes[d] };\r\n        final String[] targetStorageIDs = { storageIDs[d] };\r\n        try {\r\n            transfer(src, targets, targetStorageTypes, targetStorageIDs, lb.getBlockToken());\r\n        } catch (IOException ioe) {\r\n            DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" + nodes[d] + \": \" + ioe.getMessage());\r\n            caughtException = ioe;\r\n            exclude.add(nodes[d]);\r\n            setPipeline(original, originalTypes, originalIDs);\r\n            tried++;\r\n            continue;\r\n        }\r\n        return;\r\n    }\r\n    throw (caughtException != null) ? caughtException : new IOException(\"Failed to add a node\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "computeTransferWriteTimeout",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long computeTransferWriteTimeout()\n{\r\n    return dfsClient.getDatanodeWriteTimeout(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "computeTransferReadTimeout",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long computeTransferReadTimeout()\n{\r\n    int multi = 2 + (int) (bytesSent / dfsClient.getConf().getWritePacketSize()) / 200;\r\n    return dfsClient.getDatanodeReadTimeout(multi);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "transfer",
  "errType" : [ "InvalidEncryptionKeyException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void transfer(final DatanodeInfo src, final DatanodeInfo[] targets, final StorageType[] targetStorageTypes, final String[] targetStorageIDs, final Token<BlockTokenIdentifier> blockToken) throws IOException\n{\r\n    RefetchEncryptionKeyPolicy policy = new RefetchEncryptionKeyPolicy(src);\r\n    do {\r\n        StreamerStreams streams = null;\r\n        try {\r\n            final long writeTimeout = computeTransferWriteTimeout();\r\n            final long readTimeout = computeTransferReadTimeout();\r\n            streams = new StreamerStreams(src, writeTimeout, readTimeout, blockToken);\r\n            streams.sendTransferBlock(targets, targetStorageTypes, targetStorageIDs, blockToken);\r\n            return;\r\n        } catch (InvalidEncryptionKeyException e) {\r\n            policy.recordFailure(e);\r\n        } finally {\r\n            IOUtils.closeStream(streams);\r\n        }\r\n    } while (policy.continueRetryingOrThrow());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setupPipelineForAppendOrRecovery",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setupPipelineForAppendOrRecovery() throws IOException\n{\r\n    if (nodes == null || nodes.length == 0) {\r\n        String msg = \"Could not get block locations. \" + \"Source file \\\"\" + src + \"\\\" - Aborting...\" + this;\r\n        LOG.warn(msg);\r\n        lastException.set(new IOException(msg));\r\n        streamerClosed = true;\r\n        return;\r\n    }\r\n    setupPipelineInternal(nodes, storageTypes, storageIDs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setupPipelineInternal",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void setupPipelineInternal(DatanodeInfo[] datanodes, StorageType[] nodeStorageTypes, String[] nodeStorageIDs) throws IOException\n{\r\n    boolean success = false;\r\n    long newGS = 0L;\r\n    while (!success && !streamerClosed && dfsClient.clientRunning) {\r\n        if (!handleRestartingDatanode()) {\r\n            return;\r\n        }\r\n        final boolean isRecovery = errorState.hasInternalError();\r\n        if (!handleBadDatanode()) {\r\n            return;\r\n        }\r\n        handleDatanodeReplacement();\r\n        final LocatedBlock lb = updateBlockForPipeline();\r\n        newGS = lb.getBlock().getGenerationStamp();\r\n        accessToken = lb.getBlockToken();\r\n        success = createBlockOutputStream(nodes, storageTypes, storageIDs, newGS, isRecovery);\r\n        failPacket4Testing();\r\n        errorState.checkRestartingNodeDeadline(nodes);\r\n    }\r\n    if (success) {\r\n        updatePipeline(newGS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "handleRestartingDatanode",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean handleRestartingDatanode()\n{\r\n    if (errorState.isRestartingNode()) {\r\n        if (!errorState.doWaitForRestart()) {\r\n            errorState.setBadNodeIndex(errorState.getRestartingNodeIndex());\r\n            return true;\r\n        }\r\n        final long delay = Math.min(errorState.datanodeRestartTimeout, 4000L);\r\n        try {\r\n            Thread.sleep(delay);\r\n        } catch (InterruptedException ie) {\r\n            lastException.set(new IOException(\"Interrupted while waiting for restarting \" + nodes[errorState.getRestartingNodeIndex()]));\r\n            streamerClosed = true;\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "handleBadDatanode",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "boolean handleBadDatanode()\n{\r\n    final int badNodeIndex = errorState.getBadNodeIndex();\r\n    if (badNodeIndex >= 0) {\r\n        if (nodes.length <= 1) {\r\n            lastException.set(new IOException(\"All datanodes \" + Arrays.toString(nodes) + \" are bad. Aborting...\"));\r\n            streamerClosed = true;\r\n            return false;\r\n        }\r\n        String reason = \"bad.\";\r\n        if (errorState.getRestartingNodeIndex() == badNodeIndex) {\r\n            reason = \"restarting.\";\r\n            restartingNodes.add(nodes[badNodeIndex]);\r\n        }\r\n        LOG.warn(\"Error Recovery for \" + block + \" in pipeline \" + Arrays.toString(nodes) + \": datanode \" + badNodeIndex + \"(\" + nodes[badNodeIndex] + \") is \" + reason);\r\n        failed.add(nodes[badNodeIndex]);\r\n        DatanodeInfo[] newnodes = new DatanodeInfo[nodes.length - 1];\r\n        arraycopy(nodes, newnodes, badNodeIndex);\r\n        final StorageType[] newStorageTypes = new StorageType[newnodes.length];\r\n        arraycopy(storageTypes, newStorageTypes, badNodeIndex);\r\n        final String[] newStorageIDs = new String[newnodes.length];\r\n        arraycopy(storageIDs, newStorageIDs, badNodeIndex);\r\n        setPipeline(newnodes, newStorageTypes, newStorageIDs);\r\n        errorState.adjustState4RestartingNode();\r\n        lastException.clear();\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "handleDatanodeReplacement",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void handleDatanodeReplacement() throws IOException\n{\r\n    if (dfsClient.dtpReplaceDatanodeOnFailure.satisfy(stat.getReplication(), nodes, isAppend, isHflushed)) {\r\n        try {\r\n            addDatanode2ExistingPipeline();\r\n        } catch (IOException ioe) {\r\n            if (!dfsClient.dtpReplaceDatanodeOnFailure.isBestEffort()) {\r\n                throw ioe;\r\n            }\r\n            LOG.warn(\"Failed to replace datanode.\" + \" Continue with the remaining datanodes since \" + BlockWrite.ReplaceDatanodeOnFailure.BEST_EFFORT_KEY + \" is set to true.\", ioe);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "failPacket4Testing",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void failPacket4Testing()\n{\r\n    if (failPacket) {\r\n        failPacket = false;\r\n        try {\r\n            Thread.sleep(2000);\r\n        } catch (InterruptedException e) {\r\n            LOG.debug(\"Thread interrupted\", e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "updateBlockForPipeline",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocatedBlock updateBlockForPipeline() throws IOException\n{\r\n    return dfsClient.namenode.updateBlockForPipeline(block.getCurrentBlock(), dfsClient.clientName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "updateBlockGS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateBlockGS(final long newGS)\n{\r\n    block.setGenerationStamp(newGS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "updatePipeline",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void updatePipeline(long newGS) throws IOException\n{\r\n    final ExtendedBlock oldBlock = block.getCurrentBlock();\r\n    updateBlockGS(newGS);\r\n    dfsClient.namenode.updatePipeline(dfsClient.clientName, oldBlock, block.getCurrentBlock(), nodes, storageIDs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getExcludedNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DatanodeInfo[] getExcludedNodes()\n{\r\n    return excludedNodes.getAllPresent(excludedNodes.asMap().keySet()).keySet().toArray(DatanodeInfo.EMPTY_ARRAY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "nextBlockOutputStream",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "LocatedBlock nextBlockOutputStream() throws IOException\n{\r\n    LocatedBlock lb;\r\n    DatanodeInfo[] nodes;\r\n    StorageType[] nextStorageTypes;\r\n    String[] nextStorageIDs;\r\n    int count = dfsClient.getConf().getNumBlockWriteRetry();\r\n    boolean success;\r\n    final ExtendedBlock oldBlock = block.getCurrentBlock();\r\n    do {\r\n        errorState.resetInternalError();\r\n        lastException.clear();\r\n        DatanodeInfo[] excluded = getExcludedNodes();\r\n        lb = locateFollowingBlock(excluded.length > 0 ? excluded : null, oldBlock);\r\n        block.setCurrentBlock(lb.getBlock());\r\n        block.setNumBytes(0);\r\n        bytesSent = 0;\r\n        accessToken = lb.getBlockToken();\r\n        nodes = lb.getLocations();\r\n        nextStorageTypes = lb.getStorageTypes();\r\n        nextStorageIDs = lb.getStorageIDs();\r\n        success = createBlockOutputStream(nodes, nextStorageTypes, nextStorageIDs, 0L, false);\r\n        if (!success) {\r\n            LOG.warn(\"Abandoning \" + block);\r\n            dfsClient.namenode.abandonBlock(block.getCurrentBlock(), stat.getFileId(), src, dfsClient.clientName);\r\n            block.setCurrentBlock(null);\r\n            final DatanodeInfo badNode = nodes[errorState.getBadNodeIndex()];\r\n            LOG.warn(\"Excluding datanode \" + badNode);\r\n            excludedNodes.put(badNode, badNode);\r\n        }\r\n    } while (!success && --count >= 0);\r\n    if (!success) {\r\n        throw new IOException(\"Unable to create new block.\");\r\n    }\r\n    return lb;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createBlockOutputStream",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 41,
  "sourceCodeText" : "boolean createBlockOutputStream(DatanodeInfo[] nodes, StorageType[] nodeStorageTypes, String[] nodeStorageIDs, long newGS, boolean recoveryFlag)\n{\r\n    if (nodes.length == 0) {\r\n        LOG.info(\"nodes are empty for write pipeline of \" + block);\r\n        return false;\r\n    }\r\n    String firstBadLink = \"\";\r\n    boolean checkRestart = false;\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"pipeline = \" + Arrays.toString(nodes) + \", \" + this);\r\n    }\r\n    persistBlocks.set(true);\r\n    int refetchEncryptionKey = 1;\r\n    while (true) {\r\n        boolean result = false;\r\n        DataOutputStream out = null;\r\n        try {\r\n            assert null == s : \"Previous socket unclosed\";\r\n            assert null == blockReplyStream : \"Previous blockReplyStream unclosed\";\r\n            s = createSocketForPipeline(nodes[0], nodes.length, dfsClient);\r\n            long writeTimeout = dfsClient.getDatanodeWriteTimeout(nodes.length);\r\n            long readTimeout = dfsClient.getDatanodeReadTimeout(nodes.length);\r\n            OutputStream unbufOut = NetUtils.getOutputStream(s, writeTimeout);\r\n            InputStream unbufIn = NetUtils.getInputStream(s, readTimeout);\r\n            IOStreamPair saslStreams = dfsClient.saslClient.socketSend(s, unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\r\n            unbufOut = saslStreams.out;\r\n            unbufIn = saslStreams.in;\r\n            out = new DataOutputStream(new BufferedOutputStream(unbufOut, DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\r\n            blockReplyStream = new DataInputStream(unbufIn);\r\n            BlockConstructionStage bcs = recoveryFlag ? stage.getRecoveryStage() : stage;\r\n            ExtendedBlock blockCopy = block.getCurrentBlock();\r\n            blockCopy.setNumBytes(stat.getBlockSize());\r\n            boolean[] targetPinnings = getPinnings(nodes);\r\n            new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken, dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, nodes.length, block.getNumBytes(), bytesSent, newGS, checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile, (targetPinnings != null && targetPinnings[0]), targetPinnings, nodeStorageIDs[0], nodeStorageIDs);\r\n            BlockOpResponseProto resp = BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(blockReplyStream));\r\n            Status pipelineStatus = resp.getStatus();\r\n            firstBadLink = resp.getFirstBadLink();\r\n            if (PipelineAck.isRestartOOBStatus(pipelineStatus) && !errorState.isRestartingNode()) {\r\n                checkRestart = true;\r\n                throw new IOException(\"A datanode is restarting.\");\r\n            }\r\n            String logInfo = \"ack with firstBadLink as \" + firstBadLink;\r\n            DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\r\n            assert null == blockStream : \"Previous blockStream unclosed\";\r\n            blockStream = out;\r\n            result = true;\r\n            errorState.resetInternalError();\r\n            lastException.clear();\r\n            failed.removeAll(restartingNodes);\r\n            restartingNodes.clear();\r\n        } catch (IOException ie) {\r\n            if (!errorState.isRestartingNode()) {\r\n                LOG.warn(\"Exception in createBlockOutputStream \" + this, ie);\r\n            }\r\n            if (ie instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0) {\r\n                LOG.info(\"Will fetch a new encryption key and retry, \" + \"encryption key was invalid when connecting to \" + nodes[0] + \" : \" + ie);\r\n                refetchEncryptionKey--;\r\n                dfsClient.clearDataEncryptionKey();\r\n                continue;\r\n            }\r\n            if (firstBadLink.length() != 0) {\r\n                for (int i = 0; i < nodes.length; i++) {\r\n                    if (firstBadLink.equals(nodes[i].getXferAddr())) {\r\n                        errorState.setBadNodeIndex(i);\r\n                        break;\r\n                    }\r\n                }\r\n            } else {\r\n                assert !checkRestart;\r\n                errorState.setBadNodeIndex(0);\r\n            }\r\n            final int i = errorState.getBadNodeIndex();\r\n            if (checkRestart) {\r\n                errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \" + nodes[i], shouldWaitForRestart(i));\r\n            }\r\n            errorState.setInternalError();\r\n            lastException.set(ie);\r\n            result = false;\r\n        } finally {\r\n            if (!result) {\r\n                IOUtils.closeSocket(s);\r\n                s = null;\r\n                IOUtils.closeStream(out);\r\n                IOUtils.closeStream(blockReplyStream);\r\n                blockReplyStream = null;\r\n            }\r\n        }\r\n        return result;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getPinnings",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean[] getPinnings(DatanodeInfo[] nodes)\n{\r\n    if (favoredNodes == null) {\r\n        return null;\r\n    } else {\r\n        boolean[] pinnings = new boolean[nodes.length];\r\n        HashSet<String> favoredSet = new HashSet<>(Arrays.asList(favoredNodes));\r\n        for (int i = 0; i < nodes.length; i++) {\r\n            pinnings[i] = favoredSet.remove(nodes[i].getXferAddrWithHostname());\r\n            LOG.debug(\"{} was chosen by name node (favored={}).\", nodes[i].getXferAddrWithHostname(), pinnings[i]);\r\n        }\r\n        if (!favoredSet.isEmpty()) {\r\n            LOG.warn(\"These favored nodes were specified but not chosen: \" + favoredSet + \" Specified favored nodes: \" + Arrays.toString(favoredNodes));\r\n        }\r\n        return pinnings;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "locateFollowingBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocatedBlock locateFollowingBlock(DatanodeInfo[] excluded, ExtendedBlock oldBlock) throws IOException\n{\r\n    return DFSOutputStream.addBlock(excluded, dfsClient, src, oldBlock, stat.getFileId(), favoredNodes, addBlockFlags);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "backOffIfNecessary",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void backOffIfNecessary() throws InterruptedException\n{\r\n    int t = 0;\r\n    synchronized (congestedNodes) {\r\n        if (!congestedNodes.isEmpty()) {\r\n            StringBuilder sb = new StringBuilder(\"DataNode\");\r\n            for (DatanodeInfo i : congestedNodes) {\r\n                sb.append(' ').append(i);\r\n            }\r\n            int range = Math.abs(lastCongestionBackoffTime * 3 - CONGESTION_BACKOFF_MEAN_TIME_IN_MS);\r\n            int base = Math.min(lastCongestionBackoffTime * 3, CONGESTION_BACKOFF_MEAN_TIME_IN_MS);\r\n            t = Math.min(CONGESTION_BACK_OFF_MAX_TIME_IN_MS, (int) (base + Math.random() * range));\r\n            lastCongestionBackoffTime = t;\r\n            sb.append(\" are congested. Backing off for \").append(t).append(\" ms\");\r\n            LOG.info(sb.toString());\r\n            congestedNodes.clear();\r\n        }\r\n    }\r\n    if (t != 0) {\r\n        Thread.sleep(t);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExtendedBlock getBlock()\n{\r\n    return block.getCurrentBlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DatanodeInfo[] getNodes()\n{\r\n    return nodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStorageIDs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] getStorageIDs()\n{\r\n    return storageIDs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockConstructionStage getStage()\n{\r\n    return stage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBlockToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Token<BlockTokenIdentifier> getBlockToken()\n{\r\n    return accessToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getErrorState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ErrorState getErrorState()\n{\r\n    return errorState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "queuePacket",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void queuePacket(DFSPacket packet)\n{\r\n    synchronized (dataQueue) {\r\n        if (packet == null)\r\n            return;\r\n        packet.addTraceParent(Tracer.getCurrentSpan());\r\n        dataQueue.addLast(packet);\r\n        lastQueuedSeqno = packet.getSeqno();\r\n        LOG.debug(\"Queued {}, {}\", packet, this);\r\n        dataQueue.notifyAll();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createHeartbeatPacket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DFSPacket createHeartbeatPacket()\n{\r\n    final byte[] buf = new byte[PacketHeader.PKT_MAX_HEADER_LEN];\r\n    return new DFSPacket(buf, 0, 0, DFSPacket.HEART_BEAT_SEQNO, 0, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "initExcludedNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LoadingCache<DatanodeInfo, DatanodeInfo> initExcludedNodes(long excludedNodesCacheExpiry)\n{\r\n    return CacheBuilder.newBuilder().expireAfterWrite(excludedNodesCacheExpiry, TimeUnit.MILLISECONDS).removalListener(new RemovalListener<DatanodeInfo, DatanodeInfo>() {\r\n\r\n        @Override\r\n        public void onRemoval(@Nonnull RemovalNotification<DatanodeInfo, DatanodeInfo> notification) {\r\n            LOG.info(\"Removing node \" + notification.getKey() + \" from the excluded nodes list\");\r\n        }\r\n    }).build(new CacheLoader<DatanodeInfo, DatanodeInfo>() {\r\n\r\n        @Override\r\n        public DatanodeInfo load(DatanodeInfo key) throws Exception {\r\n            return key;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "arraycopy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void arraycopy(T[] srcs, T[] dsts, int skipIndex)\n{\r\n    System.arraycopy(srcs, 0, dsts, 0, skipIndex);\r\n    System.arraycopy(srcs, skipIndex + 1, dsts, skipIndex, dsts.length - skipIndex);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getPersistBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AtomicBoolean getPersistBlocks()\n{\r\n    return persistBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setAppendChunk",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAppendChunk(boolean appendChunk)\n{\r\n    this.appendChunk = appendChunk;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAppendChunk",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getAppendChunk()\n{\r\n    return appendChunk;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLastException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LastExceptionInStreamer getLastException()\n{\r\n    return lastException;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setSocketToNull",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSocketToNull()\n{\r\n    this.s = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAndIncCurrentSeqno",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getAndIncCurrentSeqno()\n{\r\n    long old = this.currentSeqno;\r\n    this.currentSeqno++;\r\n    return old;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLastQueuedSeqno",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLastQueuedSeqno()\n{\r\n    return lastQueuedSeqno;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBytesCurBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesCurBlock()\n{\r\n    return bytesCurBlock;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setBytesCurBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setBytesCurBlock(long bytesCurBlock)\n{\r\n    this.bytesCurBlock = bytesCurBlock;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "incBytesCurBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void incBytesCurBlock(long len)\n{\r\n    this.bytesCurBlock += len;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setArtificialSlowdown",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setArtificialSlowdown(long period)\n{\r\n    this.artificialSlowdown = period;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "streamerClosed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean streamerClosed()\n{\r\n    return streamerClosed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getPipelineRecoveryCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getPipelineRecoveryCount()\n{\r\n    return pipelineRecoveryCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "closeSocket",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void closeSocket() throws IOException\n{\r\n    if (s != null) {\r\n        s.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    final ExtendedBlock extendedBlock = block.getCurrentBlock();\r\n    return extendedBlock == null ? \"block==null\" : \"\" + extendedBlock.getLocalBlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getPeer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Peer getPeer()\n{\r\n    return peer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "int read(byte[] buf, int off, int len) throws IOException\n{\r\n    boolean logTraceEnabled = LOG.isTraceEnabled();\r\n    UUID randomId = null;\r\n    if (logTraceEnabled) {\r\n        randomId = UUID.randomUUID();\r\n        LOG.trace(\"Starting read #{} file {} from datanode {}\", randomId, filename, datanodeID.getHostName());\r\n    }\r\n    if (curDataSlice == null || curDataSlice.remaining() == 0 && bytesNeededToFinish > 0) {\r\n        readNextPacket();\r\n    }\r\n    if (logTraceEnabled) {\r\n        LOG.trace(\"Finishing read #{}\", randomId);\r\n    }\r\n    if (curDataSlice.remaining() == 0) {\r\n        return -1;\r\n    }\r\n    int nRead = Math.min(curDataSlice.remaining(), len);\r\n    curDataSlice.get(buf, off, nRead);\r\n    return nRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "int read(ByteBuffer buf) throws IOException\n{\r\n    if (curDataSlice == null || (curDataSlice.remaining() == 0 && bytesNeededToFinish > 0)) {\r\n        readNextPacket();\r\n    }\r\n    if (curDataSlice.remaining() == 0) {\r\n        return -1;\r\n    }\r\n    int nRead = Math.min(curDataSlice.remaining(), buf.remaining());\r\n    ByteBuffer writeSlice = curDataSlice.duplicate();\r\n    writeSlice.limit(writeSlice.position() + nRead);\r\n    buf.put(writeSlice);\r\n    curDataSlice.position(writeSlice.position());\r\n    return nRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "readNextPacket",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void readNextPacket() throws IOException\n{\r\n    packetReceiver.receiveNextPacket(in);\r\n    PacketHeader curHeader = packetReceiver.getHeader();\r\n    curDataSlice = packetReceiver.getDataSlice();\r\n    assert curDataSlice.capacity() == curHeader.getDataLen();\r\n    LOG.trace(\"DFSClient readNextPacket got header {}\", curHeader);\r\n    if (!curHeader.sanityCheck(lastSeqNo)) {\r\n        throw new IOException(\"BlockReader: error in packet header \" + curHeader);\r\n    }\r\n    if (curHeader.getDataLen() > 0) {\r\n        int chunks = 1 + (curHeader.getDataLen() - 1) / bytesPerChecksum;\r\n        int checksumsLen = chunks * checksumSize;\r\n        assert packetReceiver.getChecksumSlice().capacity() == checksumsLen : \"checksum slice capacity=\" + packetReceiver.getChecksumSlice().capacity() + \" checksumsLen=\" + checksumsLen;\r\n        lastSeqNo = curHeader.getSeqno();\r\n        if (verifyChecksum && curDataSlice.remaining() > 0) {\r\n            checksum.verifyChunkedSums(curDataSlice, packetReceiver.getChecksumSlice(), filename, curHeader.getOffsetInBlock());\r\n        }\r\n        bytesNeededToFinish -= curHeader.getDataLen();\r\n    }\r\n    if (curHeader.getOffsetInBlock() < startOffset) {\r\n        int newPos = (int) (startOffset - curHeader.getOffsetInBlock());\r\n        curDataSlice.position(newPos);\r\n    }\r\n    if (bytesNeededToFinish <= 0) {\r\n        readTrailingEmptyPacket();\r\n        if (verifyChecksum) {\r\n            sendReadResult(Status.CHECKSUM_OK);\r\n        } else {\r\n            sendReadResult(Status.SUCCESS);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "skip",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long skip(long n) throws IOException\n{\r\n    long skipped = 0;\r\n    while (skipped < n) {\r\n        long needToSkip = n - skipped;\r\n        if (curDataSlice == null || curDataSlice.remaining() == 0 && bytesNeededToFinish > 0) {\r\n            readNextPacket();\r\n        }\r\n        if (curDataSlice.remaining() == 0) {\r\n            break;\r\n        }\r\n        int skip = (int) Math.min(curDataSlice.remaining(), needToSkip);\r\n        curDataSlice.position(curDataSlice.position() + skip);\r\n        skipped += skip;\r\n    }\r\n    return skipped;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "readTrailingEmptyPacket",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void readTrailingEmptyPacket() throws IOException\n{\r\n    LOG.trace(\"Reading empty packet at end of read\");\r\n    packetReceiver.receiveNextPacket(in);\r\n    PacketHeader trailer = packetReceiver.getHeader();\r\n    if (!trailer.isLastPacketInBlock() || trailer.getDataLen() != 0) {\r\n        throw new IOException(\"Expected empty end-of-read packet! Header: \" + trailer);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    packetReceiver.close();\r\n    startOffset = -1;\r\n    checksum = null;\r\n    if (peerCache != null && sentStatusCode) {\r\n        peerCache.put(datanodeID, peer);\r\n    } else {\r\n        peer.close();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "sendReadResult",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void sendReadResult(Status statusCode)\n{\r\n    assert !sentStatusCode : \"already sent status code to \" + peer;\r\n    try {\r\n        writeReadResult(peer.getOutputStream(), statusCode);\r\n        sentStatusCode = true;\r\n    } catch (IOException e) {\r\n        LOG.info(\"Could not send read status (\" + statusCode + \") to datanode \" + peer.getRemoteAddressString() + \": \" + e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "writeReadResult",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writeReadResult(OutputStream out, Status statusCode) throws IOException\n{\r\n    ClientReadStatusProto.newBuilder().setStatus(statusCode).build().writeDelimitedTo(out);\r\n    out.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getFileName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getFileName(final InetSocketAddress s, final String poolId, final long blockId)\n{\r\n    return s.toString() + \":\" + poolId + \":\" + blockId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "readAll",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int readAll(byte[] buf, int offset, int len) throws IOException\n{\r\n    return BlockReaderUtil.readAll(this, buf, offset, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "readFully",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readFully(byte[] buf, int off, int len) throws IOException\n{\r\n    BlockReaderUtil.readFully(this, buf, off, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "newBlockReader",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "BlockReader newBlockReader(String file, ExtendedBlock block, Token<BlockTokenIdentifier> blockToken, long startOffset, long len, boolean verifyChecksum, String clientName, Peer peer, DatanodeID datanodeID, PeerCache peerCache, CachingStrategy cachingStrategy, int networkDistance, Configuration configuration) throws IOException\n{\r\n    int bufferSize = configuration.getInt(DFS_CLIENT_BLOCK_READER_REMOTE_BUFFER_SIZE_KEY, DFS_CLIENT_BLOCK_READER_REMOTE_BUFFER_SIZE_DEFAULT);\r\n    final DataOutputStream out = new DataOutputStream(new BufferedOutputStream(peer.getOutputStream(), bufferSize));\r\n    new Sender(out).readBlock(block, blockToken, clientName, startOffset, len, verifyChecksum, cachingStrategy);\r\n    DataInputStream in = new DataInputStream(peer.getInputStream());\r\n    BlockOpResponseProto status = BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\r\n    checkSuccess(status, peer, block, file);\r\n    ReadOpChecksumInfoProto checksumInfo = status.getReadOpChecksumInfo();\r\n    DataChecksum checksum = DataTransferProtoUtil.fromProto(checksumInfo.getChecksum());\r\n    long firstChunkOffset = checksumInfo.getChunkOffset();\r\n    if (firstChunkOffset < 0 || firstChunkOffset > startOffset || firstChunkOffset <= (startOffset - checksum.getBytesPerChecksum())) {\r\n        throw new IOException(\"BlockReader: error in first chunk offset (\" + firstChunkOffset + \") startOffset is \" + startOffset + \" for file \" + file);\r\n    }\r\n    return new BlockReaderRemote(file, block.getBlockId(), checksum, verifyChecksum, startOffset, firstChunkOffset, len, peer, datanodeID, peerCache, networkDistance);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "checkSuccess",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkSuccess(BlockOpResponseProto status, Peer peer, ExtendedBlock block, String file) throws IOException\n{\r\n    String logInfo = \"for OP_READ_BLOCK\" + \", self=\" + peer.getLocalAddressString() + \", remote=\" + peer.getRemoteAddressString() + \", for file \" + file + \", for pool \" + block.getBlockPoolId() + \" block \" + block.getBlockId() + \"_\" + block.getGenerationStamp();\r\n    DataTransferProtoUtil.checkBlockOpStatus(status, logInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "available",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int available()\n{\r\n    return TCP_WINDOW_SIZE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "isShortCircuit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isShortCircuit()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getClientMmap",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientMmap getClientMmap(EnumSet<ReadOption> opts)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getDataChecksum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DataChecksum getDataChecksum()\n{\r\n    return checksum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getNetworkDistance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNetworkDistance()\n{\r\n    return networkDistance;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getValueString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getValueString()\n{\r\n    return value.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getEncoding",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "XAttrCodec getEncoding()\n{\r\n    return getValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBytesNeeded",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesNeeded()\n{\r\n    return bytesNeeded;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBytesCached",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesCached()\n{\r\n    return bytesCached;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBytesOverlimit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesOverlimit()\n{\r\n    return bytesOverlimit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFilesNeeded",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFilesNeeded()\n{\r\n    return filesNeeded;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFilesCached",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFilesCached()\n{\r\n    return filesCached;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"{\" + \"bytesNeeded:\" + bytesNeeded + \", bytesCached:\" + bytesCached + \", bytesOverlimit:\" + bytesOverlimit + \", filesNeeded:\" + filesNeeded + \", filesCached:\" + filesCached + \"}\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writeFile(final String hostsFile, final Set<DatanodeAdminProperties> allDNs) throws IOException\n{\r\n    final ObjectMapper objectMapper = new ObjectMapper();\r\n    try (Writer output = new OutputStreamWriter(Files.newOutputStream(Paths.get(hostsFile)), \"UTF-8\")) {\r\n        objectMapper.writeValue(output, allDNs);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "newDefaultURLConnectionFactory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URLConnectionFactory newDefaultURLConnectionFactory(Configuration conf)\n{\r\n    ConnectionConfigurator conn = getSSLConnectionConfiguration(DEFAULT_SOCKET_TIMEOUT, DEFAULT_SOCKET_TIMEOUT, conf);\r\n    return new URLConnectionFactory(conn);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "newDefaultURLConnectionFactory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URLConnectionFactory newDefaultURLConnectionFactory(int connectTimeout, int readTimeout, Configuration conf)\n{\r\n    ConnectionConfigurator conn = getSSLConnectionConfiguration(connectTimeout, readTimeout, conf);\r\n    return new URLConnectionFactory(conn);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getSSLConnectionConfiguration",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ConnectionConfigurator getSSLConnectionConfiguration(final int connectTimeout, final int readTimeout, Configuration conf)\n{\r\n    ConnectionConfigurator conn;\r\n    try {\r\n        conn = new SSLConnectionConfigurator(connectTimeout, readTimeout, conf);\r\n    } catch (Exception e) {\r\n        LOG.warn(\"Cannot load customized ssl related configuration. Fallback to\" + \" system-generic settings.\", e);\r\n        if (connectTimeout == DEFAULT_SOCKET_TIMEOUT && readTimeout == DEFAULT_SOCKET_TIMEOUT) {\r\n            conn = DEFAULT_TIMEOUT_CONN_CONFIGURATOR;\r\n        } else {\r\n            conn = new ConnectionConfigurator() {\r\n\r\n                @Override\r\n                public HttpURLConnection configure(HttpURLConnection connection) throws IOException {\r\n                    URLConnectionFactory.setTimeouts(connection, connectTimeout, readTimeout);\r\n                    return connection;\r\n                }\r\n            };\r\n        }\r\n    }\r\n    return conn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "newOAuth2URLConnectionFactory",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URLConnectionFactory newOAuth2URLConnectionFactory(int connectTimeout, int readTimeout, Configuration conf) throws IOException\n{\r\n    ConnectionConfigurator conn;\r\n    try {\r\n        ConnectionConfigurator sslConnConfigurator = new SSLConnectionConfigurator(connectTimeout, readTimeout, conf);\r\n        conn = new OAuth2ConnectionConfigurator(conf, sslConnConfigurator);\r\n    } catch (Exception e) {\r\n        throw new IOException(\"Unable to load OAuth2 connection factory.\", e);\r\n    }\r\n    return new URLConnectionFactory(conn);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "openConnection",
  "errType" : [ "AuthenticationException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "URLConnection openConnection(URL url) throws IOException\n{\r\n    try {\r\n        return openConnection(url, false);\r\n    } catch (AuthenticationException e) {\r\n        LOG.error(\"Open connection {} failed\", url, e);\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "openConnection",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "URLConnection openConnection(URL url, boolean isSpnego) throws IOException, AuthenticationException\n{\r\n    if (isSpnego) {\r\n        LOG.debug(\"open AuthenticatedURL connection {}\", url);\r\n        UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();\r\n        final AuthenticatedURL.Token authToken = new AuthenticatedURL.Token();\r\n        return new AuthenticatedURL(new KerberosUgiAuthenticator(), connConfigurator).openConnection(url, authToken);\r\n    } else {\r\n        LOG.debug(\"open URL connection\");\r\n        URLConnection connection = url.openConnection();\r\n        if (connection instanceof HttpURLConnection) {\r\n            connConfigurator.configure((HttpURLConnection) connection);\r\n        }\r\n        return connection;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "setTimeouts",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setTimeouts(URLConnection connection, int connectTimeout, int readTimeout)\n{\r\n    connection.setConnectTimeout(connectTimeout);\r\n    connection.setReadTimeout(readTimeout);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void destroy()\n{\r\n    if (connConfigurator instanceof SSLConnectionConfigurator) {\r\n        ((SSLConnectionConfigurator) connConfigurator).destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getIndex",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getIndex()\n{\r\n    return index;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isHealthy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isHealthy()\n{\r\n    return !streamerClosed() && !getErrorState().hasInternalError();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "endBlock",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void endBlock()\n{\r\n    coordinator.offerEndBlock(index, block.getCurrentBlock());\r\n    super.endBlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFollowingBlock",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "LocatedBlock getFollowingBlock() throws IOException\n{\r\n    if (!this.isHealthy()) {\r\n        this.getLastException().check(false);\r\n    }\r\n    return coordinator.getFollowingBlocks().poll(index);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "nextBlockOutputStream",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "LocatedBlock nextBlockOutputStream() throws IOException\n{\r\n    boolean success;\r\n    LocatedBlock lb = getFollowingBlock();\r\n    block.setCurrentBlock(lb.getBlock());\r\n    block.setNumBytes(0);\r\n    bytesSent = 0;\r\n    accessToken = lb.getBlockToken();\r\n    DatanodeInfo[] nodes = lb.getLocations();\r\n    StorageType[] storageTypes = lb.getStorageTypes();\r\n    String[] storageIDs = lb.getStorageIDs();\r\n    success = createBlockOutputStream(nodes, storageTypes, storageIDs, 0L, false);\r\n    if (!success) {\r\n        block.setCurrentBlock(null);\r\n        final DatanodeInfo badNode = nodes[getErrorState().getBadNodeIndex()];\r\n        LOG.warn(\"Excluding datanode \" + badNode);\r\n        excludedNodes.put(badNode, badNode);\r\n        throw new IOException(\"Unable to create new block.\" + this);\r\n    }\r\n    return lb;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "peekFollowingBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocatedBlock peekFollowingBlock()\n{\r\n    return coordinator.getFollowingBlocks().peek(index);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setupPipelineInternal",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void setupPipelineInternal(DatanodeInfo[] nodes, StorageType[] nodeStorageTypes, String[] nodeStorageIDs) throws IOException\n{\r\n    boolean success = false;\r\n    while (!success && !streamerClosed() && dfsClient.clientRunning) {\r\n        if (!handleRestartingDatanode()) {\r\n            return;\r\n        }\r\n        if (!handleBadDatanode()) {\r\n            return;\r\n        }\r\n        final LocatedBlock lb = coordinator.getNewBlocks().take(index);\r\n        long newGS = lb.getBlock().getGenerationStamp();\r\n        setAccessToken(lb.getBlockToken());\r\n        assert getErrorState().hasExternalError() || getErrorState().doWaitForRestart();\r\n        success = createBlockOutputStream(nodes, nodeStorageTypes, nodeStorageIDs, newGS, true);\r\n        failPacket4Testing();\r\n        getErrorState().checkRestartingNodeDeadline(nodes);\r\n        synchronized (coordinator) {\r\n            if (!streamerClosed()) {\r\n                coordinator.updateStreamer(this, success);\r\n                coordinator.notify();\r\n            } else {\r\n                success = false;\r\n            }\r\n        }\r\n        if (success) {\r\n            success = coordinator.takeStreamerUpdateResult(index);\r\n            if (success) {\r\n                updateBlockGS(newGS);\r\n            } else {\r\n                closeStream();\r\n            }\r\n        } else {\r\n            closeStream();\r\n            setStreamerAsClosed();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setExternalError",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setExternalError()\n{\r\n    getErrorState().setExternalError();\r\n    synchronized (dataQueue) {\r\n        dataQueue.notifyAll();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return \"#\" + index + \": \" + (!isHealthy() ? \"failed, \" : \"\") + super.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "resetMetrics",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void resetMetrics()\n{\r\n    zonesReencrypted = 0;\r\n    for (Map.Entry<Long, ZoneReencryptionStatus> entry : zoneStatuses.entrySet()) {\r\n        entry.getValue().resetMetrics();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getZoneStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ZoneReencryptionStatus getZoneStatus(final Long zondId)\n{\r\n    return zoneStatuses.get(zondId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "markZoneForRetry",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void markZoneForRetry(final Long zoneId)\n{\r\n    final ZoneReencryptionStatus zs = zoneStatuses.get(zoneId);\r\n    Preconditions.checkNotNull(zs, \"Cannot find zone \" + zoneId);\r\n    LOG.info(\"Zone {} will retry re-encryption\", zoneId);\r\n    zs.setState(State.Submitted);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "markZoneStarted",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void markZoneStarted(final Long zoneId)\n{\r\n    final ZoneReencryptionStatus zs = zoneStatuses.get(zoneId);\r\n    Preconditions.checkNotNull(zs, \"Cannot find zone \" + zoneId);\r\n    LOG.info(\"Zone {} starts re-encryption processing\", zoneId);\r\n    zs.setState(State.Processing);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "markZoneCompleted",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void markZoneCompleted(final Long zoneId)\n{\r\n    final ZoneReencryptionStatus zs = zoneStatuses.get(zoneId);\r\n    Preconditions.checkNotNull(zs, \"Cannot find zone \" + zoneId);\r\n    LOG.info(\"Zone {} completed re-encryption.\", zoneId);\r\n    zs.setState(State.Completed);\r\n    zonesReencrypted++;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getNextUnprocessedZone",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Long getNextUnprocessedZone()\n{\r\n    for (Map.Entry<Long, ZoneReencryptionStatus> entry : zoneStatuses.entrySet()) {\r\n        if (entry.getValue().getState() == State.Submitted) {\r\n            return entry.getKey();\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hasRunningZone",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean hasRunningZone(final Long zoneId)\n{\r\n    return zoneStatuses.containsKey(zoneId) && zoneStatuses.get(zoneId).getState() != State.Completed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "addZoneIfNecessary",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "boolean addZoneIfNecessary(final Long zoneId, final String name, final ReencryptionInfoProto reProto)\n{\r\n    if (!zoneStatuses.containsKey(zoneId)) {\r\n        LOG.debug(\"Adding zone {} for re-encryption status\", zoneId);\r\n        Preconditions.checkNotNull(reProto);\r\n        final ZoneReencryptionStatus.Builder builder = new ZoneReencryptionStatus.Builder();\r\n        builder.id(zoneId).zoneName(name).ezKeyVersionName(reProto.getEzKeyVersionName()).submissionTime(reProto.getSubmissionTime()).canceled(reProto.getCanceled()).filesReencrypted(reProto.getNumReencrypted()).fileReencryptionFailures(reProto.getNumFailures());\r\n        if (reProto.hasCompletionTime()) {\r\n            builder.completionTime(reProto.getCompletionTime());\r\n            builder.state(State.Completed);\r\n            zonesReencrypted++;\r\n        } else {\r\n            builder.state(State.Submitted);\r\n        }\r\n        if (reProto.hasLastFile()) {\r\n            builder.lastCheckpointFile(reProto.getLastFile());\r\n        }\r\n        return zoneStatuses.put(zoneId, builder.build()) == null;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "updateZoneStatus",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void updateZoneStatus(final Long zoneId, final String zonePath, final ReencryptionInfoProto reProto)\n{\r\n    Preconditions.checkArgument(zoneId != null, \"zoneId can't be null\");\r\n    if (addZoneIfNecessary(zoneId, zonePath, reProto)) {\r\n        return;\r\n    }\r\n    final ZoneReencryptionStatus zs = getZoneStatus(zoneId);\r\n    assert zs != null;\r\n    if (reProto.hasCompletionTime()) {\r\n        zs.markZoneCompleted(reProto);\r\n    } else if (!reProto.hasLastFile() && !reProto.hasCompletionTime()) {\r\n        zs.markZoneSubmitted(reProto);\r\n    } else {\r\n        zs.updateZoneProcess(reProto);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "removeZone",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean removeZone(final Long zoneId)\n{\r\n    LOG.debug(\"Removing re-encryption status of zone {} \", zoneId);\r\n    return zoneStatuses.remove(zoneId) != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "zonesQueued",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int zonesQueued()\n{\r\n    int ret = 0;\r\n    for (Map.Entry<Long, ZoneReencryptionStatus> entry : zoneStatuses.entrySet()) {\r\n        if (entry.getValue().getState() == State.Submitted) {\r\n            ret++;\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "zonesTotal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int zonesTotal()\n{\r\n    return zoneStatuses.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getNumZonesReencrypted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumZonesReencrypted()\n{\r\n    return zonesReencrypted;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    for (Map.Entry<Long, ZoneReencryptionStatus> entry : zoneStatuses.entrySet()) {\r\n        sb.append(\"[zone:\" + entry.getKey()).append(\" state:\" + entry.getValue().getState()).append(\" lastProcessed:\" + entry.getValue().getLastCheckpointFile()).append(\" filesReencrypted:\" + entry.getValue().getFilesReencrypted()).append(\" fileReencryptionFailures:\" + entry.getValue().getNumReencryptionFailures() + \"]\");\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getZoneStatuses",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NavigableMap<Long, ZoneReencryptionStatus> getZoneStatuses()\n{\r\n    return zoneStatuses;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getName()\n{\r\n    return getXferAddr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getCapacity",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCapacity()\n{\r\n    return capacity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getDfsUsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDfsUsed()\n{\r\n    return dfsUsed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockPoolUsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBlockPoolUsed()\n{\r\n    return blockPoolUsed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getNonDfsUsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNonDfsUsed()\n{\r\n    return nonDfsUsed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getDfsUsedPercent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getDfsUsedPercent()\n{\r\n    return DFSUtilClient.getPercentUsed(dfsUsed, capacity);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getRemaining",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getRemaining()\n{\r\n    return remaining;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockPoolUsedPercent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getBlockPoolUsedPercent()\n{\r\n    return DFSUtilClient.getPercentUsed(blockPoolUsed, capacity);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getRemainingPercent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getRemainingPercent()\n{\r\n    return DFSUtilClient.getPercentRemaining(remaining, capacity);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getCacheCapacity",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCacheCapacity()\n{\r\n    return cacheCapacity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getCacheUsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCacheUsed()\n{\r\n    return cacheUsed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getCacheUsedPercent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getCacheUsedPercent()\n{\r\n    return DFSUtilClient.getPercentUsed(cacheUsed, cacheCapacity);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getCacheRemaining",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCacheRemaining()\n{\r\n    return cacheCapacity - cacheUsed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getCacheRemainingPercent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getCacheRemainingPercent()\n{\r\n    return DFSUtilClient.getPercentRemaining(getCacheRemaining(), cacheCapacity);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLastUpdate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLastUpdate()\n{\r\n    return lastUpdate;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLastUpdateMonotonic",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLastUpdateMonotonic()\n{\r\n    return lastUpdateMonotonic;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getNumBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumBlocks()\n{\r\n    return numBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setLastUpdateMonotonic",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLastUpdateMonotonic(long lastUpdateMonotonic)\n{\r\n    this.lastUpdateMonotonic = lastUpdateMonotonic;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getXceiverCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getXceiverCount()\n{\r\n    return xceiverCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setCapacity",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCapacity(long capacity)\n{\r\n    this.capacity = capacity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setDfsUsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDfsUsed(long dfsUsed)\n{\r\n    this.dfsUsed = dfsUsed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setNonDfsUsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNonDfsUsed(long nonDfsUsed)\n{\r\n    this.nonDfsUsed = nonDfsUsed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setRemaining",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRemaining(long remaining)\n{\r\n    this.remaining = remaining;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setBlockPoolUsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setBlockPoolUsed(long bpUsed)\n{\r\n    this.blockPoolUsed = bpUsed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setCacheCapacity",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCacheCapacity(long cacheCapacity)\n{\r\n    this.cacheCapacity = cacheCapacity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setCacheUsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCacheUsed(long cacheUsed)\n{\r\n    this.cacheUsed = cacheUsed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setLastUpdate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLastUpdate(long lastUpdate)\n{\r\n    this.lastUpdate = lastUpdate;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setXceiverCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setXceiverCount(int xceiverCount)\n{\r\n    this.xceiverCount = xceiverCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setNumBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumBlocks(int blockCount)\n{\r\n    this.numBlocks = blockCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getNetworkLocation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNetworkLocation()\n{\r\n    return location;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setNetworkLocation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNetworkLocation(String location)\n{\r\n    this.location = NodeBase.normalize(location);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setUpgradeDomain",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setUpgradeDomain(String upgradeDomain)\n{\r\n    this.upgradeDomain = upgradeDomain;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getUpgradeDomain",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUpgradeDomain()\n{\r\n    return upgradeDomain;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "addDependentHostName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addDependentHostName(String hostname)\n{\r\n    dependentHostNames.add(hostname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getDependentHostNames",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<String> getDependentHostNames()\n{\r\n    return dependentHostNames;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setDependentHostNames",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDependentHostNames(List<String> dependencyList)\n{\r\n    dependentHostNames = dependencyList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getDatanodeReport",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "String getDatanodeReport()\n{\r\n    StringBuilder buffer = new StringBuilder();\r\n    long c = getCapacity();\r\n    long r = getRemaining();\r\n    long u = getDfsUsed();\r\n    long nonDFSUsed = getNonDfsUsed();\r\n    float usedPercent = getDfsUsedPercent();\r\n    float remainingPercent = getRemainingPercent();\r\n    long cc = getCacheCapacity();\r\n    long cr = getCacheRemaining();\r\n    long cu = getCacheUsed();\r\n    float cacheUsedPercent = getCacheUsedPercent();\r\n    float cacheRemainingPercent = getCacheRemainingPercent();\r\n    String lookupName = NetUtils.getHostNameOfIP(getName());\r\n    int blockCount = getNumBlocks();\r\n    buffer.append(\"Name: \").append(getName());\r\n    if (lookupName != null) {\r\n        buffer.append(\" (\").append(lookupName).append(\")\");\r\n    }\r\n    buffer.append(\"\\n\").append(\"Hostname: \").append(getHostName()).append(\"\\n\");\r\n    if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\r\n        buffer.append(\"Rack: \").append(location).append(\"\\n\");\r\n    }\r\n    if (upgradeDomain != null) {\r\n        buffer.append(\"Upgrade domain: \").append(upgradeDomain).append(\"\\n\");\r\n    }\r\n    buffer.append(\"Decommission Status : \");\r\n    if (isDecommissioned()) {\r\n        buffer.append(\"Decommissioned\\n\");\r\n    } else if (isDecommissionInProgress()) {\r\n        buffer.append(\"Decommission in progress\\n\");\r\n    } else if (isInMaintenance()) {\r\n        buffer.append(\"In maintenance\\n\");\r\n    } else if (isEnteringMaintenance()) {\r\n        buffer.append(\"Entering maintenance\\n\");\r\n    } else {\r\n        buffer.append(\"Normal\\n\");\r\n    }\r\n    buffer.append(\"Configured Capacity: \").append(c).append(\" (\").append(StringUtils.byteDesc(c)).append(\")\").append(\"\\n\").append(\"DFS Used: \").append(u).append(\" (\").append(StringUtils.byteDesc(u)).append(\")\").append(\"\\n\").append(\"Non DFS Used: \").append(nonDFSUsed).append(\" (\").append(StringUtils.byteDesc(nonDFSUsed)).append(\")\").append(\"\\n\").append(\"DFS Remaining: \").append(r).append(\" (\").append(StringUtils.byteDesc(r)).append(\")\").append(\"\\n\").append(\"DFS Used%: \").append(percent2String(usedPercent)).append(\"\\n\").append(\"DFS Remaining%: \").append(percent2String(remainingPercent)).append(\"\\n\").append(\"Configured Cache Capacity: \").append(cc).append(\" (\").append(StringUtils.byteDesc(cc)).append(\")\").append(\"\\n\").append(\"Cache Used: \").append(cu).append(\" (\").append(StringUtils.byteDesc(cu)).append(\")\").append(\"\\n\").append(\"Cache Remaining: \").append(cr).append(\" (\").append(StringUtils.byteDesc(cr)).append(\")\").append(\"\\n\").append(\"Cache Used%: \").append(percent2String(cacheUsedPercent)).append(\"\\n\").append(\"Cache Remaining%: \").append(percent2String(cacheRemainingPercent)).append(\"\\n\").append(\"Xceivers: \").append(getXceiverCount()).append(\"\\n\").append(\"Last contact: \").append(new Date(lastUpdate)).append(\"\\n\").append(\"Last Block Report: \").append(lastBlockReportTime != 0 ? new Date(lastBlockReportTime) : \"Never\").append(\"\\n\").append(\"Num of Blocks: \").append(blockCount).append(\"\\n\");\r\n    return buffer.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "dumpDatanode",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "String dumpDatanode()\n{\r\n    StringBuilder buffer = new StringBuilder();\r\n    long c = getCapacity();\r\n    long r = getRemaining();\r\n    long u = getDfsUsed();\r\n    float usedPercent = getDfsUsedPercent();\r\n    long cc = getCacheCapacity();\r\n    long cr = getCacheRemaining();\r\n    long cu = getCacheUsed();\r\n    float cacheUsedPercent = getCacheUsedPercent();\r\n    buffer.append(getName());\r\n    if (!NetworkTopology.DEFAULT_RACK.equals(location)) {\r\n        buffer.append(\" \").append(location);\r\n    }\r\n    if (upgradeDomain != null) {\r\n        buffer.append(\" \").append(upgradeDomain);\r\n    }\r\n    if (isDecommissioned()) {\r\n        buffer.append(\" DD\");\r\n    } else if (isDecommissionInProgress()) {\r\n        buffer.append(\" DP\");\r\n    } else if (isInMaintenance()) {\r\n        buffer.append(\" IM\");\r\n    } else if (isEnteringMaintenance()) {\r\n        buffer.append(\" EM\");\r\n    } else {\r\n        buffer.append(\" IN\");\r\n    }\r\n    buffer.append(\" \").append(c).append(\"(\").append(StringUtils.byteDesc(c)).append(\")\").append(\" \").append(u).append(\"(\").append(StringUtils.byteDesc(u)).append(\")\").append(\" \").append(percent2String(usedPercent)).append(\" \").append(r).append(\"(\").append(StringUtils.byteDesc(r)).append(\")\").append(\" \").append(cc).append(\"(\").append(StringUtils.byteDesc(cc)).append(\")\").append(\" \").append(cu).append(\"(\").append(StringUtils.byteDesc(cu)).append(\")\").append(\" \").append(percent2String(cacheUsedPercent)).append(\" \").append(cr).append(\"(\").append(StringUtils.byteDesc(cr)).append(\")\").append(\" \").append(new Date(lastUpdate));\r\n    return buffer.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "startDecommission",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void startDecommission()\n{\r\n    adminState = AdminStates.DECOMMISSION_INPROGRESS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "stopDecommission",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void stopDecommission()\n{\r\n    adminState = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isDecommissionInProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isDecommissionInProgress()\n{\r\n    return adminState == AdminStates.DECOMMISSION_INPROGRESS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isDecommissioned",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isDecommissioned()\n{\r\n    return adminState == AdminStates.DECOMMISSIONED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setDecommissioned",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDecommissioned()\n{\r\n    adminState = AdminStates.DECOMMISSIONED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "startMaintenance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void startMaintenance()\n{\r\n    this.adminState = AdminStates.ENTERING_MAINTENANCE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setInMaintenance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setInMaintenance()\n{\r\n    this.adminState = AdminStates.IN_MAINTENANCE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setMaintenanceExpireTimeInMS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMaintenanceExpireTimeInMS(long maintenanceExpireTimeInMS)\n{\r\n    this.maintenanceExpireTimeInMS = maintenanceExpireTimeInMS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getMaintenanceExpireTimeInMS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMaintenanceExpireTimeInMS()\n{\r\n    return this.maintenanceExpireTimeInMS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setLastBlockReportTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLastBlockReportTime(long lastBlockReportTime)\n{\r\n    this.lastBlockReportTime = lastBlockReportTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setLastBlockReportMonotonic",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLastBlockReportMonotonic(long lastBlockReportMonotonic)\n{\r\n    this.lastBlockReportMonotonic = lastBlockReportMonotonic;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLastBlockReportTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLastBlockReportTime()\n{\r\n    return lastBlockReportTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLastBlockReportMonotonic",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLastBlockReportMonotonic()\n{\r\n    return lastBlockReportMonotonic;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "stopMaintenance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void stopMaintenance()\n{\r\n    adminState = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "maintenanceNotExpired",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean maintenanceNotExpired(long maintenanceExpireTimeInMS)\n{\r\n    return Time.now() < maintenanceExpireTimeInMS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isEnteringMaintenance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isEnteringMaintenance()\n{\r\n    return adminState == AdminStates.ENTERING_MAINTENANCE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isInMaintenance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isInMaintenance()\n{\r\n    return adminState == AdminStates.IN_MAINTENANCE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isMaintenance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isMaintenance()\n{\r\n    return (adminState == AdminStates.ENTERING_MAINTENANCE || adminState == AdminStates.IN_MAINTENANCE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "maintenanceExpired",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean maintenanceExpired()\n{\r\n    return !maintenanceNotExpired(this.maintenanceExpireTimeInMS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isInService",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isInService()\n{\r\n    return getAdminState() == AdminStates.NORMAL;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getAdminState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AdminStates getAdminState()\n{\r\n    if (adminState == null) {\r\n        return AdminStates.NORMAL;\r\n    }\r\n    return adminState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isStale",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isStale(long staleInterval)\n{\r\n    return (Time.monotonicNow() - lastUpdateMonotonic) >= staleInterval;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setAdminState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAdminState(AdminStates newState)\n{\r\n    if (newState == AdminStates.NORMAL) {\r\n        adminState = null;\r\n    } else {\r\n        adminState = newState;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getParent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Node getParent()\n{\r\n    return parent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setParent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setParent(Node parent)\n{\r\n    this.parent = parent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLevel",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getLevel()\n{\r\n    return level;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setLevel",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLevel(int level)\n{\r\n    this.level = level;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return super.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    return (this == obj) || super.equals(obj);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getSoftwareVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSoftwareVersion()\n{\r\n    return softwareVersion;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setSoftwareVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSoftwareVersion(String softwareVersion)\n{\r\n    this.softwareVersion = softwareVersion;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "makeRequest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BatchedEntries<ZoneReencryptionStatus> makeRequest(Long prevId) throws IOException\n{\r\n    try (TraceScope ignored = tracer.newScope(\"listReencryptionStatus\")) {\r\n        return namenode.listReencryptionStatus(prevId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "elementToPrevKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Long elementToPrevKey(ZoneReencryptionStatus entry)\n{\r\n    return entry.getId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\delegation",
  "methodName" : "clearCache",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearCache()\n{\r\n    ugiCache.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\delegation",
  "methodName" : "getKind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getKind()\n{\r\n    return HDFS_DELEGATION_KIND;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\delegation",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "UserGroupInformation getUser()\n{\r\n    UserGroupInformation ugi = ugiCache.get(this);\r\n    if (ugi == null) {\r\n        ugi = super.getUser();\r\n        ugiCache.put(this, ugi);\r\n    }\r\n    return ugi;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\delegation",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder sbld = new StringBuilder();\r\n    sbld.append(\"token for \").append(getUser().getShortUserName()).append(\": \").append(super.toString());\r\n    return sbld.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\delegation",
  "methodName" : "toStringStable",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toStringStable()\n{\r\n    StringBuilder sbld = new StringBuilder();\r\n    sbld.append(getKind()).append(\" token \").append(getSequenceNumber()).append(\" for \").append(getUser().getShortUserName()).append(\" with renewer \").append(getRenewer());\r\n    return sbld.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\delegation",
  "methodName" : "stringifyToken",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String stringifyToken(final Token<?> token) throws IOException\n{\r\n    DelegationTokenIdentifier ident = new DelegationTokenIdentifier();\r\n    ByteArrayInputStream buf = new ByteArrayInputStream(token.getIdentifier());\r\n    DataInputStream in = new DataInputStream(buf);\r\n    ident.readFields(in);\r\n    if (token.getService().getLength() > 0) {\r\n        return ident + \" on \" + token.getService();\r\n    } else {\r\n        return ident.toString();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isDaemonStarted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isDaemonStarted()\n{\r\n    return daemon != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "startExpiryDaemon",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void startExpiryDaemon()\n{\r\n    if (isDaemonStarted()) {\r\n        return;\r\n    }\r\n    daemon = new Daemon(new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            try {\r\n                PeerCache.this.run();\r\n            } catch (InterruptedException e) {\r\n            } finally {\r\n                PeerCache.this.clear();\r\n            }\r\n        }\r\n\r\n        @Override\r\n        public String toString() {\r\n            return String.valueOf(PeerCache.this);\r\n        }\r\n    });\r\n    daemon.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Peer get(DatanodeID dnId, boolean isDomain)\n{\r\n    if (capacity <= 0) {\r\n        return null;\r\n    }\r\n    return getInternal(dnId, isDomain);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getInternal",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "Peer getInternal(DatanodeID dnId, boolean isDomain)\n{\r\n    List<Value> sockStreamList = multimap.get(new Key(dnId, isDomain));\r\n    if (sockStreamList == null) {\r\n        return null;\r\n    }\r\n    Iterator<Value> iter = sockStreamList.iterator();\r\n    while (iter.hasNext()) {\r\n        Value candidate = iter.next();\r\n        iter.remove();\r\n        long ageMs = Time.monotonicNow() - candidate.getTime();\r\n        Peer peer = candidate.getPeer();\r\n        if (ageMs >= expiryPeriod) {\r\n            try {\r\n                peer.close();\r\n            } catch (IOException e) {\r\n                LOG.warn(\"got IOException closing stale peer \" + peer + \", which is \" + ageMs + \" ms old\");\r\n            }\r\n        } else if (!peer.isClosed()) {\r\n            return peer;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "put",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void put(DatanodeID dnId, Peer peer)\n{\r\n    Preconditions.checkNotNull(dnId);\r\n    Preconditions.checkNotNull(peer);\r\n    if (peer.isClosed())\r\n        return;\r\n    if (capacity <= 0) {\r\n        IOUtilsClient.cleanupWithLogger(LOG, peer);\r\n        return;\r\n    }\r\n    putInternal(dnId, peer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "putInternal",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void putInternal(DatanodeID dnId, Peer peer)\n{\r\n    startExpiryDaemon();\r\n    if (capacity == multimap.size()) {\r\n        evictOldest();\r\n    }\r\n    multimap.put(new Key(dnId, peer.getDomainSocket() != null), new Value(peer, Time.monotonicNow()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "size",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int size()\n{\r\n    return multimap.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "evictExpired",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void evictExpired(long expiryPeriod)\n{\r\n    while (multimap.size() != 0) {\r\n        Iterator<Entry<Key, Value>> iter = multimap.entries().iterator();\r\n        Entry<Key, Value> entry = iter.next();\r\n        if (entry == null || Time.monotonicNow() - entry.getValue().getTime() < expiryPeriod) {\r\n            break;\r\n        }\r\n        IOUtilsClient.cleanupWithLogger(LOG, entry.getValue().getPeer());\r\n        iter.remove();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "evictOldest",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void evictOldest()\n{\r\n    Iterator<Entry<Key, Value>> iter = multimap.entries().iterator();\r\n    if (!iter.hasNext()) {\r\n        throw new IllegalStateException(\"Cannot evict from empty cache! \" + \"capacity: \" + capacity);\r\n    }\r\n    Entry<Key, Value> entry = iter.next();\r\n    IOUtilsClient.cleanupWithLogger(LOG, entry.getValue().getPeer());\r\n    iter.remove();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void run() throws InterruptedException\n{\r\n    for (long lastExpiryTime = Time.monotonicNow(); !Thread.interrupted(); Thread.sleep(expiryPeriod)) {\r\n        final long elapsed = Time.monotonicNow() - lastExpiryTime;\r\n        if (elapsed >= expiryPeriod) {\r\n            evictExpired(expiryPeriod);\r\n            lastExpiryTime = Time.monotonicNow();\r\n        }\r\n    }\r\n    clear();\r\n    throw new InterruptedException(\"Daemon Interrupted\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "clear",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void clear()\n{\r\n    for (Value value : multimap.values()) {\r\n        IOUtilsClient.cleanupWithLogger(LOG, value.getPeer());\r\n    }\r\n    multimap.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "close",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void close()\n{\r\n    clear();\r\n    if (daemon != null) {\r\n        daemon.interrupt();\r\n        try {\r\n            daemon.join();\r\n        } catch (InterruptedException e) {\r\n            throw new RuntimeException(\"failed to join thread\");\r\n        }\r\n    }\r\n    daemon = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(int b) throws IOException\n{\r\n    buf.put((byte) b);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(byte[] b, int off, int len) throws IOException\n{\r\n    buf.put(b, off, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "getCredential",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCredential()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    super.setConf(conf);\r\n    clientId = notNull(conf, OAUTH_CLIENT_ID_KEY);\r\n    refreshURL = notNull(conf, OAUTH_REFRESH_URL_KEY);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "getAccessToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getAccessToken() throws IOException\n{\r\n    if (timer.shouldRefresh() || !initialCredentialObtained) {\r\n        refresh();\r\n        initialCredentialObtained = true;\r\n    }\r\n    return accessToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "refresh",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void refresh() throws IOException\n{\r\n    try {\r\n        OkHttpClient client = new OkHttpClient();\r\n        client.setConnectTimeout(URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT, TimeUnit.MILLISECONDS);\r\n        client.setReadTimeout(URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT, TimeUnit.MILLISECONDS);\r\n        String bodyString = Utils.postBody(CLIENT_SECRET, getCredential(), GRANT_TYPE, CLIENT_CREDENTIALS, CLIENT_ID, clientId);\r\n        RequestBody body = RequestBody.create(URLENCODED, bodyString);\r\n        Request request = new Request.Builder().url(refreshURL).post(body).build();\r\n        Response responseBody = client.newCall(request).execute();\r\n        if (responseBody.code() != HttpStatus.SC_OK) {\r\n            throw new IllegalArgumentException(\"Received invalid http response: \" + responseBody.code() + \", text = \" + responseBody.toString());\r\n        }\r\n        Map<?, ?> response = JsonSerialization.mapReader().readValue(responseBody.body().string());\r\n        String newExpiresIn = response.get(EXPIRES_IN).toString();\r\n        timer.setExpiresIn(newExpiresIn);\r\n        accessToken = response.get(ACCESS_TOKEN).toString();\r\n    } catch (Exception e) {\r\n        throw new IOException(\"Unable to obtain access token from credential\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getAclPermissionPattern",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Domain getAclPermissionPattern()\n{\r\n    return DOMAIN;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "setAclPermissionPattern",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAclPermissionPattern(Domain dm)\n{\r\n    DOMAIN = dm;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "setAclPermissionPattern",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setAclPermissionPattern(String pattern)\n{\r\n    DOMAIN = new Domain(NAME, Pattern.compile(pattern));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getAclPermission",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<AclEntry> getAclPermission(boolean includePermission)\n{\r\n    final String v = getValue();\r\n    return (v != null ? AclEntry.parseAclSpec(v, includePermission) : AclEntry.parseAclSpec(DEFAULT, includePermission));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "parseAclSpec",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "String parseAclSpec(List<AclEntry> aclEntries)\n{\r\n    if (aclEntries == null) {\r\n        return null;\r\n    }\r\n    if (aclEntries.isEmpty()) {\r\n        return \"\";\r\n    }\r\n    if (aclEntries.size() == 1) {\r\n        AclEntry entry = aclEntries.get(0);\r\n        return entry == null ? \"\" : entry.toStringStable();\r\n    }\r\n    StringBuilder sb = new StringBuilder();\r\n    Iterator<AclEntry> iter = aclEntries.iterator();\r\n    sb.append(iter.next().toStringStable());\r\n    while (iter.hasNext()) {\r\n        AclEntry entry = iter.next();\r\n        sb.append(',').append(entry == null ? \"\" : entry.toStringStable());\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ErasureCodingPolicy getPolicy()\n{\r\n    return policy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ErasureCodingPolicyState getState()\n{\r\n    return state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setState(final ErasureCodingPolicyState newState)\n{\r\n    Preconditions.checkNotNull(newState, \"New state should not be null.\");\r\n    state = newState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isEnabled()\n{\r\n    return (this.state == ErasureCodingPolicyState.ENABLED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isDisabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isDisabled()\n{\r\n    return (this.state == ErasureCodingPolicyState.DISABLED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isRemoved",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isRemoved()\n{\r\n    return (this.state == ErasureCodingPolicyState.REMOVED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (o == null) {\r\n        return false;\r\n    }\r\n    if (o == this) {\r\n        return true;\r\n    }\r\n    if (o.getClass() != getClass()) {\r\n        return false;\r\n    }\r\n    ErasureCodingPolicyInfo rhs = (ErasureCodingPolicyInfo) o;\r\n    return new EqualsBuilder().append(policy, rhs.policy).append(state, rhs.state).isEquals();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return new HashCodeBuilder(303855623, 582626729).append(policy).append(state).toHashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return policy.toString() + \", State=\" + state.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLocatedBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<LocatedBlock> getLocatedBlocks()\n{\r\n    return blocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLastLocatedBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LocatedBlock getLastLocatedBlock()\n{\r\n    return lastLocatedBlock;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isLastBlockComplete",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isLastBlockComplete()\n{\r\n    return isLastBlockComplete;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocatedBlock get(int index)\n{\r\n    return blocks.get(index);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "locatedBlockCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int locatedBlockCount()\n{\r\n    return blocks == null ? 0 : blocks.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFileLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFileLength()\n{\r\n    return this.fileLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isUnderConstruction",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isUnderConstruction()\n{\r\n    return underConstruction;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFileEncryptionInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileEncryptionInfo getFileEncryptionInfo()\n{\r\n    return fileEncryptionInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ErasureCodingPolicy getErasureCodingPolicy()\n{\r\n    return ecPolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "findBlock",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int findBlock(long offset)\n{\r\n    LocatedBlock key = new LocatedBlock(new ExtendedBlock(), DatanodeInfo.EMPTY_ARRAY);\r\n    key.setStartOffset(offset);\r\n    key.getBlock().setNumBytes(1);\r\n    Comparator<LocatedBlock> comp = new Comparator<LocatedBlock>() {\r\n\r\n        @Override\r\n        public int compare(LocatedBlock a, LocatedBlock b) {\r\n            long aBeg = a.getStartOffset();\r\n            long bBeg = b.getStartOffset();\r\n            long aEnd = aBeg + a.getBlockSize();\r\n            long bEnd = bBeg + b.getBlockSize();\r\n            if (aBeg <= bBeg && bEnd <= aEnd || bBeg <= aBeg && aEnd <= bEnd)\r\n                return 0;\r\n            if (aBeg < bBeg)\r\n                return -1;\r\n            return 1;\r\n        }\r\n    };\r\n    return Collections.binarySearch(blocks, key, comp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "insertRange",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void insertRange(int blockIdx, List<LocatedBlock> newBlocks)\n{\r\n    int oldIdx = blockIdx;\r\n    int insStart = 0, insEnd = 0;\r\n    for (int newIdx = 0; newIdx < newBlocks.size() && oldIdx < blocks.size(); newIdx++) {\r\n        long newOff = newBlocks.get(newIdx).getStartOffset();\r\n        long oldOff = blocks.get(oldIdx).getStartOffset();\r\n        if (newOff < oldOff) {\r\n            insEnd++;\r\n        } else if (newOff == oldOff) {\r\n            blocks.set(oldIdx, newBlocks.get(newIdx));\r\n            if (insStart < insEnd) {\r\n                blocks.addAll(oldIdx, newBlocks.subList(insStart, insEnd));\r\n                oldIdx += insEnd - insStart;\r\n            }\r\n            insStart = insEnd = newIdx + 1;\r\n            oldIdx++;\r\n        } else {\r\n            assert false : \"List of LocatedBlock must be sorted by startOffset\";\r\n        }\r\n    }\r\n    insEnd = newBlocks.size();\r\n    if (insStart < insEnd) {\r\n        blocks.addAll(oldIdx, newBlocks.subList(insStart, insEnd));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getInsertIndex",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getInsertIndex(int binSearchResult)\n{\r\n    return binSearchResult >= 0 ? binSearchResult : -(binSearchResult + 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return getClass().getSimpleName() + \"{\" + \";  fileLength=\" + fileLength + \";  underConstruction=\" + underConstruction + \";  blocks=\" + blocks + \";  lastLocatedBlock=\" + lastLocatedBlock + \";  isLastBlockComplete=\" + isLastBlockComplete + \";  ecPolicy=\" + ecPolicy + \"}\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "isIncremental",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isIncremental()\n{\r\n    return incremental;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getNamenodeAddr",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InetSocketAddress getNamenodeAddr()\n{\r\n    return namenodeAddr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"BlockReportOptions{incremental=\" + incremental + \", namenodeAddr=\" + namenodeAddr + \"}\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "useDirectBuffer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean useDirectBuffer()\n{\r\n    return encoder.preferDirectBuffer();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStripedDataStreamer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StripedDataStreamer getStripedDataStreamer(int i)\n{\r\n    return streamers.get(i);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getCurrentIndex",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getCurrentIndex()\n{\r\n    return getCurrentStreamer().getIndex();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getCurrentStreamer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StripedDataStreamer getCurrentStreamer()\n{\r\n    return (StripedDataStreamer) streamer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setCurrentStreamer",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "StripedDataStreamer setCurrentStreamer(int newIdx)\n{\r\n    if (streamer != null) {\r\n        int oldIdx = streamers.indexOf(getCurrentStreamer());\r\n        if (oldIdx >= 0) {\r\n            currentPackets[oldIdx] = currentPacket;\r\n        }\r\n    }\r\n    streamer = getStripedDataStreamer(newIdx);\r\n    currentPacket = currentPackets[newIdx];\r\n    adjustChunkBoundary();\r\n    return getCurrentStreamer();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "encode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void encode(RawErasureEncoder encoder, int numData, ByteBuffer[] buffers) throws IOException\n{\r\n    final ByteBuffer[] dataBuffers = new ByteBuffer[numData];\r\n    final ByteBuffer[] parityBuffers = new ByteBuffer[buffers.length - numData];\r\n    System.arraycopy(buffers, 0, dataBuffers, 0, dataBuffers.length);\r\n    System.arraycopy(buffers, numData, parityBuffers, 0, parityBuffers.length);\r\n    encoder.encode(dataBuffers, parityBuffers);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkStreamers",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "Set<StripedDataStreamer> checkStreamers() throws IOException\n{\r\n    Set<StripedDataStreamer> newFailed = new HashSet<>();\r\n    for (StripedDataStreamer s : streamers) {\r\n        if (!s.isHealthy() && !failedStreamers.contains(s)) {\r\n            newFailed.add(s);\r\n        }\r\n    }\r\n    final int failCount = failedStreamers.size() + newFailed.size();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"checkStreamers: \" + streamers);\r\n        LOG.debug(\"healthy streamer count=\" + (numAllBlocks - failCount));\r\n        LOG.debug(\"original failed streamers: \" + failedStreamers);\r\n        LOG.debug(\"newly failed streamers: \" + newFailed);\r\n    }\r\n    if (failCount > (numAllBlocks - numDataBlocks)) {\r\n        closeAllStreamers();\r\n        throw new IOException(\"Failed: the number of failed blocks = \" + failCount + \" > the number of parity blocks = \" + (numAllBlocks - numDataBlocks));\r\n    }\r\n    return newFailed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "closeAllStreamers",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void closeAllStreamers()\n{\r\n    for (StripedDataStreamer streamer : streamers) {\r\n        streamer.close(true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "handleCurrentStreamerFailure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void handleCurrentStreamerFailure(String err, Exception e) throws IOException\n{\r\n    currentPacket = null;\r\n    handleStreamerFailure(err, e, getCurrentStreamer());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "handleStreamerFailure",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void handleStreamerFailure(String err, Exception e, StripedDataStreamer streamer) throws IOException\n{\r\n    LOG.warn(\"Failed: \" + err + \", \" + this, e);\r\n    streamer.getErrorState().setInternalError();\r\n    streamer.close(true);\r\n    checkStreamers();\r\n    currentPackets[streamer.getIndex()] = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "replaceFailedStreamers",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void replaceFailedStreamers()\n{\r\n    assert streamers.size() == numAllBlocks;\r\n    final int currentIndex = getCurrentIndex();\r\n    assert currentIndex == 0;\r\n    for (short i = 0; i < numAllBlocks; i++) {\r\n        final StripedDataStreamer oldStreamer = getStripedDataStreamer(i);\r\n        if (!oldStreamer.isHealthy()) {\r\n            LOG.info(\"replacing previously failed streamer \" + oldStreamer);\r\n            StripedDataStreamer streamer = new StripedDataStreamer(oldStreamer.stat, dfsClient, src, oldStreamer.progress, oldStreamer.checksum4WriteBlock, cachingStrategy, byteArrayManager, favoredNodes, i, coordinator, getAddBlockFlags());\r\n            streamers.set(i, streamer);\r\n            currentPackets[i] = null;\r\n            if (i == currentIndex) {\r\n                this.streamer = streamer;\r\n                this.currentPacket = null;\r\n            }\r\n            streamer.start();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "waitEndBlocks",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void waitEndBlocks(int i) throws IOException\n{\r\n    while (getStripedDataStreamer(i).isHealthy()) {\r\n        final ExtendedBlock b = coordinator.endBlocks.takeWithTimeout(i);\r\n        if (b != null) {\r\n            StripedBlockUtil.checkBlocks(currentBlockGroup, i, b);\r\n            return;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getExcludedNodes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DatanodeInfo[] getExcludedNodes()\n{\r\n    List<DatanodeInfo> excluded = new ArrayList<>();\r\n    for (StripedDataStreamer streamer : streamers) {\r\n        for (DatanodeInfo e : streamer.getExcludedNodes()) {\r\n            if (e != null) {\r\n                excluded.add(e);\r\n            }\r\n        }\r\n    }\r\n    return excluded.toArray(new DatanodeInfo[excluded.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "allocateNewBlock",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void allocateNewBlock() throws IOException\n{\r\n    if (currentBlockGroup != null) {\r\n        for (int i = 0; i < numAllBlocks; i++) {\r\n            waitEndBlocks(i);\r\n        }\r\n    }\r\n    failedStreamers.clear();\r\n    DatanodeInfo[] excludedNodes = getExcludedNodes();\r\n    LOG.debug(\"Excluding DataNodes when allocating new block: \" + Arrays.asList(excludedNodes));\r\n    ExtendedBlock prevBlockGroup = currentBlockGroup;\r\n    if (prevBlockGroup4Append != null) {\r\n        prevBlockGroup = prevBlockGroup4Append;\r\n        prevBlockGroup4Append = null;\r\n    }\r\n    replaceFailedStreamers();\r\n    LOG.debug(\"Allocating new block group. The previous block group: \" + prevBlockGroup);\r\n    final LocatedBlock lb;\r\n    try {\r\n        lb = addBlock(excludedNodes, dfsClient, src, prevBlockGroup, fileId, favoredNodes, getAddBlockFlags());\r\n    } catch (IOException ioe) {\r\n        closeAllStreamers();\r\n        throw ioe;\r\n    }\r\n    assert lb.isStriped();\r\n    currentBlockGroup = lb.getBlock();\r\n    blockGroupIndex++;\r\n    final LocatedBlock[] blocks = StripedBlockUtil.parseStripedBlockGroup((LocatedStripedBlock) lb, cellSize, numDataBlocks, numAllBlocks - numDataBlocks);\r\n    for (int i = 0; i < blocks.length; i++) {\r\n        StripedDataStreamer si = getStripedDataStreamer(i);\r\n        assert si.isHealthy();\r\n        if (blocks[i] == null) {\r\n            assert i >= numDataBlocks;\r\n            LOG.warn(\"Cannot allocate parity block(index={}, policy={}). \" + \"Exclude nodes={}. There may not be enough datanodes or \" + \"racks. You can check if the cluster topology supports \" + \"the enabled erasure coding policies by running the command \" + \"'hdfs ec -verifyClusterSetup'.\", i, ecPolicy.getName(), excludedNodes);\r\n            si.getLastException().set(new IOException(\"Failed to get parity block, index=\" + i));\r\n            si.getErrorState().setInternalError();\r\n            si.close(true);\r\n        } else {\r\n            coordinator.getFollowingBlocks().offer(i, blocks[i]);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "shouldEndBlockGroup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldEndBlockGroup()\n{\r\n    return currentBlockGroup != null && currentBlockGroup.getNumBytes() == blockSize * numDataBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "writeChunk",
  "errType" : [ "Exception", "IOException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void writeChunk(byte[] bytes, int offset, int len, byte[] checksum, int ckoff, int cklen) throws IOException\n{\r\n    final int index = getCurrentIndex();\r\n    final int pos = cellBuffers.addTo(index, bytes, offset, len);\r\n    final boolean cellFull = pos == cellSize;\r\n    if (currentBlockGroup == null || shouldEndBlockGroup()) {\r\n        allocateNewBlock();\r\n    }\r\n    currentBlockGroup.setNumBytes(currentBlockGroup.getNumBytes() + len);\r\n    final StripedDataStreamer current = getCurrentStreamer();\r\n    if (current.isHealthy()) {\r\n        try {\r\n            super.writeChunk(bytes, offset, len, checksum, ckoff, cklen);\r\n        } catch (Exception e) {\r\n            handleCurrentStreamerFailure(\"offset=\" + offset + \", length=\" + len, e);\r\n        }\r\n    }\r\n    if (cellFull) {\r\n        int next = index + 1;\r\n        if (next == numDataBlocks) {\r\n            cellBuffers.flipDataBuffers();\r\n            writeParityCells();\r\n            next = 0;\r\n            if (shouldEndBlockGroup()) {\r\n                flushAllInternals();\r\n                checkStreamerFailures(false);\r\n                for (int i = 0; i < numAllBlocks; i++) {\r\n                    final StripedDataStreamer s = setCurrentStreamer(i);\r\n                    if (s.isHealthy()) {\r\n                        try {\r\n                            endBlock();\r\n                        } catch (IOException ignored) {\r\n                        }\r\n                    }\r\n                }\r\n            } else {\r\n                checkStreamerFailures(true);\r\n            }\r\n        }\r\n        setCurrentStreamer(next);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "enqueueCurrentPacketFull",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void enqueueCurrentPacketFull() throws IOException\n{\r\n    LOG.debug(\"enqueue full {}, src={}, bytesCurBlock={}, blockSize={},\" + \" appendChunk={}, {}\", currentPacket, src, getStreamer().getBytesCurBlock(), blockSize, getStreamer().getAppendChunk(), getStreamer());\r\n    enqueueCurrentPacket();\r\n    adjustChunkBoundary();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isStreamerWriting",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isStreamerWriting(int streamerIndex)\n{\r\n    final long length = currentBlockGroup == null ? 0 : currentBlockGroup.getNumBytes();\r\n    if (length == 0) {\r\n        return false;\r\n    }\r\n    if (streamerIndex >= numDataBlocks) {\r\n        return true;\r\n    }\r\n    final int numCells = (int) ((length - 1) / cellSize + 1);\r\n    return streamerIndex < numCells;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "markExternalErrorOnStreamers",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "Set<StripedDataStreamer> markExternalErrorOnStreamers()\n{\r\n    Set<StripedDataStreamer> healthySet = new HashSet<>();\r\n    for (int i = 0; i < numAllBlocks; i++) {\r\n        final StripedDataStreamer streamer = getStripedDataStreamer(i);\r\n        if (streamer.isHealthy() && isStreamerWriting(i)) {\r\n            Preconditions.checkState(streamer.getStage() == BlockConstructionStage.DATA_STREAMING, \"streamer: \" + streamer);\r\n            streamer.setExternalError();\r\n            healthySet.add(streamer);\r\n        } else if (!streamer.streamerClosed() && streamer.getErrorState().hasDatanodeError() && streamer.getErrorState().doWaitForRestart()) {\r\n            healthySet.add(streamer);\r\n            failedStreamers.remove(streamer);\r\n        }\r\n    }\r\n    return healthySet;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkStreamerFailures",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void checkStreamerFailures(boolean isNeedFlushAllPackets) throws IOException\n{\r\n    Set<StripedDataStreamer> newFailed = checkStreamers();\r\n    if (newFailed.size() == 0) {\r\n        return;\r\n    }\r\n    if (isNeedFlushAllPackets) {\r\n        flushAllInternals();\r\n    }\r\n    newFailed = checkStreamers();\r\n    while (newFailed.size() > 0) {\r\n        failedStreamers.addAll(newFailed);\r\n        coordinator.clearFailureStates();\r\n        corruptBlockCountMap.put(blockGroupIndex, failedStreamers.size());\r\n        Set<StripedDataStreamer> healthySet = markExternalErrorOnStreamers();\r\n        final ExtendedBlock newBG = updateBlockForPipeline(healthySet);\r\n        newFailed = waitCreatingStreamers(healthySet);\r\n        if (newFailed.size() + failedStreamers.size() > numAllBlocks - numDataBlocks) {\r\n            closeAllStreamers();\r\n            throw new IOException(\"Data streamers failed while creating new block streams: \" + newFailed + \". There are not enough healthy streamers.\");\r\n        }\r\n        for (StripedDataStreamer failedStreamer : newFailed) {\r\n            assert !failedStreamer.isHealthy();\r\n        }\r\n        if (newFailed.size() == 0) {\r\n            for (StripedDataStreamer streamer : healthySet) {\r\n                assert streamer.isHealthy();\r\n                streamer.getErrorState().reset();\r\n            }\r\n            updatePipeline(newBG);\r\n        }\r\n        for (int i = 0; i < numAllBlocks; i++) {\r\n            coordinator.offerStreamerUpdateResult(i, newFailed.size() == 0);\r\n        }\r\n        if (newFailed.size() != 0) {\r\n            try {\r\n                Thread.sleep(datanodeRestartTimeout);\r\n            } catch (InterruptedException e) {\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkStreamerUpdates",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int checkStreamerUpdates(Set<StripedDataStreamer> failed, Set<StripedDataStreamer> streamers)\n{\r\n    for (StripedDataStreamer streamer : streamers) {\r\n        if (!coordinator.updateStreamerMap.containsKey(streamer)) {\r\n            if (!streamer.isHealthy() && coordinator.getNewBlocks().peek(streamer.getIndex()) != null) {\r\n                failed.add(streamer);\r\n            }\r\n        }\r\n    }\r\n    return coordinator.updateStreamerMap.size() + failed.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "waitCreatingStreamers",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "Set<StripedDataStreamer> waitCreatingStreamers(Set<StripedDataStreamer> healthyStreamers) throws IOException\n{\r\n    Set<StripedDataStreamer> failed = new HashSet<>();\r\n    final int expectedNum = healthyStreamers.size();\r\n    final long socketTimeout = dfsClient.getConf().getSocketTimeout();\r\n    long remaingTime = socketTimeout > 0 ? socketTimeout / 2 : Long.MAX_VALUE;\r\n    final long waitInterval = 1000;\r\n    synchronized (coordinator) {\r\n        while (checkStreamerUpdates(failed, healthyStreamers) < expectedNum && remaingTime > 0) {\r\n            try {\r\n                long start = Time.monotonicNow();\r\n                coordinator.wait(waitInterval);\r\n                remaingTime -= Time.monotonicNow() - start;\r\n            } catch (InterruptedException e) {\r\n                throw DFSUtilClient.toInterruptedIOException(\"Interrupted when waiting\" + \" for results of updating striped streamers\", e);\r\n            }\r\n        }\r\n    }\r\n    synchronized (coordinator) {\r\n        for (StripedDataStreamer streamer : healthyStreamers) {\r\n            if (!coordinator.updateStreamerMap.containsKey(streamer)) {\r\n                LOG.info(\"close the slow stream \" + streamer);\r\n                streamer.setStreamerAsClosed();\r\n                failed.add(streamer);\r\n            }\r\n        }\r\n    }\r\n    for (Map.Entry<StripedDataStreamer, Boolean> entry : coordinator.updateStreamerMap.entrySet()) {\r\n        if (!entry.getValue()) {\r\n            failed.add(entry.getKey());\r\n        }\r\n    }\r\n    for (StripedDataStreamer failedStreamer : failed) {\r\n        healthyStreamers.remove(failedStreamer);\r\n    }\r\n    return failed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "updateBlockForPipeline",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "ExtendedBlock updateBlockForPipeline(Set<StripedDataStreamer> healthyStreamers) throws IOException\n{\r\n    final LocatedBlock updated = dfsClient.namenode.updateBlockForPipeline(currentBlockGroup, dfsClient.clientName);\r\n    final long newGS = updated.getBlock().getGenerationStamp();\r\n    ExtendedBlock newBlock = new ExtendedBlock(currentBlockGroup);\r\n    newBlock.setGenerationStamp(newGS);\r\n    final LocatedBlock[] updatedBlks = StripedBlockUtil.parseStripedBlockGroup((LocatedStripedBlock) updated, cellSize, numDataBlocks, numAllBlocks - numDataBlocks);\r\n    for (int i = 0; i < numAllBlocks; i++) {\r\n        StripedDataStreamer si = getStripedDataStreamer(i);\r\n        if (healthyStreamers.contains(si)) {\r\n            final LocatedBlock lb = new LocatedBlock(new ExtendedBlock(newBlock), null, null, null, -1, updated.isCorrupt(), null);\r\n            lb.setBlockToken(updatedBlks[i].getBlockToken());\r\n            coordinator.getNewBlocks().offer(i, lb);\r\n        }\r\n    }\r\n    return newBlock;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "updatePipeline",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void updatePipeline(ExtendedBlock newBG) throws IOException\n{\r\n    final DatanodeInfo[] newNodes = new DatanodeInfo[numAllBlocks];\r\n    final String[] newStorageIDs = new String[numAllBlocks];\r\n    for (int i = 0; i < numAllBlocks; i++) {\r\n        final StripedDataStreamer streamer = getStripedDataStreamer(i);\r\n        final DatanodeInfo[] nodes = streamer.getNodes();\r\n        final String[] storageIDs = streamer.getStorageIDs();\r\n        if (streamer.isHealthy() && nodes != null && storageIDs != null) {\r\n            newNodes[i] = nodes[0];\r\n            newStorageIDs[i] = storageIDs[0];\r\n        } else {\r\n            newNodes[i] = new DatanodeInfoBuilder().setNodeID(DatanodeID.EMPTY_DATANODE_ID).build();\r\n            newStorageIDs[i] = \"\";\r\n        }\r\n    }\r\n    final long sentBytes = currentBlockGroup.getNumBytes();\r\n    final long ackedBytes = getAckedLength();\r\n    Preconditions.checkState(ackedBytes <= sentBytes, \"Acked:\" + ackedBytes + \", Sent:\" + sentBytes);\r\n    currentBlockGroup.setNumBytes(ackedBytes);\r\n    newBG.setNumBytes(ackedBytes);\r\n    dfsClient.namenode.updatePipeline(dfsClient.clientName, currentBlockGroup, newBG, newNodes, newStorageIDs);\r\n    currentBlockGroup = newBG;\r\n    currentBlockGroup.setNumBytes(sentBytes);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBlockLengths",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<Long> getBlockLengths()\n{\r\n    List<Long> blockLengths = new ArrayList<>(numAllBlocks);\r\n    for (int i = 0; i < numAllBlocks; i++) {\r\n        final StripedDataStreamer streamer = getStripedDataStreamer(i);\r\n        long numBytes = -1;\r\n        if (streamer.isHealthy()) {\r\n            if (streamer.getBlock() != null) {\r\n                numBytes = streamer.getBlock().getNumBytes();\r\n            }\r\n        }\r\n        blockLengths.add(numBytes);\r\n    }\r\n    return blockLengths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAckedLength",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "long getAckedLength()\n{\r\n    final long sentBytes = currentBlockGroup.getNumBytes();\r\n    final long numFullStripes = sentBytes / numDataBlocks / cellSize;\r\n    final long fullStripeLength = numFullStripes * numDataBlocks * cellSize;\r\n    assert fullStripeLength <= sentBytes : \"Full stripe length can't be \" + \"greater than the block group length\";\r\n    long ackedLength = 0;\r\n    List<Long> blockLengths = Collections.unmodifiableList(getBlockLengths());\r\n    List<Long> sortedBlockLengths = new ArrayList<>(blockLengths);\r\n    Collections.sort(sortedBlockLengths);\r\n    if (numFullStripes > 0) {\r\n        final int offset = sortedBlockLengths.size() - numDataBlocks;\r\n        ackedLength = sortedBlockLengths.get(offset) * numDataBlocks;\r\n    }\r\n    if (ackedLength < fullStripeLength) {\r\n        return ackedLength;\r\n    }\r\n    if (ackedLength == sentBytes) {\r\n        return ackedLength;\r\n    }\r\n    final int numFullDataCells = (int) ((sentBytes - fullStripeLength) / cellSize);\r\n    final int partialLength = (int) (sentBytes - fullStripeLength) % cellSize;\r\n    final int numPartialDataCells = partialLength == 0 ? 0 : 1;\r\n    final int numEmptyDataCells = numDataBlocks - numFullDataCells - numPartialDataCells;\r\n    final int parityLength = numFullDataCells > 0 ? cellSize : partialLength;\r\n    final long fullStripeBlockOffset = fullStripeLength / numDataBlocks;\r\n    long[] expectedBlockLengths = new long[numAllBlocks];\r\n    int idx = 0;\r\n    for (; idx < numFullDataCells; idx++) {\r\n        expectedBlockLengths[idx] = fullStripeBlockOffset + cellSize;\r\n    }\r\n    for (; idx < numFullDataCells + numPartialDataCells; idx++) {\r\n        expectedBlockLengths[idx] = fullStripeBlockOffset + partialLength;\r\n    }\r\n    for (; idx < numFullDataCells + numPartialDataCells + numEmptyDataCells; idx++) {\r\n        expectedBlockLengths[idx] = fullStripeBlockOffset;\r\n    }\r\n    for (; idx < numAllBlocks; idx++) {\r\n        expectedBlockLengths[idx] = fullStripeBlockOffset + parityLength;\r\n    }\r\n    int numBlocksWithCorrectLength = 0;\r\n    for (int i = 0; i < numAllBlocks; i++) {\r\n        if (blockLengths.get(i) == expectedBlockLengths[i]) {\r\n            numBlocksWithCorrectLength++;\r\n        }\r\n    }\r\n    if (numBlocksWithCorrectLength >= numDataBlocks) {\r\n        ackedLength = sentBytes;\r\n    }\r\n    return ackedLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "stripeDataSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int stripeDataSize()\n{\r\n    return numDataBlocks * cellSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "hasCapability",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasCapability(String capability)\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "hflush",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void hflush()\n{\r\n    LOG.debug(\"DFSStripedOutputStream does not support hflush. \" + \"Caller should check StreamCapabilities before calling.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "hsync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void hsync()\n{\r\n    LOG.debug(\"DFSStripedOutputStream does not support hsync. \" + \"Caller should check StreamCapabilities before calling.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "hsync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void hsync(EnumSet<SyncFlag> syncFlags)\n{\r\n    LOG.debug(\"DFSStripedOutputStream does not support hsync {}. \" + \"Caller should check StreamCapabilities before calling.\", syncFlags);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "start",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void start()\n{\r\n    for (StripedDataStreamer streamer : streamers) {\r\n        streamer.start();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "abort",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void abort() throws IOException\n{\r\n    final MultipleIOException.Builder b = new MultipleIOException.Builder();\r\n    synchronized (this) {\r\n        if (isClosed()) {\r\n            return;\r\n        }\r\n        exceptionLastSeen.set(new IOException(\"Lease timeout of \" + (dfsClient.getConf().getHdfsTimeout() / 1000) + \" seconds expired.\"));\r\n        try {\r\n            closeThreads(true);\r\n        } catch (IOException e) {\r\n            b.add(e);\r\n        }\r\n    }\r\n    dfsClient.endFileLease(fileId);\r\n    final IOException ioe = b.build();\r\n    if (ioe != null) {\r\n        throw ioe;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isClosed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isClosed()\n{\r\n    if (closed) {\r\n        return true;\r\n    }\r\n    for (StripedDataStreamer s : streamers) {\r\n        if (!s.streamerClosed()) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "closeThreads",
  "errType" : [ "Exception", "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void closeThreads(boolean force) throws IOException\n{\r\n    final MultipleIOException.Builder b = new MultipleIOException.Builder();\r\n    try {\r\n        for (StripedDataStreamer streamer : streamers) {\r\n            try {\r\n                streamer.close(force);\r\n                streamer.join();\r\n                streamer.closeSocket();\r\n            } catch (Exception e) {\r\n                try {\r\n                    handleStreamerFailure(\"force=\" + force, e, streamer);\r\n                } catch (IOException ioe) {\r\n                    b.add(ioe);\r\n                }\r\n            } finally {\r\n                streamer.setSocketToNull();\r\n            }\r\n        }\r\n    } finally {\r\n        setClosed();\r\n    }\r\n    final IOException ioe = b.build();\r\n    if (ioe != null) {\r\n        throw ioe;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "generateParityCellsForLastStripe",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean generateParityCellsForLastStripe()\n{\r\n    final long currentBlockGroupBytes = currentBlockGroup == null ? 0 : currentBlockGroup.getNumBytes();\r\n    final long lastStripeSize = currentBlockGroupBytes % stripeDataSize();\r\n    if (lastStripeSize == 0) {\r\n        return false;\r\n    }\r\n    final long parityCellSize = lastStripeSize < cellSize ? lastStripeSize : cellSize;\r\n    final ByteBuffer[] buffers = cellBuffers.getBuffers();\r\n    for (int i = 0; i < numAllBlocks; i++) {\r\n        final int position = buffers[i].position();\r\n        assert position <= parityCellSize : \"If an internal block is smaller\" + \" than parity block, then its last cell should be small than last\" + \" parity cell\";\r\n        for (int j = 0; j < parityCellSize - position; j++) {\r\n            buffers[i].put((byte) 0);\r\n        }\r\n        buffers[i].flip();\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "writeParityCells",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void writeParityCells() throws IOException\n{\r\n    final ByteBuffer[] buffers = cellBuffers.getBuffers();\r\n    if (!checkAnyParityStreamerIsHealthy()) {\r\n        return;\r\n    }\r\n    encode(encoder, numDataBlocks, buffers);\r\n    for (int i = numDataBlocks; i < numAllBlocks; i++) {\r\n        writeParity(i, buffers[i], cellBuffers.getChecksumArray(i));\r\n    }\r\n    cellBuffers.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkAnyParityStreamerIsHealthy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean checkAnyParityStreamerIsHealthy()\n{\r\n    for (int i = numDataBlocks; i < numAllBlocks; i++) {\r\n        if (streamers.get(i).isHealthy()) {\r\n            return true;\r\n        }\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Skips encoding and writing parity cells as there are \" + \"no healthy parity data streamers: \" + streamers);\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "writeParity",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void writeParity(int index, ByteBuffer buffer, byte[] checksumBuf) throws IOException\n{\r\n    final StripedDataStreamer current = setCurrentStreamer(index);\r\n    final int len = buffer.limit();\r\n    final long oldBytes = current.getBytesCurBlock();\r\n    if (current.isHealthy()) {\r\n        try {\r\n            DataChecksum sum = getDataChecksum();\r\n            if (buffer.isDirect()) {\r\n                ByteBuffer directCheckSumBuf = BUFFER_POOL.getBuffer(true, checksumBuf.length);\r\n                sum.calculateChunkedSums(buffer, directCheckSumBuf);\r\n                directCheckSumBuf.get(checksumBuf);\r\n                BUFFER_POOL.putBuffer(directCheckSumBuf);\r\n            } else {\r\n                sum.calculateChunkedSums(buffer.array(), 0, len, checksumBuf, 0);\r\n            }\r\n            for (int i = 0; i < len; i += sum.getBytesPerChecksum()) {\r\n                int chunkLen = Math.min(sum.getBytesPerChecksum(), len - i);\r\n                int ckOffset = i / sum.getBytesPerChecksum() * getChecksumSize();\r\n                super.writeChunk(buffer, chunkLen, checksumBuf, ckOffset, getChecksumSize());\r\n            }\r\n        } catch (Exception e) {\r\n            handleCurrentStreamerFailure(\"oldBytes=\" + oldBytes + \", len=\" + len, e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setClosed",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setClosed()\n{\r\n    super.setClosed();\r\n    for (int i = 0; i < numAllBlocks; i++) {\r\n        getStripedDataStreamer(i).release();\r\n    }\r\n    cellBuffers.release();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "closeImpl",
  "errType" : [ "ClosedChannelException", "IOException", "IOException", "Exception" ],
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void closeImpl() throws IOException\n{\r\n    boolean recoverLeaseOnCloseException = dfsClient.getConfiguration().getBoolean(RECOVER_LEASE_ON_CLOSE_EXCEPTION_KEY, RECOVER_LEASE_ON_CLOSE_EXCEPTION_DEFAULT);\r\n    try {\r\n        if (isClosed()) {\r\n            exceptionLastSeen.check(true);\r\n            final int minReplication = ecPolicy.getNumDataUnits();\r\n            int goodStreamers = 0;\r\n            final MultipleIOException.Builder b = new MultipleIOException.Builder();\r\n            for (final StripedDataStreamer si : streamers) {\r\n                try {\r\n                    si.getLastException().check(true);\r\n                    goodStreamers++;\r\n                } catch (IOException e) {\r\n                    b.add(e);\r\n                }\r\n            }\r\n            if (goodStreamers < minReplication) {\r\n                final IOException ioe = b.build();\r\n                if (ioe != null) {\r\n                    throw ioe;\r\n                }\r\n            }\r\n            return;\r\n        }\r\n        try {\r\n            flushBuffer();\r\n            if (generateParityCellsForLastStripe()) {\r\n                writeParityCells();\r\n            }\r\n            enqueueAllCurrentPackets();\r\n            flushAllInternals();\r\n            checkStreamerFailures(false);\r\n            for (int i = 0; i < numAllBlocks; i++) {\r\n                final StripedDataStreamer s = setCurrentStreamer(i);\r\n                if (s.isHealthy()) {\r\n                    try {\r\n                        if (s.getBytesCurBlock() > 0) {\r\n                            setCurrentPacketToEmpty();\r\n                        }\r\n                        flushInternal();\r\n                    } catch (Exception e) {\r\n                    }\r\n                }\r\n            }\r\n        } finally {\r\n            closeThreads(true);\r\n        }\r\n        try (TraceScope ignored = dfsClient.getTracer().newScope(\"completeFile\")) {\r\n            completeFile(currentBlockGroup);\r\n        }\r\n        logCorruptBlocks();\r\n    } catch (ClosedChannelException ignored) {\r\n    } catch (IOException ioe) {\r\n        recoverLease(recoverLeaseOnCloseException);\r\n        throw ioe;\r\n    } finally {\r\n        setClosed();\r\n        flushAllExecutor.shutdownNow();\r\n        encoder.release();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 5,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "enqueueAllCurrentPackets",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void enqueueAllCurrentPackets() throws IOException\n{\r\n    int idx = streamers.indexOf(getCurrentStreamer());\r\n    for (int i = 0; i < streamers.size(); i++) {\r\n        final StripedDataStreamer si = setCurrentStreamer(i);\r\n        if (si.isHealthy() && currentPacket != null) {\r\n            try {\r\n                enqueueCurrentPacket();\r\n            } catch (IOException e) {\r\n                handleCurrentStreamerFailure(\"enqueueAllCurrentPackets, i=\" + i, e);\r\n            }\r\n        }\r\n    }\r\n    setCurrentStreamer(idx);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "flushAllInternals",
  "errType" : [ "Exception", "InterruptedException", "ExecutionException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void flushAllInternals() throws IOException\n{\r\n    Map<Future<Void>, Integer> flushAllFuturesMap = new HashMap<>();\r\n    Future<Void> future = null;\r\n    int current = getCurrentIndex();\r\n    for (int i = 0; i < numAllBlocks; i++) {\r\n        final StripedDataStreamer s = setCurrentStreamer(i);\r\n        if (s.isHealthy()) {\r\n            try {\r\n                final long toWaitFor = flushInternalWithoutWaitingAck();\r\n                future = flushAllExecutorCompletionService.submit(new Callable<Void>() {\r\n\r\n                    @Override\r\n                    public Void call() throws Exception {\r\n                        s.waitForAckedSeqno(toWaitFor);\r\n                        return null;\r\n                    }\r\n                });\r\n                flushAllFuturesMap.put(future, i);\r\n            } catch (Exception e) {\r\n                handleCurrentStreamerFailure(\"flushInternal \" + s, e);\r\n            }\r\n        }\r\n    }\r\n    setCurrentStreamer(current);\r\n    for (int i = 0; i < flushAllFuturesMap.size(); i++) {\r\n        try {\r\n            future = flushAllExecutorCompletionService.take();\r\n            future.get();\r\n        } catch (InterruptedException ie) {\r\n            throw DFSUtilClient.toInterruptedIOException(\"Interrupted during waiting all streamer flush, \", ie);\r\n        } catch (ExecutionException ee) {\r\n            LOG.warn(\"Caught ExecutionException while waiting all streamer flush, \", ee);\r\n            StripedDataStreamer s = streamers.get(flushAllFuturesMap.get(future));\r\n            handleStreamerFailure(\"flushInternal \" + s, (Exception) ee.getCause(), s);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "sleep",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void sleep(long ms, String op) throws InterruptedIOException\n{\r\n    try {\r\n        Thread.sleep(ms);\r\n    } catch (InterruptedException ie) {\r\n        throw DFSUtilClient.toInterruptedIOException(\"Sleep interrupted during \" + op, ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "logCorruptBlocks",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void logCorruptBlocks()\n{\r\n    for (Map.Entry<Integer, Integer> entry : corruptBlockCountMap.entrySet()) {\r\n        int bgIndex = entry.getKey();\r\n        int corruptBlockCount = entry.getValue();\r\n        StringBuilder sb = new StringBuilder();\r\n        sb.append(\"Block group <\").append(bgIndex).append(\"> failed to write \").append(corruptBlockCount).append(\" blocks.\");\r\n        if (corruptBlockCount == numAllBlocks - numDataBlocks) {\r\n            sb.append(\" It's at high risk of losing data.\");\r\n        }\r\n        LOG.warn(sb.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ExtendedBlock getBlock()\n{\r\n    return currentBlockGroup;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "createSWebHdfsFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SWebHdfsFileSystem createSWebHdfsFileSystem(Configuration conf)\n{\r\n    SWebHdfsFileSystem fs = new SWebHdfsFileSystem();\r\n    fs.setConf(conf);\r\n    return fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setFileName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockReaderFactory setFileName(String fileName)\n{\r\n    this.fileName = fileName;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockReaderFactory setBlock(ExtendedBlock block)\n{\r\n    this.block = block;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setBlockToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockReaderFactory setBlockToken(Token<BlockTokenIdentifier> token)\n{\r\n    this.token = token;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setStartOffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockReaderFactory setStartOffset(long startOffset)\n{\r\n    this.startOffset = startOffset;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setVerifyChecksum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockReaderFactory setVerifyChecksum(boolean verifyChecksum)\n{\r\n    this.verifyChecksum = verifyChecksum;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setClientName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockReaderFactory setClientName(String clientName)\n{\r\n    this.clientName = clientName;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setDatanodeInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockReaderFactory setDatanodeInfo(DatanodeInfo datanode)\n{\r\n    this.datanode = datanode;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setStorageType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockReaderFactory setStorageType(StorageType storageType)\n{\r\n    this.storageType = storageType;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setAllowShortCircuitLocalReads",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockReaderFactory setAllowShortCircuitLocalReads(boolean allowShortCircuitLocalReads)\n{\r\n    this.allowShortCircuitLocalReads = allowShortCircuitLocalReads;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setClientCacheContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockReaderFactory setClientCacheContext(ClientContext clientContext)\n{\r\n    this.clientContext = clientContext;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockReaderFactory setLength(long length)\n{\r\n    this.length = length;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setCachingStrategy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockReaderFactory setCachingStrategy(CachingStrategy cachingStrategy)\n{\r\n    this.cachingStrategy = cachingStrategy;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setInetSocketAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockReaderFactory setInetSocketAddress(InetSocketAddress inetSocketAddress)\n{\r\n    this.inetSocketAddress = inetSocketAddress;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setUserGroupInformation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockReaderFactory setUserGroupInformation(UserGroupInformation userGroupInformation)\n{\r\n    this.userGroupInformation = userGroupInformation;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setRemotePeerFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockReaderFactory setRemotePeerFactory(RemotePeerFactory remotePeerFactory)\n{\r\n    this.remotePeerFactory = remotePeerFactory;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setConfiguration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockReaderFactory setConfiguration(Configuration configuration)\n{\r\n    this.configuration = configuration;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setFailureInjectorForTesting",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFailureInjectorForTesting(FailureInjector injector)\n{\r\n    failureInjector = injector;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "build",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "BlockReader build() throws IOException\n{\r\n    Preconditions.checkNotNull(configuration);\r\n    Preconditions.checkState(length >= 0, \"Length must be set to a non-negative value\");\r\n    BlockReader reader = tryToCreateExternalBlockReader();\r\n    if (reader != null) {\r\n        return reader;\r\n    }\r\n    final ShortCircuitConf scConf = conf.getShortCircuitConf();\r\n    try {\r\n        if (scConf.isShortCircuitLocalReads() && allowShortCircuitLocalReads) {\r\n            if (clientContext.getUseLegacyBlockReaderLocal()) {\r\n                reader = getLegacyBlockReaderLocal();\r\n                if (reader != null) {\r\n                    LOG.trace(\"{}: returning new legacy block reader local.\", this);\r\n                    return reader;\r\n                }\r\n            } else {\r\n                reader = getBlockReaderLocal();\r\n                if (reader != null) {\r\n                    LOG.trace(\"{}: returning new block reader local.\", this);\r\n                    return reader;\r\n                }\r\n            }\r\n        }\r\n        if (scConf.isDomainSocketDataTraffic()) {\r\n            reader = getRemoteBlockReaderFromDomain();\r\n            if (reader != null) {\r\n                LOG.trace(\"{}: returning new remote block reader using UNIX domain \" + \"socket on {}\", this, pathInfo.getPath());\r\n                return reader;\r\n            }\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Block read failed. Getting remote block reader using TCP\", e);\r\n    }\r\n    Preconditions.checkState(!DFSInputStream.tcpReadsDisabledForTesting, \"TCP reads were disabled for testing, but we failed to \" + \"do a non-TCP read.\");\r\n    return getRemoteBlockReaderFromTcp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "tryToCreateExternalBlockReader",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "BlockReader tryToCreateExternalBlockReader()\n{\r\n    List<Class<? extends ReplicaAccessorBuilder>> clses = conf.getReplicaAccessorBuilderClasses();\r\n    for (Class<? extends ReplicaAccessorBuilder> cls : clses) {\r\n        try {\r\n            ByteArrayDataOutput bado = ByteStreams.newDataOutput();\r\n            token.write(bado);\r\n            byte[] tokenBytes = bado.toByteArray();\r\n            Constructor<? extends ReplicaAccessorBuilder> ctor = cls.getConstructor();\r\n            ReplicaAccessorBuilder builder = ctor.newInstance();\r\n            long visibleLength = startOffset + length;\r\n            ReplicaAccessor accessor = builder.setAllowShortCircuitReads(allowShortCircuitLocalReads).setBlock(block.getBlockId(), block.getBlockPoolId()).setGenerationStamp(block.getGenerationStamp()).setBlockAccessToken(tokenBytes).setClientName(clientName).setConfiguration(configuration).setFileName(fileName).setVerifyChecksum(verifyChecksum).setVisibleLength(visibleLength).build();\r\n            if (accessor == null) {\r\n                LOG.trace(\"{}: No ReplicaAccessor created by {}\", this, cls.getName());\r\n            } else {\r\n                return new ExternalBlockReader(accessor, visibleLength, startOffset);\r\n            }\r\n        } catch (Throwable t) {\r\n            LOG.warn(\"Failed to construct new object of type \" + cls.getName(), t);\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getLegacyBlockReaderLocal",
  "errType" : [ "RemoteException", "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "BlockReader getLegacyBlockReaderLocal() throws IOException\n{\r\n    LOG.trace(\"{}: trying to construct BlockReaderLocalLegacy\", this);\r\n    if (!DFSUtilClient.isLocalAddress(inetSocketAddress)) {\r\n        LOG.trace(\"{}: can't construct BlockReaderLocalLegacy because the address\" + \"{} is not local\", this, inetSocketAddress);\r\n        return null;\r\n    }\r\n    if (clientContext.getDisableLegacyBlockReaderLocal()) {\r\n        PerformanceAdvisory.LOG.debug(\"{}: can't construct \" + \"BlockReaderLocalLegacy because \" + \"disableLegacyBlockReaderLocal is set.\", this);\r\n        return null;\r\n    }\r\n    IOException ioe;\r\n    try {\r\n        return BlockReaderLocalLegacy.newBlockReader(conf, userGroupInformation, configuration, fileName, block, token, datanode, startOffset, length, storageType);\r\n    } catch (RemoteException remoteException) {\r\n        ioe = remoteException.unwrapRemoteException(InvalidToken.class, AccessControlException.class);\r\n    } catch (IOException e) {\r\n        ioe = e;\r\n    }\r\n    if ((!(ioe instanceof AccessControlException)) && isSecurityException(ioe)) {\r\n        throw ioe;\r\n    }\r\n    LOG.warn(this + \": error creating legacy BlockReaderLocal.  \" + \"Disabling legacy local reads.\", ioe);\r\n    clientContext.setDisableLegacyBlockReaderLocal();\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getBlockReaderLocal",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "BlockReader getBlockReaderLocal() throws IOException\n{\r\n    LOG.trace(\"{}: trying to construct a BlockReaderLocal for short-circuit \" + \" reads.\", this);\r\n    if (pathInfo == null) {\r\n        pathInfo = clientContext.getDomainSocketFactory().getPathInfo(inetSocketAddress, conf.getShortCircuitConf());\r\n    }\r\n    if (!pathInfo.getPathState().getUsableForShortCircuit()) {\r\n        PerformanceAdvisory.LOG.debug(\"{}: {} is not usable for short circuit; \" + \"giving up on BlockReaderLocal.\", this, pathInfo);\r\n        return null;\r\n    }\r\n    ShortCircuitCache cache = clientContext.getShortCircuitCache(block.getBlockId());\r\n    ExtendedBlockId key = new ExtendedBlockId(block.getBlockId(), block.getBlockPoolId());\r\n    ShortCircuitReplicaInfo info = cache.fetchOrCreate(key, this);\r\n    InvalidToken exc = info.getInvalidTokenException();\r\n    if (exc != null) {\r\n        LOG.trace(\"{}: got InvalidToken exception while trying to construct \" + \"BlockReaderLocal via {}\", this, pathInfo.getPath());\r\n        throw exc;\r\n    }\r\n    if (info.getReplica() == null) {\r\n        PerformanceAdvisory.LOG.debug(\"{}: failed to get \" + \"ShortCircuitReplica. Cannot construct \" + \"BlockReaderLocal via {}\", this, pathInfo.getPath());\r\n        return null;\r\n    }\r\n    return new BlockReaderLocal.Builder(conf.getShortCircuitConf()).setFilename(fileName).setBlock(block).setStartOffset(startOffset).setShortCircuitReplica(info.getReplica()).setVerifyChecksum(verifyChecksum).setCachingStrategy(cachingStrategy).setStorageType(storageType).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "createShortCircuitReplicaInfo",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "ShortCircuitReplicaInfo createShortCircuitReplicaInfo()\n{\r\n    if (createShortCircuitReplicaInfoCallback != null) {\r\n        ShortCircuitReplicaInfo info = createShortCircuitReplicaInfoCallback.createShortCircuitReplicaInfo();\r\n        if (info != null)\r\n            return info;\r\n    }\r\n    LOG.trace(\"{}: trying to create ShortCircuitReplicaInfo.\", this);\r\n    BlockReaderPeer curPeer;\r\n    while (true) {\r\n        curPeer = nextDomainPeer();\r\n        if (curPeer == null)\r\n            break;\r\n        if (curPeer.fromCache)\r\n            remainingCacheTries--;\r\n        DomainPeer peer = (DomainPeer) curPeer.peer;\r\n        Slot slot = null;\r\n        ShortCircuitCache cache = clientContext.getShortCircuitCache(block.getBlockId());\r\n        try {\r\n            MutableBoolean usedPeer = new MutableBoolean(false);\r\n            slot = cache.allocShmSlot(datanode, peer, usedPeer, new ExtendedBlockId(block.getBlockId(), block.getBlockPoolId()), clientName);\r\n            if (usedPeer.booleanValue()) {\r\n                LOG.trace(\"{}: allocShmSlot used up our previous socket {}.  \" + \"Allocating a new one...\", this, peer.getDomainSocket());\r\n                curPeer = nextDomainPeer();\r\n                if (curPeer == null)\r\n                    break;\r\n                peer = (DomainPeer) curPeer.peer;\r\n            }\r\n            ShortCircuitReplicaInfo info = requestFileDescriptors(peer, slot);\r\n            clientContext.getPeerCache().put(datanode, peer);\r\n            return info;\r\n        } catch (IOException e) {\r\n            if (slot != null) {\r\n                cache.freeSlot(slot);\r\n            }\r\n            if (curPeer.fromCache) {\r\n                LOG.debug(\"{}: closing stale domain peer {}\", this, peer, e);\r\n                IOUtilsClient.cleanupWithLogger(LOG, peer);\r\n            } else {\r\n                LOG.warn(this + \": I/O error requesting file descriptors.  \" + \"Disabling domain socket \" + peer.getDomainSocket(), e);\r\n                IOUtilsClient.cleanupWithLogger(LOG, peer);\r\n                clientContext.getDomainSocketFactory().disableDomainSocketPath(pathInfo.getPath());\r\n                return null;\r\n            }\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "requestFileDescriptors",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 28,
  "sourceCodeText" : "ShortCircuitReplicaInfo requestFileDescriptors(DomainPeer peer, Slot slot) throws IOException\n{\r\n    ShortCircuitCache cache = clientContext.getShortCircuitCache(block.getBlockId());\r\n    final DataOutputStream out = new DataOutputStream(new BufferedOutputStream(peer.getOutputStream(), SMALL_BUFFER_SIZE));\r\n    SlotId slotId = slot == null ? null : slot.getSlotId();\r\n    new Sender(out).requestShortCircuitFds(block, token, slotId, 1, failureInjector.getSupportsReceiptVerification());\r\n    DataInputStream in = new DataInputStream(peer.getInputStream());\r\n    BlockOpResponseProto resp = BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(in));\r\n    DomainSocket sock = peer.getDomainSocket();\r\n    failureInjector.injectRequestFileDescriptorsFailure();\r\n    switch(resp.getStatus()) {\r\n        case SUCCESS:\r\n            byte[] buf = new byte[1];\r\n            FileInputStream[] fis = new FileInputStream[2];\r\n            sock.recvFileInputStreams(fis, buf, 0, buf.length);\r\n            ShortCircuitReplica replica = null;\r\n            try {\r\n                if (fis[0] == null || fis[1] == null) {\r\n                    throw new IOException(\"the datanode \" + datanode + \" failed to \" + \"pass a file descriptor (might have reached open file limit).\");\r\n                }\r\n                ExtendedBlockId key = new ExtendedBlockId(block.getBlockId(), block.getBlockPoolId());\r\n                if (buf[0] == USE_RECEIPT_VERIFICATION.getNumber()) {\r\n                    LOG.trace(\"Sending receipt verification byte for slot {}\", slot);\r\n                    sock.getOutputStream().write(0);\r\n                }\r\n                replica = new ShortCircuitReplica(key, fis[0], fis[1], cache, Time.monotonicNow(), slot);\r\n                return new ShortCircuitReplicaInfo(replica);\r\n            } catch (IOException e) {\r\n                LOG.warn(this + \": error creating ShortCircuitReplica.\", e);\r\n                return null;\r\n            } finally {\r\n                if (replica == null) {\r\n                    IOUtilsClient.cleanupWithLogger(DFSClient.LOG, fis[0], fis[1]);\r\n                }\r\n            }\r\n        case ERROR_UNSUPPORTED:\r\n            if (!resp.hasShortCircuitAccessVersion()) {\r\n                LOG.warn(\"short-circuit read access is disabled for \" + \"DataNode \" + datanode + \".  reason: \" + resp.getMessage());\r\n                clientContext.getDomainSocketFactory().disableShortCircuitForPath(pathInfo.getPath());\r\n            } else {\r\n                LOG.warn(\"short-circuit read access for the file \" + fileName + \" is disabled for DataNode \" + datanode + \".  reason: \" + resp.getMessage());\r\n            }\r\n            return null;\r\n        case ERROR_ACCESS_TOKEN:\r\n            String msg = \"access control error while \" + \"attempting to set up short-circuit access to \" + fileName + resp.getMessage();\r\n            LOG.debug(\"{}:{}\", this, msg);\r\n            if (slot != null) {\r\n                cache.freeSlot(slot);\r\n            }\r\n            return new ShortCircuitReplicaInfo(new InvalidToken(msg));\r\n        default:\r\n            final long expiration = clientContext.getDomainSocketFactory().getPathExpireSeconds();\r\n            String disableMsg = \"disabled temporarily for \" + expiration + \" seconds\";\r\n            if (expiration == 0) {\r\n                disableMsg = \"not disabled\";\r\n            }\r\n            LOG.warn(\"{}: unknown response code {} while attempting to set up \" + \"short-circuit access. {}. Short-circuit read for \" + \"DataNode {} is {} based on {}.\", this, resp.getStatus(), resp.getMessage(), datanode, disableMsg, DFS_DOMAIN_SOCKET_DISABLE_INTERVAL_SECOND_KEY);\r\n            clientContext.getDomainSocketFactory().disableShortCircuitForPath(pathInfo.getPath());\r\n            return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getRemoteBlockReaderFromDomain",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "BlockReader getRemoteBlockReaderFromDomain() throws IOException\n{\r\n    if (pathInfo == null) {\r\n        pathInfo = clientContext.getDomainSocketFactory().getPathInfo(inetSocketAddress, conf.getShortCircuitConf());\r\n    }\r\n    if (!pathInfo.getPathState().getUsableForDataTransfer()) {\r\n        PerformanceAdvisory.LOG.debug(\"{}: not trying to create a \" + \"remote block reader because the UNIX domain socket at {}\" + \" is not usable.\", this, pathInfo);\r\n        return null;\r\n    }\r\n    LOG.trace(\"{}: trying to create a remote block reader from the UNIX domain \" + \"socket at {}\", this, pathInfo.getPath());\r\n    while (true) {\r\n        BlockReaderPeer curPeer = nextDomainPeer();\r\n        if (curPeer == null)\r\n            break;\r\n        if (curPeer.fromCache)\r\n            remainingCacheTries--;\r\n        DomainPeer peer = (DomainPeer) curPeer.peer;\r\n        BlockReader blockReader = null;\r\n        try {\r\n            blockReader = getRemoteBlockReader(peer);\r\n            return blockReader;\r\n        } catch (IOException ioe) {\r\n            IOUtilsClient.cleanupWithLogger(LOG, peer);\r\n            if (isSecurityException(ioe)) {\r\n                LOG.trace(\"{}: got security exception while constructing a remote \" + \" block reader from the unix domain socket at {}\", this, pathInfo.getPath(), ioe);\r\n                throw ioe;\r\n            }\r\n            if (curPeer.fromCache) {\r\n                LOG.debug(\"Closed potentially stale domain peer {}\", peer, ioe);\r\n            } else {\r\n                LOG.warn(\"I/O error constructing remote block reader.  Disabling \" + \"domain socket \" + peer.getDomainSocket(), ioe);\r\n                clientContext.getDomainSocketFactory().disableDomainSocketPath(pathInfo.getPath());\r\n                return null;\r\n            }\r\n        } finally {\r\n            if (blockReader == null) {\r\n                IOUtilsClient.cleanupWithLogger(LOG, peer);\r\n            }\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getRemoteBlockReaderFromTcp",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "BlockReader getRemoteBlockReaderFromTcp() throws IOException\n{\r\n    LOG.trace(\"{}: trying to create a remote block reader from a TCP socket\", this);\r\n    BlockReader blockReader = null;\r\n    while (true) {\r\n        BlockReaderPeer curPeer = null;\r\n        Peer peer = null;\r\n        try {\r\n            curPeer = nextTcpPeer();\r\n            if (curPeer.fromCache)\r\n                remainingCacheTries--;\r\n            peer = curPeer.peer;\r\n            blockReader = getRemoteBlockReader(peer);\r\n            return blockReader;\r\n        } catch (IOException ioe) {\r\n            if (isSecurityException(ioe)) {\r\n                LOG.trace(\"{}: got security exception while constructing a remote \" + \"block reader from {}\", this, peer, ioe);\r\n                throw ioe;\r\n            }\r\n            if ((curPeer != null) && curPeer.fromCache) {\r\n                LOG.debug(\"Closed potentially stale remote peer {}\", peer, ioe);\r\n            } else {\r\n                LOG.warn(\"I/O error constructing remote block reader.\", ioe);\r\n                throw ioe;\r\n            }\r\n        } finally {\r\n            if (blockReader == null) {\r\n                IOUtilsClient.cleanupWithLogger(LOG, peer);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "nextDomainPeer",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "BlockReaderPeer nextDomainPeer()\n{\r\n    if (remainingCacheTries > 0) {\r\n        Peer peer = clientContext.getPeerCache().get(datanode, true);\r\n        if (peer != null) {\r\n            LOG.trace(\"nextDomainPeer: reusing existing peer {}\", peer);\r\n            return new BlockReaderPeer(peer, true);\r\n        }\r\n    }\r\n    DomainSocket sock = clientContext.getDomainSocketFactory().createSocket(pathInfo, conf.getSocketTimeout());\r\n    if (sock == null)\r\n        return null;\r\n    return new BlockReaderPeer(new DomainPeer(sock), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "nextTcpPeer",
  "errType" : [ "IOException|UnresolvedAddressException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "BlockReaderPeer nextTcpPeer() throws IOException\n{\r\n    if (remainingCacheTries > 0) {\r\n        Peer peer = clientContext.getPeerCache().get(datanode, false);\r\n        if (peer != null) {\r\n            LOG.trace(\"nextTcpPeer: reusing existing peer {}\", peer);\r\n            return new BlockReaderPeer(peer, true);\r\n        }\r\n    }\r\n    try {\r\n        Peer peer = remotePeerFactory.newConnectedPeer(inetSocketAddress, token, datanode);\r\n        LOG.trace(\"nextTcpPeer: created newConnectedPeer {}\", peer);\r\n        return new BlockReaderPeer(peer, false);\r\n    } catch (IOException | UnresolvedAddressException e) {\r\n        LOG.trace(\"nextTcpPeer: failed to create newConnectedPeer connected to\" + \"{}\", datanode);\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "isSecurityException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSecurityException(IOException ioe)\n{\r\n    return (ioe instanceof InvalidToken) || (ioe instanceof InvalidEncryptionKeyException) || (ioe instanceof InvalidBlockTokenException) || (ioe instanceof AccessControlException);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getRemoteBlockReader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BlockReader getRemoteBlockReader(Peer peer) throws IOException\n{\r\n    int networkDistance = clientContext.getNetworkDistance(datanode);\r\n    return BlockReaderRemote.newBlockReader(fileName, block, token, startOffset, length, verifyChecksum, clientName, peer, datanode, clientContext.getPeerCache(), cachingStrategy, networkDistance, configuration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"BlockReaderFactory(fileName=\" + fileName + \", block=\" + block + \")\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getFileName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getFileName(final InetSocketAddress s, final String poolId, final long blockId)\n{\r\n    return s.toString() + \":\" + poolId + \":\" + blockId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "checkEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkEnabled()\n{\r\n    if (policy == Policy.DISABLE) {\r\n        throw new UnsupportedOperationException(\"This feature is disabled.  Please refer to \" + HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.ENABLE_KEY + \" configuration property.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "isBestEffort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isBestEffort()\n{\r\n    return bestEffort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "satisfy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean satisfy(final short replication, final DatanodeInfo[] existings, final boolean isAppend, final boolean isHflushed)\n{\r\n    final int n = existings == null ? 0 : existings.length;\r\n    return !(n == 0 || n >= replication) && policy.getCondition().satisfy(replication, existings, n, isAppend, isHflushed);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return policy.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ReplaceDatanodeOnFailure get(final Configuration conf)\n{\r\n    final Policy policy = getPolicy(conf);\r\n    final boolean bestEffort = conf.getBoolean(HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.BEST_EFFORT_KEY, HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.BEST_EFFORT_DEFAULT);\r\n    return new ReplaceDatanodeOnFailure(policy, bestEffort);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getPolicy",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Policy getPolicy(final Configuration conf)\n{\r\n    final boolean enabled = conf.getBoolean(HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.ENABLE_KEY, HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.ENABLE_DEFAULT);\r\n    if (!enabled) {\r\n        return Policy.DISABLE;\r\n    }\r\n    final String policy = conf.get(HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.POLICY_KEY, HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.POLICY_DEFAULT);\r\n    for (int i = 1; i < Policy.values().length; i++) {\r\n        final Policy p = Policy.values()[i];\r\n        if (p.name().equalsIgnoreCase(policy)) {\r\n            return p;\r\n        }\r\n    }\r\n    throw new HadoopIllegalArgumentException(\"Illegal configuration value for \" + HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.POLICY_KEY + \": \" + policy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void write(final Policy policy, final boolean bestEffort, final Configuration conf)\n{\r\n    conf.setBoolean(HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.ENABLE_KEY, policy != Policy.DISABLE);\r\n    conf.set(HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.POLICY_KEY, policy.name());\r\n    conf.setBoolean(HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.BEST_EFFORT_KEY, bestEffort);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getValue(final Configuration conf)\n{\r\n    return getValue() != null ? getValue() : conf.getInt(CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_KEY, CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void run()\n{\r\n    while (!Thread.currentThread().isInterrupted()) {\r\n        if (!waitForInterval()) {\r\n            return;\r\n        }\r\n        LOG.debug(\"Running refresh for {} streams\", registeredInputStreams.size());\r\n        long start = Time.monotonicNow();\r\n        AtomicInteger neededRefresh = new AtomicInteger(0);\r\n        Phaser phaser = new Phaser(1);\r\n        Map<String, InetSocketAddress> addressCache = new ConcurrentHashMap<>();\r\n        for (DFSInputStream inputStream : getInputStreams()) {\r\n            phaser.register();\r\n            refreshThreadPool.submit(() -> {\r\n                try {\r\n                    if (isInputStreamTracked(inputStream) && inputStream.refreshBlockLocations(addressCache)) {\r\n                        neededRefresh.incrementAndGet();\r\n                    }\r\n                } finally {\r\n                    phaser.arriveAndDeregister();\r\n                }\r\n            });\r\n        }\r\n        phaser.arriveAndAwaitAdvance();\r\n        synchronized (this) {\r\n            runCount++;\r\n            refreshCount += neededRefresh.get();\r\n        }\r\n        LOG.debug(\"Finished refreshing {} of {} streams in {}ms\", neededRefresh, registeredInputStreams.size(), Time.monotonicNow() - start);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getRunCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRunCount()\n{\r\n    return runCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getRefreshCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRefreshCount()\n{\r\n    return refreshCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "waitForInterval",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean waitForInterval()\n{\r\n    try {\r\n        Thread.sleep(interval + ThreadLocalRandom.current().nextLong(-jitter, jitter));\r\n        return true;\r\n    } catch (InterruptedException e) {\r\n        LOG.debug(\"Interrupted during wait interval\", e);\r\n        Thread.currentThread().interrupt();\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "shutdown",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void shutdown()\n{\r\n    if (isAlive()) {\r\n        interrupt();\r\n        try {\r\n            join();\r\n        } catch (InterruptedException e) {\r\n        }\r\n    }\r\n    refreshThreadPool.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getInputStreams",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Collection<DFSInputStream> getInputStreams()\n{\r\n    return new ArrayList<>(registeredInputStreams);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addInputStream",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addInputStream(DFSInputStream dfsInputStream)\n{\r\n    LOG.trace(\"Registering {} for {}\", dfsInputStream, dfsInputStream.getSrc());\r\n    registeredInputStreams.add(dfsInputStream);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeInputStream",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeInputStream(DFSInputStream dfsInputStream)\n{\r\n    if (isInputStreamTracked(dfsInputStream)) {\r\n        LOG.trace(\"De-registering {} for {}\", dfsInputStream, dfsInputStream.getSrc());\r\n        registeredInputStreams.remove(dfsInputStream);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isInputStreamTracked",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isInputStreamTracked(DFSInputStream dfsInputStream)\n{\r\n    return registeredInputStreams.contains(dfsInputStream);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getInterval",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getInterval()\n{\r\n    return interval;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getPath()\n{\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getOffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getOffset()\n{\r\n    return offset;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLength()\n{\r\n    return length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getNonce",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getNonce()\n{\r\n    return Arrays.copyOf(nonce, nonce.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (this == o) {\r\n        return true;\r\n    }\r\n    if (o == null || getClass() != o.getClass()) {\r\n        return false;\r\n    }\r\n    ProvidedStorageLocation that = (ProvidedStorageLocation) o;\r\n    if ((offset != that.offset) || (length != that.length) || !path.equals(that.path)) {\r\n        return false;\r\n    }\r\n    return Arrays.equals(nonce, that.nonce);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int hashCode()\n{\r\n    int result = path.hashCode();\r\n    result = 31 * result + (int) (offset ^ (offset >>> 32));\r\n    result = 31 * result + (int) (length ^ (length >>> 32));\r\n    result = 31 * result + Arrays.hashCode(nonce);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getFlag",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EnumSet<XAttrSetFlag> getFlag()\n{\r\n    return getValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "readFile",
  "errType" : [ "JsonMappingException", "Throwable" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "DatanodeAdminProperties[] readFile(final String hostsFilePath) throws IOException\n{\r\n    DatanodeAdminProperties[] allDNs = new DatanodeAdminProperties[0];\r\n    ObjectMapper objectMapper = new ObjectMapper();\r\n    File hostFile = new File(hostsFilePath);\r\n    boolean tryOldFormat = false;\r\n    if (hostFile.length() > 0) {\r\n        try (Reader input = new InputStreamReader(Files.newInputStream(hostFile.toPath()), \"UTF-8\")) {\r\n            allDNs = objectMapper.readValue(input, DatanodeAdminProperties[].class);\r\n        } catch (JsonMappingException jme) {\r\n            tryOldFormat = true;\r\n        }\r\n    } else {\r\n        LOG.warn(hostsFilePath + \" is empty.\" + REFER_TO_DOC_MSG);\r\n    }\r\n    if (tryOldFormat) {\r\n        ObjectReader objectReader = objectMapper.readerFor(DatanodeAdminProperties.class);\r\n        JsonFactory jsonFactory = new JsonFactory();\r\n        List<DatanodeAdminProperties> all = new ArrayList<>();\r\n        try (Reader input = new InputStreamReader(Files.newInputStream(Paths.get(hostsFilePath)), \"UTF-8\")) {\r\n            Iterator<DatanodeAdminProperties> iterator = objectReader.readValues(jsonFactory.createParser(input));\r\n            while (iterator.hasNext()) {\r\n                DatanodeAdminProperties properties = iterator.next();\r\n                all.add(properties);\r\n            }\r\n            LOG.warn(hostsFilePath + \" has legacy JSON format.\" + REFER_TO_DOC_MSG);\r\n        } catch (Throwable ex) {\r\n            LOG.warn(hostsFilePath + \" has invalid JSON format.\" + REFER_TO_DOC_MSG, ex);\r\n        }\r\n        allDNs = all.toArray(new DatanodeAdminProperties[all.size()]);\r\n    }\r\n    return allDNs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "retrieve",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long retrieve(long record)\n{\r\n    return (record & MASK) >>> OFFSET;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "combine",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long combine(long value, long record)\n{\r\n    if (value < MIN) {\r\n        throw new IllegalArgumentException(\"Illagal value: \" + NAME + \" = \" + value + \" < MIN = \" + MIN);\r\n    }\r\n    if (value > MAX) {\r\n        throw new IllegalArgumentException(\"Illagal value: \" + NAME + \" = \" + value + \" > MAX = \" + MAX);\r\n    }\r\n    return (record & ~MASK) | (value << OFFSET);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "getMin",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMin()\n{\r\n    return MIN;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getLength()\n{\r\n    return LENGTH;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getOp",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Op getOp(String str)\n{\r\n    try {\r\n        return DOMAIN.parse(str);\r\n    } catch (IllegalArgumentException e) {\r\n        throw new IllegalArgumentException(str + \" is not a valid \" + Type.GET + \" operation.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    IOUtils.closeStream(in);\r\n    IOUtils.closeStream(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getValueString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getValueString()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "LeaseRenewer getInstance(final String authority, final UserGroupInformation ugi, final DFSClient dfsc)\n{\r\n    final LeaseRenewer r = Factory.INSTANCE.get(authority, ugi);\r\n    r.addClient(dfsc);\r\n    return r;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "remove",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void remove(LeaseRenewer renewer)\n{\r\n    synchronized (renewer) {\r\n        Factory.INSTANCE.remove(renewer);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getRenewalTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getRenewalTime()\n{\r\n    return renewal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setRenewalTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRenewalTime(final long renewal)\n{\r\n    this.renewal = renewal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "addClient",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addClient(final DFSClient dfsc)\n{\r\n    for (DFSClient c : dfsclients) {\r\n        if (c == dfsc) {\r\n            return;\r\n        }\r\n    }\r\n    dfsclients.add(dfsc);\r\n    final int hdfsTimeout = dfsc.getConf().getHdfsTimeout();\r\n    if (hdfsTimeout > 0) {\r\n        final long half = hdfsTimeout / 2;\r\n        if (half < renewal) {\r\n            this.renewal = half;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "clearClients",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearClients()\n{\r\n    dfsclients.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "clientsRunning",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean clientsRunning()\n{\r\n    for (Iterator<DFSClient> i = dfsclients.iterator(); i.hasNext(); ) {\r\n        if (!i.next().isClientRunning()) {\r\n            i.remove();\r\n        }\r\n    }\r\n    return !dfsclients.isEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getSleepPeriod",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSleepPeriod()\n{\r\n    return sleepPeriod;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setGraceSleepPeriod",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setGraceSleepPeriod(final long gracePeriod)\n{\r\n    unsyncSetGraceSleepPeriod(gracePeriod);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "unsyncSetGraceSleepPeriod",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void unsyncSetGraceSleepPeriod(final long gracePeriod)\n{\r\n    if (gracePeriod < 100L) {\r\n        throw new HadoopIllegalArgumentException(gracePeriod + \" = gracePeriod < 100ms is too small.\");\r\n    }\r\n    this.gracePeriod = gracePeriod;\r\n    final long half = gracePeriod / 2;\r\n    this.sleepPeriod = half < LEASE_RENEWER_SLEEP_DEFAULT ? half : LEASE_RENEWER_SLEEP_DEFAULT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "isRunning",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isRunning()\n{\r\n    return daemon != null && daemon.isAlive();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "isEmpty",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isEmpty()\n{\r\n    return dfsclients.isEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getDaemonName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDaemonName()\n{\r\n    return daemon.getName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "isRenewerExpired",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isRenewerExpired()\n{\r\n    return emptyTime != Long.MAX_VALUE && Time.monotonicNow() - emptyTime > gracePeriod;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "put",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "boolean put(final DFSClient dfsc)\n{\r\n    if (dfsc.isClientRunning()) {\r\n        if (!isRunning() || isRenewerExpired()) {\r\n            final int id = ++currentId;\r\n            if (isLSRunning.get()) {\r\n                return false;\r\n            }\r\n            isLSRunning.getAndSet(true);\r\n            daemon = new Daemon(new Runnable() {\r\n\r\n                @Override\r\n                public void run() {\r\n                    try {\r\n                        if (LOG.isDebugEnabled()) {\r\n                            LOG.debug(\"Lease renewer daemon for \" + clientsString() + \" with renew id \" + id + \" started\");\r\n                        }\r\n                        LeaseRenewer.this.run(id);\r\n                    } catch (InterruptedException e) {\r\n                        LOG.debug(\"LeaseRenewer is interrupted.\", e);\r\n                    } finally {\r\n                        synchronized (LeaseRenewer.this) {\r\n                            Factory.INSTANCE.remove(LeaseRenewer.this);\r\n                        }\r\n                        if (LOG.isDebugEnabled()) {\r\n                            LOG.debug(\"Lease renewer daemon for \" + clientsString() + \" with renew id \" + id + \" exited\");\r\n                        }\r\n                    }\r\n                }\r\n\r\n                @Override\r\n                public String toString() {\r\n                    return String.valueOf(LeaseRenewer.this);\r\n                }\r\n            });\r\n            daemon.start();\r\n        }\r\n        emptyTime = Long.MAX_VALUE;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setEmptyTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setEmptyTime(long time)\n{\r\n    emptyTime = time;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "closeClient",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void closeClient(final DFSClient dfsc)\n{\r\n    dfsclients.remove(dfsc);\r\n    if (dfsclients.isEmpty()) {\r\n        if (!isRunning() || isRenewerExpired()) {\r\n            Factory.INSTANCE.remove(LeaseRenewer.this);\r\n            return;\r\n        }\r\n        if (emptyTime == Long.MAX_VALUE) {\r\n            emptyTime = Time.monotonicNow();\r\n        }\r\n    }\r\n    if (renewal == dfsc.getConf().getHdfsTimeout() / 2) {\r\n        long min = HdfsConstants.LEASE_SOFTLIMIT_PERIOD;\r\n        for (DFSClient c : dfsclients) {\r\n            final int timeout = c.getConf().getHdfsTimeout();\r\n            if (timeout > 0 && timeout < min) {\r\n                min = timeout;\r\n            }\r\n        }\r\n        renewal = min / 2;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "interruptAndJoin",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void interruptAndJoin() throws InterruptedException\n{\r\n    Daemon daemonCopy = null;\r\n    synchronized (this) {\r\n        if (isRunning()) {\r\n            daemon.interrupt();\r\n            daemonCopy = daemon;\r\n        }\r\n    }\r\n    if (daemonCopy != null) {\r\n        LOG.debug(\"Wait for lease checker to terminate\");\r\n        daemonCopy.join();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "renew",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void renew() throws IOException\n{\r\n    final List<DFSClient> copies;\r\n    synchronized (this) {\r\n        copies = new ArrayList<>(dfsclients);\r\n    }\r\n    Collections.sort(copies, new Comparator<DFSClient>() {\r\n\r\n        @Override\r\n        public int compare(final DFSClient left, final DFSClient right) {\r\n            return left.getClientName().compareTo(right.getClientName());\r\n        }\r\n    });\r\n    String previousName = \"\";\r\n    for (final DFSClient c : copies) {\r\n        if (!c.getClientName().equals(previousName)) {\r\n            if (!c.renewLease()) {\r\n                LOG.debug(\"Did not renew lease for client {}\", c);\r\n                continue;\r\n            }\r\n            previousName = c.getClientName();\r\n            LOG.debug(\"Lease renewed for client {}\", previousName);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "run",
  "errType" : [ "SocketTimeoutException", "IOException" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void run(final int id) throws InterruptedException\n{\r\n    for (long lastRenewed = Time.monotonicNow(); !Thread.interrupted(); Thread.sleep(getSleepPeriod())) {\r\n        final long elapsed = Time.monotonicNow() - lastRenewed;\r\n        if (elapsed >= getRenewalTime()) {\r\n            try {\r\n                renew();\r\n                if (LOG.isDebugEnabled()) {\r\n                    LOG.debug(\"Lease renewer daemon for \" + clientsString() + \" with renew id \" + id + \" executed\");\r\n                }\r\n                lastRenewed = Time.monotonicNow();\r\n            } catch (SocketTimeoutException ie) {\r\n                LOG.warn(\"Failed to renew lease for \" + clientsString() + \" for \" + (elapsed / 1000) + \" seconds.  Aborting ...\", ie);\r\n                List<DFSClient> dfsclientsCopy;\r\n                synchronized (this) {\r\n                    DFSClientFaultInjector.get().delayWhenRenewLeaseTimeout();\r\n                    dfsclientsCopy = new ArrayList<>(dfsclients);\r\n                    Factory.INSTANCE.remove(LeaseRenewer.this);\r\n                }\r\n                for (DFSClient dfsClient : dfsclientsCopy) {\r\n                    dfsClient.closeAllFilesBeingWritten(true);\r\n                }\r\n                break;\r\n            } catch (IOException ie) {\r\n                LOG.warn(\"Failed to renew lease for \" + clientsString() + \" for \" + (elapsed / 1000) + \" seconds.  Will retry shortly ...\", ie);\r\n            }\r\n        }\r\n        synchronized (this) {\r\n            if (id != currentId || isRenewerExpired()) {\r\n                if (LOG.isDebugEnabled()) {\r\n                    if (id != currentId) {\r\n                        LOG.debug(\"Lease renewer daemon for \" + clientsString() + \" with renew id \" + id + \" is not current\");\r\n                    } else {\r\n                        LOG.debug(\"Lease renewer daemon for \" + clientsString() + \" with renew id \" + id + \" expired\");\r\n                    }\r\n                }\r\n                return;\r\n            }\r\n            if (!clientsRunning() && emptyTime == Long.MAX_VALUE) {\r\n                emptyTime = Time.monotonicNow();\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    String s = getClass().getSimpleName() + \":\" + factorykey;\r\n    if (LOG.isTraceEnabled()) {\r\n        return s + \", clients=\" + clientsString() + \", created at \" + instantiationTrace;\r\n    }\r\n    return s;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "clientsString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String clientsString()\n{\r\n    if (dfsclients.isEmpty()) {\r\n        return \"[]\";\r\n    } else {\r\n        final StringBuilder b = new StringBuilder(\"[\").append(dfsclients.get(0).getClientName());\r\n        for (int i = 1; i < dfsclients.size(); i++) {\r\n            b.append(\", \").append(dfsclients.get(i).getClientName());\r\n        }\r\n        return b.append(\"]\").toString();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setLeaseRenewerGraceDefault",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLeaseRenewerGraceDefault(long leaseRenewerGraceDefault)\n{\r\n    LeaseRenewer.leaseRenewerGraceDefault = leaseRenewerGraceDefault;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getStorageID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStorageID()\n{\r\n    return storageID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "State getState()\n{\r\n    return state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getStorageType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StorageType getStorageType()\n{\r\n    return storageType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "generateUuid",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String generateUuid()\n{\r\n    return STORAGE_ID_PREFIX + UUID.randomUUID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "isValidStorageId",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isValidStorageId(final String storageID)\n{\r\n    try {\r\n        if (storageID != null && storageID.indexOf(STORAGE_ID_PREFIX) == 0) {\r\n            UUID.fromString(storageID.substring(STORAGE_ID_PREFIX.length()));\r\n            return true;\r\n        }\r\n    } catch (IllegalArgumentException ignored) {\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"DatanodeStorage[\" + storageID + \",\" + storageType + \",\" + state + \"]\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object other)\n{\r\n    if (other == this) {\r\n        return true;\r\n    }\r\n    if ((other == null) || !(other instanceof DatanodeStorage)) {\r\n        return false;\r\n    }\r\n    DatanodeStorage otherStorage = (DatanodeStorage) other;\r\n    return otherStorage.getStorageID().compareTo(getStorageID()) == 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return getStorageID().hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "removeIdFromFilter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "CacheDirectiveInfo removeIdFromFilter(CacheDirectiveInfo filter)\n{\r\n    CacheDirectiveInfo.Builder builder = new CacheDirectiveInfo.Builder(filter);\r\n    builder.setId(null);\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "makeRequest",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "BatchedEntries<CacheDirectiveEntry> makeRequest(Long prevKey) throws IOException\n{\r\n    BatchedEntries<CacheDirectiveEntry> entries;\r\n    try (TraceScope ignored = tracer.newScope(\"listCacheDirectives\")) {\r\n        entries = namenode.listCacheDirectives(prevKey, filter);\r\n    } catch (IOException e) {\r\n        if (e.getMessage().contains(\"Filtering by ID is unsupported\")) {\r\n            long id = filter.getId();\r\n            filter = removeIdFromFilter(filter);\r\n            entries = namenode.listCacheDirectives(id - 1, filter);\r\n            for (int i = 0; i < entries.size(); i++) {\r\n                CacheDirectiveEntry entry = entries.get(i);\r\n                if (entry.getInfo().getId().equals(id)) {\r\n                    return new SingleEntry(entry);\r\n                }\r\n            }\r\n            throw new RemoteException(InvalidRequestException.class.getName(), \"Did not find requested id \" + id);\r\n        }\r\n        throw e;\r\n    }\r\n    Preconditions.checkNotNull(entries);\r\n    return entries;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "elementToPrevKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Long elementToPrevKey(CacheDirectiveEntry entry)\n{\r\n    return entry.getInfo().getId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getEndpointShmManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EndpointShmManager getEndpointShmManager()\n{\r\n    return manager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getPeer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DomainPeer getPeer()\n{\r\n    return peer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "isDisconnected",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isDisconnected()\n{\r\n    return disconnected;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "handle",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean handle(DomainSocket sock)\n{\r\n    manager.unregisterShm(getShmId());\r\n    synchronized (this) {\r\n        Preconditions.checkState(!disconnected);\r\n        disconnected = true;\r\n        boolean hadSlots = false;\r\n        for (Iterator<Slot> iter = slotIterator(); iter.hasNext(); ) {\r\n            Slot slot = iter.next();\r\n            slot.makeInvalid();\r\n            hadSlots = true;\r\n        }\r\n        if (!hadSlots) {\r\n            free();\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "run",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void run()\n{\r\n    while (!Thread.currentThread().isInterrupted()) {\r\n        clearAndGetDetectedDeadNodes();\r\n        LOG.debug(\"Current detector state {}, the detected nodes: {}.\", state, deadNodes.values());\r\n        switch(state) {\r\n            case INIT:\r\n                init();\r\n                break;\r\n            case CHECK_DEAD:\r\n                checkDeadNodes();\r\n                break;\r\n            case IDLE:\r\n                idle();\r\n                break;\r\n            case ERROR:\r\n                try {\r\n                    Thread.sleep(ERROR_SLEEP_MS);\r\n                } catch (InterruptedException e) {\r\n                    LOG.debug(\"Got interrupted while DeadNodeDetector is error.\", e);\r\n                    Thread.currentThread().interrupt();\r\n                }\r\n                return;\r\n            default:\r\n                break;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void shutdown()\n{\r\n    threadShutDown(this);\r\n    threadShutDown(probeDeadNodesSchedulerThr);\r\n    threadShutDown(probeSuspectNodesSchedulerThr);\r\n    probeDeadNodesThreadPool.shutdown();\r\n    probeSuspectNodesThreadPool.shutdown();\r\n    rpcThreadPool.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "threadShutDown",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void threadShutDown(Thread thread)\n{\r\n    if (thread != null && thread.isAlive()) {\r\n        thread.interrupt();\r\n        try {\r\n            thread.join();\r\n        } catch (InterruptedException e) {\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isThreadsShutdown",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean isThreadsShutdown()\n{\r\n    return !this.isAlive() && !probeDeadNodesSchedulerThr.isAlive() && !probeSuspectNodesSchedulerThr.isAlive() && probeDeadNodesThreadPool.isShutdown() && probeSuspectNodesThreadPool.isShutdown() && rpcThreadPool.isShutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setDisabledProbeThreadForTest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDisabledProbeThreadForTest(boolean disabledProbeThreadForTest)\n{\r\n    DeadNodeDetector.disabledProbeThreadForTest = disabledProbeThreadForTest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "startProbeScheduler",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void startProbeScheduler()\n{\r\n    probeDeadNodesSchedulerThr = new Thread(new ProbeScheduler(this, ProbeType.CHECK_DEAD));\r\n    probeDeadNodesSchedulerThr.setDaemon(true);\r\n    probeDeadNodesSchedulerThr.start();\r\n    probeSuspectNodesSchedulerThr = new Thread(new ProbeScheduler(this, ProbeType.CHECK_SUSPECT));\r\n    probeSuspectNodesSchedulerThr.setDaemon(true);\r\n    probeSuspectNodesSchedulerThr.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "scheduleProbe",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void scheduleProbe(ProbeType type)\n{\r\n    LOG.debug(\"Schedule probe datanode for probe type: {}.\", type);\r\n    DatanodeInfo datanodeInfo = null;\r\n    if (type == ProbeType.CHECK_DEAD) {\r\n        while ((datanodeInfo = deadNodesProbeQueue.poll()) != null) {\r\n            if (probeInProg.containsKey(datanodeInfo.getDatanodeUuid())) {\r\n                LOG.debug(\"The datanode {} is already contained in probe queue, \" + \"skip to add it.\", datanodeInfo);\r\n                continue;\r\n            }\r\n            probeInProg.put(datanodeInfo.getDatanodeUuid(), datanodeInfo);\r\n            Probe probe = new Probe(this, datanodeInfo, ProbeType.CHECK_DEAD);\r\n            probeDeadNodesThreadPool.execute(probe);\r\n        }\r\n    } else if (type == ProbeType.CHECK_SUSPECT) {\r\n        while ((datanodeInfo = suspectNodesProbeQueue.poll()) != null) {\r\n            if (probeInProg.containsKey(datanodeInfo.getDatanodeUuid())) {\r\n                continue;\r\n            }\r\n            probeInProg.put(datanodeInfo.getDatanodeUuid(), datanodeInfo);\r\n            Probe probe = new Probe(this, datanodeInfo, ProbeType.CHECK_SUSPECT);\r\n            probeSuspectNodesThreadPool.execute(probe);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "probeCallBack",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void probeCallBack(Probe probe, boolean success)\n{\r\n    LOG.debug(\"Probe datanode: {} result: {}, type: {}\", probe.getDatanodeInfo(), success, probe.getType());\r\n    probeInProg.remove(probe.getDatanodeInfo().getDatanodeUuid());\r\n    if (success) {\r\n        if (probe.getType() == ProbeType.CHECK_DEAD) {\r\n            LOG.info(\"Remove the node out from dead node list: {}.\", probe.getDatanodeInfo());\r\n            removeDeadNode(probe.getDatanodeInfo());\r\n        } else if (probe.getType() == ProbeType.CHECK_SUSPECT) {\r\n            LOG.debug(\"Remove the node out from suspect node list: {}.\", probe.getDatanodeInfo());\r\n            removeNodeFromDeadNodeDetector(probe.getDatanodeInfo());\r\n        }\r\n    } else {\r\n        if (probe.getType() == ProbeType.CHECK_SUSPECT) {\r\n            LOG.warn(\"Probe failed, add suspect node to dead node list: {}.\", probe.getDatanodeInfo());\r\n            addToDead(probe.getDatanodeInfo());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkDeadNodes",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void checkDeadNodes()\n{\r\n    Set<DatanodeInfo> datanodeInfos = clearAndGetDetectedDeadNodes();\r\n    for (DatanodeInfo datanodeInfo : datanodeInfos) {\r\n        if (!deadNodesProbeQueue.offer(datanodeInfo)) {\r\n            LOG.debug(\"Skip to add dead node {} to check \" + \"since the node is already in the probe queue.\", datanodeInfo);\r\n        } else {\r\n            LOG.debug(\"Add dead node to check: {}.\", datanodeInfo);\r\n        }\r\n    }\r\n    state = State.IDLE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "idle",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void idle()\n{\r\n    try {\r\n        Thread.sleep(idleSleepMs);\r\n    } catch (InterruptedException e) {\r\n        LOG.debug(\"Got interrupted while DeadNodeDetector is idle.\", e);\r\n        Thread.currentThread().interrupt();\r\n    }\r\n    state = State.CHECK_DEAD;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void init()\n{\r\n    state = State.CHECK_DEAD;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addToDead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addToDead(DatanodeInfo datanodeInfo)\n{\r\n    deadNodes.put(datanodeInfo.getDatanodeUuid(), datanodeInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isDeadNode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isDeadNode(DatanodeInfo datanodeInfo)\n{\r\n    return deadNodes.containsKey(datanodeInfo.getDatanodeUuid());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeFromDead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeFromDead(DatanodeInfo datanodeInfo)\n{\r\n    deadNodes.remove(datanodeInfo.getDatanodeUuid());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDeadNodesProbeQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "UniqueQueue<DatanodeInfo> getDeadNodesProbeQueue()\n{\r\n    return deadNodesProbeQueue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSuspectNodesProbeQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "UniqueQueue<DatanodeInfo> getSuspectNodesProbeQueue()\n{\r\n    return suspectNodesProbeQueue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setSuspectQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSuspectQueue(UniqueQueue<DatanodeInfo> queue)\n{\r\n    this.suspectNodesProbeQueue = queue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setDeadQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDeadQueue(UniqueQueue<DatanodeInfo> queue)\n{\r\n    this.deadNodesProbeQueue = queue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addNodeToDetect",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void addNodeToDetect(DFSInputStream dfsInputStream, DatanodeInfo datanodeInfo)\n{\r\n    HashSet<DatanodeInfo> datanodeInfos = suspectAndDeadNodes.get(dfsInputStream);\r\n    if (datanodeInfos == null) {\r\n        datanodeInfos = new HashSet<DatanodeInfo>();\r\n        datanodeInfos.add(datanodeInfo);\r\n        suspectAndDeadNodes.putIfAbsent(dfsInputStream, datanodeInfos);\r\n    } else {\r\n        datanodeInfos.add(datanodeInfo);\r\n    }\r\n    LOG.debug(\"Add datanode {} to suspectAndDeadNodes.\", datanodeInfo);\r\n    addSuspectNodeToDetect(datanodeInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addSuspectNodeToDetect",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean addSuspectNodeToDetect(DatanodeInfo datanodeInfo)\n{\r\n    return suspectNodesProbeQueue.offer(datanodeInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "clearAndGetDetectedDeadNodes",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Set<DatanodeInfo> clearAndGetDetectedDeadNodes()\n{\r\n    Set<DatanodeInfo> newDeadNodes = new HashSet<DatanodeInfo>();\r\n    for (HashSet<DatanodeInfo> datanodeInfos : suspectAndDeadNodes.values()) {\r\n        newDeadNodes.addAll(datanodeInfos);\r\n    }\r\n    for (DatanodeInfo datanodeInfo : deadNodes.values()) {\r\n        if (!newDeadNodes.contains(datanodeInfo)) {\r\n            deadNodes.remove(datanodeInfo.getDatanodeUuid());\r\n        }\r\n    }\r\n    return new HashSet<>(deadNodes.values());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeNodeFromDeadNodeDetector",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void removeNodeFromDeadNodeDetector(DFSInputStream dfsInputStream, DatanodeInfo datanodeInfo)\n{\r\n    Set<DatanodeInfo> datanodeInfos = suspectAndDeadNodes.get(dfsInputStream);\r\n    if (datanodeInfos != null) {\r\n        datanodeInfos.remove(datanodeInfo);\r\n        dfsInputStream.removeFromLocalDeadNodes(datanodeInfo);\r\n        if (datanodeInfos.isEmpty()) {\r\n            suspectAndDeadNodes.remove(dfsInputStream);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeNodeFromDeadNodeDetector",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void removeNodeFromDeadNodeDetector(DatanodeInfo datanodeInfo)\n{\r\n    for (Map.Entry<DFSInputStream, HashSet<DatanodeInfo>> entry : suspectAndDeadNodes.entrySet()) {\r\n        Set<DatanodeInfo> datanodeInfos = entry.getValue();\r\n        if (datanodeInfos.remove(datanodeInfo)) {\r\n            DFSInputStream dfsInputStream = entry.getKey();\r\n            dfsInputStream.removeFromLocalDeadNodes(datanodeInfo);\r\n            if (datanodeInfos.isEmpty()) {\r\n                suspectAndDeadNodes.remove(dfsInputStream);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeDeadNode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void removeDeadNode(DatanodeInfo datanodeInfo)\n{\r\n    removeNodeFromDeadNodeDetector(datanodeInfo);\r\n    removeFromDead(datanodeInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "probeSleep",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void probeSleep(long time)\n{\r\n    try {\r\n        Thread.sleep(time);\r\n    } catch (InterruptedException e) {\r\n        LOG.debug(\"Got interrupted while probe is scheduling.\", e);\r\n        Thread.currentThread().interrupt();\r\n        return;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getNameSpace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NameSpace getNameSpace()\n{\r\n    return ns;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getValue()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return new HashCodeBuilder(811, 67).append(name).append(ns).append(value).toHashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (obj == null) {\r\n        return false;\r\n    }\r\n    if (obj == this) {\r\n        return true;\r\n    }\r\n    if (obj.getClass() != getClass()) {\r\n        return false;\r\n    }\r\n    XAttr rhs = (XAttr) obj;\r\n    return new EqualsBuilder().append(ns, rhs.ns).append(name, rhs.name).append(value, rhs.value).isEquals();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "equalsIgnoreValue",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equalsIgnoreValue(Object obj)\n{\r\n    if (obj == null) {\r\n        return false;\r\n    }\r\n    if (obj == this) {\r\n        return true;\r\n    }\r\n    if (obj.getClass() != getClass()) {\r\n        return false;\r\n    }\r\n    XAttr rhs = (XAttr) obj;\r\n    return new EqualsBuilder().append(ns, rhs.ns).append(name, rhs.name).isEquals();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"XAttr [ns=\" + ns + \", name=\" + name + \", value=\" + Arrays.toString(value) + \"]\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getPolicies",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<ErasureCodingPolicy> getPolicies()\n{\r\n    return SYS_POLICIES;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getByID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ErasureCodingPolicy getByID(byte id)\n{\r\n    return SYSTEM_POLICIES_BY_ID.get(id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getByName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ErasureCodingPolicy getByName(String name)\n{\r\n    return SYSTEM_POLICIES_BY_NAME.get(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getReplicationPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ErasureCodingPolicy getReplicationPolicy()\n{\r\n    return REPLICATION_POLICY;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "parseStripedBlockGroup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "LocatedBlock[] parseStripedBlockGroup(LocatedStripedBlock bg, int cellSize, int dataBlkNum, int parityBlkNum)\n{\r\n    int locatedBGSize = bg.getBlockIndices().length;\r\n    LocatedBlock[] lbs = new LocatedBlock[dataBlkNum + parityBlkNum];\r\n    for (short i = 0; i < locatedBGSize; i++) {\r\n        final int idx = bg.getBlockIndices()[i];\r\n        if (idx < (dataBlkNum + parityBlkNum) && lbs[idx] == null) {\r\n            lbs[idx] = constructInternalBlock(bg, i, cellSize, dataBlkNum, idx);\r\n        }\r\n    }\r\n    return lbs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "constructInternalBlock",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "LocatedBlock constructInternalBlock(LocatedStripedBlock bg, int idxInReturnedLocs, int cellSize, int dataBlkNum, int idxInBlockGroup)\n{\r\n    final ExtendedBlock blk = constructInternalBlock(bg.getBlock(), cellSize, dataBlkNum, idxInBlockGroup);\r\n    final LocatedBlock locatedBlock;\r\n    if (idxInReturnedLocs < bg.getLocations().length) {\r\n        locatedBlock = new LocatedBlock(blk, new DatanodeInfo[] { bg.getLocations()[idxInReturnedLocs] }, new String[] { bg.getStorageIDs()[idxInReturnedLocs] }, new StorageType[] { bg.getStorageTypes()[idxInReturnedLocs] }, bg.getStartOffset(), bg.isCorrupt(), null);\r\n    } else {\r\n        locatedBlock = new LocatedBlock(blk, null, null, null, bg.getStartOffset(), bg.isCorrupt(), null);\r\n    }\r\n    Token<BlockTokenIdentifier>[] blockTokens = bg.getBlockTokens();\r\n    if (idxInReturnedLocs < blockTokens.length) {\r\n        locatedBlock.setBlockToken(blockTokens[idxInReturnedLocs]);\r\n    }\r\n    return locatedBlock;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "constructInternalBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExtendedBlock constructInternalBlock(ExtendedBlock blockGroup, ErasureCodingPolicy ecPolicy, int idxInBlockGroup)\n{\r\n    return constructInternalBlock(blockGroup, ecPolicy.getCellSize(), ecPolicy.getNumDataUnits(), idxInBlockGroup);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "constructInternalBlock",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ExtendedBlock constructInternalBlock(ExtendedBlock blockGroup, int cellSize, int dataBlkNum, int idxInBlockGroup)\n{\r\n    ExtendedBlock block = new ExtendedBlock(blockGroup);\r\n    block.setBlockId(blockGroup.getBlockId() + idxInBlockGroup);\r\n    block.setNumBytes(getInternalBlockLength(blockGroup.getNumBytes(), cellSize, dataBlkNum, idxInBlockGroup));\r\n    return block;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "getInternalBlockLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getInternalBlockLength(long dataSize, ErasureCodingPolicy ecPolicy, int idxInBlockGroup)\n{\r\n    return getInternalBlockLength(dataSize, ecPolicy.getCellSize(), ecPolicy.getNumDataUnits(), idxInBlockGroup);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "getInternalBlockLength",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long getInternalBlockLength(long dataSize, int cellSize, int numDataBlocks, int idxInBlockGroup)\n{\r\n    Preconditions.checkArgument(dataSize >= 0);\r\n    Preconditions.checkArgument(cellSize > 0);\r\n    Preconditions.checkArgument(numDataBlocks > 0);\r\n    Preconditions.checkArgument(idxInBlockGroup >= 0);\r\n    final int stripeSize = cellSize * numDataBlocks;\r\n    final int lastStripeDataLen = (int) (dataSize % stripeSize);\r\n    if (lastStripeDataLen == 0) {\r\n        return dataSize / numDataBlocks;\r\n    }\r\n    final int numStripes = (int) ((dataSize - 1) / stripeSize + 1);\r\n    return (numStripes - 1L) * cellSize + lastCellSize(lastStripeDataLen, cellSize, numDataBlocks, idxInBlockGroup);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "getSafeLength",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long getSafeLength(ErasureCodingPolicy ecPolicy, long[] blockLens)\n{\r\n    final int cellSize = ecPolicy.getCellSize();\r\n    final int dataBlkNum = ecPolicy.getNumDataUnits();\r\n    Preconditions.checkArgument(blockLens.length >= dataBlkNum);\r\n    final int stripeSize = dataBlkNum * cellSize;\r\n    long[] cpy = Arrays.copyOf(blockLens, blockLens.length);\r\n    Arrays.sort(cpy);\r\n    long lastFullStripeIdx = cpy[cpy.length - dataBlkNum] / cellSize;\r\n    return lastFullStripeIdx * stripeSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "lastCellSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int lastCellSize(int size, int cellSize, int numDataBlocks, int i)\n{\r\n    if (i < numDataBlocks) {\r\n        size -= i * cellSize;\r\n        if (size < 0) {\r\n            size = 0;\r\n        }\r\n    }\r\n    return size > cellSize ? cellSize : size;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "offsetInBlkToOffsetInBG",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long offsetInBlkToOffsetInBG(int cellSize, int dataBlkNum, long offsetInBlk, int idxInBlockGroup)\n{\r\n    long cellIdxInBlk = offsetInBlk / cellSize;\r\n    return cellIdxInBlk * cellSize * dataBlkNum + (long) idxInBlockGroup * cellSize + offsetInBlk % cellSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "getNextCompletedStripedRead",
  "errType" : [ "ExecutionException", "CancellationException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "StripingChunkReadResult getNextCompletedStripedRead(CompletionService<BlockReadStats> readService, Map<Future<BlockReadStats>, Integer> futures, final long timeoutMillis) throws InterruptedException\n{\r\n    Preconditions.checkArgument(!futures.isEmpty());\r\n    Future<BlockReadStats> future = null;\r\n    try {\r\n        if (timeoutMillis > 0) {\r\n            future = readService.poll(timeoutMillis, TimeUnit.MILLISECONDS);\r\n        } else {\r\n            future = readService.take();\r\n        }\r\n        if (future != null) {\r\n            final BlockReadStats stats = future.get();\r\n            return new StripingChunkReadResult(futures.remove(future), StripingChunkReadResult.SUCCESSFUL, stats);\r\n        } else {\r\n            return new StripingChunkReadResult(StripingChunkReadResult.TIMEOUT);\r\n        }\r\n    } catch (ExecutionException e) {\r\n        LOG.debug(\"Exception during striped read task\", e);\r\n        return new StripingChunkReadResult(futures.remove(future), StripingChunkReadResult.FAILED);\r\n    } catch (CancellationException e) {\r\n        LOG.debug(\"Exception during striped read task\", e);\r\n        return new StripingChunkReadResult(futures.remove(future), StripingChunkReadResult.CANCELLED);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "spaceConsumedByStripedBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long spaceConsumedByStripedBlock(long numDataBlkBytes, int dataBlkNum, int parityBlkNum, int cellSize)\n{\r\n    int parityIndex = dataBlkNum + 1;\r\n    long numParityBlkBytes = getInternalBlockLength(numDataBlkBytes, cellSize, dataBlkNum, parityIndex) * parityBlkNum;\r\n    return numDataBlkBytes + numParityBlkBytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "divideOneStripe",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "AlignedStripe[] divideOneStripe(ErasureCodingPolicy ecPolicy, int cellSize, LocatedStripedBlock blockGroup, long rangeStartInBlockGroup, long rangeEndInBlockGroup, ByteBuffer buf)\n{\r\n    final int dataBlkNum = ecPolicy.getNumDataUnits();\r\n    StripingCell[] cells = getStripingCellsOfByteRange(ecPolicy, cellSize, blockGroup, rangeStartInBlockGroup, rangeEndInBlockGroup);\r\n    VerticalRange[] ranges = getRangesForInternalBlocks(ecPolicy, cellSize, cells);\r\n    AlignedStripe[] stripes = mergeRangesForInternalBlocks(ecPolicy, ranges, blockGroup, cellSize);\r\n    int bufOffset = (int) (rangeStartInBlockGroup % ((long) cellSize * dataBlkNum));\r\n    for (StripingCell cell : cells) {\r\n        long cellStart = cell.idxInInternalBlk * cellSize + cell.offset;\r\n        long cellEnd = cellStart + cell.size - 1;\r\n        for (AlignedStripe s : stripes) {\r\n            long stripeEnd = s.getOffsetInBlock() + s.getSpanInBlock() - 1;\r\n            long overlapStart = Math.max(cellStart, s.getOffsetInBlock());\r\n            long overlapEnd = Math.min(cellEnd, stripeEnd);\r\n            int overLapLen = (int) (overlapEnd - overlapStart + 1);\r\n            if (overLapLen > 0) {\r\n                Preconditions.checkState(s.chunks[cell.idxInStripe] == null);\r\n                final int pos = (int) (bufOffset + overlapStart - cellStart);\r\n                buf.position(pos);\r\n                buf.limit(pos + overLapLen);\r\n                s.chunks[cell.idxInStripe] = new StripingChunk(buf.slice());\r\n            }\r\n        }\r\n        bufOffset += cell.size;\r\n    }\r\n    prepareAllZeroChunks(blockGroup, stripes, cellSize, dataBlkNum);\r\n    return stripes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "divideByteRangeIntoStripes",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "AlignedStripe[] divideByteRangeIntoStripes(ErasureCodingPolicy ecPolicy, int cellSize, LocatedStripedBlock blockGroup, long rangeStartInBlockGroup, long rangeEndInBlockGroup, ByteBuffer buf)\n{\r\n    final int dataBlkNum = ecPolicy.getNumDataUnits();\r\n    StripingCell[] cells = getStripingCellsOfByteRange(ecPolicy, cellSize, blockGroup, rangeStartInBlockGroup, rangeEndInBlockGroup);\r\n    VerticalRange[] ranges = getRangesForInternalBlocks(ecPolicy, cellSize, cells);\r\n    AlignedStripe[] stripes = mergeRangesForInternalBlocks(ecPolicy, ranges, blockGroup, cellSize);\r\n    calcualteChunkPositionsInBuf(cellSize, stripes, cells, buf);\r\n    prepareAllZeroChunks(blockGroup, stripes, cellSize, dataBlkNum);\r\n    return stripes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "getStripingCellsOfByteRange",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "StripingCell[] getStripingCellsOfByteRange(ErasureCodingPolicy ecPolicy, int cellSize, LocatedStripedBlock blockGroup, long rangeStartInBlockGroup, long rangeEndInBlockGroup)\n{\r\n    Preconditions.checkArgument(rangeStartInBlockGroup <= rangeEndInBlockGroup && rangeEndInBlockGroup < blockGroup.getBlockSize(), \"start=%s end=%s blockSize=%s\", rangeStartInBlockGroup, rangeEndInBlockGroup, blockGroup.getBlockSize());\r\n    long len = rangeEndInBlockGroup - rangeStartInBlockGroup + 1;\r\n    int firstCellIdxInBG = (int) (rangeStartInBlockGroup / cellSize);\r\n    int lastCellIdxInBG = (int) (rangeEndInBlockGroup / cellSize);\r\n    int numCells = lastCellIdxInBG - firstCellIdxInBG + 1;\r\n    StripingCell[] cells = new StripingCell[numCells];\r\n    final int firstCellOffset = (int) (rangeStartInBlockGroup % cellSize);\r\n    final int firstCellSize = (int) Math.min(cellSize - (rangeStartInBlockGroup % cellSize), len);\r\n    cells[0] = new StripingCell(ecPolicy, firstCellSize, firstCellIdxInBG, firstCellOffset);\r\n    if (lastCellIdxInBG != firstCellIdxInBG) {\r\n        final int lastCellSize = (int) (rangeEndInBlockGroup % cellSize) + 1;\r\n        cells[numCells - 1] = new StripingCell(ecPolicy, lastCellSize, lastCellIdxInBG, 0);\r\n    }\r\n    for (int i = 1; i < numCells - 1; i++) {\r\n        cells[i] = new StripingCell(ecPolicy, cellSize, i + firstCellIdxInBG, 0);\r\n    }\r\n    return cells;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "getRangesForInternalBlocks",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "VerticalRange[] getRangesForInternalBlocks(ErasureCodingPolicy ecPolicy, int cellSize, StripingCell[] cells)\n{\r\n    int dataBlkNum = ecPolicy.getNumDataUnits();\r\n    int parityBlkNum = ecPolicy.getNumParityUnits();\r\n    VerticalRange[] ranges = new VerticalRange[dataBlkNum + parityBlkNum];\r\n    long earliestStart = Long.MAX_VALUE;\r\n    long latestEnd = -1;\r\n    for (StripingCell cell : cells) {\r\n        if (ranges[cell.idxInStripe] == null) {\r\n            ranges[cell.idxInStripe] = new VerticalRange(cell.idxInInternalBlk * cellSize + cell.offset, cell.size);\r\n        } else {\r\n            ranges[cell.idxInStripe].spanInBlock += cell.size;\r\n        }\r\n        VerticalRange range = ranges[cell.idxInStripe];\r\n        if (range.offsetInBlock < earliestStart) {\r\n            earliestStart = range.offsetInBlock;\r\n        }\r\n        if (range.offsetInBlock + range.spanInBlock - 1 > latestEnd) {\r\n            latestEnd = range.offsetInBlock + range.spanInBlock - 1;\r\n        }\r\n    }\r\n    for (int i = dataBlkNum; i < dataBlkNum + parityBlkNum; i++) {\r\n        ranges[i] = new VerticalRange(earliestStart, latestEnd - earliestStart + 1);\r\n    }\r\n    return ranges;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "mergeRangesForInternalBlocks",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "AlignedStripe[] mergeRangesForInternalBlocks(ErasureCodingPolicy ecPolicy, VerticalRange[] ranges, LocatedStripedBlock blockGroup, int cellSize)\n{\r\n    int dataBlkNum = ecPolicy.getNumDataUnits();\r\n    int parityBlkNum = ecPolicy.getNumParityUnits();\r\n    List<AlignedStripe> stripes = new ArrayList<>();\r\n    SortedSet<Long> stripePoints = new TreeSet<>();\r\n    for (VerticalRange r : ranges) {\r\n        if (r != null) {\r\n            stripePoints.add(r.offsetInBlock);\r\n            stripePoints.add(r.offsetInBlock + r.spanInBlock);\r\n        }\r\n    }\r\n    int lastCellIdxInBG = (int) (blockGroup.getBlockSize() / cellSize);\r\n    int idxInInternalBlk = lastCellIdxInBG / ecPolicy.getNumDataUnits();\r\n    long lastCellEndOffset = (idxInInternalBlk * (long) cellSize) + (blockGroup.getBlockSize() % cellSize);\r\n    if (stripePoints.first() < lastCellEndOffset && stripePoints.last() > lastCellEndOffset) {\r\n        stripePoints.add(lastCellEndOffset);\r\n    }\r\n    long prev = -1;\r\n    for (long point : stripePoints) {\r\n        if (prev >= 0) {\r\n            stripes.add(new AlignedStripe(prev, point - prev, dataBlkNum + parityBlkNum));\r\n        }\r\n        prev = point;\r\n    }\r\n    return stripes.toArray(new AlignedStripe[stripes.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "calcualteChunkPositionsInBuf",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void calcualteChunkPositionsInBuf(int cellSize, AlignedStripe[] stripes, StripingCell[] cells, ByteBuffer buf)\n{\r\n    int done = 0;\r\n    for (StripingCell cell : cells) {\r\n        long cellStart = cell.idxInInternalBlk * cellSize + cell.offset;\r\n        long cellEnd = cellStart + cell.size - 1;\r\n        StripingChunk chunk;\r\n        for (AlignedStripe s : stripes) {\r\n            long stripeEnd = s.getOffsetInBlock() + s.getSpanInBlock() - 1;\r\n            long overlapStart = Math.max(cellStart, s.getOffsetInBlock());\r\n            long overlapEnd = Math.min(cellEnd, stripeEnd);\r\n            int overLapLen = (int) (overlapEnd - overlapStart + 1);\r\n            if (overLapLen <= 0) {\r\n                continue;\r\n            }\r\n            chunk = s.chunks[cell.idxInStripe];\r\n            if (chunk == null) {\r\n                chunk = new StripingChunk();\r\n                s.chunks[cell.idxInStripe] = chunk;\r\n            }\r\n            chunk.getChunkBuffer().addSlice(buf, (int) (done + overlapStart - cellStart), overLapLen);\r\n        }\r\n        done += cell.size;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "prepareAllZeroChunks",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void prepareAllZeroChunks(LocatedStripedBlock blockGroup, AlignedStripe[] stripes, int cellSize, int dataBlkNum)\n{\r\n    for (AlignedStripe s : stripes) {\r\n        for (int i = 0; i < dataBlkNum; i++) {\r\n            long internalBlkLen = getInternalBlockLength(blockGroup.getBlockSize(), cellSize, dataBlkNum, i);\r\n            if (internalBlkLen <= s.getOffsetInBlock()) {\r\n                Preconditions.checkState(s.chunks[i] == null);\r\n                s.chunks[i] = new StripingChunk(StripingChunk.ALLZERO);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "checkBlocks",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void checkBlocks(ExtendedBlock blockGroup, int i, ExtendedBlock blocki) throws IOException\n{\r\n    if (!blocki.getBlockPoolId().equals(blockGroup.getBlockPoolId())) {\r\n        throw new IOException(\"Block pool IDs mismatched: block\" + i + \"=\" + blocki + \", expected block group=\" + blockGroup);\r\n    }\r\n    if (blocki.getBlockId() - i != blockGroup.getBlockId()) {\r\n        throw new IOException(\"Block IDs mismatched: block\" + i + \"=\" + blocki + \", expected block group=\" + blockGroup);\r\n    }\r\n    if (blocki.getGenerationStamp() != blockGroup.getGenerationStamp()) {\r\n        throw new IOException(\"Generation stamps mismatched: block\" + i + \"=\" + blocki + \", expected block group=\" + blockGroup);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "getBlockIndex",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getBlockIndex(Block reportedBlock)\n{\r\n    long BLOCK_GROUP_INDEX_MASK = 15;\r\n    return (int) (reportedBlock.getBlockId() & BLOCK_GROUP_INDEX_MASK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getId()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPath()\n{\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getSuite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CipherSuite getSuite()\n{\r\n    return suite;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CryptoProtocolVersion getVersion()\n{\r\n    return version;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getKeyName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getKeyName()\n{\r\n    return keyName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return new HashCodeBuilder(13, 31).append(id).append(path).append(suite).append(version).append(keyName).toHashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (obj == null) {\r\n        return false;\r\n    }\r\n    if (obj == this) {\r\n        return true;\r\n    }\r\n    if (obj.getClass() != getClass()) {\r\n        return false;\r\n    }\r\n    EncryptionZone rhs = (EncryptionZone) obj;\r\n    return new EqualsBuilder().append(id, rhs.id).append(path, rhs.path).append(suite, rhs.suite).append(version, rhs.version).append(keyName, rhs.keyName).isEquals();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"EncryptionZone [id=\" + id + \", path=\" + path + \", suite=\" + suite + \", version=\" + version + \", keyName=\" + keyName + \"]\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close()\n{\r\n    if (replica != null) {\r\n        if (anchored) {\r\n            replica.removeNoChecksumAnchor();\r\n        }\r\n        replica.unref();\r\n    }\r\n    replica = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "getMappedByteBuffer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MappedByteBuffer getMappedByteBuffer()\n{\r\n    return map;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "fromExtendedBlock",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ExtendedBlockId fromExtendedBlock(ExtendedBlock block)\n{\r\n    return new ExtendedBlockId(block.getBlockId(), block.getBlockPoolId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBlockId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBlockId()\n{\r\n    return this.blockId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBlockPoolId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockPoolId()\n{\r\n    return this.bpId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if ((o == null) || (o.getClass() != this.getClass())) {\r\n        return false;\r\n    }\r\n    ExtendedBlockId other = (ExtendedBlockId) o;\r\n    return new EqualsBuilder().append(blockId, other.blockId).append(bpId, other.bpId).isEquals();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return new HashCodeBuilder().append(this.blockId).append(this.bpId).toHashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return blockId + \"_\" + bpId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getInputStreamChannel",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReadableByteChannel getInputStreamChannel()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "setReadTimeout",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setReadTimeout(int timeoutMs) throws IOException\n{\r\n    socket.setSoTimeout(timeoutMs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getReceiveBufferSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getReceiveBufferSize() throws IOException\n{\r\n    return socket.getReceiveBufferSize();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getTcpNoDelay",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getTcpNoDelay() throws IOException\n{\r\n    return socket.getTcpNoDelay();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "setWriteTimeout",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setWriteTimeout(int timeoutMs)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "isClosed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isClosed()\n{\r\n    return socket.isClosed();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    socket.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getRemoteAddressString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getRemoteAddressString()\n{\r\n    SocketAddress address = socket.getRemoteSocketAddress();\r\n    return address == null ? null : address.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getLocalAddressString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getLocalAddressString()\n{\r\n    return socket.getLocalSocketAddress().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getInputStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InputStream getInputStream() throws IOException\n{\r\n    return in;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getOutputStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OutputStream getOutputStream() throws IOException\n{\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "isLocal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isLocal()\n{\r\n    return isLocal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"BasicInetPeer(\" + socket.toString() + \")\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getDomainSocket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DomainSocket getDomainSocket()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "hasSecureChannel",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasSecureChannel()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "newSocketSend",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "IOStreamPair newSocketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token<BlockTokenIdentifier> accessToken, DatanodeID datanodeId) throws IOException\n{\r\n    DataEncryptionKey encryptionKey = !trustedChannelResolver.isTrusted() ? encryptionKeyFactory.newDataEncryptionKey() : null;\r\n    IOStreamPair ios = send(socket.getInetAddress(), underlyingOut, underlyingIn, encryptionKey, accessToken, datanodeId, null);\r\n    return ios != null ? ios : new IOStreamPair(underlyingIn, underlyingOut);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "peerSend",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Peer peerSend(Peer peer, DataEncryptionKeyFactory encryptionKeyFactory, Token<BlockTokenIdentifier> accessToken, DatanodeID datanodeId) throws IOException\n{\r\n    IOStreamPair ios = checkTrustAndSend(getPeerAddress(peer), peer.getOutputStream(), peer.getInputStream(), encryptionKeyFactory, accessToken, datanodeId, null);\r\n    return ios != null ? new EncryptedPeer(peer, ios) : peer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "socketSend",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOStreamPair socketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token<BlockTokenIdentifier> accessToken, DatanodeID datanodeId) throws IOException\n{\r\n    return socketSend(socket, underlyingOut, underlyingIn, encryptionKeyFactory, accessToken, datanodeId, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "socketSend",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOStreamPair socketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token<BlockTokenIdentifier> accessToken, DatanodeID datanodeId, SecretKey secretKey) throws IOException\n{\r\n    IOStreamPair ios = checkTrustAndSend(socket.getInetAddress(), underlyingOut, underlyingIn, encryptionKeyFactory, accessToken, datanodeId, secretKey);\r\n    return ios != null ? ios : new IOStreamPair(underlyingIn, underlyingOut);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "checkTrustAndSend",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "IOStreamPair checkTrustAndSend(InetAddress addr, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token<BlockTokenIdentifier> accessToken, DatanodeID datanodeId, SecretKey secretKey) throws IOException\n{\r\n    boolean localTrusted = trustedChannelResolver.isTrusted();\r\n    boolean remoteTrusted = trustedChannelResolver.isTrusted(addr);\r\n    LOG.debug(\"SASL encryption trust check: localHostTrusted = {}, \" + \"remoteHostTrusted = {}\", localTrusted, remoteTrusted);\r\n    if (!localTrusted || !remoteTrusted) {\r\n        DataEncryptionKey encryptionKey = encryptionKeyFactory.newDataEncryptionKey();\r\n        return send(addr, underlyingOut, underlyingIn, encryptionKey, accessToken, datanodeId, secretKey);\r\n    } else {\r\n        LOG.debug(\"SASL client skipping handshake on trusted connection for addr = {}, \" + \"datanodeId = {}\", addr, datanodeId);\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "send",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "IOStreamPair send(InetAddress addr, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKey encryptionKey, Token<BlockTokenIdentifier> accessToken, DatanodeID datanodeId, SecretKey secretKey) throws IOException\n{\r\n    if (encryptionKey != null) {\r\n        LOG.debug(\"SASL client doing encrypted handshake for addr = {}, \" + \"datanodeId = {}\", addr, datanodeId);\r\n        return getEncryptedStreams(addr, underlyingOut, underlyingIn, encryptionKey, accessToken, secretKey);\r\n    } else if (!UserGroupInformation.isSecurityEnabled()) {\r\n        LOG.debug(\"SASL client skipping handshake in unsecured configuration for \" + \"addr = {}, datanodeId = {}\", addr, datanodeId);\r\n        return null;\r\n    } else if (SecurityUtil.isPrivilegedPort(datanodeId.getXferPort())) {\r\n        LOG.debug(\"SASL client skipping handshake in secured configuration with \" + \"privileged port for addr = {}, datanodeId = {}\", addr, datanodeId);\r\n        return null;\r\n    } else if (fallbackToSimpleAuth != null && fallbackToSimpleAuth.get()) {\r\n        LOG.debug(\"SASL client skipping handshake in secured configuration with \" + \"unsecured cluster for addr = {}, datanodeId = {}\", addr, datanodeId);\r\n        return null;\r\n    } else if (saslPropsResolver != null) {\r\n        LOG.debug(\"SASL client doing general handshake for addr = {}, datanodeId = {}\", addr, datanodeId);\r\n        return getSaslStreams(addr, underlyingOut, underlyingIn, accessToken, secretKey);\r\n    } else {\r\n        LOG.debug(\"SASL client skipping handshake in secured configuration with \" + \"no SASL protection configured for addr = {}, datanodeId = {}\", addr, datanodeId);\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "getEncryptedStreams",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "IOStreamPair getEncryptedStreams(InetAddress addr, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKey encryptionKey, Token<BlockTokenIdentifier> accessToken, SecretKey secretKey) throws IOException\n{\r\n    Map<String, String> saslProps = createSaslPropertiesForEncryption(encryptionKey.encryptionAlgorithm);\r\n    if (secretKey != null) {\r\n        LOG.debug(\"DataNode overwriting downstream QOP\" + saslProps.get(Sasl.QOP));\r\n        updateToken(accessToken, secretKey, saslProps);\r\n    }\r\n    LOG.debug(\"Client using encryption algorithm {}\", encryptionKey.encryptionAlgorithm);\r\n    String userName = getUserNameFromEncryptionKey(encryptionKey);\r\n    char[] password = encryptionKeyToPassword(encryptionKey.encryptionKey);\r\n    CallbackHandler callbackHandler = new SaslClientCallbackHandler(userName, password);\r\n    return doSaslHandshake(addr, underlyingOut, underlyingIn, userName, saslProps, callbackHandler, accessToken);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "getUserNameFromEncryptionKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getUserNameFromEncryptionKey(DataEncryptionKey encryptionKey)\n{\r\n    return encryptionKey.keyId + NAME_DELIMITER + encryptionKey.blockPoolId + NAME_DELIMITER + new String(Base64.encodeBase64(encryptionKey.nonce, false), Charsets.UTF_8);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "getTargetQOP",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTargetQOP()\n{\r\n    return targetQOP;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "getSaslStreams",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "IOStreamPair getSaslStreams(InetAddress addr, OutputStream underlyingOut, InputStream underlyingIn, Token<BlockTokenIdentifier> accessToken, SecretKey secretKey) throws IOException\n{\r\n    Map<String, String> saslProps = saslPropsResolver.getClientProperties(addr);\r\n    if (secretKey != null) {\r\n        String newQOP = conf.get(DFS_ENCRYPT_DATA_OVERWRITE_DOWNSTREAM_NEW_QOP_KEY);\r\n        if (newQOP != null) {\r\n            saslProps.put(Sasl.QOP, newQOP);\r\n        }\r\n        LOG.debug(\"DataNode overwriting downstream QOP \" + saslProps.get(Sasl.QOP));\r\n        updateToken(accessToken, secretKey, saslProps);\r\n    }\r\n    targetQOP = saslProps.get(Sasl.QOP);\r\n    String userName = buildUserName(accessToken);\r\n    char[] password = buildClientPassword(accessToken);\r\n    CallbackHandler callbackHandler = new SaslClientCallbackHandler(userName, password);\r\n    return doSaslHandshake(addr, underlyingOut, underlyingIn, userName, saslProps, callbackHandler, accessToken);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "updateToken",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void updateToken(Token<BlockTokenIdentifier> accessToken, SecretKey secretKey, Map<String, String> saslProps) throws IOException\n{\r\n    byte[] newSecret = saslProps.get(Sasl.QOP).getBytes(Charsets.UTF_8);\r\n    BlockTokenIdentifier bkid = accessToken.decodeIdentifier();\r\n    bkid.setHandshakeMsg(newSecret);\r\n    byte[] bkidBytes = bkid.getBytes();\r\n    accessToken.setPassword(SecretManager.createPassword(bkidBytes, secretKey));\r\n    accessToken.setID(bkidBytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "buildUserName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String buildUserName(Token<BlockTokenIdentifier> blockToken)\n{\r\n    return new String(Base64.encodeBase64(blockToken.getIdentifier(), false), Charsets.UTF_8);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "buildClientPassword",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "char[] buildClientPassword(Token<BlockTokenIdentifier> blockToken)\n{\r\n    return new String(Base64.encodeBase64(blockToken.getPassword(), false), Charsets.UTF_8).toCharArray();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer\\sasl",
  "methodName" : "doSaslHandshake",
  "errType" : [ "IOException", "Exception" ],
  "containingMethodsNum" : 39,
  "sourceCodeText" : "IOStreamPair doSaslHandshake(InetAddress addr, OutputStream underlyingOut, InputStream underlyingIn, String userName, Map<String, String> saslProps, CallbackHandler callbackHandler, Token<BlockTokenIdentifier> accessToken) throws IOException\n{\r\n    DataOutputStream out = new DataOutputStream(underlyingOut);\r\n    DataInputStream in = new DataInputStream(underlyingIn);\r\n    SaslParticipant sasl = SaslParticipant.createClientSaslParticipant(userName, saslProps, callbackHandler);\r\n    out.writeInt(SASL_TRANSFER_MAGIC_NUMBER);\r\n    out.flush();\r\n    try {\r\n        BlockTokenIdentifier blockTokenIdentifier = accessToken.decodeIdentifier();\r\n        if (blockTokenIdentifier != null) {\r\n            byte[] handshakeSecret = accessToken.decodeIdentifier().getHandshakeMsg();\r\n            if (handshakeSecret == null || handshakeSecret.length == 0) {\r\n                LOG.debug(\"Handshake secret is null, \" + \"sending without handshake secret.\");\r\n                sendSaslMessage(out, new byte[0]);\r\n            } else {\r\n                LOG.debug(\"Sending handshake secret.\");\r\n                BlockTokenIdentifier identifier = new BlockTokenIdentifier();\r\n                identifier.readFields(new DataInputStream(new ByteArrayInputStream(accessToken.getIdentifier())));\r\n                String bpid = identifier.getBlockPoolId();\r\n                sendSaslMessageHandshakeSecret(out, new byte[0], handshakeSecret, bpid);\r\n            }\r\n        } else {\r\n            LOG.debug(\"Block token id is null, sending without handshake secret.\");\r\n            sendSaslMessage(out, new byte[0]);\r\n        }\r\n        byte[] remoteResponse = readSaslMessage(in);\r\n        byte[] localResponse = sasl.evaluateChallengeOrResponse(remoteResponse);\r\n        List<CipherOption> cipherOptions = null;\r\n        String cipherSuites = conf.get(DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY);\r\n        if (requestedQopContainsPrivacy(saslProps)) {\r\n            if (cipherSuites != null && !cipherSuites.isEmpty()) {\r\n                CipherOption option = null;\r\n                if (cipherSuites.equals(CipherSuite.AES_CTR_NOPADDING.getName())) {\r\n                    option = new CipherOption(CipherSuite.AES_CTR_NOPADDING);\r\n                } else if (cipherSuites.equals(CipherSuite.SM4_CTR_NOPADDING.getName())) {\r\n                    option = new CipherOption(CipherSuite.SM4_CTR_NOPADDING);\r\n                } else {\r\n                    throw new IOException(String.format(\"Invalid cipher suite, %s=%s\", DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY, cipherSuites));\r\n                }\r\n                cipherOptions = Lists.newArrayListWithCapacity(1);\r\n                cipherOptions.add(option);\r\n            }\r\n        }\r\n        sendSaslMessageAndNegotiationCipherOptions(out, localResponse, cipherOptions);\r\n        SaslResponseWithNegotiatedCipherOption response = readSaslMessageAndNegotiatedCipherOption(in);\r\n        localResponse = sasl.evaluateChallengeOrResponse(response.payload);\r\n        assert localResponse == null;\r\n        checkSaslComplete(sasl, saslProps);\r\n        CipherOption cipherOption = null;\r\n        if (sasl.isNegotiatedQopPrivacy()) {\r\n            cipherOption = unwrap(response.cipherOption, sasl);\r\n            if (LOG.isDebugEnabled()) {\r\n                if (cipherOption == null) {\r\n                    if (cipherSuites != null && !cipherSuites.isEmpty()) {\r\n                        LOG.debug(\"Client accepts cipher suites {}, \" + \"but server {} does not accept any of them\", cipherSuites, addr);\r\n                    }\r\n                } else {\r\n                    LOG.debug(\"Client using cipher suite {} with server {}\", cipherOption.getCipherSuite().getName(), addr);\r\n                }\r\n            }\r\n        }\r\n        return cipherOption != null ? createStreamPair(conf, cipherOption, underlyingOut, underlyingIn, false) : sasl.createStreamPair(out, in);\r\n    } catch (IOException ioe) {\r\n        String message = ioe.getMessage();\r\n        try {\r\n            sendGenericSaslErrorMessage(out, message);\r\n        } catch (Exception e) {\r\n            LOG.debug(\"Failed to send generic sasl error to server {} (message: {}), \" + \"suppress exception\", addr, message, e);\r\n            ioe.addSuppressed(e);\r\n        }\r\n        throw ioe;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "incrementOpCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementOpCounter(OpType op)\n{\r\n    opsCount.get(op).increment();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return HdfsConstants.HDFS_URI_SCHEME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLongStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Iterator<LongStatistic> getLongStatistics()\n{\r\n    return new LongIterator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLong",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Long getLong(String key)\n{\r\n    final OpType type = OpType.fromSymbol(key);\r\n    return type == null ? null : opsCount.get(type).longValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isTracked",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isTracked(String key)\n{\r\n    return OpType.fromSymbol(key) != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void reset()\n{\r\n    for (LongAdder count : opsCount.values()) {\r\n        count.reset();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getMessage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getMessage()\n{\r\n    String msg = super.getMessage();\r\n    if (msg == null) {\r\n        msg = \"The NameSpace quota (directories and files)\" + (pathName == null ? \"\" : (\" of directory \" + pathName)) + \" is exceeded: quota=\" + quota + \" file count=\" + count;\r\n        if (prefix != null) {\r\n            msg = prefix + \": \" + msg;\r\n        }\r\n    }\r\n    return msg;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setMessagePrefix",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMessagePrefix(final String prefix)\n{\r\n    this.prefix = prefix;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "short getVersion()\n{\r\n    return version;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getChecksum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DataChecksum getChecksum()\n{\r\n    return checksum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "readDataChecksum",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DataChecksum readDataChecksum(FileInputStream inputStream, int bufSize, File metaFile) throws IOException\n{\r\n    DataInputStream in = new DataInputStream(new BufferedInputStream(inputStream, bufSize));\r\n    return readDataChecksum(in, metaFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "readDataChecksum",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "DataChecksum readDataChecksum(final DataInputStream metaIn, final Object name) throws IOException\n{\r\n    final BlockMetadataHeader header = readHeader(metaIn);\r\n    if (header.getVersion() != VERSION) {\r\n        LOG.warn(\"Unexpected meta-file version for \" + name + \": version in file is \" + header.getVersion() + \" but expected version is \" + VERSION);\r\n    }\r\n    return header.getChecksum();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "preadHeader",
  "errType" : [ "InvalidChecksumSizeException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "BlockMetadataHeader preadHeader(FileChannel fc) throws IOException\n{\r\n    final byte[] arr = new byte[getHeaderSize()];\r\n    ByteBuffer buf = ByteBuffer.wrap(arr);\r\n    while (buf.hasRemaining()) {\r\n        if (fc.read(buf, buf.position()) <= 0) {\r\n            throw new CorruptMetaHeaderException(\"EOF while reading header from \" + \"the metadata file. The meta file may be truncated or corrupt\");\r\n        }\r\n    }\r\n    short version = (short) ((arr[0] << 8) | (arr[1] & 0xff));\r\n    DataChecksum dataChecksum;\r\n    try {\r\n        dataChecksum = DataChecksum.newDataChecksum(arr, 2);\r\n    } catch (InvalidChecksumSizeException e) {\r\n        throw new CorruptMetaHeaderException(\"The block meta file header is \" + \"corrupt\", e);\r\n    }\r\n    return new BlockMetadataHeader(version, dataChecksum);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "readHeader",
  "errType" : [ "EOFException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockMetadataHeader readHeader(DataInputStream in) throws IOException\n{\r\n    try {\r\n        return readHeader(in.readShort(), in);\r\n    } catch (EOFException eof) {\r\n        throw new CorruptMetaHeaderException(\"EOF while reading header from meta\" + \". The meta file may be truncated or corrupt\", eof);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "readHeader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockMetadataHeader readHeader(FileInputStream fis) throws IOException\n{\r\n    try (DataInputStream in = new DataInputStream(new BufferedInputStream(fis))) {\r\n        return readHeader(in);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "readHeader",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "BlockMetadataHeader readHeader(RandomAccessFile raf) throws IOException\n{\r\n    byte[] buf = new byte[getHeaderSize()];\r\n    raf.seek(0);\r\n    raf.readFully(buf, 0, buf.length);\r\n    return readHeader(new DataInputStream(new ByteArrayInputStream(buf)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "readHeader",
  "errType" : [ "InvalidChecksumSizeException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockMetadataHeader readHeader(short version, DataInputStream in) throws IOException\n{\r\n    DataChecksum checksum = null;\r\n    try {\r\n        checksum = DataChecksum.newDataChecksum(in);\r\n    } catch (InvalidChecksumSizeException e) {\r\n        throw new CorruptMetaHeaderException(\"The block meta file header is \" + \"corrupt\", e);\r\n    }\r\n    return new BlockMetadataHeader(version, checksum);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "writeHeader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writeHeader(DataOutputStream out, BlockMetadataHeader header) throws IOException\n{\r\n    out.writeShort(header.getVersion());\r\n    header.getChecksum().writeHeader(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "writeHeader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void writeHeader(DataOutputStream out, DataChecksum checksum) throws IOException\n{\r\n    writeHeader(out, new BlockMetadataHeader(VERSION, checksum));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getHeaderSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getHeaderSize()\n{\r\n    return Short.SIZE / Byte.SIZE + DataChecksum.getChecksumHeaderSize();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl\\metrics",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int read(FileChannel dataIn, ByteBuffer dst, long position) throws IOException\n{\r\n    final int nRead;\r\n    if (isEnabled && (ThreadLocalRandom.current().nextInt() < sampleRangeMax)) {\r\n        long begin = timer.monotonicNow();\r\n        nRead = dataIn.read(dst, position);\r\n        long latency = timer.monotonicNow() - begin;\r\n        addLatency(latency);\r\n    } else {\r\n        nRead = dataIn.read(dst, position);\r\n    }\r\n    return nRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl\\metrics",
  "methodName" : "addLatency",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addLatency(long latency)\n{\r\n    metrics.addShortCircuitReadLatency(latency);\r\n    if (latency > SLOW_READ_WARNING_THRESHOLD_MS && !isWarningLogged) {\r\n        LOG.warn(String.format(\"The Short Circuit Local Read latency, %d ms, \" + \"is higher then the threshold (%d ms). Suppressing further warnings\" + \" for this BlockReaderLocal.\", latency, SLOW_READ_WARNING_THRESHOLD_MS));\r\n        isWarningLogged = true;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close()\n{\r\n    RPC.stopProxy(rpcProxy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getBlockLocations",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "LocatedBlocks getBlockLocations(String src, long offset, long length) throws IOException\n{\r\n    GetBlockLocationsRequestProto req = GetBlockLocationsRequestProto.newBuilder().setSrc(src).setOffset(offset).setLength(length).build();\r\n    try {\r\n        GetBlockLocationsResponseProto resp = rpcProxy.getBlockLocations(null, req);\r\n        return resp.hasLocations() ? PBHelperClient.convert(resp.getLocations()) : null;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getServerDefaults",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FsServerDefaults getServerDefaults() throws IOException\n{\r\n    GetServerDefaultsRequestProto req = VOID_GET_SERVER_DEFAULT_REQUEST;\r\n    try {\r\n        return PBHelperClient.convert(rpcProxy.getServerDefaults(null, req).getServerDefaults());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "create",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable<CreateFlag> flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions, String ecPolicyName, String storagePolicy) throws IOException\n{\r\n    CreateRequestProto.Builder builder = CreateRequestProto.newBuilder().setSrc(src).setMasked(PBHelperClient.convert(masked)).setClientName(clientName).setCreateFlag(PBHelperClient.convertCreateFlag(flag)).setCreateParent(createParent).setReplication(replication).setBlockSize(blockSize);\r\n    if (ecPolicyName != null) {\r\n        builder.setEcPolicyName(ecPolicyName);\r\n    }\r\n    if (storagePolicy != null) {\r\n        builder.setStoragePolicy(storagePolicy);\r\n    }\r\n    FsPermission unmasked = masked.getUnmasked();\r\n    if (unmasked != null) {\r\n        builder.setUnmasked(PBHelperClient.convert(unmasked));\r\n    }\r\n    builder.addAllCryptoProtocolVersion(PBHelperClient.convert(supportedVersions));\r\n    CreateRequestProto req = builder.build();\r\n    try {\r\n        CreateResponseProto res = rpcProxy.create(null, req);\r\n        return res.hasFs() ? PBHelperClient.convert(res.getFs()) : null;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "truncate",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean truncate(String src, long newLength, String clientName) throws IOException\n{\r\n    TruncateRequestProto req = TruncateRequestProto.newBuilder().setSrc(src).setNewLength(newLength).setClientName(clientName).build();\r\n    try {\r\n        return rpcProxy.truncate(null, req).getResult();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "append",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "LastBlockWithStatus append(String src, String clientName, EnumSetWritable<CreateFlag> flag) throws IOException\n{\r\n    AppendRequestProto req = AppendRequestProto.newBuilder().setSrc(src).setClientName(clientName).setFlag(PBHelperClient.convertCreateFlag(flag)).build();\r\n    try {\r\n        AppendResponseProto res = rpcProxy.append(null, req);\r\n        LocatedBlock lastBlock = res.hasBlock() ? PBHelperClient.convertLocatedBlockProto(res.getBlock()) : null;\r\n        HdfsFileStatus stat = (res.hasStat()) ? PBHelperClient.convert(res.getStat()) : null;\r\n        return new LastBlockWithStatus(lastBlock, stat);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "setReplication",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean setReplication(String src, short replication) throws IOException\n{\r\n    SetReplicationRequestProto req = SetReplicationRequestProto.newBuilder().setSrc(src).setReplication(replication).build();\r\n    try {\r\n        return rpcProxy.setReplication(null, req).getResult();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "setPermission",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setPermission(String src, FsPermission permission) throws IOException\n{\r\n    SetPermissionRequestProto req = SetPermissionRequestProto.newBuilder().setSrc(src).setPermission(PBHelperClient.convert(permission)).build();\r\n    try {\r\n        if (Client.isAsynchronousMode()) {\r\n            rpcProxy.setPermission(null, req);\r\n            setAsyncReturnValue();\r\n        } else {\r\n            rpcProxy.setPermission(null, req);\r\n        }\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "setAsyncReturnValue",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setAsyncReturnValue()\n{\r\n    final AsyncGet<Message, Exception> asyncReturnMessage = ProtobufRpcEngine2.getAsyncReturnMessage();\r\n    final AsyncGet<Void, Exception> asyncGet = new AsyncGet<Void, Exception>() {\r\n\r\n        @Override\r\n        public Void get(long timeout, TimeUnit unit) throws Exception {\r\n            asyncReturnMessage.get(timeout, unit);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public boolean isDone() {\r\n            return asyncReturnMessage.isDone();\r\n        }\r\n    };\r\n    AsyncCallHandler.setLowerLayerAsyncReturn(asyncGet);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "setOwner",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setOwner(String src, String username, String groupname) throws IOException\n{\r\n    SetOwnerRequestProto.Builder req = SetOwnerRequestProto.newBuilder().setSrc(src);\r\n    if (username != null)\r\n        req.setUsername(username);\r\n    if (groupname != null)\r\n        req.setGroupname(groupname);\r\n    try {\r\n        if (Client.isAsynchronousMode()) {\r\n            rpcProxy.setOwner(null, req.build());\r\n            setAsyncReturnValue();\r\n        } else {\r\n            rpcProxy.setOwner(null, req.build());\r\n        }\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "abandonBlock",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void abandonBlock(ExtendedBlock b, long fileId, String src, String holder) throws IOException\n{\r\n    AbandonBlockRequestProto req = AbandonBlockRequestProto.newBuilder().setB(PBHelperClient.convert(b)).setSrc(src).setHolder(holder).setFileId(fileId).build();\r\n    try {\r\n        rpcProxy.abandonBlock(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "addBlock",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes, EnumSet<AddBlockFlag> addBlockFlags) throws IOException\n{\r\n    AddBlockRequestProto.Builder req = AddBlockRequestProto.newBuilder().setSrc(src).setClientName(clientName).setFileId(fileId);\r\n    if (previous != null)\r\n        req.setPrevious(PBHelperClient.convert(previous));\r\n    if (excludeNodes != null)\r\n        req.addAllExcludeNodes(PBHelperClient.convert(excludeNodes));\r\n    if (favoredNodes != null) {\r\n        req.addAllFavoredNodes(Arrays.asList(favoredNodes));\r\n    }\r\n    if (addBlockFlags != null) {\r\n        req.addAllFlags(PBHelperClient.convertAddBlockFlags(addBlockFlags));\r\n    }\r\n    try {\r\n        return PBHelperClient.convertLocatedBlockProto(rpcProxy.addBlock(null, req.build()).getBlock());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getAdditionalDatanode",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName) throws IOException\n{\r\n    GetAdditionalDatanodeRequestProto req = GetAdditionalDatanodeRequestProto.newBuilder().setSrc(src).setFileId(fileId).setBlk(PBHelperClient.convert(blk)).addAllExistings(PBHelperClient.convert(existings)).addAllExistingStorageUuids(Arrays.asList(existingStorageIDs)).addAllExcludes(PBHelperClient.convert(excludes)).setNumAdditionalNodes(numAdditionalNodes).setClientName(clientName).build();\r\n    try {\r\n        return PBHelperClient.convertLocatedBlockProto(rpcProxy.getAdditionalDatanode(null, req).getBlock());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "complete",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean complete(String src, String clientName, ExtendedBlock last, long fileId) throws IOException\n{\r\n    CompleteRequestProto.Builder req = CompleteRequestProto.newBuilder().setSrc(src).setClientName(clientName).setFileId(fileId);\r\n    if (last != null)\r\n        req.setLast(PBHelperClient.convert(last));\r\n    try {\r\n        return rpcProxy.complete(null, req.build()).getResult();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "reportBadBlocks",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void reportBadBlocks(LocatedBlock[] blocks) throws IOException\n{\r\n    ReportBadBlocksRequestProto req = ReportBadBlocksRequestProto.newBuilder().addAllBlocks(Arrays.asList(PBHelperClient.convertLocatedBlocks(blocks))).build();\r\n    try {\r\n        rpcProxy.reportBadBlocks(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "rename",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean rename(String src, String dst) throws IOException\n{\r\n    RenameRequestProto req = RenameRequestProto.newBuilder().setSrc(src).setDst(dst).build();\r\n    try {\r\n        return rpcProxy.rename(null, req).getResult();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "rename2",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void rename2(String src, String dst, Rename... options) throws IOException\n{\r\n    boolean overwrite = false;\r\n    boolean toTrash = false;\r\n    if (options != null) {\r\n        for (Rename option : options) {\r\n            if (option == Rename.OVERWRITE) {\r\n                overwrite = true;\r\n            }\r\n            if (option == Rename.TO_TRASH) {\r\n                toTrash = true;\r\n            }\r\n        }\r\n    }\r\n    Rename2RequestProto req = Rename2RequestProto.newBuilder().setSrc(src).setDst(dst).setOverwriteDest(overwrite).setMoveToTrash(toTrash).build();\r\n    try {\r\n        if (Client.isAsynchronousMode()) {\r\n            rpcProxy.rename2(null, req);\r\n            setAsyncReturnValue();\r\n        } else {\r\n            rpcProxy.rename2(null, req);\r\n        }\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "concat",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void concat(String trg, String[] srcs) throws IOException\n{\r\n    ConcatRequestProto req = ConcatRequestProto.newBuilder().setTrg(trg).addAllSrcs(Arrays.asList(srcs)).build();\r\n    try {\r\n        rpcProxy.concat(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "delete",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean delete(String src, boolean recursive) throws IOException\n{\r\n    DeleteRequestProto req = DeleteRequestProto.newBuilder().setSrc(src).setRecursive(recursive).build();\r\n    try {\r\n        return rpcProxy.delete(null, req).getResult();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "mkdirs",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean mkdirs(String src, FsPermission masked, boolean createParent) throws IOException\n{\r\n    MkdirsRequestProto.Builder builder = MkdirsRequestProto.newBuilder().setSrc(src).setMasked(PBHelperClient.convert(masked)).setCreateParent(createParent);\r\n    FsPermission unmasked = masked.getUnmasked();\r\n    if (unmasked != null) {\r\n        builder.setUnmasked(PBHelperClient.convert(unmasked));\r\n    }\r\n    MkdirsRequestProto req = builder.build();\r\n    try {\r\n        return rpcProxy.mkdirs(null, req).getResult();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getListing",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation) throws IOException\n{\r\n    GetListingRequestProto req = GetListingRequestProto.newBuilder().setSrc(src).setStartAfter(ByteString.copyFrom(startAfter)).setNeedLocation(needLocation).build();\r\n    try {\r\n        GetListingResponseProto result = rpcProxy.getListing(null, req);\r\n        if (result.hasDirList()) {\r\n            return PBHelperClient.convert(result.getDirList());\r\n        }\r\n        return null;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getBatchedListing",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "BatchedDirectoryListing getBatchedListing(String[] srcs, byte[] startAfter, boolean needLocation) throws IOException\n{\r\n    GetBatchedListingRequestProto req = GetBatchedListingRequestProto.newBuilder().addAllPaths(Arrays.asList(srcs)).setStartAfter(ByteString.copyFrom(startAfter)).setNeedLocation(needLocation).build();\r\n    try {\r\n        GetBatchedListingResponseProto result = rpcProxy.getBatchedListing(null, req);\r\n        if (result.getListingsCount() > 0) {\r\n            HdfsPartialListing[] listingArray = new HdfsPartialListing[result.getListingsCount()];\r\n            int listingIdx = 0;\r\n            for (BatchedDirectoryListingProto proto : result.getListingsList()) {\r\n                HdfsPartialListing listing;\r\n                if (proto.hasException()) {\r\n                    HdfsProtos.RemoteExceptionProto reProto = proto.getException();\r\n                    RemoteException ex = new RemoteException(reProto.getClassName(), reProto.getMessage());\r\n                    listing = new HdfsPartialListing(proto.getParentIdx(), ex);\r\n                } else {\r\n                    List<HdfsFileStatus> statuses = PBHelperClient.convertHdfsFileStatus(proto.getPartialListingList());\r\n                    listing = new HdfsPartialListing(proto.getParentIdx(), statuses);\r\n                }\r\n                listingArray[listingIdx++] = listing;\r\n            }\r\n            BatchedDirectoryListing batchedListing = new BatchedDirectoryListing(listingArray, result.getHasMore(), result.getStartAfter().toByteArray());\r\n            return batchedListing;\r\n        }\r\n        return null;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "renewLease",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void renewLease(String clientName) throws IOException\n{\r\n    RenewLeaseRequestProto req = RenewLeaseRequestProto.newBuilder().setClientName(clientName).build();\r\n    try {\r\n        rpcProxy.renewLease(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "recoverLease",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean recoverLease(String src, String clientName) throws IOException\n{\r\n    RecoverLeaseRequestProto req = RecoverLeaseRequestProto.newBuilder().setSrc(src).setClientName(clientName).build();\r\n    try {\r\n        return rpcProxy.recoverLease(null, req).getResult();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getStats",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long[] getStats() throws IOException\n{\r\n    try {\r\n        return PBHelperClient.convert(rpcProxy.getFsStats(null, VOID_GET_FSSTATUS_REQUEST));\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getReplicatedBlockStats",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ReplicatedBlockStats getReplicatedBlockStats() throws IOException\n{\r\n    try {\r\n        return PBHelperClient.convert(rpcProxy.getFsReplicatedBlockStats(null, VOID_GET_FS_REPLICATED_BLOCK_STATS_REQUEST));\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getECBlockGroupStats",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ECBlockGroupStats getECBlockGroupStats() throws IOException\n{\r\n    try {\r\n        return PBHelperClient.convert(rpcProxy.getFsECBlockGroupStats(null, VOID_GET_FS_ECBLOCKGROUP_STATS_REQUEST));\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getDatanodeReport",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DatanodeInfo[] getDatanodeReport(DatanodeReportType type) throws IOException\n{\r\n    GetDatanodeReportRequestProto req = GetDatanodeReportRequestProto.newBuilder().setType(PBHelperClient.convert(type)).build();\r\n    try {\r\n        return PBHelperClient.convert(rpcProxy.getDatanodeReport(null, req).getDiList());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getDatanodeStorageReport",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DatanodeStorageReport[] getDatanodeStorageReport(DatanodeReportType type) throws IOException\n{\r\n    final GetDatanodeStorageReportRequestProto req = GetDatanodeStorageReportRequestProto.newBuilder().setType(PBHelperClient.convert(type)).build();\r\n    try {\r\n        return PBHelperClient.convertDatanodeStorageReports(rpcProxy.getDatanodeStorageReport(null, req).getDatanodeStorageReportsList());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getPreferredBlockSize",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getPreferredBlockSize(String filename) throws IOException\n{\r\n    GetPreferredBlockSizeRequestProto req = GetPreferredBlockSizeRequestProto.newBuilder().setFilename(filename).build();\r\n    try {\r\n        return rpcProxy.getPreferredBlockSize(null, req).getBsize();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "setSafeMode",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean setSafeMode(SafeModeAction action, boolean isChecked) throws IOException\n{\r\n    SetSafeModeRequestProto req = SetSafeModeRequestProto.newBuilder().setAction(PBHelperClient.convert(action)).setChecked(isChecked).build();\r\n    try {\r\n        return rpcProxy.setSafeMode(null, req).getResult();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "saveNamespace",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean saveNamespace(long timeWindow, long txGap) throws IOException\n{\r\n    try {\r\n        SaveNamespaceRequestProto req = SaveNamespaceRequestProto.newBuilder().setTimeWindow(timeWindow).setTxGap(txGap).build();\r\n        return rpcProxy.saveNamespace(null, req).getSaved();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "rollEdits",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long rollEdits() throws IOException\n{\r\n    try {\r\n        RollEditsResponseProto resp = rpcProxy.rollEdits(null, VOID_ROLLEDITS_REQUEST);\r\n        return resp.getNewSegmentTxId();\r\n    } catch (ServiceException se) {\r\n        throw ProtobufHelper.getRemoteException(se);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "restoreFailedStorage",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean restoreFailedStorage(String arg) throws IOException\n{\r\n    RestoreFailedStorageRequestProto req = RestoreFailedStorageRequestProto.newBuilder().setArg(arg).build();\r\n    try {\r\n        return rpcProxy.restoreFailedStorage(null, req).getResult();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "refreshNodes",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void refreshNodes() throws IOException\n{\r\n    try {\r\n        rpcProxy.refreshNodes(null, VOID_REFRESH_NODES_REQUEST);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "finalizeUpgrade",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void finalizeUpgrade() throws IOException\n{\r\n    try {\r\n        rpcProxy.finalizeUpgrade(null, VOID_FINALIZE_UPGRADE_REQUEST);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "upgradeStatus",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean upgradeStatus() throws IOException\n{\r\n    try {\r\n        final UpgradeStatusResponseProto proto = rpcProxy.upgradeStatus(null, VOID_UPGRADE_STATUS_REQUEST);\r\n        return proto.getUpgradeFinalized();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "rollingUpgrade",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "RollingUpgradeInfo rollingUpgrade(RollingUpgradeAction action) throws IOException\n{\r\n    final RollingUpgradeRequestProto r = RollingUpgradeRequestProto.newBuilder().setAction(PBHelperClient.convert(action)).build();\r\n    try {\r\n        final RollingUpgradeResponseProto proto = rpcProxy.rollingUpgrade(null, r);\r\n        if (proto.hasRollingUpgradeInfo()) {\r\n            return PBHelperClient.convert(proto.getRollingUpgradeInfo());\r\n        }\r\n        return null;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "listCorruptFileBlocks",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "CorruptFileBlocks listCorruptFileBlocks(String path, String cookie) throws IOException\n{\r\n    ListCorruptFileBlocksRequestProto.Builder req = ListCorruptFileBlocksRequestProto.newBuilder().setPath(path);\r\n    if (cookie != null)\r\n        req.setCookie(cookie);\r\n    try {\r\n        return PBHelperClient.convert(rpcProxy.listCorruptFileBlocks(null, req.build()).getCorrupt());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "metaSave",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void metaSave(String filename) throws IOException\n{\r\n    MetaSaveRequestProto req = MetaSaveRequestProto.newBuilder().setFilename(filename).build();\r\n    try {\r\n        rpcProxy.metaSave(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getFileInfo",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "HdfsFileStatus getFileInfo(String src) throws IOException\n{\r\n    GetFileInfoRequestProto req = GetFileInfoRequestProto.newBuilder().setSrc(src).build();\r\n    try {\r\n        GetFileInfoResponseProto res = rpcProxy.getFileInfo(null, req);\r\n        return res.hasFs() ? PBHelperClient.convert(res.getFs()) : null;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getLocatedFileInfo",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "HdfsLocatedFileStatus getLocatedFileInfo(String src, boolean needBlockToken) throws IOException\n{\r\n    GetLocatedFileInfoRequestProto req = GetLocatedFileInfoRequestProto.newBuilder().setSrc(src).setNeedBlockToken(needBlockToken).build();\r\n    try {\r\n        GetLocatedFileInfoResponseProto res = rpcProxy.getLocatedFileInfo(null, req);\r\n        return (HdfsLocatedFileStatus) (res.hasFs() ? PBHelperClient.convert(res.getFs()) : null);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getFileLinkInfo",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "HdfsFileStatus getFileLinkInfo(String src) throws IOException\n{\r\n    GetFileLinkInfoRequestProto req = GetFileLinkInfoRequestProto.newBuilder().setSrc(src).build();\r\n    try {\r\n        GetFileLinkInfoResponseProto result = rpcProxy.getFileLinkInfo(null, req);\r\n        return result.hasFs() ? PBHelperClient.convert(result.getFs()) : null;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getContentSummary",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ContentSummary getContentSummary(String path) throws IOException\n{\r\n    GetContentSummaryRequestProto req = GetContentSummaryRequestProto.newBuilder().setPath(path).build();\r\n    try {\r\n        return PBHelperClient.convert(rpcProxy.getContentSummary(null, req).getSummary());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "setQuota",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setQuota(String path, long namespaceQuota, long storagespaceQuota, StorageType type) throws IOException\n{\r\n    final SetQuotaRequestProto.Builder builder = SetQuotaRequestProto.newBuilder().setPath(path).setNamespaceQuota(namespaceQuota).setStoragespaceQuota(storagespaceQuota);\r\n    if (type != null) {\r\n        builder.setStorageType(PBHelperClient.convertStorageType(type));\r\n    }\r\n    final SetQuotaRequestProto req = builder.build();\r\n    try {\r\n        rpcProxy.setQuota(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "fsync",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void fsync(String src, long fileId, String client, long lastBlockLength) throws IOException\n{\r\n    FsyncRequestProto req = FsyncRequestProto.newBuilder().setSrc(src).setClient(client).setLastBlockLength(lastBlockLength).setFileId(fileId).build();\r\n    try {\r\n        rpcProxy.fsync(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "setTimes",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setTimes(String src, long mtime, long atime) throws IOException\n{\r\n    SetTimesRequestProto req = SetTimesRequestProto.newBuilder().setSrc(src).setMtime(mtime).setAtime(atime).build();\r\n    try {\r\n        rpcProxy.setTimes(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "createSymlink",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent) throws IOException\n{\r\n    CreateSymlinkRequestProto req = CreateSymlinkRequestProto.newBuilder().setTarget(target).setLink(link).setDirPerm(PBHelperClient.convert(dirPerm)).setCreateParent(createParent).build();\r\n    try {\r\n        rpcProxy.createSymlink(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getLinkTarget",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getLinkTarget(String path) throws IOException\n{\r\n    GetLinkTargetRequestProto req = GetLinkTargetRequestProto.newBuilder().setPath(path).build();\r\n    try {\r\n        GetLinkTargetResponseProto rsp = rpcProxy.getLinkTarget(null, req);\r\n        return rsp.hasTargetPath() ? rsp.getTargetPath() : null;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "updateBlockForPipeline",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "LocatedBlock updateBlockForPipeline(ExtendedBlock block, String clientName) throws IOException\n{\r\n    UpdateBlockForPipelineRequestProto req = UpdateBlockForPipelineRequestProto.newBuilder().setBlock(PBHelperClient.convert(block)).setClientName(clientName).build();\r\n    try {\r\n        return PBHelperClient.convertLocatedBlockProto(rpcProxy.updateBlockForPipeline(null, req).getBlock());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "updatePipeline",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void updatePipeline(String clientName, ExtendedBlock oldBlock, ExtendedBlock newBlock, DatanodeID[] newNodes, String[] storageIDs) throws IOException\n{\r\n    UpdatePipelineRequestProto req = UpdatePipelineRequestProto.newBuilder().setClientName(clientName).setOldBlock(PBHelperClient.convert(oldBlock)).setNewBlock(PBHelperClient.convert(newBlock)).addAllNewNodes(Arrays.asList(PBHelperClient.convert(newNodes))).addAllStorageIDs(storageIDs == null ? null : Arrays.asList(storageIDs)).build();\r\n    try {\r\n        rpcProxy.updatePipeline(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getDelegationToken",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> getDelegationToken(Text renewer) throws IOException\n{\r\n    GetDelegationTokenRequestProto req = GetDelegationTokenRequestProto.newBuilder().setRenewer(renewer == null ? \"\" : renewer.toString()).build();\r\n    try {\r\n        GetDelegationTokenResponseProto resp = rpcProxy.getDelegationToken(null, req);\r\n        return resp.hasToken() ? PBHelperClient.convertDelegationToken(resp.getToken()) : null;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "renewDelegationToken",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long renewDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException\n{\r\n    RenewDelegationTokenRequestProto req = RenewDelegationTokenRequestProto.newBuilder().setToken(PBHelperClient.convert(token)).build();\r\n    try {\r\n        return rpcProxy.renewDelegationToken(null, req).getNewExpiryTime();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "cancelDelegationToken",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cancelDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException\n{\r\n    CancelDelegationTokenRequestProto req = CancelDelegationTokenRequestProto.newBuilder().setToken(PBHelperClient.convert(token)).build();\r\n    try {\r\n        rpcProxy.cancelDelegationToken(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "setBalancerBandwidth",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setBalancerBandwidth(long bandwidth) throws IOException\n{\r\n    SetBalancerBandwidthRequestProto req = SetBalancerBandwidthRequestProto.newBuilder().setBandwidth(bandwidth).build();\r\n    try {\r\n        rpcProxy.setBalancerBandwidth(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "isMethodSupported",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isMethodSupported(String methodName) throws IOException\n{\r\n    return RpcClientUtil.isMethodSupported(rpcProxy, ClientNamenodeProtocolPB.class, RPC.RpcKind.RPC_PROTOCOL_BUFFER, RPC.getProtocolVersion(ClientNamenodeProtocolPB.class), methodName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getDataEncryptionKey",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "DataEncryptionKey getDataEncryptionKey() throws IOException\n{\r\n    try {\r\n        GetDataEncryptionKeyResponseProto rsp = rpcProxy.getDataEncryptionKey(null, VOID_GET_DATA_ENCRYPTIONKEY_REQUEST);\r\n        return rsp.hasDataEncryptionKey() ? PBHelperClient.convert(rsp.getDataEncryptionKey()) : null;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "isFileClosed",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isFileClosed(String src) throws IOException\n{\r\n    IsFileClosedRequestProto req = IsFileClosedRequestProto.newBuilder().setSrc(src).build();\r\n    try {\r\n        return rpcProxy.isFileClosed(null, req).getResult();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getUnderlyingProxyObject",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getUnderlyingProxyObject()\n{\r\n    return rpcProxy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "createSnapshot",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String createSnapshot(String snapshotRoot, String snapshotName) throws IOException\n{\r\n    final CreateSnapshotRequestProto.Builder builder = CreateSnapshotRequestProto.newBuilder().setSnapshotRoot(snapshotRoot);\r\n    if (snapshotName != null) {\r\n        builder.setSnapshotName(snapshotName);\r\n    }\r\n    final CreateSnapshotRequestProto req = builder.build();\r\n    try {\r\n        return rpcProxy.createSnapshot(null, req).getSnapshotPath();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "deleteSnapshot",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void deleteSnapshot(String snapshotRoot, String snapshotName) throws IOException\n{\r\n    DeleteSnapshotRequestProto req = DeleteSnapshotRequestProto.newBuilder().setSnapshotRoot(snapshotRoot).setSnapshotName(snapshotName).build();\r\n    try {\r\n        rpcProxy.deleteSnapshot(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "allowSnapshot",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void allowSnapshot(String snapshotRoot) throws IOException\n{\r\n    AllowSnapshotRequestProto req = AllowSnapshotRequestProto.newBuilder().setSnapshotRoot(snapshotRoot).build();\r\n    try {\r\n        rpcProxy.allowSnapshot(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "disallowSnapshot",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void disallowSnapshot(String snapshotRoot) throws IOException\n{\r\n    DisallowSnapshotRequestProto req = DisallowSnapshotRequestProto.newBuilder().setSnapshotRoot(snapshotRoot).build();\r\n    try {\r\n        rpcProxy.disallowSnapshot(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "renameSnapshot",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void renameSnapshot(String snapshotRoot, String snapshotOldName, String snapshotNewName) throws IOException\n{\r\n    RenameSnapshotRequestProto req = RenameSnapshotRequestProto.newBuilder().setSnapshotRoot(snapshotRoot).setSnapshotOldName(snapshotOldName).setSnapshotNewName(snapshotNewName).build();\r\n    try {\r\n        rpcProxy.renameSnapshot(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getSnapshottableDirListing",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "SnapshottableDirectoryStatus[] getSnapshottableDirListing() throws IOException\n{\r\n    GetSnapshottableDirListingRequestProto req = GetSnapshottableDirListingRequestProto.newBuilder().build();\r\n    try {\r\n        GetSnapshottableDirListingResponseProto result = rpcProxy.getSnapshottableDirListing(null, req);\r\n        if (result.hasSnapshottableDirList()) {\r\n            return PBHelperClient.convert(result.getSnapshottableDirList());\r\n        }\r\n        return null;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getSnapshotListing",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "SnapshotStatus[] getSnapshotListing(String path) throws IOException\n{\r\n    GetSnapshotListingRequestProto req = GetSnapshotListingRequestProto.newBuilder().setSnapshotRoot(path).build();\r\n    try {\r\n        GetSnapshotListingResponseProto result = rpcProxy.getSnapshotListing(null, req);\r\n        if (result.hasSnapshotList()) {\r\n            return PBHelperClient.convert(result.getSnapshotList());\r\n        }\r\n        return null;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getSnapshotDiffReport",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "SnapshotDiffReport getSnapshotDiffReport(String snapshotRoot, String fromSnapshot, String toSnapshot) throws IOException\n{\r\n    GetSnapshotDiffReportRequestProto req = GetSnapshotDiffReportRequestProto.newBuilder().setSnapshotRoot(snapshotRoot).setFromSnapshot(fromSnapshot).setToSnapshot(toSnapshot).build();\r\n    try {\r\n        GetSnapshotDiffReportResponseProto result = rpcProxy.getSnapshotDiffReport(null, req);\r\n        return PBHelperClient.convert(result.getDiffReport());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getSnapshotDiffReportListing",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "SnapshotDiffReportListing getSnapshotDiffReportListing(String snapshotRoot, String fromSnapshot, String toSnapshot, byte[] startPath, int index) throws IOException\n{\r\n    GetSnapshotDiffReportListingRequestProto req = GetSnapshotDiffReportListingRequestProto.newBuilder().setSnapshotRoot(snapshotRoot).setFromSnapshot(fromSnapshot).setToSnapshot(toSnapshot).setCursor(HdfsProtos.SnapshotDiffReportCursorProto.newBuilder().setStartPath(PBHelperClient.getByteString(startPath)).setIndex(index).build()).build();\r\n    try {\r\n        GetSnapshotDiffReportListingResponseProto result = rpcProxy.getSnapshotDiffReportListing(null, req);\r\n        return PBHelperClient.convert(result.getDiffReport());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "addCacheDirective",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long addCacheDirective(CacheDirectiveInfo directive, EnumSet<CacheFlag> flags) throws IOException\n{\r\n    try {\r\n        AddCacheDirectiveRequestProto.Builder builder = AddCacheDirectiveRequestProto.newBuilder().setInfo(PBHelperClient.convert(directive));\r\n        if (!flags.isEmpty()) {\r\n            builder.setCacheFlags(PBHelperClient.convertCacheFlags(flags));\r\n        }\r\n        return rpcProxy.addCacheDirective(null, builder.build()).getId();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "modifyCacheDirective",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void modifyCacheDirective(CacheDirectiveInfo directive, EnumSet<CacheFlag> flags) throws IOException\n{\r\n    try {\r\n        ModifyCacheDirectiveRequestProto.Builder builder = ModifyCacheDirectiveRequestProto.newBuilder().setInfo(PBHelperClient.convert(directive));\r\n        if (!flags.isEmpty()) {\r\n            builder.setCacheFlags(PBHelperClient.convertCacheFlags(flags));\r\n        }\r\n        rpcProxy.modifyCacheDirective(null, builder.build());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "removeCacheDirective",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void removeCacheDirective(long id) throws IOException\n{\r\n    try {\r\n        rpcProxy.removeCacheDirective(null, RemoveCacheDirectiveRequestProto.newBuilder().setId(id).build());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "listCacheDirectives",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "BatchedEntries<CacheDirectiveEntry> listCacheDirectives(long prevId, CacheDirectiveInfo filter) throws IOException\n{\r\n    if (filter == null) {\r\n        filter = new CacheDirectiveInfo.Builder().build();\r\n    }\r\n    try {\r\n        return new BatchedCacheEntries(rpcProxy.listCacheDirectives(null, ListCacheDirectivesRequestProto.newBuilder().setPrevId(prevId).setFilter(PBHelperClient.convert(filter)).build()));\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "addCachePool",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addCachePool(CachePoolInfo info) throws IOException\n{\r\n    AddCachePoolRequestProto.Builder builder = AddCachePoolRequestProto.newBuilder();\r\n    builder.setInfo(PBHelperClient.convert(info));\r\n    try {\r\n        rpcProxy.addCachePool(null, builder.build());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "modifyCachePool",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void modifyCachePool(CachePoolInfo req) throws IOException\n{\r\n    ModifyCachePoolRequestProto.Builder builder = ModifyCachePoolRequestProto.newBuilder();\r\n    builder.setInfo(PBHelperClient.convert(req));\r\n    try {\r\n        rpcProxy.modifyCachePool(null, builder.build());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "removeCachePool",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void removeCachePool(String cachePoolName) throws IOException\n{\r\n    try {\r\n        rpcProxy.removeCachePool(null, RemoveCachePoolRequestProto.newBuilder().setPoolName(cachePoolName).build());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "listCachePools",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BatchedEntries<CachePoolEntry> listCachePools(String prevKey) throws IOException\n{\r\n    try {\r\n        return new BatchedCachePoolEntries(rpcProxy.listCachePools(null, ListCachePoolsRequestProto.newBuilder().setPrevPoolName(prevKey).build()));\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "modifyAclEntries",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void modifyAclEntries(String src, List<AclEntry> aclSpec) throws IOException\n{\r\n    ModifyAclEntriesRequestProto req = ModifyAclEntriesRequestProto.newBuilder().setSrc(src).addAllAclSpec(PBHelperClient.convertAclEntryProto(aclSpec)).build();\r\n    try {\r\n        rpcProxy.modifyAclEntries(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "removeAclEntries",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeAclEntries(String src, List<AclEntry> aclSpec) throws IOException\n{\r\n    RemoveAclEntriesRequestProto req = RemoveAclEntriesRequestProto.newBuilder().setSrc(src).addAllAclSpec(PBHelperClient.convertAclEntryProto(aclSpec)).build();\r\n    try {\r\n        rpcProxy.removeAclEntries(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "removeDefaultAcl",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeDefaultAcl(String src) throws IOException\n{\r\n    RemoveDefaultAclRequestProto req = RemoveDefaultAclRequestProto.newBuilder().setSrc(src).build();\r\n    try {\r\n        rpcProxy.removeDefaultAcl(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "removeAcl",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeAcl(String src) throws IOException\n{\r\n    RemoveAclRequestProto req = RemoveAclRequestProto.newBuilder().setSrc(src).build();\r\n    try {\r\n        rpcProxy.removeAcl(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "setAcl",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setAcl(String src, List<AclEntry> aclSpec) throws IOException\n{\r\n    SetAclRequestProto req = SetAclRequestProto.newBuilder().setSrc(src).addAllAclSpec(PBHelperClient.convertAclEntryProto(aclSpec)).build();\r\n    try {\r\n        if (Client.isAsynchronousMode()) {\r\n            rpcProxy.setAcl(null, req);\r\n            setAsyncReturnValue();\r\n        } else {\r\n            rpcProxy.setAcl(null, req);\r\n        }\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getAclStatus",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "AclStatus getAclStatus(String src) throws IOException\n{\r\n    GetAclStatusRequestProto req = GetAclStatusRequestProto.newBuilder().setSrc(src).build();\r\n    try {\r\n        if (Client.isAsynchronousMode()) {\r\n            rpcProxy.getAclStatus(null, req);\r\n            final AsyncGet<Message, Exception> asyncReturnMessage = ProtobufRpcEngine2.getAsyncReturnMessage();\r\n            final AsyncGet<AclStatus, Exception> asyncGet = new AsyncGet<AclStatus, Exception>() {\r\n\r\n                @Override\r\n                public AclStatus get(long timeout, TimeUnit unit) throws Exception {\r\n                    return PBHelperClient.convert((GetAclStatusResponseProto) asyncReturnMessage.get(timeout, unit));\r\n                }\r\n\r\n                @Override\r\n                public boolean isDone() {\r\n                    return asyncReturnMessage.isDone();\r\n                }\r\n            };\r\n            AsyncCallHandler.setLowerLayerAsyncReturn(asyncGet);\r\n            return null;\r\n        } else {\r\n            return PBHelperClient.convert(rpcProxy.getAclStatus(null, req));\r\n        }\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "createEncryptionZone",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void createEncryptionZone(String src, String keyName) throws IOException\n{\r\n    final CreateEncryptionZoneRequestProto.Builder builder = CreateEncryptionZoneRequestProto.newBuilder();\r\n    builder.setSrc(src);\r\n    if (keyName != null && !keyName.isEmpty()) {\r\n        builder.setKeyName(keyName);\r\n    }\r\n    CreateEncryptionZoneRequestProto req = builder.build();\r\n    try {\r\n        rpcProxy.createEncryptionZone(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getEZForPath",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "EncryptionZone getEZForPath(String src) throws IOException\n{\r\n    final GetEZForPathRequestProto.Builder builder = GetEZForPathRequestProto.newBuilder();\r\n    builder.setSrc(src);\r\n    final GetEZForPathRequestProto req = builder.build();\r\n    try {\r\n        final EncryptionZonesProtos.GetEZForPathResponseProto response = rpcProxy.getEZForPath(null, req);\r\n        if (response.hasZone()) {\r\n            return PBHelperClient.convert(response.getZone());\r\n        } else {\r\n            return null;\r\n        }\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "listEncryptionZones",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "BatchedEntries<EncryptionZone> listEncryptionZones(long id) throws IOException\n{\r\n    final ListEncryptionZonesRequestProto req = ListEncryptionZonesRequestProto.newBuilder().setId(id).build();\r\n    try {\r\n        EncryptionZonesProtos.ListEncryptionZonesResponseProto response = rpcProxy.listEncryptionZones(null, req);\r\n        List<EncryptionZone> elements = Lists.newArrayListWithCapacity(response.getZonesCount());\r\n        for (EncryptionZoneProto p : response.getZonesList()) {\r\n            elements.add(PBHelperClient.convert(p));\r\n        }\r\n        return new BatchedListEntries<>(elements, response.getHasMore());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "setErasureCodingPolicy",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setErasureCodingPolicy(String src, String ecPolicyName) throws IOException\n{\r\n    final SetErasureCodingPolicyRequestProto.Builder builder = SetErasureCodingPolicyRequestProto.newBuilder();\r\n    builder.setSrc(src);\r\n    if (ecPolicyName != null) {\r\n        builder.setEcPolicyName(ecPolicyName);\r\n    }\r\n    SetErasureCodingPolicyRequestProto req = builder.build();\r\n    try {\r\n        rpcProxy.setErasureCodingPolicy(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "unsetErasureCodingPolicy",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void unsetErasureCodingPolicy(String src) throws IOException\n{\r\n    final UnsetErasureCodingPolicyRequestProto.Builder builder = UnsetErasureCodingPolicyRequestProto.newBuilder();\r\n    builder.setSrc(src);\r\n    UnsetErasureCodingPolicyRequestProto req = builder.build();\r\n    try {\r\n        rpcProxy.unsetErasureCodingPolicy(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getECTopologyResultForPolicies",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ECTopologyVerifierResult getECTopologyResultForPolicies(final String... policyNames) throws IOException\n{\r\n    final GetECTopologyResultForPoliciesRequestProto.Builder builder = GetECTopologyResultForPoliciesRequestProto.newBuilder();\r\n    builder.addAllPolicies(Arrays.asList(policyNames));\r\n    GetECTopologyResultForPoliciesRequestProto req = builder.build();\r\n    try {\r\n        GetECTopologyResultForPoliciesResponseProto response = rpcProxy.getECTopologyResultForPolicies(null, req);\r\n        return PBHelperClient.convertECTopologyVerifierResultProto(response.getResponse());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "reencryptEncryptionZone",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void reencryptEncryptionZone(String zone, ReencryptAction action) throws IOException\n{\r\n    final ReencryptEncryptionZoneRequestProto.Builder builder = ReencryptEncryptionZoneRequestProto.newBuilder();\r\n    builder.setZone(zone).setAction(PBHelperClient.convert(action));\r\n    ReencryptEncryptionZoneRequestProto req = builder.build();\r\n    try {\r\n        rpcProxy.reencryptEncryptionZone(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "listReencryptionStatus",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "BatchedEntries<ZoneReencryptionStatus> listReencryptionStatus(long id) throws IOException\n{\r\n    final ListReencryptionStatusRequestProto req = ListReencryptionStatusRequestProto.newBuilder().setId(id).build();\r\n    try {\r\n        ListReencryptionStatusResponseProto response = rpcProxy.listReencryptionStatus(null, req);\r\n        List<ZoneReencryptionStatus> elements = Lists.newArrayListWithCapacity(response.getStatusesCount());\r\n        for (ZoneReencryptionStatusProto p : response.getStatusesList()) {\r\n            elements.add(PBHelperClient.convert(p));\r\n        }\r\n        return new BatchedListEntries<>(elements, response.getHasMore());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "setXAttr",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setXAttr(String src, XAttr xAttr, EnumSet<XAttrSetFlag> flag) throws IOException\n{\r\n    SetXAttrRequestProto req = SetXAttrRequestProto.newBuilder().setSrc(src).setXAttr(PBHelperClient.convertXAttrProto(xAttr)).setFlag(PBHelperClient.convert(flag)).build();\r\n    try {\r\n        rpcProxy.setXAttr(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getXAttrs",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "List<XAttr> getXAttrs(String src, List<XAttr> xAttrs) throws IOException\n{\r\n    GetXAttrsRequestProto.Builder builder = GetXAttrsRequestProto.newBuilder();\r\n    builder.setSrc(src);\r\n    if (xAttrs != null) {\r\n        builder.addAllXAttrs(PBHelperClient.convertXAttrProto(xAttrs));\r\n    }\r\n    GetXAttrsRequestProto req = builder.build();\r\n    try {\r\n        return PBHelperClient.convert(rpcProxy.getXAttrs(null, req));\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "listXAttrs",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<XAttr> listXAttrs(String src) throws IOException\n{\r\n    ListXAttrsRequestProto.Builder builder = ListXAttrsRequestProto.newBuilder();\r\n    builder.setSrc(src);\r\n    ListXAttrsRequestProto req = builder.build();\r\n    try {\r\n        return PBHelperClient.convert(rpcProxy.listXAttrs(null, req));\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "removeXAttr",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeXAttr(String src, XAttr xAttr) throws IOException\n{\r\n    RemoveXAttrRequestProto req = RemoveXAttrRequestProto.newBuilder().setSrc(src).setXAttr(PBHelperClient.convertXAttrProto(xAttr)).build();\r\n    try {\r\n        rpcProxy.removeXAttr(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "checkAccess",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkAccess(String path, FsAction mode) throws IOException\n{\r\n    CheckAccessRequestProto req = CheckAccessRequestProto.newBuilder().setPath(path).setMode(PBHelperClient.convert(mode)).build();\r\n    try {\r\n        rpcProxy.checkAccess(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "setStoragePolicy",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setStoragePolicy(String src, String policyName) throws IOException\n{\r\n    SetStoragePolicyRequestProto req = SetStoragePolicyRequestProto.newBuilder().setSrc(src).setPolicyName(policyName).build();\r\n    try {\r\n        rpcProxy.setStoragePolicy(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "unsetStoragePolicy",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void unsetStoragePolicy(String src) throws IOException\n{\r\n    UnsetStoragePolicyRequestProto req = UnsetStoragePolicyRequestProto.newBuilder().setSrc(src).build();\r\n    try {\r\n        rpcProxy.unsetStoragePolicy(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getStoragePolicy",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "BlockStoragePolicy getStoragePolicy(String path) throws IOException\n{\r\n    GetStoragePolicyRequestProto request = GetStoragePolicyRequestProto.newBuilder().setPath(path).build();\r\n    try {\r\n        return PBHelperClient.convert(rpcProxy.getStoragePolicy(null, request).getStoragePolicy());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getStoragePolicies",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "BlockStoragePolicy[] getStoragePolicies() throws IOException\n{\r\n    try {\r\n        GetStoragePoliciesResponseProto response = rpcProxy.getStoragePolicies(null, VOID_GET_STORAGE_POLICIES_REQUEST);\r\n        return PBHelperClient.convertStoragePolicies(response.getPoliciesList());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getCurrentEditLogTxid",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getCurrentEditLogTxid() throws IOException\n{\r\n    GetCurrentEditLogTxidRequestProto req = GetCurrentEditLogTxidRequestProto.getDefaultInstance();\r\n    try {\r\n        return rpcProxy.getCurrentEditLogTxid(null, req).getTxid();\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getEditsFromTxid",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "EventBatchList getEditsFromTxid(long txid) throws IOException\n{\r\n    GetEditsFromTxidRequestProto req = GetEditsFromTxidRequestProto.newBuilder().setTxid(txid).build();\r\n    try {\r\n        return PBHelperClient.convert(rpcProxy.getEditsFromTxid(null, req));\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "addErasureCodingPolicies",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AddErasureCodingPolicyResponse[] addErasureCodingPolicies(ErasureCodingPolicy[] policies) throws IOException\n{\r\n    List<ErasureCodingPolicyProto> protos = Arrays.stream(policies).map(PBHelperClient::convertErasureCodingPolicy).collect(Collectors.toList());\r\n    AddErasureCodingPoliciesRequestProto req = AddErasureCodingPoliciesRequestProto.newBuilder().addAllEcPolicies(protos).build();\r\n    try {\r\n        AddErasureCodingPoliciesResponseProto rep = rpcProxy.addErasureCodingPolicies(null, req);\r\n        AddErasureCodingPolicyResponse[] responses = rep.getResponsesList().stream().map(PBHelperClient::convertAddErasureCodingPolicyResponse).toArray(AddErasureCodingPolicyResponse[]::new);\r\n        return responses;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "removeErasureCodingPolicy",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void removeErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    RemoveErasureCodingPolicyRequestProto.Builder builder = RemoveErasureCodingPolicyRequestProto.newBuilder();\r\n    builder.setEcPolicyName(ecPolicyName);\r\n    RemoveErasureCodingPolicyRequestProto req = builder.build();\r\n    try {\r\n        rpcProxy.removeErasureCodingPolicy(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "enableErasureCodingPolicy",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void enableErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    EnableErasureCodingPolicyRequestProto.Builder builder = EnableErasureCodingPolicyRequestProto.newBuilder();\r\n    builder.setEcPolicyName(ecPolicyName);\r\n    EnableErasureCodingPolicyRequestProto req = builder.build();\r\n    try {\r\n        rpcProxy.enableErasureCodingPolicy(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "disableErasureCodingPolicy",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void disableErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    DisableErasureCodingPolicyRequestProto.Builder builder = DisableErasureCodingPolicyRequestProto.newBuilder();\r\n    builder.setEcPolicyName(ecPolicyName);\r\n    DisableErasureCodingPolicyRequestProto req = builder.build();\r\n    try {\r\n        rpcProxy.disableErasureCodingPolicy(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getErasureCodingPolicies",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ErasureCodingPolicyInfo[] getErasureCodingPolicies() throws IOException\n{\r\n    try {\r\n        GetErasureCodingPoliciesResponseProto response = rpcProxy.getErasureCodingPolicies(null, VOID_GET_EC_POLICIES_REQUEST);\r\n        ErasureCodingPolicyInfo[] ecPolicies = new ErasureCodingPolicyInfo[response.getEcPoliciesCount()];\r\n        int i = 0;\r\n        for (ErasureCodingPolicyProto proto : response.getEcPoliciesList()) {\r\n            ecPolicies[i++] = PBHelperClient.convertErasureCodingPolicyInfo(proto);\r\n        }\r\n        return ecPolicies;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getErasureCodingCodecs",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<String, String> getErasureCodingCodecs() throws IOException\n{\r\n    try {\r\n        GetErasureCodingCodecsResponseProto response = rpcProxy.getErasureCodingCodecs(null, VOID_GET_EC_CODEC_REQUEST);\r\n        Map<String, String> ecCodecs = new HashMap<>();\r\n        for (CodecProto codec : response.getCodecList()) {\r\n            ecCodecs.put(codec.getCodec(), codec.getCoders());\r\n        }\r\n        return ecCodecs;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getErasureCodingPolicy",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ErasureCodingPolicy getErasureCodingPolicy(String src) throws IOException\n{\r\n    GetErasureCodingPolicyRequestProto req = GetErasureCodingPolicyRequestProto.newBuilder().setSrc(src).build();\r\n    try {\r\n        GetErasureCodingPolicyResponseProto response = rpcProxy.getErasureCodingPolicy(null, req);\r\n        if (response.hasEcPolicy()) {\r\n            return PBHelperClient.convertErasureCodingPolicy(response.getEcPolicy());\r\n        }\r\n        return null;\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getQuotaUsage",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "QuotaUsage getQuotaUsage(String path) throws IOException\n{\r\n    GetQuotaUsageRequestProto req = GetQuotaUsageRequestProto.newBuilder().setPath(path).build();\r\n    try {\r\n        return PBHelperClient.convert(rpcProxy.getQuotaUsage(null, req).getUsage());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BatchedEntries<OpenFileEntry> listOpenFiles(long prevId) throws IOException\n{\r\n    return listOpenFiles(prevId, EnumSet.of(OpenFilesType.ALL_OPEN_FILES), OpenFilesIterator.FILTER_PATH_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "listOpenFiles",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "BatchedEntries<OpenFileEntry> listOpenFiles(long prevId, EnumSet<OpenFilesType> openFilesTypes, String path) throws IOException\n{\r\n    ListOpenFilesRequestProto.Builder req = ListOpenFilesRequestProto.newBuilder().setId(prevId);\r\n    if (openFilesTypes != null) {\r\n        req.addAllTypes(PBHelperClient.convertOpenFileTypes(openFilesTypes));\r\n    }\r\n    req.setPath(path);\r\n    try {\r\n        ListOpenFilesResponseProto response = rpcProxy.listOpenFiles(null, req.build());\r\n        List<OpenFileEntry> openFileEntries = Lists.newArrayListWithCapacity(response.getEntriesCount());\r\n        for (OpenFilesBatchResponseProto p : response.getEntriesList()) {\r\n            openFileEntries.add(PBHelperClient.convert(p));\r\n        }\r\n        return new BatchedListEntries<>(openFileEntries, response.getHasMore());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "msync",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void msync() throws IOException\n{\r\n    MsyncRequestProto.Builder req = MsyncRequestProto.newBuilder();\r\n    try {\r\n        rpcProxy.msync(null, req.build());\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "satisfyStoragePolicy",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void satisfyStoragePolicy(String src) throws IOException\n{\r\n    SatisfyStoragePolicyRequestProto req = SatisfyStoragePolicyRequestProto.newBuilder().setSrc(src).build();\r\n    try {\r\n        rpcProxy.satisfyStoragePolicy(null, req);\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getHAServiceState",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "HAServiceProtocol.HAServiceState getHAServiceState() throws IOException\n{\r\n    HAServiceStateRequestProto req = HAServiceStateRequestProto.newBuilder().build();\r\n    try {\r\n        HAServiceStateProto res = rpcProxy.getHAServiceState(null, req).getState();\r\n        switch(res) {\r\n            case ACTIVE:\r\n                return HAServiceProtocol.HAServiceState.ACTIVE;\r\n            case STANDBY:\r\n                return HAServiceProtocol.HAServiceState.STANDBY;\r\n            case OBSERVER:\r\n                return HAServiceProtocol.HAServiceState.OBSERVER;\r\n            case INITIALIZING:\r\n            default:\r\n                return HAServiceProtocol.HAServiceState.INITIALIZING;\r\n        }\r\n    } catch (ServiceException e) {\r\n        throw ProtobufHelper.getRemoteException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getPartialListing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HdfsFileStatus[] getPartialListing()\n{\r\n    return partialListing;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getRemainingEntries",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRemainingEntries()\n{\r\n    return remainingEntries;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hasMore",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasMore()\n{\r\n    return remainingEntries != 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLastName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getLastName()\n{\r\n    if (partialListing.length == 0) {\r\n        return null;\r\n    }\r\n    return partialListing[partialListing.length - 1].getLocalNameInBytes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "setQuota",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setQuota(Path src, long quota) throws IOException\n{\r\n    dfs.setQuota(src, quota, HdfsConstants.QUOTA_DONT_SET);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "clearQuota",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearQuota(Path src) throws IOException\n{\r\n    dfs.setQuota(src, HdfsConstants.QUOTA_RESET, HdfsConstants.QUOTA_DONT_SET);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "setSpaceQuota",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSpaceQuota(Path src, long spaceQuota) throws IOException\n{\r\n    dfs.setQuota(src, HdfsConstants.QUOTA_DONT_SET, spaceQuota);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "clearSpaceQuota",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearSpaceQuota(Path src) throws IOException\n{\r\n    dfs.setQuota(src, HdfsConstants.QUOTA_DONT_SET, HdfsConstants.QUOTA_RESET);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "setQuotaByStorageType",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setQuotaByStorageType(Path src, StorageType type, long quota) throws IOException\n{\r\n    dfs.setQuotaByStorageType(src, type, quota);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "clearQuotaByStorageType",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearQuotaByStorageType(Path src, StorageType type) throws IOException\n{\r\n    dfs.setQuotaByStorageType(src, type, HdfsConstants.QUOTA_RESET);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "allowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void allowSnapshot(Path path) throws IOException\n{\r\n    dfs.allowSnapshot(path);\r\n    if (dfs.isSnapshotTrashRootEnabled()) {\r\n        dfs.provisionSnapshotTrash(path, TRASH_PERMISSION);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "provisionSnapshotTrash",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path provisionSnapshotTrash(Path path) throws IOException\n{\r\n    return dfs.provisionSnapshotTrash(path, TRASH_PERMISSION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "disallowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void disallowSnapshot(Path path) throws IOException\n{\r\n    dfs.disallowSnapshot(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "addCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long addCacheDirective(CacheDirectiveInfo info, EnumSet<CacheFlag> flags) throws IOException\n{\r\n    return dfs.addCacheDirective(info, flags);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "modifyCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void modifyCacheDirective(CacheDirectiveInfo info, EnumSet<CacheFlag> flags) throws IOException\n{\r\n    dfs.modifyCacheDirective(info, flags);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "removeCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeCacheDirective(long id) throws IOException\n{\r\n    dfs.removeCacheDirective(id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "listCacheDirectives",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<CacheDirectiveEntry> listCacheDirectives(CacheDirectiveInfo filter) throws IOException\n{\r\n    return dfs.listCacheDirectives(filter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "addCachePool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addCachePool(CachePoolInfo info) throws IOException\n{\r\n    dfs.addCachePool(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "modifyCachePool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void modifyCachePool(CachePoolInfo info) throws IOException\n{\r\n    dfs.modifyCachePool(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "removeCachePool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeCachePool(String poolName) throws IOException\n{\r\n    dfs.removeCachePool(poolName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "listCachePools",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<CachePoolEntry> listCachePools() throws IOException\n{\r\n    return dfs.listCachePools();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getKeyProvider",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "KeyProvider getKeyProvider() throws IOException\n{\r\n    return dfs.getClient().getKeyProvider();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "createEncryptionZone",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createEncryptionZone(Path path, String keyName) throws IOException, AccessControlException, FileNotFoundException\n{\r\n    dfs.createEncryptionZone(path, keyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "createEncryptionZone",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createEncryptionZone(Path path, String keyName, EnumSet<CreateEncryptionZoneFlag> flags) throws IOException, AccessControlException, FileNotFoundException, HadoopIllegalArgumentException\n{\r\n    dfs.createEncryptionZone(path, keyName);\r\n    if (flags.contains(CreateEncryptionZoneFlag.PROVISION_TRASH)) {\r\n        if (flags.contains(CreateEncryptionZoneFlag.NO_TRASH)) {\r\n            throw new HadoopIllegalArgumentException(\"can not have both PROVISION_TRASH and NO_TRASH flags\");\r\n        }\r\n        dfs.provisionEZTrash(path, TRASH_PERMISSION);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "provisionEncryptionZoneTrash",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void provisionEncryptionZoneTrash(Path path) throws IOException\n{\r\n    dfs.provisionEZTrash(path, TRASH_PERMISSION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getEncryptionZoneForPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EncryptionZone getEncryptionZoneForPath(Path path) throws IOException, AccessControlException\n{\r\n    return dfs.getEZForPath(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "listEncryptionZones",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<EncryptionZone> listEncryptionZones() throws IOException\n{\r\n    return dfs.listEncryptionZones();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "reencryptEncryptionZone",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void reencryptEncryptionZone(final Path zone, final ReencryptAction action) throws IOException\n{\r\n    dfs.reencryptEncryptionZone(zone, action);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "listReencryptionStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<ZoneReencryptionStatus> listReencryptionStatus() throws IOException\n{\r\n    return dfs.listReencryptionStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getFileEncryptionInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileEncryptionInfo getFileEncryptionInfo(final Path path) throws IOException\n{\r\n    return dfs.getFileEncryptionInfo(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getInotifyEventStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DFSInotifyEventInputStream getInotifyEventStream() throws IOException\n{\r\n    return dfs.getInotifyEventStream();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getInotifyEventStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DFSInotifyEventInputStream getInotifyEventStream(long lastReadTxid) throws IOException\n{\r\n    return dfs.getInotifyEventStream(lastReadTxid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "setStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStoragePolicy(final Path src, final String policyName) throws IOException\n{\r\n    dfs.setStoragePolicy(src, policyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "unsetStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void unsetStoragePolicy(final Path src) throws IOException\n{\r\n    dfs.unsetStoragePolicy(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockStoragePolicySpi getStoragePolicy(final Path src) throws IOException\n{\r\n    return dfs.getStoragePolicy(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getAllStoragePolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<? extends BlockStoragePolicySpi> getAllStoragePolicies() throws IOException\n{\r\n    return dfs.getAllStoragePolicies();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "setErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setErasureCodingPolicy(final Path path, final String ecPolicyName) throws IOException\n{\r\n    dfs.setErasureCodingPolicy(path, ecPolicyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ErasureCodingPolicy getErasureCodingPolicy(final Path path) throws IOException\n{\r\n    return dfs.getErasureCodingPolicy(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "satisfyStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void satisfyStoragePolicy(final Path path) throws IOException\n{\r\n    dfs.satisfyStoragePolicy(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "getErasureCodingPolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ErasureCodingPolicyInfo[] getErasureCodingPolicies() throws IOException\n{\r\n    return dfs.getClient().getErasureCodingPolicies();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "unsetErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void unsetErasureCodingPolicy(final Path path) throws IOException\n{\r\n    dfs.unsetErasureCodingPolicy(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "addErasureCodingPolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AddErasureCodingPolicyResponse[] addErasureCodingPolicies(ErasureCodingPolicy[] policies) throws IOException\n{\r\n    return dfs.addErasureCodingPolicies(policies);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "removeErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    dfs.removeErasureCodingPolicy(ecPolicyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "enableErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void enableErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    dfs.enableErasureCodingPolicy(ecPolicyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "disableErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void disableErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    dfs.disableErasureCodingPolicy(ecPolicyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<OpenFileEntry> listOpenFiles() throws IOException\n{\r\n    return dfs.listOpenFiles();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<OpenFileEntry> listOpenFiles(EnumSet<OpenFilesType> openFilesTypes) throws IOException\n{\r\n    return dfs.listOpenFiles(openFilesTypes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<OpenFileEntry> listOpenFiles(EnumSet<OpenFilesType> openFilesTypes, String path) throws IOException\n{\r\n    return dfs.listOpenFiles(openFilesTypes, path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "resetMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void resetMetrics()\n{\r\n    filesReencrypted = 0;\r\n    numReencryptionFailures = 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getId()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getZoneName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getZoneName()\n{\r\n    return zoneName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setState(final State s)\n{\r\n    state = s;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "State getState()\n{\r\n    return state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getEzKeyVersionName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getEzKeyVersionName()\n{\r\n    return ezKeyVersionName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getSubmissionTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSubmissionTime()\n{\r\n    return submissionTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getCompletionTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCompletionTime()\n{\r\n    return completionTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isCanceled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isCanceled()\n{\r\n    return canceled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLastCheckpointFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLastCheckpointFile()\n{\r\n    return lastCheckpointFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFilesReencrypted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFilesReencrypted()\n{\r\n    return filesReencrypted;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getNumReencryptionFailures",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumReencryptionFailures()\n{\r\n    return numReencryptionFailures;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void reset()\n{\r\n    state = State.Submitted;\r\n    ezKeyVersionName = null;\r\n    submissionTime = 0;\r\n    completionTime = 0;\r\n    canceled = false;\r\n    lastCheckpointFile = null;\r\n    resetMetrics();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setZoneName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setZoneName(final String name)\n{\r\n    Preconditions.checkNotNull(name, \"zone name cannot be null\");\r\n    zoneName = name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "cancel",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void cancel()\n{\r\n    canceled = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "markZoneCompleted",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void markZoneCompleted(final ReencryptionInfoProto proto)\n{\r\n    state = ZoneReencryptionStatus.State.Completed;\r\n    completionTime = proto.getCompletionTime();\r\n    lastCheckpointFile = null;\r\n    canceled = proto.getCanceled();\r\n    filesReencrypted = proto.getNumReencrypted();\r\n    numReencryptionFailures = proto.getNumFailures();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "markZoneSubmitted",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void markZoneSubmitted(final ReencryptionInfoProto proto)\n{\r\n    reset();\r\n    state = ZoneReencryptionStatus.State.Submitted;\r\n    ezKeyVersionName = proto.getEzKeyVersionName();\r\n    submissionTime = proto.getSubmissionTime();\r\n    filesReencrypted = proto.getNumReencrypted();\r\n    numReencryptionFailures = proto.getNumFailures();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "updateZoneProcess",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void updateZoneProcess(final ReencryptionInfoProto proto)\n{\r\n    lastCheckpointFile = proto.getLastFile();\r\n    filesReencrypted = proto.getNumReencrypted();\r\n    numReencryptionFailures = proto.getNumFailures();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setOwner",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOwner(String owner)\n{\r\n    super.setOwner(owner);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setGroup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setGroup(String group)\n{\r\n    super.setOwner(group);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isSymlink",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSymlink()\n{\r\n    return uSymlink != null && uSymlink.length > 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getSymlink",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path getSymlink() throws IOException\n{\r\n    if (isSymlink()) {\r\n        return new Path(DFSUtilClient.bytes2String(getSymlinkInBytes()));\r\n    }\r\n    throw new IOException(\"Path \" + getPath() + \" is not a symbolic link\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setPermission",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setPermission(FsPermission permission)\n{\r\n    super.setPermission(permission);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLocalNameInBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getLocalNameInBytes()\n{\r\n    return uPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setSymlink",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSymlink(Path sym)\n{\r\n    uSymlink = DFSUtilClient.string2Bytes(sym.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getSymlinkInBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getSymlinkInBytes()\n{\r\n    return uSymlink;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFileId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFileId()\n{\r\n    return fileId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFileEncryptionInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileEncryptionInfo getFileEncryptionInfo()\n{\r\n    return feInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ErasureCodingPolicy getErasureCodingPolicy()\n{\r\n    return ecPolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getChildrenNum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getChildrenNum()\n{\r\n    return childrenNum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte getStoragePolicy()\n{\r\n    return storagePolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    return super.equals(o);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return super.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLocatedBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LocatedBlocks getLocatedBlocks()\n{\r\n    return hdfsloc;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "makeQualifiedLocated",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "LocatedFileStatus makeQualifiedLocated(URI defaultUri, Path path)\n{\r\n    makeQualified(defaultUri, path);\r\n    setBlockLocations(DFSUtilClient.locatedBlocks2Locations(getLocatedBlocks()));\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBytesInFutureBlockGroups",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesInFutureBlockGroups()\n{\r\n    return bytesInFutureBlockGroups;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getCorruptBlockGroups",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCorruptBlockGroups()\n{\r\n    return corruptBlockGroups;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLowRedundancyBlockGroups",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLowRedundancyBlockGroups()\n{\r\n    return lowRedundancyBlockGroups;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getMissingBlockGroups",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMissingBlockGroups()\n{\r\n    return missingBlockGroups;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getPendingDeletionBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPendingDeletionBlocks()\n{\r\n    return pendingDeletionBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hasHighestPriorityLowRedundancyBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasHighestPriorityLowRedundancyBlocks()\n{\r\n    return getHighestPriorityLowRedundancyBlocks() != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getHighestPriorityLowRedundancyBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Long getHighestPriorityLowRedundancyBlocks()\n{\r\n    return highestPriorityLowRedundancyBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder statsBuilder = new StringBuilder();\r\n    statsBuilder.append(\"ECBlockGroupStats=[\").append(\"LowRedundancyBlockGroups=\").append(getLowRedundancyBlockGroups()).append(\", CorruptBlockGroups=\").append(getCorruptBlockGroups()).append(\", MissingBlockGroups=\").append(getMissingBlockGroups()).append(\", BytesInFutureBlockGroups=\").append(getBytesInFutureBlockGroups()).append(\", PendingDeletionBlocks=\").append(getPendingDeletionBlocks());\r\n    if (hasHighestPriorityLowRedundancyBlocks()) {\r\n        statsBuilder.append(\", HighestPriorityLowRedundancyBlocks=\").append(getHighestPriorityLowRedundancyBlocks());\r\n    }\r\n    statsBuilder.append(\"]\");\r\n    return statsBuilder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return new HashCodeBuilder().append(lowRedundancyBlockGroups).append(corruptBlockGroups).append(missingBlockGroups).append(bytesInFutureBlockGroups).append(pendingDeletionBlocks).append(highestPriorityLowRedundancyBlocks).toHashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (this == o) {\r\n        return true;\r\n    }\r\n    if (o == null || getClass() != o.getClass()) {\r\n        return false;\r\n    }\r\n    ECBlockGroupStats other = (ECBlockGroupStats) o;\r\n    return new EqualsBuilder().append(lowRedundancyBlockGroups, other.lowRedundancyBlockGroups).append(corruptBlockGroups, other.corruptBlockGroups).append(missingBlockGroups, other.missingBlockGroups).append(bytesInFutureBlockGroups, other.bytesInFutureBlockGroups).append(pendingDeletionBlocks, other.pendingDeletionBlocks).append(highestPriorityLowRedundancyBlocks, other.highestPriorityLowRedundancyBlocks).isEquals();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "merge",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "ECBlockGroupStats merge(Collection<ECBlockGroupStats> stats)\n{\r\n    long lowRedundancyBlockGroups = 0;\r\n    long corruptBlockGroups = 0;\r\n    long missingBlockGroups = 0;\r\n    long bytesInFutureBlockGroups = 0;\r\n    long pendingDeletionBlocks = 0;\r\n    long highestPriorityLowRedundancyBlocks = 0;\r\n    boolean hasHighestPriorityLowRedundancyBlocks = false;\r\n    for (ECBlockGroupStats stat : stats) {\r\n        lowRedundancyBlockGroups += stat.getLowRedundancyBlockGroups();\r\n        corruptBlockGroups += stat.getCorruptBlockGroups();\r\n        missingBlockGroups += stat.getMissingBlockGroups();\r\n        bytesInFutureBlockGroups += stat.getBytesInFutureBlockGroups();\r\n        pendingDeletionBlocks += stat.getPendingDeletionBlocks();\r\n        if (stat.hasHighestPriorityLowRedundancyBlocks()) {\r\n            hasHighestPriorityLowRedundancyBlocks = true;\r\n            highestPriorityLowRedundancyBlocks += stat.getHighestPriorityLowRedundancyBlocks();\r\n        }\r\n    }\r\n    if (hasHighestPriorityLowRedundancyBlocks) {\r\n        return new ECBlockGroupStats(lowRedundancyBlockGroups, corruptBlockGroups, missingBlockGroups, bytesInFutureBlockGroups, pendingDeletionBlocks, highestPriorityLowRedundancyBlocks);\r\n    }\r\n    return new ECBlockGroupStats(lowRedundancyBlockGroups, corruptBlockGroups, missingBlockGroups, bytesInFutureBlockGroups, pendingDeletionBlocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getSnapshotNumber",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getSnapshotNumber()\n{\r\n    return snapshotNumber;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getSnapshotQuota",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getSnapshotQuota()\n{\r\n    return snapshotQuota;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getParentFullPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getParentFullPath()\n{\r\n    return parentFullPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getDirStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HdfsFileStatus getDirStatus()\n{\r\n    return dirStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFullPath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path getFullPath()\n{\r\n    String parentFullPathStr = (parentFullPath == null || parentFullPath.length == 0) ? null : DFSUtilClient.bytes2String(parentFullPath);\r\n    if (parentFullPathStr == null && dirStatus.getLocalNameInBytes().length == 0) {\r\n        return new Path(\"/\");\r\n    } else {\r\n        return parentFullPathStr == null ? new Path(dirStatus.getLocalName()) : new Path(parentFullPathStr, dirStatus.getLocalName());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "print",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void print(SnapshottableDirectoryStatus[] stats, PrintStream out)\n{\r\n    if (stats == null || stats.length == 0) {\r\n        out.println();\r\n        return;\r\n    }\r\n    int maxRepl = 0, maxLen = 0, maxOwner = 0, maxGroup = 0;\r\n    int maxSnapshotNum = 0, maxSnapshotQuota = 0;\r\n    for (SnapshottableDirectoryStatus status : stats) {\r\n        maxRepl = maxLength(maxRepl, status.dirStatus.getReplication());\r\n        maxLen = maxLength(maxLen, status.dirStatus.getLen());\r\n        maxOwner = maxLength(maxOwner, status.dirStatus.getOwner());\r\n        maxGroup = maxLength(maxGroup, status.dirStatus.getGroup());\r\n        maxSnapshotNum = maxLength(maxSnapshotNum, status.snapshotNumber);\r\n        maxSnapshotQuota = maxLength(maxSnapshotQuota, status.snapshotQuota);\r\n    }\r\n    String lineFormat = \"%s%s \" + \"%\" + maxRepl + \"s \" + (maxOwner > 0 ? \"%-\" + maxOwner + \"s \" : \"%s\") + (maxGroup > 0 ? \"%-\" + maxGroup + \"s \" : \"%s\") + \"%\" + maxLen + \"s \" + \"%s \" + \"%\" + maxSnapshotNum + \"s \" + \"%\" + maxSnapshotQuota + \"s \" + \"%s\";\r\n    SimpleDateFormat dateFormat = new SimpleDateFormat(\"yyyy-MM-dd HH:mm\");\r\n    for (SnapshottableDirectoryStatus status : stats) {\r\n        String line = String.format(lineFormat, \"d\", status.dirStatus.getPermission(), status.dirStatus.getReplication(), status.dirStatus.getOwner(), status.dirStatus.getGroup(), String.valueOf(status.dirStatus.getLen()), dateFormat.format(new Date(status.dirStatus.getModificationTime())), status.snapshotNumber, status.snapshotQuota, status.getFullPath().toString());\r\n        out.println(line);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "maxLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int maxLength(int n, Object value)\n{\r\n    return Math.max(n, String.valueOf(value).length());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "chooseStorageTypes",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<StorageType> chooseStorageTypes(final short replication)\n{\r\n    final List<StorageType> types = new LinkedList<>();\r\n    int i = 0, j = 0;\r\n    for (; i < replication && j < storageTypes.length; ++j) {\r\n        if (!storageTypes[j].isTransient()) {\r\n            types.add(storageTypes[j]);\r\n            ++i;\r\n        }\r\n    }\r\n    final StorageType last = storageTypes[storageTypes.length - 1];\r\n    if (!last.isTransient()) {\r\n        for (; i < replication; i++) {\r\n            types.add(last);\r\n        }\r\n    }\r\n    return types;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "chooseStorageTypes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<StorageType> chooseStorageTypes(final short replication, final Iterable<StorageType> chosen)\n{\r\n    return chooseStorageTypes(replication, chosen, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "chooseStorageTypes",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<StorageType> chooseStorageTypes(final short replication, final Iterable<StorageType> chosen, final List<StorageType> excess)\n{\r\n    final List<StorageType> types = chooseStorageTypes(replication);\r\n    diff(types, chosen, excess);\r\n    return types;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "chooseStorageTypes",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "List<StorageType> chooseStorageTypes(final short replication, final Iterable<StorageType> chosen, final EnumSet<StorageType> unavailables, final boolean isNewBlock)\n{\r\n    final List<StorageType> excess = new LinkedList<>();\r\n    final List<StorageType> storageTypes = chooseStorageTypes(replication, chosen, excess);\r\n    final int expectedSize = storageTypes.size() - excess.size();\r\n    final List<StorageType> removed = new LinkedList<>();\r\n    for (int i = storageTypes.size() - 1; i >= 0; i--) {\r\n        final StorageType t = storageTypes.get(i);\r\n        if (unavailables.contains(t)) {\r\n            final StorageType fallback = isNewBlock ? getCreationFallback(unavailables) : getReplicationFallback(unavailables);\r\n            if (fallback == null) {\r\n                removed.add(storageTypes.remove(i));\r\n            } else {\r\n                storageTypes.set(i, fallback);\r\n            }\r\n        }\r\n    }\r\n    diff(storageTypes, excess, null);\r\n    if (storageTypes.size() < expectedSize) {\r\n        LOG.warn(\"Failed to place enough replicas: expected size is {}\" + \" but only {} storage types can be selected (replication={},\" + \" selected={}, unavailable={}\" + \", removed={}\" + \", policy={}\" + \")\", expectedSize, storageTypes.size(), replication, storageTypes, unavailables, removed, this);\r\n    }\r\n    return storageTypes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "diff",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void diff(List<StorageType> t, Iterable<StorageType> c, List<StorageType> e)\n{\r\n    for (StorageType storagetype : c) {\r\n        final int i = t.indexOf(storagetype);\r\n        if (i >= 0) {\r\n            t.remove(i);\r\n        } else if (e != null) {\r\n            e.add(storagetype);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "chooseExcess",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<StorageType> chooseExcess(final short replication, final Iterable<StorageType> chosen)\n{\r\n    final List<StorageType> types = chooseStorageTypes(replication);\r\n    final List<StorageType> excess = new LinkedList<>();\r\n    diff(types, chosen, excess);\r\n    return excess;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getCreationFallback",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StorageType getCreationFallback(EnumSet<StorageType> unavailables)\n{\r\n    return getFallback(unavailables, creationFallbacks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getReplicationFallback",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StorageType getReplicationFallback(EnumSet<StorageType> unavailables)\n{\r\n    return getFallback(unavailables, replicationFallbacks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return Byte.valueOf(id).hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (obj == this) {\r\n        return true;\r\n    } else if (!(obj instanceof BlockStoragePolicy)) {\r\n        return false;\r\n    }\r\n    final BlockStoragePolicy that = (BlockStoragePolicy) obj;\r\n    return this.id == that.id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String toString()\n{\r\n    return getClass().getSimpleName() + \"{\" + name + \":\" + id + \", storageTypes=\" + Arrays.asList(storageTypes) + \", creationFallbacks=\" + Arrays.asList(creationFallbacks) + \", replicationFallbacks=\" + Arrays.asList(replicationFallbacks) + \"}\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte getId()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getStorageTypes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StorageType[] getStorageTypes()\n{\r\n    return this.storageTypes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getCreationFallbacks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StorageType[] getCreationFallbacks()\n{\r\n    return this.creationFallbacks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getReplicationFallbacks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StorageType[] getReplicationFallbacks()\n{\r\n    return this.replicationFallbacks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFallback",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StorageType getFallback(EnumSet<StorageType> unavailables, StorageType[] fallbacks)\n{\r\n    for (StorageType fb : fallbacks) {\r\n        if (!unavailables.contains(fb)) {\r\n            return fb;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isCopyOnCreateFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isCopyOnCreateFile()\n{\r\n    return copyOnCreateFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getReadBuffer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ByteBuffer getReadBuffer()\n{\r\n    return ByteBuffer.wrap(readBuf, offset, targetLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getTargetLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTargetLength()\n{\r\n    return targetLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readFromBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int readFromBlock(BlockReader blockReader) throws IOException\n{\r\n    return readFromBlock(blockReader, targetLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readFromBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int readFromBlock(BlockReader blockReader, int length) throws IOException\n{\r\n    int nRead = blockReader.read(readBuf, offset, length);\r\n    if (nRead > 0) {\r\n        offset += nRead;\r\n    }\r\n    return nRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readFromBuffer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int readFromBuffer(ByteBuffer src)\n{\r\n    return readFromBuffer(src, src.remaining());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readFromBuffer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int readFromBuffer(ByteBuffer src, int length)\n{\r\n    ByteBuffer dup = src.duplicate();\r\n    dup.get(readBuf, offset, length);\r\n    offset += length;\r\n    return length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getReadBuffer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ByteBuffer getReadBuffer()\n{\r\n    return readBuf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readFromBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int readFromBlock(BlockReader blockReader) throws IOException\n{\r\n    return readFromBlock(blockReader, readBuf.remaining());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readFromBlock",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int readFromBlock(BlockReader blockReader, int length) throws IOException\n{\r\n    ByteBuffer tmpBuf = readBuf.duplicate();\r\n    tmpBuf.limit(tmpBuf.position() + length);\r\n    int nRead = blockReader.read(tmpBuf);\r\n    if (nRead > 0) {\r\n        readBuf.position(readBuf.position() + nRead);\r\n    }\r\n    return nRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getTargetLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTargetLength()\n{\r\n    return targetLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readFromBuffer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int readFromBuffer(ByteBuffer src)\n{\r\n    return readFromBuffer(src, src.remaining());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "readFromBuffer",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int readFromBuffer(ByteBuffer src, int length)\n{\r\n    ByteBuffer dup = src.duplicate();\r\n    int newLen = Math.min(readBuf.remaining(), dup.remaining());\r\n    newLen = Math.min(newLen, length);\r\n    dup.limit(dup.position() + newLen);\r\n    readBuf.put(dup);\r\n    return newLen;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "toSortedString",
  "errType" : [ "UnsupportedEncodingException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String toSortedString(final String separator, final Param<?, ?>... parameters)\n{\r\n    Arrays.sort(parameters, NAME_CMP);\r\n    final StringBuilder b = new StringBuilder();\r\n    try {\r\n        for (Param<?, ?> p : parameters) {\r\n            if (p.getValue() != null) {\r\n                b.append(separator).append(URLEncoder.encode(p.getName(), \"UTF-8\")).append(\"=\").append(URLEncoder.encode(p.getValueString(), \"UTF-8\"));\r\n            }\r\n        }\r\n    } catch (UnsupportedEncodingException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n    return b.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T getValue()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getValueString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getValueString()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return getName() + \"=\" + value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "useLogicalURI",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean useLogicalURI()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "cloneDelegationTokenForVirtualIP",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void cloneDelegationTokenForVirtualIP(Configuration conf, URI haURI)\n{\r\n    URI virtualIPURI = getFailoverVirtualIP(conf, haURI.getHost());\r\n    InetSocketAddress vipAddress = new InetSocketAddress(virtualIPURI.getHost(), virtualIPURI.getPort());\r\n    HAUtilClient.cloneDelegationTokenForLogicalUri(ugi, haURI, Collections.singleton(vipAddress));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getFailoverVirtualIP",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "URI getFailoverVirtualIP(Configuration conf, String nameServiceID)\n{\r\n    String configKey = IPFAILOVER_CONFIG_PREFIX + \".\" + nameServiceID;\r\n    String virtualIP = conf.get(configKey);\r\n    LOG.info(\"Name service ID {} will use virtual IP {} for failover\", nameServiceID, virtualIP);\r\n    if (virtualIP == null) {\r\n        throw new IllegalArgumentException(\"Virtual IP for failover not found,\" + \"misconfigured \" + configKey + \"?\");\r\n    }\r\n    return URI.create(virtualIP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getDataLen",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getDataLen()\n{\r\n    return proto.getDataLen();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "isLastPacketInBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isLastPacketInBlock()\n{\r\n    return proto.getLastPacketInBlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getSeqno",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getSeqno()\n{\r\n    return proto.getSeqno();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getOffsetInBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getOffsetInBlock()\n{\r\n    return proto.getOffsetInBlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getPacketLen",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getPacketLen()\n{\r\n    return packetLen;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getSyncBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getSyncBlock()\n{\r\n    return proto.getSyncBlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"PacketHeader with packetLen=\" + packetLen + \" header data: \" + proto.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "setFieldsFromData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFieldsFromData(int packetLen, byte[] headerData) throws InvalidProtocolBufferException\n{\r\n    this.packetLen = packetLen;\r\n    proto = PacketHeaderProto.parseFrom(headerData);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void readFields(ByteBuffer buf) throws IOException\n{\r\n    packetLen = buf.getInt();\r\n    short protoLen = buf.getShort();\r\n    byte[] data = new byte[protoLen];\r\n    buf.get(data);\r\n    proto = PacketHeaderProto.parseFrom(data);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void readFields(DataInputStream in) throws IOException\n{\r\n    this.packetLen = in.readInt();\r\n    short protoLen = in.readShort();\r\n    byte[] data = new byte[protoLen];\r\n    in.readFully(data);\r\n    proto = PacketHeaderProto.parseFrom(data);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getSerializedSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getSerializedSize()\n{\r\n    return PKT_LENGTHS_LEN + proto.getSerializedSize();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "putInBuffer",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void putInBuffer(final ByteBuffer buf)\n{\r\n    assert proto.getSerializedSize() <= MAX_PROTO_SIZE : \"Expected \" + (MAX_PROTO_SIZE) + \" got: \" + proto.getSerializedSize();\r\n    try {\r\n        buf.putInt(packetLen);\r\n        buf.putShort((short) proto.getSerializedSize());\r\n        proto.writeTo(new ByteBufferOutputStream(buf));\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void write(DataOutputStream out) throws IOException\n{\r\n    assert proto.getSerializedSize() <= MAX_PROTO_SIZE : \"Expected \" + (MAX_PROTO_SIZE) + \" got: \" + proto.getSerializedSize();\r\n    out.writeInt(packetLen);\r\n    out.writeShort(proto.getSerializedSize());\r\n    proto.writeTo(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "getBytes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "byte[] getBytes()\n{\r\n    ByteBuffer buf = ByteBuffer.allocate(getSerializedSize());\r\n    putInBuffer(buf);\r\n    return buf.array();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "sanityCheck",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean sanityCheck(long lastSeqNo)\n{\r\n    if (proto.getDataLen() <= 0 && !proto.getLastPacketInBlock())\r\n        return false;\r\n    if (proto.getLastPacketInBlock() && proto.getDataLen() != 0)\r\n        return false;\r\n    return proto.getSeqno() == lastSeqNo + 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (!(o instanceof PacketHeader))\r\n        return false;\r\n    PacketHeader other = (PacketHeader) o;\r\n    return this.proto.equals(other.proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol\\datatransfer",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return (int) proto.getSeqno();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String toString()\n{\r\n    return getClass().getSimpleName() + \"{\" + getBlock() + \"; getBlockSize()=\" + getBlockSize() + \"; corrupt=\" + isCorrupt() + \"; offset=\" + getStartOffset() + \"; locs=\" + Arrays.asList(getLocations()) + \"; indices=\" + Arrays.toString(blockIndices) + \"}\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockIndices",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getBlockIndices()\n{\r\n    return this.blockIndices;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isStriped",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isStriped()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockType getBlockType()\n{\r\n    return BlockType.STRIPED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockTokens",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Token<BlockTokenIdentifier>[] getBlockTokens()\n{\r\n    return blockTokens;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setBlockTokens",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setBlockTokens(Token<BlockTokenIdentifier>[] tokens)\n{\r\n    this.blockTokens = tokens;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getValueString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getValueString()\n{\r\n    return value.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setOwner",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOwner(String owner)\n{\r\n    super.setOwner(owner);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setGroup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setGroup(String group)\n{\r\n    super.setOwner(group);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isSymlink",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSymlink()\n{\r\n    return uSymlink != null && uSymlink.length > 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getSymlink",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path getSymlink() throws IOException\n{\r\n    if (isSymlink()) {\r\n        return new Path(DFSUtilClient.bytes2String(getSymlinkInBytes()));\r\n    }\r\n    throw new IOException(\"Path \" + getPath() + \" is not a symbolic link\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setPermission",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setPermission(FsPermission permission)\n{\r\n    super.setPermission(permission);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLocalNameInBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getLocalNameInBytes()\n{\r\n    return uPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setSymlink",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSymlink(Path sym)\n{\r\n    uSymlink = DFSUtilClient.string2Bytes(sym.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getSymlinkInBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getSymlinkInBytes()\n{\r\n    return uSymlink;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFileId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFileId()\n{\r\n    return fileId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getFileEncryptionInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileEncryptionInfo getFileEncryptionInfo()\n{\r\n    return feInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ErasureCodingPolicy getErasureCodingPolicy()\n{\r\n    return ecPolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getChildrenNum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getChildrenNum()\n{\r\n    return childrenNum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte getStoragePolicy()\n{\r\n    return storagePolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    return super.equals(o);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return super.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "checkRange",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkRange(final Integer min, final Integer max)\n{\r\n    if (value == null) {\r\n        return;\r\n    }\r\n    if (min != null && value < min) {\r\n        throw new IllegalArgumentException(\"Invalid parameter range: \" + getName() + \" = \" + domain.toString(value) + \" < \" + domain.toString(min));\r\n    }\r\n    if (max != null && value > max) {\r\n        throw new IllegalArgumentException(\"Invalid parameter range: \" + getName() + \" = \" + domain.toString(value) + \" > \" + domain.toString(max));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return getName() + \"=\" + domain.toString(getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getValueString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getValueString()\n{\r\n    return domain.toString(getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "createDataBufIfNeeded",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createDataBufIfNeeded()\n{\r\n    if (dataBuf == null) {\r\n        dataBuf = bufferPool.getBuffer(maxAllocatedChunks * bytesPerChecksum);\r\n        dataBuf.position(0);\r\n        dataBuf.limit(0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "freeDataBufIfExists",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void freeDataBufIfExists()\n{\r\n    if (dataBuf != null) {\r\n        dataPos -= dataBuf.remaining();\r\n        dataBuf.clear();\r\n        bufferPool.returnBuffer(dataBuf);\r\n        dataBuf = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "createChecksumBufIfNeeded",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createChecksumBufIfNeeded()\n{\r\n    if (checksumBuf == null) {\r\n        checksumBuf = bufferPool.getBuffer(maxAllocatedChunks * checksumSize);\r\n        checksumBuf.position(0);\r\n        checksumBuf.limit(0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "freeChecksumBufIfExists",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void freeChecksumBufIfExists()\n{\r\n    if (checksumBuf != null) {\r\n        checksumBuf.clear();\r\n        bufferPool.returnBuffer(checksumBuf);\r\n        checksumBuf = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "drainDataBuf",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int drainDataBuf(ByteBuffer buf)\n{\r\n    if (dataBuf == null)\r\n        return -1;\r\n    int oldLimit = dataBuf.limit();\r\n    int nRead = Math.min(dataBuf.remaining(), buf.remaining());\r\n    if (nRead == 0) {\r\n        return (dataBuf.remaining() == 0) ? -1 : 0;\r\n    }\r\n    try {\r\n        dataBuf.limit(dataBuf.position() + nRead);\r\n        buf.put(dataBuf);\r\n    } finally {\r\n        dataBuf.limit(oldLimit);\r\n    }\r\n    return nRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "fillBuffer",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "int fillBuffer(ByteBuffer buf, boolean canSkipChecksum) throws IOException\n{\r\n    int total = 0;\r\n    long startDataPos = dataPos;\r\n    int startBufPos = buf.position();\r\n    while (buf.hasRemaining()) {\r\n        int nRead = blockReaderIoProvider.read(dataIn, buf, dataPos);\r\n        if (nRead < 0) {\r\n            break;\r\n        }\r\n        dataPos += nRead;\r\n        total += nRead;\r\n    }\r\n    if (canSkipChecksum) {\r\n        freeChecksumBufIfExists();\r\n        return total;\r\n    }\r\n    if (total > 0) {\r\n        try {\r\n            buf.limit(buf.position());\r\n            buf.position(startBufPos);\r\n            createChecksumBufIfNeeded();\r\n            int checksumsNeeded = (total + bytesPerChecksum - 1) / bytesPerChecksum;\r\n            checksumBuf.clear();\r\n            checksumBuf.limit(checksumsNeeded * checksumSize);\r\n            long checksumPos = BlockMetadataHeader.getHeaderSize() + ((startDataPos / bytesPerChecksum) * checksumSize);\r\n            while (checksumBuf.hasRemaining()) {\r\n                int nRead = checksumIn.read(checksumBuf, checksumPos);\r\n                if (nRead < 0) {\r\n                    throw new IOException(\"Got unexpected checksum file EOF at \" + checksumPos + \", block file position \" + startDataPos + \" for block \" + block + \" of file \" + filename);\r\n                }\r\n                checksumPos += nRead;\r\n            }\r\n            checksumBuf.flip();\r\n            checksum.verifyChunkedSums(buf, checksumBuf, filename, startDataPos);\r\n        } finally {\r\n            buf.position(buf.limit());\r\n        }\r\n    }\r\n    return total;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "createNoChecksumContext",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean createNoChecksumContext()\n{\r\n    return !verifyChecksum || (storageType != null && storageType.isTransient()) || replica.addNoChecksumAnchor();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "releaseNoChecksumContext",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void releaseNoChecksumContext()\n{\r\n    if (verifyChecksum) {\r\n        if (storageType == null || !storageType.isTransient()) {\r\n            replica.removeNoChecksumAnchor();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "read",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int read(ByteBuffer buf) throws IOException\n{\r\n    boolean canSkipChecksum = createNoChecksumContext();\r\n    try {\r\n        String traceFormatStr = \"read(buf.remaining={}, block={}, filename={}, \" + \"canSkipChecksum={})\";\r\n        LOG.trace(traceFormatStr + \": starting\", buf.remaining(), block, filename, canSkipChecksum);\r\n        int nRead;\r\n        try {\r\n            if (canSkipChecksum && zeroReadaheadRequested) {\r\n                nRead = readWithoutBounceBuffer(buf);\r\n            } else {\r\n                nRead = readWithBounceBuffer(buf, canSkipChecksum);\r\n            }\r\n        } catch (IOException e) {\r\n            LOG.trace(traceFormatStr + \": I/O error\", buf.remaining(), block, filename, canSkipChecksum, e);\r\n            throw e;\r\n        }\r\n        LOG.trace(traceFormatStr + \": returning {}\", buf.remaining(), block, filename, canSkipChecksum, nRead);\r\n        return nRead;\r\n    } finally {\r\n        if (canSkipChecksum)\r\n            releaseNoChecksumContext();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "readWithoutBounceBuffer",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int readWithoutBounceBuffer(ByteBuffer buf) throws IOException\n{\r\n    freeDataBufIfExists();\r\n    freeChecksumBufIfExists();\r\n    int total = 0;\r\n    while (buf.hasRemaining()) {\r\n        int nRead = blockReaderIoProvider.read(dataIn, buf, dataPos);\r\n        if (nRead <= 0)\r\n            break;\r\n        dataPos += nRead;\r\n        total += nRead;\r\n    }\r\n    return (total == 0 && (dataPos == dataIn.size())) ? -1 : total;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "fillDataBuf",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "boolean fillDataBuf(boolean canSkipChecksum) throws IOException\n{\r\n    createDataBufIfNeeded();\r\n    final int slop = (int) (dataPos % bytesPerChecksum);\r\n    final long oldDataPos = dataPos;\r\n    dataBuf.limit(maxReadaheadLength);\r\n    if (canSkipChecksum) {\r\n        dataBuf.position(slop);\r\n        fillBuffer(dataBuf, true);\r\n    } else {\r\n        dataPos -= slop;\r\n        dataBuf.position(0);\r\n        fillBuffer(dataBuf, false);\r\n    }\r\n    dataBuf.limit(dataBuf.position());\r\n    dataBuf.position(Math.min(dataBuf.position(), slop));\r\n    LOG.trace(\"loaded {} bytes into bounce buffer from offset {} of {}\", dataBuf.remaining(), oldDataPos, block);\r\n    return dataBuf.limit() != maxReadaheadLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "readWithBounceBuffer",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "int readWithBounceBuffer(ByteBuffer buf, boolean canSkipChecksum) throws IOException\n{\r\n    int total = 0;\r\n    int bb = drainDataBuf(buf);\r\n    if (bb >= 0) {\r\n        total += bb;\r\n        if (buf.remaining() == 0)\r\n            return total;\r\n    }\r\n    boolean eof = true, done = false;\r\n    do {\r\n        if (buf.isDirect() && (buf.remaining() >= maxReadaheadLength) && ((dataPos % bytesPerChecksum) == 0)) {\r\n            int oldLimit = buf.limit();\r\n            int nRead;\r\n            try {\r\n                buf.limit(buf.position() + maxReadaheadLength);\r\n                nRead = fillBuffer(buf, canSkipChecksum);\r\n            } finally {\r\n                buf.limit(oldLimit);\r\n            }\r\n            if (nRead < maxReadaheadLength) {\r\n                done = true;\r\n            }\r\n            if (nRead > 0) {\r\n                eof = false;\r\n            }\r\n            total += nRead;\r\n        } else {\r\n            if (fillDataBuf(canSkipChecksum)) {\r\n                done = true;\r\n            }\r\n            bb = drainDataBuf(buf);\r\n            if (bb >= 0) {\r\n                eof = false;\r\n                total += bb;\r\n            }\r\n        }\r\n    } while ((!done) && (buf.remaining() > 0));\r\n    return (eof && total == 0) ? -1 : total;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "read",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int read(byte[] arr, int off, int len) throws IOException\n{\r\n    boolean canSkipChecksum = createNoChecksumContext();\r\n    int nRead;\r\n    try {\r\n        final String traceFormatStr = \"read(arr.length={}, off={}, len={}, \" + \"filename={}, block={}, canSkipChecksum={})\";\r\n        LOG.trace(traceFormatStr + \": starting\", arr.length, off, len, filename, block, canSkipChecksum);\r\n        try {\r\n            if (canSkipChecksum && zeroReadaheadRequested) {\r\n                nRead = readWithoutBounceBuffer(arr, off, len);\r\n            } else {\r\n                nRead = readWithBounceBuffer(arr, off, len, canSkipChecksum);\r\n            }\r\n        } catch (IOException e) {\r\n            LOG.trace(traceFormatStr + \": I/O error\", arr.length, off, len, filename, block, canSkipChecksum, e);\r\n            throw e;\r\n        }\r\n        LOG.trace(traceFormatStr + \": returning {}\", arr.length, off, len, filename, block, canSkipChecksum, nRead);\r\n    } finally {\r\n        if (canSkipChecksum)\r\n            releaseNoChecksumContext();\r\n    }\r\n    return nRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "readWithoutBounceBuffer",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int readWithoutBounceBuffer(byte[] arr, int off, int len) throws IOException\n{\r\n    freeDataBufIfExists();\r\n    freeChecksumBufIfExists();\r\n    int nRead = blockReaderIoProvider.read(dataIn, ByteBuffer.wrap(arr, off, len), dataPos);\r\n    if (nRead > 0) {\r\n        dataPos += nRead;\r\n    } else if ((nRead == 0) && (dataPos == dataIn.size())) {\r\n        return -1;\r\n    }\r\n    return nRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "readWithBounceBuffer",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "int readWithBounceBuffer(byte[] arr, int off, int len, boolean canSkipChecksum) throws IOException\n{\r\n    createDataBufIfNeeded();\r\n    if (!dataBuf.hasRemaining()) {\r\n        dataBuf.position(0);\r\n        dataBuf.limit(maxReadaheadLength);\r\n        fillDataBuf(canSkipChecksum);\r\n    }\r\n    if (dataBuf.remaining() == 0)\r\n        return -1;\r\n    int toRead = Math.min(dataBuf.remaining(), len);\r\n    dataBuf.get(arr, off, toRead);\r\n    return toRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "skip",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long skip(long n) throws IOException\n{\r\n    int discardedFromBuf = 0;\r\n    long remaining = n;\r\n    if ((dataBuf != null) && dataBuf.hasRemaining()) {\r\n        discardedFromBuf = (int) Math.min(dataBuf.remaining(), n);\r\n        dataBuf.position(dataBuf.position() + discardedFromBuf);\r\n        remaining -= discardedFromBuf;\r\n    }\r\n    LOG.trace(\"skip(n={}, block={}, filename={}): discarded {} bytes from \" + \"dataBuf and advanced dataPos by {}\", n, block, filename, discardedFromBuf, remaining);\r\n    dataPos += remaining;\r\n    return n;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "available",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int available()\n{\r\n    return Integer.MAX_VALUE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (closed)\r\n        return;\r\n    closed = true;\r\n    LOG.trace(\"close(filename={}, block={})\", filename, block);\r\n    replica.unref();\r\n    freeDataBufIfExists();\r\n    freeChecksumBufIfExists();\r\n    if (metrics != null) {\r\n        metrics.collectThreadLocalStates();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "readFully",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readFully(byte[] arr, int off, int len) throws IOException\n{\r\n    BlockReaderUtil.readFully(this, arr, off, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "readAll",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int readAll(byte[] buf, int off, int len) throws IOException\n{\r\n    return BlockReaderUtil.readAll(this, buf, off, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "isShortCircuit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isShortCircuit()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getClientMmap",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ClientMmap getClientMmap(EnumSet<ReadOption> opts)\n{\r\n    boolean anchor = verifyChecksum && !opts.contains(ReadOption.SKIP_CHECKSUMS);\r\n    if (anchor) {\r\n        if (!createNoChecksumContext()) {\r\n            LOG.trace(\"can't get an mmap for {} of {} since SKIP_CHECKSUMS was not \" + \"given, we aren't skipping checksums, and the block is not \" + \"mlocked.\", block, filename);\r\n            return null;\r\n        }\r\n    }\r\n    ClientMmap clientMmap = null;\r\n    try {\r\n        clientMmap = replica.getOrCreateClientMmap(anchor);\r\n    } finally {\r\n        if ((clientMmap == null) && anchor) {\r\n            releaseNoChecksumContext();\r\n        }\r\n    }\r\n    return clientMmap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getVerifyChecksum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getVerifyChecksum()\n{\r\n    return this.verifyChecksum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getMaxReadaheadLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxReadaheadLength()\n{\r\n    return this.maxReadaheadLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "forceAnchorable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void forceAnchorable()\n{\r\n    replica.getSlot().makeAnchorable();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "forceUnanchorable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void forceUnanchorable()\n{\r\n    replica.getSlot().makeUnanchorable();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getDataChecksum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DataChecksum getDataChecksum()\n{\r\n    return checksum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getNetworkDistance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNetworkDistance()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getOp",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Op getOp(String str)\n{\r\n    try {\r\n        return DOMAIN.parse(str);\r\n    } catch (IllegalArgumentException e) {\r\n        throw new IllegalArgumentException(str + \" is not a valid \" + Type.POST + \" operation.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockPath()\n{\r\n    return localBlockPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ExtendedBlock getBlock()\n{\r\n    return block;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getMetaPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getMetaPath()\n{\r\n    return localMetaPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getNumBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumBytes()\n{\r\n    return block.getNumBytes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "newDefaultStrategy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CachingStrategy newDefaultStrategy()\n{\r\n    return new CachingStrategy(null, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "newDropBehind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CachingStrategy newDropBehind()\n{\r\n    return new CachingStrategy(true, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getDropBehind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Boolean getDropBehind()\n{\r\n    return dropBehind;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "getReadahead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Long getReadahead()\n{\r\n    return readahead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\datanode",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"CachingStrategy(dropBehind=\" + dropBehind + \", readahead=\" + readahead + \")\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockChecksumType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockChecksumType getBlockChecksumType()\n{\r\n    return blockChecksumType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getStripeLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStripeLength()\n{\r\n    return stripeLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return String.format(\"blockChecksumType=%s, stripedLength=%d\", blockChecksumType, stripeLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getDefaultDirFsPermission",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FsPermission getDefaultDirFsPermission()\n{\r\n    return new FsPermission(DEFAULT_DIR_PERMISSION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getDefaultFileFsPermission",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FsPermission getDefaultFileFsPermission()\n{\r\n    return new FsPermission(DEFAULT_FILE_PERMISSION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getDefaultSymLinkFsPermission",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FsPermission getDefaultSymLinkFsPermission()\n{\r\n    return new FsPermission(DEFAULT_SYMLINK_PERMISSION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getFileFsPermission",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FsPermission getFileFsPermission()\n{\r\n    return this.getFsPermission(DEFAULT_FILE_PERMISSION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getDirFsPermission",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FsPermission getDirFsPermission()\n{\r\n    return this.getFsPermission(DEFAULT_DIR_PERMISSION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getFsPermission",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FsPermission getFsPermission(short defaultPermission)\n{\r\n    final Short v = getValue();\r\n    return new FsPermission(v != null ? v : defaultPermission);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "initialize",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void initialize(URI uri, Configuration conf) throws IOException\n{\r\n    super.initialize(uri, conf);\r\n    try {\r\n        this.vfs = tryInitializeMountingViewFs(uri, conf);\r\n    } catch (IOException ioe) {\r\n        LOGGER.debug(new StringBuilder(\"Mount tree initialization failed with \").append(\"the reason => {}. Falling back to regular DFS\").append(\" initialization. Please re-initialize the fs after updating\").append(\" mount point.\").toString(), ioe.getMessage());\r\n        super.initDFSClient(uri, conf);\r\n        super.setWorkingDirectory(super.getHomeDirectory());\r\n        return;\r\n    }\r\n    setConf(conf);\r\n    defaultDFS = (DistributedFileSystem) this.vfs.getFallbackFileSystem();\r\n    dfs = (defaultDFS != null) ? defaultDFS.dfs : null;\r\n    super.setWorkingDirectory(this.vfs.getHomeDirectory());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "initDFSClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initDFSClient(URI uri, Configuration conf) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "tryInitializeMountingViewFs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ViewFileSystemOverloadScheme tryInitializeMountingViewFs(URI theUri, Configuration conf) throws IOException\n{\r\n    ViewFileSystemOverloadScheme viewFs = new ViewFileSystemOverloadScheme();\r\n    viewFs.setSupportAutoAddingFallbackOnNoMounts(false);\r\n    viewFs.initialize(theUri, conf);\r\n    return viewFs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getUri",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "URI getUri()\n{\r\n    if (this.vfs == null) {\r\n        return super.getUri();\r\n    }\r\n    return this.vfs.getUri();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getScheme()\n{\r\n    if (this.vfs == null) {\r\n        return super.getScheme();\r\n    }\r\n    return this.vfs.getScheme();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getWorkingDirectory()\n{\r\n    if (this.vfs == null) {\r\n        return super.getWorkingDirectory();\r\n    }\r\n    return this.vfs.getWorkingDirectory();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setWorkingDirectory(Path dir)\n{\r\n    if (this.vfs == null) {\r\n        super.setWorkingDirectory(dir);\r\n        return;\r\n    }\r\n    this.vfs.setWorkingDirectory(dir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getHomeDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getHomeDirectory()\n{\r\n    if (super.dfs == null) {\r\n        return null;\r\n    }\r\n    if (this.vfs == null) {\r\n        return super.getHomeDirectory();\r\n    }\r\n    return this.vfs.getHomeDirectory();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getHedgedReadMetrics",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DFSHedgedReadMetrics getHedgedReadMetrics()\n{\r\n    if (this.vfs == null) {\r\n        return super.getHedgedReadMetrics();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getHedgedReadMetrics\");\r\n    return defaultDFS.getHedgedReadMetrics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileBlockLocations",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BlockLocation[] getFileBlockLocations(FileStatus fs, long start, long len) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getFileBlockLocations(fs, start, len);\r\n    }\r\n    return this.vfs.getFileBlockLocations(fs, start, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileBlockLocations",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BlockLocation[] getFileBlockLocations(Path p, final long start, final long len) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getFileBlockLocations(p, start, len);\r\n    }\r\n    return this.vfs.getFileBlockLocations(p, start, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setVerifyChecksum",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setVerifyChecksum(final boolean verifyChecksum)\n{\r\n    if (this.vfs == null) {\r\n        super.setVerifyChecksum(verifyChecksum);\r\n        return;\r\n    }\r\n    this.vfs.setVerifyChecksum(verifyChecksum);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "recoverLease",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean recoverLease(final Path f) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.recoverLease(f);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(f, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"recoverLease\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).recoverLease(mountPathInfo.getPathOnTarget());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataInputStream open(final Path f, final int bufferSize) throws AccessControlException, FileNotFoundException, IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.open(f, bufferSize);\r\n    }\r\n    return this.vfs.open(f, bufferSize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataInputStream open(PathHandle fd, int bufferSize) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.open(fd, bufferSize);\r\n    }\r\n    return this.vfs.open(fd, bufferSize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createPathHandle",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsPathHandle createPathHandle(FileStatus st, Options.HandleOpt... opts)\n{\r\n    if (this.vfs == null) {\r\n        return super.createPathHandle(st, opts);\r\n    }\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataOutputStream append(final Path f, final int bufferSize, final Progressable progress) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.append(f, bufferSize, progress);\r\n    }\r\n    return this.vfs.append(f, bufferSize, progress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FSDataOutputStream append(Path f, final EnumSet<CreateFlag> flag, final int bufferSize, final Progressable progress) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.append(f, flag, bufferSize, progress);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(f, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"append\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).append(mountPathInfo.getPathOnTarget(), flag, bufferSize, progress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FSDataOutputStream append(Path f, final EnumSet<CreateFlag> flag, final int bufferSize, final Progressable progress, final InetSocketAddress[] favoredNodes) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.append(f, flag, bufferSize, progress, favoredNodes);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(f, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"append\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).append(mountPathInfo.getPathOnTarget(), flag, bufferSize, progress, favoredNodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.create(f, permission, overwrite, bufferSize, replication, blockSize, progress);\r\n    }\r\n    return this.vfs.create(f, permission, overwrite, bufferSize, replication, blockSize, progress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HdfsDataOutputStream create(final Path f, final FsPermission permission, final boolean overwrite, final int bufferSize, final short replication, final long blockSize, final Progressable progress, final InetSocketAddress[] favoredNodes) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.create(f, permission, overwrite, bufferSize, replication, blockSize, progress, favoredNodes);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(f, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"create\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).create(mountPathInfo.getPathOnTarget(), permission, overwrite, bufferSize, replication, blockSize, progress, favoredNodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataOutputStream create(final Path f, final FsPermission permission, final EnumSet<CreateFlag> cflags, final int bufferSize, final short replication, final long blockSize, final Progressable progress, final Options.ChecksumOpt checksumOpt) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.create(f, permission, cflags, bufferSize, replication, blockSize, progress, checksumOpt);\r\n    }\r\n    return vfs.create(f, permission, cflags, bufferSize, replication, blockSize, progress, checksumOpt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkDFS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkDFS(FileSystem fs, String methodName)\n{\r\n    if (!(fs instanceof DistributedFileSystem)) {\r\n        String msg = new StringBuilder(\"This API:\").append(methodName).append(\" is specific to DFS. Can't run on other fs:\").append(fs.getUri()).toString();\r\n        throw new UnsupportedOperationException(msg);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkDefaultDFS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkDefaultDFS(FileSystem fs, String methodName)\n{\r\n    if (fs == null) {\r\n        String msg = new StringBuilder(\"This API:\").append(methodName).append(\" cannot be supported without default cluster(that is linkFallBack).\").toString();\r\n        throw new UnsupportedOperationException(msg);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "primitiveCreate",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HdfsDataOutputStream primitiveCreate(Path f, FsPermission absolutePermission, EnumSet<CreateFlag> flag, int bufferSize, short replication, long blockSize, Progressable progress, Options.ChecksumOpt checksumOpt) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.primitiveCreate(f, absolutePermission, flag, bufferSize, replication, blockSize, progress, checksumOpt);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(f, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"primitiveCreate\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).primitiveCreate(f, absolutePermission, flag, bufferSize, replication, blockSize, progress, checksumOpt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet<CreateFlag> flags, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.createNonRecursive(f, permission, flags, bufferSize, replication, bufferSize, progress);\r\n    }\r\n    return this.vfs.createNonRecursive(f, permission, flags, bufferSize, replication, bufferSize, progress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setReplication",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean setReplication(final Path f, final short replication) throws AccessControlException, FileNotFoundException, IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.setReplication(f, replication);\r\n    }\r\n    return this.vfs.setReplication(f, replication);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setStoragePolicy(Path src, String policyName) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.setStoragePolicy(src, policyName);\r\n        return;\r\n    }\r\n    this.vfs.setStoragePolicy(src, policyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "unsetStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void unsetStoragePolicy(Path src) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.unsetStoragePolicy(src);\r\n        return;\r\n    }\r\n    this.vfs.unsetStoragePolicy(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BlockStoragePolicySpi getStoragePolicy(Path src) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getStoragePolicy(src);\r\n    }\r\n    return this.vfs.getStoragePolicy(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAllStoragePolicies",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Collection<BlockStoragePolicy> getAllStoragePolicies() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getAllStoragePolicies();\r\n    }\r\n    Collection<? extends BlockStoragePolicySpi> allStoragePolicies = this.vfs.getAllStoragePolicies();\r\n    return (Collection<BlockStoragePolicy>) allStoragePolicies;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBytesWithFutureGenerationStamps",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getBytesWithFutureGenerationStamps() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getBytesWithFutureGenerationStamps();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getBytesWithFutureGenerationStamps\");\r\n    return defaultDFS.getBytesWithFutureGenerationStamps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStoragePolicies",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "BlockStoragePolicy[] getStoragePolicies() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getStoragePolicies();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getStoragePolicies\");\r\n    return defaultDFS.getStoragePolicies();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "concat",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void concat(Path trg, Path[] psrcs) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.concat(trg, psrcs);\r\n        return;\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(trg, getConf());\r\n    mountPathInfo.getTargetFs().concat(mountPathInfo.getPathOnTarget(), psrcs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "rename",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean rename(final Path src, final Path dst) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.rename(src, dst);\r\n    }\r\n    return this.vfs.rename(src, dst);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "rename",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void rename(Path src, Path dst, final Options.Rename... options) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.rename(src, dst, options);\r\n        return;\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountSrcPathInfo = this.vfs.getMountPathInfo(src, getConf());\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountDstPathInfo = this.vfs.getMountPathInfo(dst, getConf());\r\n    if (!mountSrcPathInfo.getTargetFs().getUri().equals(mountDstPathInfo.getTargetFs().getUri())) {\r\n        throw new HadoopIllegalArgumentException(\"Can't rename across file systems.\");\r\n    }\r\n    FileUtil.rename(mountSrcPathInfo.getTargetFs(), mountSrcPathInfo.getPathOnTarget(), mountDstPathInfo.getPathOnTarget(), options);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "truncate",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean truncate(final Path f, final long newLength) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.truncate(f, newLength);\r\n    }\r\n    return this.vfs.truncate(f, newLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean delete(final Path f, final boolean recursive) throws AccessControlException, FileNotFoundException, IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.delete(f, recursive);\r\n    }\r\n    return this.vfs.delete(f, recursive);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getContentSummary",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ContentSummary getContentSummary(Path f) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getContentSummary(f);\r\n    }\r\n    return this.vfs.getContentSummary(f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getQuotaUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "QuotaUsage getQuotaUsage(Path f) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getQuotaUsage(f);\r\n    }\r\n    return this.vfs.getQuotaUsage(f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setQuota",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setQuota(Path src, final long namespaceQuota, final long storagespaceQuota) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.setQuota(src, namespaceQuota, storagespaceQuota);\r\n        return;\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(src, getConf());\r\n    mountPathInfo.getTargetFs().setQuota(mountPathInfo.getPathOnTarget(), namespaceQuota, storagespaceQuota);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setQuotaByStorageType",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setQuotaByStorageType(Path src, final StorageType type, final long quota) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.setQuotaByStorageType(src, type, quota);\r\n        return;\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(src, getConf());\r\n    mountPathInfo.getTargetFs().setQuotaByStorageType(mountPathInfo.getPathOnTarget(), type, quota);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileStatus[] listStatus(Path p) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.listStatus(p);\r\n    }\r\n    return this.vfs.listStatus(p);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listLocatedStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<LocatedFileStatus> listLocatedStatus(final Path f, final PathFilter filter) throws FileNotFoundException, IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.listLocatedStatus(f, filter);\r\n    }\r\n    return this.vfs.listLocatedStatus(f, filter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RemoteIterator<FileStatus> listStatusIterator(final Path p) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.listStatusIterator(p);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(p, getConf());\r\n    return mountPathInfo.getTargetFs().listStatusIterator(mountPathInfo.getPathOnTarget());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "batchedListStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<PartialListing<FileStatus>> batchedListStatusIterator(final List<Path> paths) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.batchedListStatusIterator(paths);\r\n    }\r\n    return this.defaultDFS.batchedListStatusIterator(paths);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "batchedListLocatedStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<PartialListing<LocatedFileStatus>> batchedListLocatedStatusIterator(final List<Path> paths) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.batchedListLocatedStatusIterator(paths);\r\n    }\r\n    return this.defaultDFS.batchedListLocatedStatusIterator(paths);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "mkdir",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean mkdir(Path f, FsPermission permission) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.mkdir(f, permission);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(f, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"mkdir\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).mkdir(mountPathInfo.getPathOnTarget(), permission);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean mkdirs(Path f, FsPermission permission) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.mkdirs(f, permission);\r\n    }\r\n    return this.vfs.mkdirs(f, permission);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "primitiveMkdir",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean primitiveMkdir(Path f, FsPermission absolutePermission) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.primitiveMkdir(f, absolutePermission);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(f, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"primitiveMkdir\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).primitiveMkdir(mountPathInfo.getPathOnTarget(), absolutePermission);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (this.vfs != null) {\r\n        this.vfs.close();\r\n    }\r\n    super.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getClient",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DFSClient getClient()\n{\r\n    if (this.vfs == null) {\r\n        return super.getClient();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getClient\");\r\n    return defaultDFS.getClient();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FsStatus getStatus(Path p) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getStatus(p);\r\n    }\r\n    return this.vfs.getStatus(p);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getMissingBlocksCount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getMissingBlocksCount() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getMissingBlocksCount();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getMissingBlocksCount\");\r\n    return defaultDFS.getMissingBlocksCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getPendingDeletionBlocksCount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getPendingDeletionBlocksCount() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getPendingDeletionBlocksCount();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getPendingDeletionBlocksCount\");\r\n    return defaultDFS.getPendingDeletionBlocksCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getMissingReplOneBlocksCount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getMissingReplOneBlocksCount() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getMissingReplOneBlocksCount();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getMissingReplOneBlocksCount\");\r\n    return defaultDFS.getMissingReplOneBlocksCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLowRedundancyBlocksCount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getLowRedundancyBlocksCount() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getLowRedundancyBlocksCount();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getLowRedundancyBlocksCount\");\r\n    return defaultDFS.getLowRedundancyBlocksCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getCorruptBlocksCount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getCorruptBlocksCount() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getCorruptBlocksCount();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getCorruptBlocksCount\");\r\n    return defaultDFS.getLowRedundancyBlocksCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listCorruptFileBlocks",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RemoteIterator<Path> listCorruptFileBlocks(final Path path) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.listCorruptFileBlocks(path);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    return mountPathInfo.getTargetFs().listCorruptFileBlocks(mountPathInfo.getPathOnTarget());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDataNodeStats",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DatanodeInfo[] getDataNodeStats() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getDataNodeStats();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getDataNodeStats\");\r\n    return defaultDFS.getDataNodeStats();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDataNodeStats",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DatanodeInfo[] getDataNodeStats(final HdfsConstants.DatanodeReportType type) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getDataNodeStats(type);\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getDataNodeStats\");\r\n    return defaultDFS.getDataNodeStats(type);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setSafeMode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean setSafeMode(HdfsConstants.SafeModeAction action) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.setSafeMode(action);\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"setSafeMode\");\r\n    return defaultDFS.setSafeMode(action);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setSafeMode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean setSafeMode(HdfsConstants.SafeModeAction action, boolean isChecked) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.setSafeMode(action, isChecked);\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"setSafeMode\");\r\n    return defaultDFS.setSafeMode(action, isChecked);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "saveNamespace",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean saveNamespace(long timeWindow, long txGap) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.saveNamespace(timeWindow, txGap);\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"saveNamespace\");\r\n    return defaultDFS.saveNamespace(timeWindow, txGap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "saveNamespace",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void saveNamespace() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.saveNamespace();\r\n        return;\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"saveNamespace\");\r\n    defaultDFS.saveNamespace();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "rollEdits",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long rollEdits() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.rollEdits();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"rollEdits\");\r\n    return defaultDFS.rollEdits();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "restoreFailedStorage",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean restoreFailedStorage(String arg) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.restoreFailedStorage(arg);\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"restoreFailedStorage\");\r\n    return defaultDFS.restoreFailedStorage(arg);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "refreshNodes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void refreshNodes() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.refreshNodes();\r\n        return;\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"refreshNodes\");\r\n    defaultDFS.refreshNodes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "finalizeUpgrade",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void finalizeUpgrade() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.finalizeUpgrade();\r\n        return;\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"finalizeUpgrade\");\r\n    defaultDFS.finalizeUpgrade();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "upgradeStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean upgradeStatus() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.upgradeStatus();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"upgradeStatus\");\r\n    return defaultDFS.upgradeStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "rollingUpgrade",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RollingUpgradeInfo rollingUpgrade(HdfsConstants.RollingUpgradeAction action) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.rollingUpgrade(action);\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"rollingUpgrade\");\r\n    return defaultDFS.rollingUpgrade(action);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "metaSave",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void metaSave(String pathname) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.metaSave(pathname);\r\n        return;\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"metaSave\");\r\n    defaultDFS.metaSave(pathname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FsServerDefaults getServerDefaults() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getServerDefaults();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getServerDefaults\");\r\n    return defaultDFS.getServerDefaults();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileStatus getFileStatus(final Path f) throws AccessControlException, FileNotFoundException, IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getFileStatus(f);\r\n    }\r\n    return this.vfs.getFileStatus(f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createSymlink",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createSymlink(final Path target, final Path link, final boolean createParent) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.createSymlink(target, link, createParent);\r\n        return;\r\n    }\r\n    throw new UnsupportedOperationException(\"createSymlink is not supported in ViewHDFS\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "supportsSymlinks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean supportsSymlinks()\n{\r\n    if (this.vfs == null) {\r\n        return super.supportsSymlinks();\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileLinkStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FileStatus getFileLinkStatus(final Path f) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getFileLinkStatus(f);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(f, getConf());\r\n    return mountPathInfo.getTargetFs().getFileLinkStatus(mountPathInfo.getPathOnTarget());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLinkTarget",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getLinkTarget(Path path) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getLinkTarget(path);\r\n    }\r\n    return this.vfs.getLinkTarget(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "resolveLink",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path resolveLink(Path f) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.resolveLink(f);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(f, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"resolveLink\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).resolveLink(mountPathInfo.getPathOnTarget());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileChecksum",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileChecksum getFileChecksum(final Path f) throws AccessControlException, FileNotFoundException, IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getFileChecksum(f);\r\n    }\r\n    return this.vfs.getFileChecksum(f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setPermission",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setPermission(final Path f, final FsPermission permission) throws AccessControlException, FileNotFoundException, IOException\n{\r\n    if (this.vfs == null) {\r\n        super.setPermission(f, permission);\r\n        return;\r\n    }\r\n    this.vfs.setPermission(f, permission);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setOwner",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOwner(final Path f, final String username, final String groupname) throws AccessControlException, FileNotFoundException, IOException\n{\r\n    if (this.vfs == null) {\r\n        super.setOwner(f, username, groupname);\r\n        return;\r\n    }\r\n    this.vfs.setOwner(f, username, groupname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setTimes",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setTimes(final Path f, final long mtime, final long atime) throws AccessControlException, FileNotFoundException, IOException\n{\r\n    if (this.vfs == null) {\r\n        super.setTimes(f, mtime, atime);\r\n        return;\r\n    }\r\n    this.vfs.setTimes(f, mtime, atime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDefaultPort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getDefaultPort()\n{\r\n    return super.getDefaultPort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> getDelegationToken(String renewer) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getDelegationToken(renewer);\r\n    }\r\n    if (defaultDFS != null) {\r\n        return defaultDFS.getDelegationToken(renewer);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setBalancerBandwidth",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setBalancerBandwidth(long bandwidth) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.setBalancerBandwidth(bandwidth);\r\n        return;\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"setBalancerBandwidth\");\r\n    defaultDFS.setBalancerBandwidth(bandwidth);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getCanonicalServiceName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getCanonicalServiceName()\n{\r\n    if (this.vfs == null) {\r\n        return super.getCanonicalServiceName();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getCanonicalServiceName\");\r\n    return defaultDFS.getCanonicalServiceName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "canonicalizeUri",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "URI canonicalizeUri(URI uri)\n{\r\n    if (this.vfs == null) {\r\n        return super.canonicalizeUri(uri);\r\n    }\r\n    return vfs.canonicalizeUri(uri);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isInSafeMode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isInSafeMode() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.isInSafeMode();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"isInSafeMode\");\r\n    return defaultDFS.isInSafeMode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "allowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void allowSnapshot(Path path) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.allowSnapshot(path);\r\n        return;\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"allowSnapshot\");\r\n    ((DistributedFileSystem) mountPathInfo.getTargetFs()).allowSnapshot(mountPathInfo.getPathOnTarget());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "disallowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void disallowSnapshot(final Path path) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.disallowSnapshot(path);\r\n        return;\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"disallowSnapshot\");\r\n    ((DistributedFileSystem) mountPathInfo.getTargetFs()).disallowSnapshot(mountPathInfo.getPathOnTarget());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createSnapshot",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path createSnapshot(Path path, String snapshotName) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.createSnapshot(path, snapshotName);\r\n    }\r\n    return this.vfs.createSnapshot(path, snapshotName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "renameSnapshot",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void renameSnapshot(Path path, String snapshotOldName, String snapshotNewName) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.renameSnapshot(path, snapshotOldName, snapshotNewName);\r\n        return;\r\n    }\r\n    this.vfs.renameSnapshot(path, snapshotOldName, snapshotNewName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSnapshottableDirListing",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "SnapshottableDirectoryStatus[] getSnapshottableDirListing() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getSnapshottableDirListing();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getSnapshottableDirListing\");\r\n    return defaultDFS.getSnapshottableDirListing();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "deleteSnapshot",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void deleteSnapshot(Path path, String snapshotName) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.deleteSnapshot(path, snapshotName);\r\n        return;\r\n    }\r\n    this.vfs.deleteSnapshot(path, snapshotName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "snapshotDiffReportListingRemoteIterator",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "RemoteIterator<SnapshotDiffReportListing> snapshotDiffReportListingRemoteIterator(final Path snapshotDir, final String fromSnapshot, final String toSnapshot) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.snapshotDiffReportListingRemoteIterator(snapshotDir, fromSnapshot, toSnapshot);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(snapshotDir, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"snapshotDiffReportListingRemoteIterator\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).snapshotDiffReportListingRemoteIterator(mountPathInfo.getPathOnTarget(), fromSnapshot, toSnapshot);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSnapshotDiffReport",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "SnapshotDiffReport getSnapshotDiffReport(final Path snapshotDir, final String fromSnapshot, final String toSnapshot) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getSnapshotDiffReport(snapshotDir, fromSnapshot, toSnapshot);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(snapshotDir, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"getSnapshotDiffReport\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).getSnapshotDiffReport(mountPathInfo.getPathOnTarget(), fromSnapshot, toSnapshot);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isFileClosed",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean isFileClosed(final Path src) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.isFileClosed(src);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(src, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"isFileClosed\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).isFileClosed(mountPathInfo.getPathOnTarget());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long addCacheDirective(CacheDirectiveInfo info) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.addCacheDirective(info);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(info.getPath(), getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"addCacheDirective\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).addCacheDirective(new CacheDirectiveInfo.Builder(info).setPath(mountPathInfo.getPathOnTarget()).build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long addCacheDirective(CacheDirectiveInfo info, EnumSet<CacheFlag> flags) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.addCacheDirective(info, flags);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(info.getPath(), getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"addCacheDirective\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).addCacheDirective(new CacheDirectiveInfo.Builder(info).setPath(mountPathInfo.getPathOnTarget()).build(), flags);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "modifyCacheDirective",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void modifyCacheDirective(CacheDirectiveInfo info) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.modifyCacheDirective(info);\r\n        return;\r\n    }\r\n    if (info.getPath() != null) {\r\n        ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(info.getPath(), getConf());\r\n        checkDFS(mountPathInfo.getTargetFs(), \"modifyCacheDirective\");\r\n        ((DistributedFileSystem) mountPathInfo.getTargetFs()).modifyCacheDirective(new CacheDirectiveInfo.Builder(info).setPath(mountPathInfo.getPathOnTarget()).build());\r\n        return;\r\n    }\r\n    List<IOException> failedExceptions = new ArrayList<>();\r\n    boolean isDFSExistsInChilds = false;\r\n    for (FileSystem fs : getChildFileSystems()) {\r\n        if (!(fs instanceof DistributedFileSystem)) {\r\n            continue;\r\n        }\r\n        isDFSExistsInChilds = true;\r\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n        try {\r\n            dfs.modifyCacheDirective(info);\r\n        } catch (IOException ioe) {\r\n            failedExceptions.add(ioe);\r\n        }\r\n    }\r\n    if (!isDFSExistsInChilds) {\r\n        throw new UnsupportedOperationException(\"No DFS available in child file systems.\");\r\n    }\r\n    if (failedExceptions.size() > 0) {\r\n        throw MultipleIOException.createIOException(failedExceptions);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "modifyCacheDirective",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void modifyCacheDirective(CacheDirectiveInfo info, EnumSet<CacheFlag> flags) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.modifyCacheDirective(info, flags);\r\n        return;\r\n    }\r\n    if (info.getPath() != null) {\r\n        ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(info.getPath(), getConf());\r\n        checkDFS(mountPathInfo.getTargetFs(), \"modifyCacheDirective\");\r\n        ((DistributedFileSystem) mountPathInfo.getTargetFs()).modifyCacheDirective(new CacheDirectiveInfo.Builder(info).setPath(mountPathInfo.getPathOnTarget()).build(), flags);\r\n        return;\r\n    }\r\n    List<IOException> failedExceptions = new ArrayList<>();\r\n    boolean isDFSExistsInChilds = false;\r\n    for (FileSystem fs : getChildFileSystems()) {\r\n        if (!(fs instanceof DistributedFileSystem)) {\r\n            continue;\r\n        }\r\n        isDFSExistsInChilds = true;\r\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n        try {\r\n            dfs.modifyCacheDirective(info, flags);\r\n        } catch (IOException ioe) {\r\n            failedExceptions.add(ioe);\r\n        }\r\n    }\r\n    if (!isDFSExistsInChilds) {\r\n        throw new UnsupportedOperationException(\"No DFS available in child file systems.\");\r\n    }\r\n    if (failedExceptions.size() > 0) {\r\n        throw MultipleIOException.createIOException(failedExceptions);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeCacheDirective",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void removeCacheDirective(long id) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.removeCacheDirective(id);\r\n        return;\r\n    }\r\n    List<IOException> failedExceptions = new ArrayList<>();\r\n    boolean isDFSExistsInChilds = false;\r\n    for (FileSystem fs : getChildFileSystems()) {\r\n        if (!(fs instanceof DistributedFileSystem)) {\r\n            continue;\r\n        }\r\n        isDFSExistsInChilds = true;\r\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n        try {\r\n            dfs.removeCacheDirective(id);\r\n        } catch (IOException ioe) {\r\n            failedExceptions.add(ioe);\r\n        }\r\n    }\r\n    if (!isDFSExistsInChilds) {\r\n        throw new UnsupportedOperationException(\"No DFS available in child file systems.\");\r\n    }\r\n    if (failedExceptions.size() > 0) {\r\n        throw MultipleIOException.createIOException(failedExceptions);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listCacheDirectives",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "RemoteIterator<CacheDirectiveEntry> listCacheDirectives(CacheDirectiveInfo filter) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.listCacheDirectives(filter);\r\n    }\r\n    if (filter != null && filter.getPath() != null) {\r\n        ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(filter.getPath(), getConf());\r\n        checkDFS(mountPathInfo.getTargetFs(), \"listCacheDirectives\");\r\n        return ((DistributedFileSystem) mountPathInfo.getTargetFs()).listCacheDirectives(new CacheDirectiveInfo.Builder(filter).setPath(mountPathInfo.getPathOnTarget()).build());\r\n    }\r\n    final List<RemoteIterator<CacheDirectiveEntry>> iters = new ArrayList<>();\r\n    for (FileSystem fs : getChildFileSystems()) {\r\n        if (fs instanceof DistributedFileSystem) {\r\n            iters.add(((DistributedFileSystem) fs).listCacheDirectives(filter));\r\n        }\r\n    }\r\n    if (iters.size() == 0) {\r\n        throw new UnsupportedOperationException(\"No DFS found in child fs. This API can't be supported in non DFS\");\r\n    }\r\n    return new RemoteIterator<CacheDirectiveEntry>() {\r\n\r\n        int currIdx = 0;\r\n\r\n        RemoteIterator<CacheDirectiveEntry> currIter = iters.get(currIdx++);\r\n\r\n        @Override\r\n        public boolean hasNext() throws IOException {\r\n            if (currIter.hasNext()) {\r\n                return true;\r\n            }\r\n            while (currIdx < iters.size()) {\r\n                currIter = iters.get(currIdx++);\r\n                if (currIter.hasNext()) {\r\n                    return true;\r\n                }\r\n            }\r\n            return false;\r\n        }\r\n\r\n        @Override\r\n        public CacheDirectiveEntry next() throws IOException {\r\n            if (hasNext()) {\r\n                return currIter.next();\r\n            }\r\n            throw new NoSuchElementException(\"No more elements\");\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addCachePool",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void addCachePool(CachePoolInfo info) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.addCachePool(info);\r\n        return;\r\n    }\r\n    List<IOException> failedExceptions = new ArrayList<>();\r\n    boolean isDFSExistsInChilds = false;\r\n    for (FileSystem fs : getChildFileSystems()) {\r\n        if (!(fs instanceof DistributedFileSystem)) {\r\n            continue;\r\n        }\r\n        isDFSExistsInChilds = true;\r\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n        try {\r\n            dfs.addCachePool(info);\r\n        } catch (IOException ioe) {\r\n            failedExceptions.add(ioe);\r\n        }\r\n    }\r\n    if (!isDFSExistsInChilds) {\r\n        throw new UnsupportedOperationException(\"No DFS available in child file systems.\");\r\n    }\r\n    if (failedExceptions.size() > 0) {\r\n        throw MultipleIOException.createIOException(failedExceptions);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "modifyCachePool",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void modifyCachePool(CachePoolInfo info) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.modifyCachePool(info);\r\n        return;\r\n    }\r\n    List<IOException> failedExceptions = new ArrayList<>();\r\n    boolean isDFSExistsInChilds = false;\r\n    for (FileSystem fs : getChildFileSystems()) {\r\n        if (!(fs instanceof DistributedFileSystem)) {\r\n            continue;\r\n        }\r\n        isDFSExistsInChilds = true;\r\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n        try {\r\n            dfs.modifyCachePool(info);\r\n        } catch (IOException ioe) {\r\n            failedExceptions.add(ioe);\r\n        }\r\n    }\r\n    if (!isDFSExistsInChilds) {\r\n        throw new UnsupportedOperationException(\"No DFS available in child file systems.\");\r\n    }\r\n    if (failedExceptions.size() > 0) {\r\n        throw MultipleIOException.createIOException(failedExceptions);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeCachePool",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void removeCachePool(String poolName) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.removeCachePool(poolName);\r\n        return;\r\n    }\r\n    List<IOException> failedExceptions = new ArrayList<>();\r\n    boolean isDFSExistsInChilds = false;\r\n    for (FileSystem fs : getChildFileSystems()) {\r\n        if (!(fs instanceof DistributedFileSystem)) {\r\n            continue;\r\n        }\r\n        isDFSExistsInChilds = true;\r\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n        try {\r\n            dfs.removeCachePool(poolName);\r\n        } catch (IOException ioe) {\r\n            failedExceptions.add(ioe);\r\n        }\r\n    }\r\n    if (!isDFSExistsInChilds) {\r\n        throw new UnsupportedOperationException(\"No DFS available in child file systems.\");\r\n    }\r\n    if (failedExceptions.size() > 0) {\r\n        throw MultipleIOException.createIOException(failedExceptions);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listCachePools",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "RemoteIterator<CachePoolEntry> listCachePools() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.listCachePools();\r\n    }\r\n    List<DistributedFileSystem> childDFSs = new ArrayList<>();\r\n    for (FileSystem fs : getChildFileSystems()) {\r\n        if (fs instanceof DistributedFileSystem) {\r\n            childDFSs.add((DistributedFileSystem) fs);\r\n        }\r\n    }\r\n    if (childDFSs.size() == 0) {\r\n        throw new UnsupportedOperationException(\"No DFS found in child fs. This API can't be supported in non DFS\");\r\n    }\r\n    return new RemoteIterator<CachePoolEntry>() {\r\n\r\n        int curDfsIdx = 0;\r\n\r\n        RemoteIterator<CachePoolEntry> currIter = childDFSs.get(curDfsIdx++).listCachePools();\r\n\r\n        @Override\r\n        public boolean hasNext() throws IOException {\r\n            if (currIter.hasNext()) {\r\n                return true;\r\n            }\r\n            while (curDfsIdx < childDFSs.size()) {\r\n                currIter = childDFSs.get(curDfsIdx++).listCachePools();\r\n                if (currIter.hasNext()) {\r\n                    return true;\r\n                }\r\n            }\r\n            return false;\r\n        }\r\n\r\n        @Override\r\n        public CachePoolEntry next() throws IOException {\r\n            if (hasNext()) {\r\n                return currIter.next();\r\n            }\r\n            throw new java.util.NoSuchElementException(\"No more entries\");\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "modifyAclEntries",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void modifyAclEntries(Path path, List<AclEntry> aclSpec) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.modifyAclEntries(path, aclSpec);\r\n        return;\r\n    }\r\n    this.vfs.modifyAclEntries(path, aclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeAclEntries",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void removeAclEntries(Path path, List<AclEntry> aclSpec) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.removeAclEntries(path, aclSpec);\r\n        return;\r\n    }\r\n    this.vfs.removeAclEntries(path, aclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeDefaultAcl",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void removeDefaultAcl(Path path) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.removeDefaultAcl(path);\r\n        return;\r\n    }\r\n    this.vfs.removeDefaultAcl(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeAcl",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void removeAcl(Path path) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.removeAcl(path);\r\n        return;\r\n    }\r\n    this.vfs.removeAcl(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setAcl",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setAcl(Path path, List<AclEntry> aclSpec) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.setAcl(path, aclSpec);\r\n        return;\r\n    }\r\n    this.vfs.setAcl(path, aclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAclStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AclStatus getAclStatus(Path path) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getAclStatus(path);\r\n    }\r\n    return this.vfs.getAclStatus(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createEncryptionZone",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createEncryptionZone(final Path path, final String keyName) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.createEncryptionZone(path, keyName);\r\n        return;\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"createEncryptionZone\");\r\n    ((DistributedFileSystem) mountPathInfo.getTargetFs()).createEncryptionZone(mountPathInfo.getPathOnTarget(), keyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getEZForPath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "EncryptionZone getEZForPath(final Path path) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getEZForPath(path);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"getEZForPath\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).getEZForPath(mountPathInfo.getPathOnTarget());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listEncryptionZones",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RemoteIterator<EncryptionZone> listEncryptionZones() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.listEncryptionZones();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"listEncryptionZones\");\r\n    return defaultDFS.listEncryptionZones();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "reencryptEncryptionZone",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void reencryptEncryptionZone(final Path zone, final HdfsConstants.ReencryptAction action) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.reencryptEncryptionZone(zone, action);\r\n        return;\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(zone, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"reencryptEncryptionZone\");\r\n    ((DistributedFileSystem) mountPathInfo.getTargetFs()).reencryptEncryptionZone(mountPathInfo.getPathOnTarget(), action);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listReencryptionStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RemoteIterator<ZoneReencryptionStatus> listReencryptionStatus() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.listReencryptionStatus();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"listReencryptionStatus\");\r\n    return defaultDFS.listReencryptionStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileEncryptionInfo",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FileEncryptionInfo getFileEncryptionInfo(final Path path) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getFileEncryptionInfo(path);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"getFileEncryptionInfo\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).getFileEncryptionInfo(mountPathInfo.getPathOnTarget());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "provisionEZTrash",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void provisionEZTrash(final Path path, final FsPermission trashPermission) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.provisionEZTrash(path, trashPermission);\r\n        return;\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"provisionEZTrash\");\r\n    ((DistributedFileSystem) mountPathInfo.getTargetFs()).provisionEZTrash(mountPathInfo.getPathOnTarget(), trashPermission);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "provisionSnapshotTrash",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path provisionSnapshotTrash(final Path path, final FsPermission trashPermission) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.provisionSnapshotTrash(path, trashPermission);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"provisionSnapshotTrash\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).provisionSnapshotTrash(mountPathInfo.getPathOnTarget(), trashPermission);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setXAttr",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setXAttr(Path path, String name, byte[] value, EnumSet<XAttrSetFlag> flag) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.setXAttr(path, name, value, flag);\r\n        return;\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    mountPathInfo.getTargetFs().setXAttr(mountPathInfo.getPathOnTarget(), name, value, flag);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getXAttr",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "byte[] getXAttr(Path path, String name) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getXAttr(path, name);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    return mountPathInfo.getTargetFs().getXAttr(mountPathInfo.getPathOnTarget(), name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(Path path) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getXAttrs(path);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    return mountPathInfo.getTargetFs().getXAttrs(mountPathInfo.getPathOnTarget());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(Path path, List<String> names) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getXAttrs(path, names);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    return mountPathInfo.getTargetFs().getXAttrs(mountPathInfo.getPathOnTarget(), names);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listXAttrs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<String> listXAttrs(Path path) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.listXAttrs(path);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    return mountPathInfo.getTargetFs().listXAttrs(mountPathInfo.getPathOnTarget());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeXAttr",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeXAttr(Path path, String name) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.removeXAttr(path, name);\r\n        return;\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    mountPathInfo.getTargetFs().removeXAttr(mountPathInfo.getPathOnTarget(), name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "access",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void access(Path path, FsAction mode) throws AccessControlException, FileNotFoundException, IOException\n{\r\n    if (this.vfs == null) {\r\n        super.access(path, mode);\r\n        return;\r\n    }\r\n    this.vfs.access(path, mode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getKeyProviderUri",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "URI getKeyProviderUri() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getKeyProviderUri();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getKeyProviderUri\");\r\n    return defaultDFS.getKeyProviderUri();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getKeyProvider",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "KeyProvider getKeyProvider() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getKeyProvider();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getKeyProvider\");\r\n    return defaultDFS.getKeyProvider();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAdditionalTokenIssuers",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DelegationTokenIssuer[] getAdditionalTokenIssuers() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getChildFileSystems();\r\n    }\r\n    return this.vfs.getChildFileSystems();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getInotifyEventStream",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DFSInotifyEventInputStream getInotifyEventStream() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getInotifyEventStream();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getInotifyEventStream\");\r\n    return defaultDFS.getInotifyEventStream();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getInotifyEventStream",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DFSInotifyEventInputStream getInotifyEventStream(long lastReadTxid) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getInotifyEventStream();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"getInotifyEventStream\");\r\n    return defaultDFS.getInotifyEventStream();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setErasureCodingPolicy(final Path path, final String ecPolicyName) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.setErasureCodingPolicy(path, ecPolicyName);\r\n        return;\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"setErasureCodingPolicy\");\r\n    ((DistributedFileSystem) mountPathInfo.getTargetFs()).setErasureCodingPolicy(mountPathInfo.getPathOnTarget(), ecPolicyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "satisfyStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void satisfyStoragePolicy(Path src) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.satisfyStoragePolicy(src);\r\n        return;\r\n    }\r\n    this.vfs.satisfyStoragePolicy(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ErasureCodingPolicy getErasureCodingPolicy(final Path path) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getErasureCodingPolicy(path);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"getErasureCodingPolicy\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).getErasureCodingPolicy(mountPathInfo.getPathOnTarget());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAllErasureCodingPolicies",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Collection<ErasureCodingPolicyInfo> getAllErasureCodingPolicies() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getAllErasureCodingPolicies();\r\n    }\r\n    FileSystem[] childFss = getChildFileSystems();\r\n    List<ErasureCodingPolicyInfo> results = new ArrayList<>();\r\n    List<IOException> failedExceptions = new ArrayList<>();\r\n    boolean isDFSExistsInChilds = false;\r\n    for (FileSystem fs : childFss) {\r\n        if (!(fs instanceof DistributedFileSystem)) {\r\n            continue;\r\n        }\r\n        isDFSExistsInChilds = true;\r\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n        try {\r\n            results.addAll(dfs.getAllErasureCodingPolicies());\r\n        } catch (IOException ioe) {\r\n            failedExceptions.add(ioe);\r\n        }\r\n    }\r\n    if (!isDFSExistsInChilds) {\r\n        throw new UnsupportedOperationException(\"No DFS available in child file systems.\");\r\n    }\r\n    if (failedExceptions.size() > 0) {\r\n        throw MultipleIOException.createIOException(failedExceptions);\r\n    }\r\n    return results;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAllErasureCodingCodecs",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Map<String, String> getAllErasureCodingCodecs() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getAllErasureCodingCodecs();\r\n    }\r\n    FileSystem[] childFss = getChildFileSystems();\r\n    Map<String, String> results = new HashMap<>();\r\n    List<IOException> failedExceptions = new ArrayList<>();\r\n    boolean isDFSExistsInChilds = false;\r\n    for (FileSystem fs : childFss) {\r\n        if (!(fs instanceof DistributedFileSystem)) {\r\n            continue;\r\n        }\r\n        isDFSExistsInChilds = true;\r\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n        try {\r\n            results.putAll(dfs.getAllErasureCodingCodecs());\r\n        } catch (IOException ioe) {\r\n            failedExceptions.add(ioe);\r\n        }\r\n    }\r\n    if (!isDFSExistsInChilds) {\r\n        throw new UnsupportedOperationException(\"No DFS available in child file systems.\");\r\n    }\r\n    if (failedExceptions.size() > 0) {\r\n        throw MultipleIOException.createIOException(failedExceptions);\r\n    }\r\n    return results;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addErasureCodingPolicies",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "AddErasureCodingPolicyResponse[] addErasureCodingPolicies(ErasureCodingPolicy[] policies) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.addErasureCodingPolicies(policies);\r\n    }\r\n    List<IOException> failedExceptions = new ArrayList<>();\r\n    List<AddErasureCodingPolicyResponse> results = new ArrayList<>();\r\n    boolean isDFSExistsInChilds = false;\r\n    for (FileSystem fs : getChildFileSystems()) {\r\n        if (!(fs instanceof DistributedFileSystem)) {\r\n            continue;\r\n        }\r\n        isDFSExistsInChilds = true;\r\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n        try {\r\n            results.addAll(Arrays.asList(dfs.addErasureCodingPolicies(policies)));\r\n        } catch (IOException ioe) {\r\n            failedExceptions.add(ioe);\r\n        }\r\n    }\r\n    if (!isDFSExistsInChilds) {\r\n        throw new UnsupportedOperationException(\"No DFS available in child file systems.\");\r\n    }\r\n    if (failedExceptions.size() > 0) {\r\n        throw MultipleIOException.createIOException(failedExceptions);\r\n    }\r\n    return results.toArray(new AddErasureCodingPolicyResponse[results.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeErasureCodingPolicy",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void removeErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.removeErasureCodingPolicy(ecPolicyName);\r\n        return;\r\n    }\r\n    List<IOException> failedExceptions = new ArrayList<>();\r\n    boolean isDFSExistsInChilds = false;\r\n    for (FileSystem fs : getChildFileSystems()) {\r\n        if (!(fs instanceof DistributedFileSystem)) {\r\n            continue;\r\n        }\r\n        isDFSExistsInChilds = true;\r\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n        try {\r\n            dfs.removeErasureCodingPolicy(ecPolicyName);\r\n        } catch (IOException ioe) {\r\n            failedExceptions.add(ioe);\r\n        }\r\n    }\r\n    if (!isDFSExistsInChilds) {\r\n        throw new UnsupportedOperationException(\"No DFS available in child file systems.\");\r\n    }\r\n    if (failedExceptions.size() > 0) {\r\n        throw MultipleIOException.createIOException(failedExceptions);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "enableErasureCodingPolicy",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void enableErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.enableErasureCodingPolicy(ecPolicyName);\r\n        return;\r\n    }\r\n    List<IOException> failedExceptions = new ArrayList<>();\r\n    boolean isDFSExistsInChilds = false;\r\n    for (FileSystem fs : getChildFileSystems()) {\r\n        if (!(fs instanceof DistributedFileSystem)) {\r\n            continue;\r\n        }\r\n        isDFSExistsInChilds = true;\r\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n        try {\r\n            dfs.enableErasureCodingPolicy(ecPolicyName);\r\n        } catch (IOException ioe) {\r\n            failedExceptions.add(ioe);\r\n        }\r\n    }\r\n    if (!isDFSExistsInChilds) {\r\n        throw new UnsupportedOperationException(\"No DFS available in child file systems.\");\r\n    }\r\n    if (failedExceptions.size() > 0) {\r\n        throw MultipleIOException.createIOException(failedExceptions);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "disableErasureCodingPolicy",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void disableErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.disableErasureCodingPolicy(ecPolicyName);\r\n        return;\r\n    }\r\n    List<IOException> failedExceptions = new ArrayList<>();\r\n    boolean isDFSExistsInChilds = false;\r\n    for (FileSystem fs : getChildFileSystems()) {\r\n        if (!(fs instanceof DistributedFileSystem)) {\r\n            continue;\r\n        }\r\n        isDFSExistsInChilds = true;\r\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n        try {\r\n            dfs.disableErasureCodingPolicy(ecPolicyName);\r\n        } catch (IOException ioe) {\r\n            failedExceptions.add(ioe);\r\n        }\r\n    }\r\n    if (!isDFSExistsInChilds) {\r\n        throw new UnsupportedOperationException(\"No DFS available in child file systems.\");\r\n    }\r\n    if (failedExceptions.size() > 0) {\r\n        throw MultipleIOException.createIOException(failedExceptions);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "unsetErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void unsetErasureCodingPolicy(final Path path) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        super.unsetErasureCodingPolicy(path);\r\n        return;\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"unsetErasureCodingPolicy\");\r\n    ((DistributedFileSystem) mountPathInfo.getTargetFs()).unsetErasureCodingPolicy(mountPathInfo.getPathOnTarget());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getECTopologyResultForPolicies",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "ECTopologyVerifierResult getECTopologyResultForPolicies(final String... policyNames) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getECTopologyResultForPolicies(policyNames);\r\n    }\r\n    List<IOException> failedExceptions = new ArrayList<>();\r\n    ECTopologyVerifierResult result = null;\r\n    for (FileSystem fs : getChildFileSystems()) {\r\n        if (!(fs instanceof DistributedFileSystem)) {\r\n            continue;\r\n        }\r\n        DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n        try {\r\n            result = dfs.getECTopologyResultForPolicies(policyNames);\r\n            if (!result.isSupported()) {\r\n                return result;\r\n            }\r\n        } catch (IOException ioe) {\r\n            failedExceptions.add(ioe);\r\n        }\r\n    }\r\n    if (result == null) {\r\n        throw new UnsupportedOperationException(\"No DFS available in child filesystems\");\r\n    }\r\n    if (failedExceptions.size() > 0) {\r\n        throw MultipleIOException.createIOException(failedExceptions);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getTrashRoot",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getTrashRoot(Path path)\n{\r\n    if (this.vfs == null) {\r\n        return super.getTrashRoot(path);\r\n    }\r\n    return this.vfs.getTrashRoot(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getTrashRoots",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Collection<FileStatus> getTrashRoots(boolean allUsers)\n{\r\n    if (this.vfs == null) {\r\n        return super.getTrashRoots(allUsers);\r\n    }\r\n    List<FileStatus> trashRoots = new ArrayList<>();\r\n    for (FileSystem fs : getChildFileSystems()) {\r\n        trashRoots.addAll(fs.getTrashRoots(allUsers));\r\n    }\r\n    return trashRoots;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "fixRelativePart",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path fixRelativePart(Path p)\n{\r\n    return super.fixRelativePart(p);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFsStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Statistics getFsStatistics()\n{\r\n    if (this.vfs == null) {\r\n        return super.getFsStatistics();\r\n    }\r\n    return statistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDFSOpsCountStatistics",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DFSOpsCountStatistics getDFSOpsCountStatistics()\n{\r\n    if (this.vfs == null) {\r\n        return super.getDFSOpsCountStatistics();\r\n    }\r\n    return defaultDFS.getDFSOpsCountStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createFile",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HdfsDataOutputStreamBuilder createFile(Path path)\n{\r\n    if (this.vfs == null) {\r\n        return super.createFile(path);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = null;\r\n    try {\r\n        mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    } catch (IOException e) {\r\n        return null;\r\n    }\r\n    checkDFS(mountPathInfo.getTargetFs(), \"createFile\");\r\n    return (HdfsDataOutputStreamBuilder) mountPathInfo.getTargetFs().createFile(mountPathInfo.getPathOnTarget());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RemoteIterator<OpenFileEntry> listOpenFiles() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.listOpenFiles();\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"listOpenFiles\");\r\n    return defaultDFS.listOpenFiles();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RemoteIterator<OpenFileEntry> listOpenFiles(EnumSet<OpenFilesIterator.OpenFilesType> openFilesTypes) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.listOpenFiles(openFilesTypes);\r\n    }\r\n    checkDefaultDFS(defaultDFS, \"listOpenFiles\");\r\n    return defaultDFS.listOpenFiles(openFilesTypes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "RemoteIterator<OpenFileEntry> listOpenFiles(EnumSet<OpenFilesIterator.OpenFilesType> openFilesTypes, String path) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.listOpenFiles(openFilesTypes, path);\r\n    }\r\n    Path absF = fixRelativePart(new Path(path));\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = this.vfs.getMountPathInfo(absF, getConf());\r\n    checkDFS(mountPathInfo.getTargetFs(), \"listOpenFiles\");\r\n    return ((DistributedFileSystem) mountPathInfo.getTargetFs()).listOpenFiles(openFilesTypes, mountPathInfo.getPathOnTarget().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "appendFile",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "HdfsDataOutputStreamBuilder appendFile(Path path)\n{\r\n    if (this.vfs == null) {\r\n        return super.appendFile(path);\r\n    }\r\n    ViewFileSystemOverloadScheme.MountPathInfo<FileSystem> mountPathInfo = null;\r\n    try {\r\n        mountPathInfo = this.vfs.getMountPathInfo(path, getConf());\r\n    } catch (IOException e) {\r\n        LOGGER.warn(\"Failed to resolve the path as mount path\", e);\r\n        return null;\r\n    }\r\n    checkDFS(mountPathInfo.getTargetFs(), \"appendFile\");\r\n    return (HdfsDataOutputStreamBuilder) mountPathInfo.getTargetFs().appendFile(mountPathInfo.getPathOnTarget());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "hasPathCapability",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean hasPathCapability(Path path, String capability) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.hasPathCapability(path, capability);\r\n    }\r\n    return this.vfs.hasPathCapability(path, capability);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "resolvePath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path resolvePath(final Path f) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.resolvePath(f);\r\n    }\r\n    return this.vfs.resolvePath(f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean delete(final Path f) throws AccessControlException, FileNotFoundException, IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.delete(f);\r\n    }\r\n    return this.vfs.delete(f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileChecksum",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileChecksum getFileChecksum(final Path f, final long length) throws AccessControlException, FileNotFoundException, IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getFileChecksum(f, length);\r\n    }\r\n    return this.vfs.getFileChecksum(f, length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean mkdirs(Path dir) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.mkdirs(dir);\r\n    }\r\n    return this.vfs.mkdirs(dir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDefaultBlockSize",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getDefaultBlockSize(Path f)\n{\r\n    if (this.vfs == null) {\r\n        return super.getDefaultBlockSize(f);\r\n    }\r\n    return this.vfs.getDefaultBlockSize(f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDefaultReplication",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "short getDefaultReplication(Path f)\n{\r\n    if (this.vfs == null) {\r\n        return super.getDefaultReplication(f);\r\n    }\r\n    return this.vfs.getDefaultReplication(f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FsServerDefaults getServerDefaults(Path f) throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getServerDefaults(f);\r\n    }\r\n    return this.vfs.getServerDefaults(f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setWriteChecksum",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setWriteChecksum(final boolean writeChecksum)\n{\r\n    if (this.vfs == null) {\r\n        super.setWriteChecksum(writeChecksum);\r\n        return;\r\n    }\r\n    this.vfs.setWriteChecksum(writeChecksum);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getChildFileSystems",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileSystem[] getChildFileSystems()\n{\r\n    if (this.vfs == null) {\r\n        return super.getChildFileSystems();\r\n    }\r\n    return this.vfs.getChildFileSystems();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getMountPoints",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ViewFileSystem.MountPoint[] getMountPoints()\n{\r\n    if (this.vfs == null) {\r\n        return null;\r\n    }\r\n    return this.vfs.getMountPoints();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FsStatus getStatus() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getStatus();\r\n    }\r\n    return this.vfs.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getUsed",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getUsed() throws IOException\n{\r\n    if (this.vfs == null) {\r\n        return super.getUsed();\r\n    }\r\n    return this.vfs.getUsed();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getInputStreamChannel",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReadableByteChannel getInputStreamChannel()\n{\r\n    return in;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "setReadTimeout",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setReadTimeout(int timeoutMs) throws IOException\n{\r\n    in.setTimeout(timeoutMs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getReceiveBufferSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getReceiveBufferSize() throws IOException\n{\r\n    return socket.getReceiveBufferSize();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getTcpNoDelay",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getTcpNoDelay() throws IOException\n{\r\n    return socket.getTcpNoDelay();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "setWriteTimeout",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setWriteTimeout(int timeoutMs) throws IOException\n{\r\n    out.setTimeout(timeoutMs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "isClosed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isClosed()\n{\r\n    return socket.isClosed();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    try {\r\n        in.close();\r\n    } finally {\r\n        out.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getRemoteAddressString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getRemoteAddressString()\n{\r\n    SocketAddress address = socket.getRemoteSocketAddress();\r\n    return address == null ? null : address.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getLocalAddressString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getLocalAddressString()\n{\r\n    return socket.getLocalSocketAddress().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getInputStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InputStream getInputStream() throws IOException\n{\r\n    return in;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getOutputStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OutputStream getOutputStream() throws IOException\n{\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "isLocal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isLocal()\n{\r\n    return isLocal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"NioInetPeer(\" + socket.toString() + \")\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "getDomainSocket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DomainSocket getDomainSocket()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\net",
  "methodName" : "hasSecureChannel",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasSecureChannel()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setDisabledStopDeadNodeDetectorThreadForTest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDisabledStopDeadNodeDetectorThreadForTest(boolean disabledStopDeadNodeDetectorThreadForTest)\n{\r\n    DFSClient.disabledStopDeadNodeDetectorThreadForTest = disabledStopDeadNodeDetectorThreadForTest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DfsClientConf getConf()\n{\r\n    return dfsClientConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLocalInterfaceAddrs",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "SocketAddress[] getLocalInterfaceAddrs(String[] interfaceNames) throws UnknownHostException\n{\r\n    List<SocketAddress> localAddrs = new ArrayList<>();\r\n    for (String interfaceName : interfaceNames) {\r\n        if (InetAddresses.isInetAddress(interfaceName)) {\r\n            localAddrs.add(new InetSocketAddress(interfaceName, 0));\r\n        } else if (NetUtils.isValidSubnet(interfaceName)) {\r\n            for (InetAddress addr : NetUtils.getIPs(interfaceName, false)) {\r\n                localAddrs.add(new InetSocketAddress(addr, 0));\r\n            }\r\n        } else {\r\n            for (String ip : DNS.getIPs(interfaceName, false)) {\r\n                localAddrs.add(new InetSocketAddress(ip, 0));\r\n            }\r\n        }\r\n    }\r\n    return localAddrs.toArray(new SocketAddress[localAddrs.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getRandomLocalInterfaceAddr",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SocketAddress getRandomLocalInterfaceAddr()\n{\r\n    if (localInterfaceAddrs.length == 0) {\r\n        return null;\r\n    }\r\n    final int idx = r.nextInt(localInterfaceAddrs.length);\r\n    final SocketAddress addr = localInterfaceAddrs[idx];\r\n    LOG.debug(\"Using local interface {}\", addr);\r\n    return addr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDatanodeWriteTimeout",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getDatanodeWriteTimeout(int numNodes)\n{\r\n    final int t = dfsClientConf.getDatanodeSocketWriteTimeout();\r\n    return t > 0 ? t + HdfsConstants.WRITE_TIMEOUT_EXTENSION * numNodes : 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDatanodeReadTimeout",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getDatanodeReadTimeout(int numNodes)\n{\r\n    final int t = dfsClientConf.getSocketTimeout();\r\n    return t > 0 ? HdfsConstants.READ_TIMEOUT_EXTENSION * numNodes + t : 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getClientName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getClientName()\n{\r\n    return clientName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkOpen",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkOpen() throws IOException\n{\r\n    if (!clientRunning) {\r\n        throw new IOException(\"Filesystem closed\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLeaseRenewer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LeaseRenewer getLeaseRenewer()\n{\r\n    return LeaseRenewer.getInstance(namenodeUri != null ? namenodeUri.getAuthority() : \"null\", ugi, this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "beginFileLease",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void beginFileLease(final long inodeId, final DFSOutputStream out)\n{\r\n    synchronized (filesBeingWritten) {\r\n        putFileBeingWritten(inodeId, out);\r\n        LeaseRenewer renewer = getLeaseRenewer();\r\n        boolean result = renewer.put(this);\r\n        if (!result) {\r\n            LeaseRenewer.remove(renewer);\r\n            renewer = getLeaseRenewer();\r\n            renewer.put(this);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "endFileLease",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void endFileLease(final long inodeId)\n{\r\n    synchronized (filesBeingWritten) {\r\n        removeFileBeingWritten(inodeId);\r\n        if (filesBeingWritten.isEmpty()) {\r\n            getLeaseRenewer().closeClient(this);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "putFileBeingWritten",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void putFileBeingWritten(final long inodeId, final DFSOutputStream out)\n{\r\n    synchronized (filesBeingWritten) {\r\n        filesBeingWritten.put(inodeId, out);\r\n        if (lastLeaseRenewal == 0) {\r\n            updateLastLeaseRenewal();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeFileBeingWritten",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void removeFileBeingWritten(final long inodeId)\n{\r\n    synchronized (filesBeingWritten) {\r\n        filesBeingWritten.remove(inodeId);\r\n        if (filesBeingWritten.isEmpty()) {\r\n            lastLeaseRenewal = 0;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isFilesBeingWrittenEmpty",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isFilesBeingWrittenEmpty()\n{\r\n    synchronized (filesBeingWritten) {\r\n        return filesBeingWritten.isEmpty();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isClientRunning",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isClientRunning()\n{\r\n    return clientRunning;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLastLeaseRenewal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLastLeaseRenewal()\n{\r\n    return lastLeaseRenewal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "updateLastLeaseRenewal",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateLastLeaseRenewal()\n{\r\n    synchronized (filesBeingWritten) {\r\n        if (filesBeingWritten.isEmpty()) {\r\n            return;\r\n        }\r\n        lastLeaseRenewal = Time.monotonicNow();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "renewLease",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "boolean renewLease() throws IOException\n{\r\n    if (clientRunning && !isFilesBeingWrittenEmpty()) {\r\n        try {\r\n            namenode.renewLease(clientName);\r\n            updateLastLeaseRenewal();\r\n            return true;\r\n        } catch (IOException e) {\r\n            final long elapsed = Time.monotonicNow() - getLastLeaseRenewal();\r\n            if (elapsed > dfsClientConf.getleaseHardLimitPeriod()) {\r\n                LOG.warn(\"Failed to renew lease for \" + clientName + \" for \" + (elapsed / 1000) + \" seconds (>= hard-limit =\" + (dfsClientConf.getleaseHardLimitPeriod() / 1000) + \" seconds.) \" + \"Closing all files being written ...\", e);\r\n                closeAllFilesBeingWritten(true);\r\n            } else {\r\n                throw e;\r\n            }\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "closeConnectionToNamenode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void closeConnectionToNamenode()\n{\r\n    RPC.stopProxy(namenode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "closeAllFilesBeingWritten",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void closeAllFilesBeingWritten(final boolean abort)\n{\r\n    for (; ; ) {\r\n        final long inodeId;\r\n        final DFSOutputStream out;\r\n        synchronized (filesBeingWritten) {\r\n            if (filesBeingWritten.isEmpty()) {\r\n                return;\r\n            }\r\n            inodeId = filesBeingWritten.keySet().iterator().next();\r\n            out = filesBeingWritten.remove(inodeId);\r\n        }\r\n        if (out != null) {\r\n            try {\r\n                if (abort) {\r\n                    out.abort();\r\n                } else {\r\n                    out.close();\r\n                }\r\n            } catch (IOException ie) {\r\n                LOG.error(\"Failed to \" + (abort ? \"abort\" : \"close\") + \" file: \" + out.getSrc() + \" with inode: \" + inodeId, ie);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (clientRunning) {\r\n        closeAllFilesBeingWritten(false);\r\n        clientRunning = false;\r\n        if (!disabledStopDeadNodeDetectorThreadForTest) {\r\n            clientContext.unreference();\r\n        }\r\n        closeConnectionToNamenode();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "closeOutputStreams",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void closeOutputStreams(boolean abort)\n{\r\n    if (clientRunning) {\r\n        closeAllFilesBeingWritten(abort);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBlockSize",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long getBlockSize(String f) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"getBlockSize\", f)) {\r\n        return namenode.getPreferredBlockSize(f);\r\n    } catch (IOException ie) {\r\n        LOG.warn(\"Problem getting block size\", ie);\r\n        throw ie;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FsServerDefaults getServerDefaults() throws IOException\n{\r\n    checkOpen();\r\n    long now = Time.monotonicNow();\r\n    if ((serverDefaults == null) || (now - serverDefaultsLastUpdate > serverDefaultsValidityPeriod)) {\r\n        serverDefaults = namenode.getServerDefaults();\r\n        serverDefaultsLastUpdate = now;\r\n    }\r\n    assert serverDefaults != null;\r\n    return serverDefaults;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getCanonicalServiceName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getCanonicalServiceName()\n{\r\n    return (dtService != null) ? dtService.toString() : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<?> getDelegationToken(String renewer) throws IOException\n{\r\n    return getDelegationToken(renewer == null ? null : new Text(renewer));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> getDelegationToken(Text renewer) throws IOException\n{\r\n    assert dtService != null;\r\n    try (TraceScope ignored = tracer.newScope(\"getDelegationToken\")) {\r\n        Token<DelegationTokenIdentifier> token = namenode.getDelegationToken(renewer);\r\n        if (token != null) {\r\n            token.setService(this.dtService);\r\n            LOG.info(\"Created \" + DelegationTokenIdentifier.stringifyToken(token));\r\n        } else {\r\n            LOG.info(\"Cannot get delegation token from \" + renewer);\r\n        }\r\n        return token;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "renewDelegationToken",
  "errType" : [ "InterruptedException", "RemoteException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long renewDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException\n{\r\n    LOG.info(\"Renewing \" + DelegationTokenIdentifier.stringifyToken(token));\r\n    try {\r\n        return token.renew(conf);\r\n    } catch (InterruptedException ie) {\r\n        throw new RuntimeException(\"caught interrupted\", ie);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(InvalidToken.class, AccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "cancelDelegationToken",
  "errType" : [ "InterruptedException", "RemoteException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cancelDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException\n{\r\n    LOG.info(\"Cancelling \" + DelegationTokenIdentifier.stringifyToken(token));\r\n    try {\r\n        token.cancel(conf);\r\n    } catch (InterruptedException ie) {\r\n        throw new RuntimeException(\"caught interrupted\", ie);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(InvalidToken.class, AccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "reportBadBlocks",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void reportBadBlocks(LocatedBlock[] blocks) throws IOException\n{\r\n    checkOpen();\r\n    namenode.reportBadBlocks(blocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getRefreshReadBlkLocationsInterval",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getRefreshReadBlkLocationsInterval()\n{\r\n    return dfsClientConf.getLocatedBlocksRefresherInterval();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLocatedBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocatedBlocks getLocatedBlocks(String src, long start) throws IOException\n{\r\n    return getLocatedBlocks(src, start, dfsClientConf.getPrefetchSize());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLocatedBlocks",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "LocatedBlocks getLocatedBlocks(String src, long start, long length) throws IOException\n{\r\n    try (TraceScope ignored = newPathTraceScope(\"getBlockLocations\", src)) {\r\n        return callGetBlockLocations(namenode, src, start, length);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "callGetBlockLocations",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "LocatedBlocks callGetBlockLocations(ClientProtocol namenode, String src, long start, long length) throws IOException\n{\r\n    try {\r\n        return namenode.getBlockLocations(src, start, length);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "recoverLease",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean recoverLease(String src) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"recoverLease\", src)) {\r\n        return namenode.recoverLease(src, clientName);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(FileNotFoundException.class, AccessControlException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBlockLocations",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "BlockLocation[] getBlockLocations(String src, long start, long length) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"getBlockLocations\", src)) {\r\n        LocatedBlocks blocks = getLocatedBlocks(src, start, length);\r\n        BlockLocation[] locations = DFSUtilClient.locatedBlocks2Locations(blocks);\r\n        HdfsBlockLocation[] hdfsLocations = new HdfsBlockLocation[locations.length];\r\n        for (int i = 0; i < locations.length; i++) {\r\n            hdfsLocations[i] = new HdfsBlockLocation(locations[i], blocks.get(i));\r\n        }\r\n        return hdfsLocations;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createWrappedInputStream",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "HdfsDataInputStream createWrappedInputStream(DFSInputStream dfsis) throws IOException\n{\r\n    FileEncryptionInfo feInfo = dfsis.getFileEncryptionInfo();\r\n    if (feInfo != null) {\r\n        CryptoInputStream cryptoIn;\r\n        try (TraceScope ignored = getTracer().newScope(\"decryptEDEK\")) {\r\n            cryptoIn = HdfsKMSUtil.createWrappedInputStream(dfsis, getKeyProvider(), feInfo, getConfiguration());\r\n        }\r\n        return new HdfsDataInputStream(cryptoIn);\r\n    } else {\r\n        return new HdfsDataInputStream(dfsis);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createWrappedOutputStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsDataOutputStream createWrappedOutputStream(DFSOutputStream dfsos, FileSystem.Statistics statistics) throws IOException\n{\r\n    return createWrappedOutputStream(dfsos, statistics, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createWrappedOutputStream",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "HdfsDataOutputStream createWrappedOutputStream(DFSOutputStream dfsos, FileSystem.Statistics statistics, long startPos) throws IOException\n{\r\n    final FileEncryptionInfo feInfo = dfsos.getFileEncryptionInfo();\r\n    if (feInfo != null) {\r\n        HdfsKMSUtil.getCryptoProtocolVersion(feInfo);\r\n        final CryptoCodec codec = HdfsKMSUtil.getCryptoCodec(conf, feInfo);\r\n        KeyVersion decrypted;\r\n        try (TraceScope ignored = tracer.newScope(\"decryptEDEK\")) {\r\n            LOG.debug(\"Start decrypting EDEK for file: {}, output stream: 0x{}\", dfsos.getSrc(), Integer.toHexString(dfsos.hashCode()));\r\n            decrypted = HdfsKMSUtil.decryptEncryptedDataEncryptionKey(feInfo, getKeyProvider());\r\n            LOG.debug(\"Decrypted EDEK for file: {}, output stream: 0x{}\", dfsos.getSrc(), Integer.toHexString(dfsos.hashCode()));\r\n        }\r\n        final CryptoOutputStream cryptoOut = new CryptoOutputStream(dfsos, codec, decrypted.getMaterial(), feInfo.getIV(), startPos);\r\n        return new HdfsDataOutputStream(cryptoOut, statistics, startPos);\r\n    } else {\r\n        return new HdfsDataOutputStream(dfsos, statistics, startPos);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DFSInputStream open(String src) throws IOException\n{\r\n    return open(src, dfsClientConf.getIoBufferSize(), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DFSInputStream open(String src, int buffersize, boolean verifyChecksum, FileSystem.Statistics stats) throws IOException\n{\r\n    return open(src, buffersize, verifyChecksum);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "DFSInputStream open(String src, int buffersize, boolean verifyChecksum) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"newDFSInputStream\", src)) {\r\n        LocatedBlocks locatedBlocks = getLocatedBlocks(src, 0);\r\n        return openInternal(locatedBlocks, src, verifyChecksum);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "DFSInputStream open(HdfsPathHandle fd, int buffersize, boolean verifyChecksum) throws IOException\n{\r\n    checkOpen();\r\n    String src = fd.getPath();\r\n    try (TraceScope ignored = newPathTraceScope(\"newDFSInputStream\", src)) {\r\n        HdfsLocatedFileStatus s = getLocatedFileInfo(src, true);\r\n        fd.verify(s);\r\n        LocatedBlocks locatedBlocks = s.getLocatedBlocks();\r\n        return openInternal(locatedBlocks, src, verifyChecksum);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "openInternal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DFSInputStream openInternal(LocatedBlocks locatedBlocks, String src, boolean verifyChecksum) throws IOException\n{\r\n    if (locatedBlocks != null) {\r\n        ErasureCodingPolicy ecPolicy = locatedBlocks.getErasureCodingPolicy();\r\n        if (ecPolicy != null) {\r\n            return new DFSStripedInputStream(this, src, verifyChecksum, ecPolicy, locatedBlocks);\r\n        }\r\n        return new DFSInputStream(this, src, verifyChecksum, locatedBlocks);\r\n    } else {\r\n        throw new IOException(\"Cannot open filename \" + src);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getNamenode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientProtocol getNamenode()\n{\r\n    return namenode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OutputStream create(String src, boolean overwrite) throws IOException\n{\r\n    return create(src, overwrite, dfsClientConf.getDefaultReplication(), dfsClientConf.getDefaultBlockSize(), null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OutputStream create(String src, boolean overwrite, Progressable progress) throws IOException\n{\r\n    return create(src, overwrite, dfsClientConf.getDefaultReplication(), dfsClientConf.getDefaultBlockSize(), progress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OutputStream create(String src, boolean overwrite, short replication, long blockSize) throws IOException\n{\r\n    return create(src, overwrite, replication, blockSize, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OutputStream create(String src, boolean overwrite, short replication, long blockSize, Progressable progress) throws IOException\n{\r\n    return create(src, overwrite, replication, blockSize, progress, dfsClientConf.getIoBufferSize());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OutputStream create(String src, boolean overwrite, short replication, long blockSize, Progressable progress, int buffersize) throws IOException\n{\r\n    return create(src, FsPermission.getFileDefault(), overwrite ? EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE) : EnumSet.of(CreateFlag.CREATE), replication, blockSize, progress, buffersize, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DFSOutputStream create(String src, FsPermission permission, EnumSet<CreateFlag> flag, short replication, long blockSize, Progressable progress, int buffersize, ChecksumOpt checksumOpt) throws IOException\n{\r\n    return create(src, permission, flag, true, replication, blockSize, progress, buffersize, checksumOpt, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DFSOutputStream create(String src, FsPermission permission, EnumSet<CreateFlag> flag, boolean createParent, short replication, long blockSize, Progressable progress, int buffersize, ChecksumOpt checksumOpt) throws IOException\n{\r\n    return create(src, permission, flag, createParent, replication, blockSize, progress, buffersize, checksumOpt, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "applyUMask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FsPermission applyUMask(FsPermission permission)\n{\r\n    if (permission == null) {\r\n        permission = FsPermission.getFileDefault();\r\n    }\r\n    return FsCreateModes.applyUMask(permission, dfsClientConf.getUMask());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "applyUMaskDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FsPermission applyUMaskDir(FsPermission permission)\n{\r\n    if (permission == null) {\r\n        permission = FsPermission.getDirDefault();\r\n    }\r\n    return FsCreateModes.applyUMask(permission, dfsClientConf.getUMask());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DFSOutputStream create(String src, FsPermission permission, EnumSet<CreateFlag> flag, boolean createParent, short replication, long blockSize, Progressable progress, int buffersize, ChecksumOpt checksumOpt, InetSocketAddress[] favoredNodes) throws IOException\n{\r\n    return create(src, permission, flag, createParent, replication, blockSize, progress, buffersize, checksumOpt, favoredNodes, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DFSOutputStream create(String src, FsPermission permission, EnumSet<CreateFlag> flag, boolean createParent, short replication, long blockSize, Progressable progress, int buffersize, ChecksumOpt checksumOpt, InetSocketAddress[] favoredNodes, String ecPolicyName) throws IOException\n{\r\n    return create(src, permission, flag, createParent, replication, blockSize, progress, buffersize, checksumOpt, favoredNodes, ecPolicyName, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "DFSOutputStream create(String src, FsPermission permission, EnumSet<CreateFlag> flag, boolean createParent, short replication, long blockSize, Progressable progress, int buffersize, ChecksumOpt checksumOpt, InetSocketAddress[] favoredNodes, String ecPolicyName, String storagePolicy) throws IOException\n{\r\n    checkOpen();\r\n    final FsPermission masked = applyUMask(permission);\r\n    LOG.debug(\"{}: masked={}\", src, masked);\r\n    final DFSOutputStream result = DFSOutputStream.newStreamForCreate(this, src, masked, flag, createParent, replication, blockSize, progress, dfsClientConf.createChecksum(checksumOpt), getFavoredNodesStr(favoredNodes), ecPolicyName, storagePolicy);\r\n    beginFileLease(result.getFileId(), result);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFavoredNodesStr",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String[] getFavoredNodesStr(InetSocketAddress[] favoredNodes)\n{\r\n    String[] favoredNodeStrs = null;\r\n    if (favoredNodes != null) {\r\n        favoredNodeStrs = new String[favoredNodes.length];\r\n        for (int i = 0; i < favoredNodes.length; i++) {\r\n            favoredNodeStrs[i] = favoredNodes[i].getHostName() + \":\" + favoredNodes[i].getPort();\r\n        }\r\n    }\r\n    return favoredNodeStrs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "primitiveAppend",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "DFSOutputStream primitiveAppend(String src, EnumSet<CreateFlag> flag, Progressable progress) throws IOException\n{\r\n    if (flag.contains(CreateFlag.APPEND)) {\r\n        HdfsFileStatus stat = getFileInfo(src);\r\n        if (stat == null) {\r\n            if (!flag.contains(CreateFlag.CREATE)) {\r\n                throw new FileNotFoundException(\"failed to append to non-existent file \" + src + \" on client \" + clientName);\r\n            }\r\n            return null;\r\n        }\r\n        return callAppend(src, flag, progress, null);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "primitiveCreate",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "DFSOutputStream primitiveCreate(String src, FsPermission absPermission, EnumSet<CreateFlag> flag, boolean createParent, short replication, long blockSize, Progressable progress, int buffersize, ChecksumOpt checksumOpt) throws IOException\n{\r\n    checkOpen();\r\n    CreateFlag.validate(flag);\r\n    DFSOutputStream result = primitiveAppend(src, flag, progress);\r\n    if (result == null) {\r\n        DataChecksum checksum = dfsClientConf.createChecksum(checksumOpt);\r\n        result = DFSOutputStream.newStreamForCreate(this, src, absPermission, flag, createParent, replication, blockSize, progress, checksum, null, null, null);\r\n    }\r\n    beginFileLease(result.getFileId(), result);\r\n    return result;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createSymlink",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createSymlink(String target, String link, boolean createParent) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"createSymlink\", target)) {\r\n        final FsPermission dirPerm = applyUMask(null);\r\n        namenode.createSymlink(target, link, dirPerm, createParent);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileAlreadyExistsException.class, FileNotFoundException.class, ParentNotDirectoryException.class, NSQuotaExceededException.class, DSQuotaExceededException.class, QuotaByStorageTypeExceededException.class, UnresolvedPathException.class, SnapshotAccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLinkTarget",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getLinkTarget(String path) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"getLinkTarget\", path)) {\r\n        return namenode.getLinkTarget(path);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "callAppend",
  "errType" : [ "RemoteException", "InterruptedException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "LastBlockWithStatus callAppend(String src, EnumSetWritable<CreateFlag> flag) throws IOException\n{\r\n    final long startTime = Time.monotonicNow();\r\n    for (; ; ) {\r\n        try {\r\n            return namenode.append(src, clientName, flag);\r\n        } catch (RemoteException re) {\r\n            if (Time.monotonicNow() - startTime > 5000 || !RetriableException.class.getName().equals(re.getClassName())) {\r\n                throw re;\r\n            }\r\n            try {\r\n                Thread.sleep(500);\r\n            } catch (InterruptedException e) {\r\n                throw DFSUtilClient.toInterruptedIOException(\"callAppend\", e);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "callAppend",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "DFSOutputStream callAppend(String src, EnumSet<CreateFlag> flag, Progressable progress, String[] favoredNodes) throws IOException\n{\r\n    CreateFlag.validateForAppend(flag);\r\n    try {\r\n        final LastBlockWithStatus blkWithStatus = callAppend(src, new EnumSetWritable<>(flag, CreateFlag.class));\r\n        HdfsFileStatus status = blkWithStatus.getFileStatus();\r\n        if (status == null) {\r\n            LOG.debug(\"NameNode is on an older version, request file \" + \"info with additional RPC call for file: {}\", src);\r\n            status = getFileInfo(src);\r\n        }\r\n        return DFSOutputStream.newStreamForAppend(this, src, flag, progress, blkWithStatus.getLastBlock(), status, dfsClientConf.createChecksum(null), favoredNodes);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, SafeModeException.class, DSQuotaExceededException.class, QuotaByStorageTypeExceededException.class, UnsupportedOperationException.class, UnresolvedPathException.class, SnapshotAccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "HdfsDataOutputStream append(final String src, final int buffersize, EnumSet<CreateFlag> flag, final Progressable progress, final FileSystem.Statistics statistics) throws IOException\n{\r\n    final DFSOutputStream out = append(src, buffersize, flag, null, progress);\r\n    return createWrappedOutputStream(out, statistics, out.getInitialLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "HdfsDataOutputStream append(final String src, final int buffersize, EnumSet<CreateFlag> flag, final Progressable progress, final FileSystem.Statistics statistics, final InetSocketAddress[] favoredNodes) throws IOException\n{\r\n    final DFSOutputStream out = append(src, buffersize, flag, getFavoredNodesStr(favoredNodes), progress);\r\n    return createWrappedOutputStream(out, statistics, out.getInitialLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DFSOutputStream append(String src, int buffersize, EnumSet<CreateFlag> flag, String[] favoredNodes, Progressable progress) throws IOException\n{\r\n    checkOpen();\r\n    final DFSOutputStream result = callAppend(src, flag, progress, favoredNodes);\r\n    beginFileLease(result.getFileId(), result);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setReplication",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean setReplication(String src, short replication) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"setReplication\", src)) {\r\n        return namenode.setReplication(src, replication);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, SafeModeException.class, DSQuotaExceededException.class, QuotaByStorageTypeExceededException.class, UnresolvedPathException.class, SnapshotAccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setStoragePolicy",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setStoragePolicy(String src, String policyName) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"setStoragePolicy\", src)) {\r\n        namenode.setStoragePolicy(src, policyName);\r\n    } catch (RemoteException e) {\r\n        throw e.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, SafeModeException.class, NSQuotaExceededException.class, UnresolvedPathException.class, SnapshotAccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "unsetStoragePolicy",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void unsetStoragePolicy(String src) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"unsetStoragePolicy\", src)) {\r\n        namenode.unsetStoragePolicy(src);\r\n    } catch (RemoteException e) {\r\n        throw e.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, SafeModeException.class, NSQuotaExceededException.class, UnresolvedPathException.class, SnapshotAccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStoragePolicy",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "BlockStoragePolicy getStoragePolicy(String path) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"getStoragePolicy\", path)) {\r\n        return namenode.getStoragePolicy(path);\r\n    } catch (RemoteException e) {\r\n        throw e.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, SafeModeException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStoragePolicies",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "BlockStoragePolicy[] getStoragePolicies() throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"getStoragePolicies\")) {\r\n        return namenode.getStoragePolicies();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "rename",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean rename(String src, String dst) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newSrcDstTraceScope(\"rename\", src, dst)) {\r\n        return namenode.rename(src, dst);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, NSQuotaExceededException.class, DSQuotaExceededException.class, QuotaByStorageTypeExceededException.class, UnresolvedPathException.class, SnapshotAccessControlException.class, ParentNotDirectoryException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "concat",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void concat(String trg, String[] srcs) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"concat\")) {\r\n        namenode.concat(trg, srcs);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, UnresolvedPathException.class, SnapshotAccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "rename",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void rename(String src, String dst, Options.Rename... options) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newSrcDstTraceScope(\"rename2\", src, dst)) {\r\n        namenode.rename2(src, dst, options);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, DSQuotaExceededException.class, QuotaByStorageTypeExceededException.class, FileAlreadyExistsException.class, FileNotFoundException.class, ParentNotDirectoryException.class, SafeModeException.class, NSQuotaExceededException.class, UnresolvedPathException.class, SnapshotAccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "truncate",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean truncate(String src, long newLength) throws IOException\n{\r\n    checkOpen();\r\n    if (newLength < 0) {\r\n        throw new HadoopIllegalArgumentException(\"Cannot truncate to a negative file size: \" + newLength + \".\");\r\n    }\r\n    try (TraceScope ignored = newPathTraceScope(\"truncate\", src)) {\r\n        return namenode.truncate(src, newLength, clientName);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean delete(String src) throws IOException\n{\r\n    checkOpen();\r\n    return delete(src, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "delete",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean delete(String src, boolean recursive) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"delete\", src)) {\r\n        return namenode.delete(src, recursive);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, SafeModeException.class, UnresolvedPathException.class, SnapshotAccessControlException.class, PathIsNotEmptyDirectoryException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "exists",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean exists(String src) throws IOException\n{\r\n    checkOpen();\r\n    return getFileInfo(src) != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DirectoryListing listPaths(String src, byte[] startAfter) throws IOException\n{\r\n    return listPaths(src, startAfter, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listPaths",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "DirectoryListing listPaths(String src, byte[] startAfter, boolean needLocation) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"listPaths\", src)) {\r\n        return namenode.getListing(src, startAfter, needLocation);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "batchedListPaths",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "BatchedDirectoryListing batchedListPaths(String[] srcs, byte[] startAfter, boolean needLocation) throws IOException\n{\r\n    checkOpen();\r\n    try {\r\n        return namenode.getBatchedListing(srcs, startAfter, needLocation);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileInfo",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HdfsFileStatus getFileInfo(String src) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"getFileInfo\", src)) {\r\n        return namenode.getFileInfo(src);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLocatedFileInfo",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HdfsLocatedFileStatus getLocatedFileInfo(String src, boolean needBlockToken) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"getLocatedFileInfo\", src)) {\r\n        return namenode.getLocatedFileInfo(src, needBlockToken);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isFileClosed",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean isFileClosed(String src) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"isFileClosed\", src)) {\r\n        return namenode.isFileClosed(src);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileLinkInfo",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HdfsFileStatus getFileLinkInfo(String src) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"getFileLinkInfo\", src)) {\r\n        return namenode.getFileLinkInfo(src);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "clearDataEncryptionKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearDataEncryptionKey()\n{\r\n    LOG.debug(\"Clearing encryption key\");\r\n    synchronized (this) {\r\n        encryptionKey = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "shouldEncryptData",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean shouldEncryptData() throws IOException\n{\r\n    FsServerDefaults d = getServerDefaults();\r\n    return d != null && d.getEncryptDataTransfer();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "newDataEncryptionKey",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "DataEncryptionKey newDataEncryptionKey() throws IOException\n{\r\n    if (shouldEncryptData()) {\r\n        synchronized (this) {\r\n            if (encryptionKey == null || encryptionKey.expiryDate < Time.now()) {\r\n                LOG.debug(\"Getting new encryption token from NN\");\r\n                encryptionKey = namenode.getDataEncryptionKey();\r\n            }\r\n            return encryptionKey;\r\n        }\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getEncryptionKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DataEncryptionKey getEncryptionKey()\n{\r\n    return encryptionKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileChecksumInternal",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "FileChecksum getFileChecksumInternal(String src, long length, ChecksumCombineMode combineMode) throws IOException\n{\r\n    checkOpen();\r\n    Preconditions.checkArgument(length >= 0);\r\n    LocatedBlocks blockLocations = null;\r\n    FileChecksumHelper.FileChecksumComputer maker = null;\r\n    ErasureCodingPolicy ecPolicy = null;\r\n    if (length > 0) {\r\n        blockLocations = getBlockLocations(src, length);\r\n        ecPolicy = blockLocations.getErasureCodingPolicy();\r\n    }\r\n    maker = ecPolicy != null ? new FileChecksumHelper.StripedFileNonStripedChecksumComputer(src, length, blockLocations, namenode, this, ecPolicy, combineMode) : new FileChecksumHelper.ReplicatedFileChecksumComputer(src, length, blockLocations, namenode, this, combineMode);\r\n    maker.compute();\r\n    return maker.getFileChecksum();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileChecksumWithCombineMode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileChecksum getFileChecksumWithCombineMode(String src, long length) throws IOException\n{\r\n    ChecksumCombineMode combineMode = getConf().getChecksumCombineMode();\r\n    return getFileChecksumInternal(src, length, combineMode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileChecksum",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MD5MD5CRC32FileChecksum getFileChecksum(String src, long length) throws IOException\n{\r\n    return (MD5MD5CRC32FileChecksum) getFileChecksumInternal(src, length, ChecksumCombineMode.MD5MD5CRC);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBlockLocations",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "LocatedBlocks getBlockLocations(String src, long length) throws IOException\n{\r\n    LocatedBlocks blockLocations = callGetBlockLocations(namenode, src, 0, length);\r\n    if (null == blockLocations) {\r\n        throw new FileNotFoundException(\"File does not exist: \" + src);\r\n    }\r\n    if (blockLocations.isUnderConstruction()) {\r\n        throw new IOException(\"Fail to get checksum, since file \" + src + \" is under construction.\");\r\n    }\r\n    return blockLocations;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "connectToDN",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOStreamPair connectToDN(DatanodeInfo dn, int timeout, Token<BlockTokenIdentifier> blockToken) throws IOException\n{\r\n    return DFSUtilClient.connectToDN(dn, timeout, conf, saslClient, socketFactory, getConf().isConnectToDnViaHostname(), this, blockToken);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "inferChecksumTypeByReading",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Type inferChecksumTypeByReading(LocatedBlock lb, DatanodeInfo dn) throws IOException\n{\r\n    IOStreamPair pair = connectToDN(dn, dfsClientConf.getSocketTimeout(), lb.getBlockToken());\r\n    try {\r\n        new Sender((DataOutputStream) pair.out).readBlock(lb.getBlock(), lb.getBlockToken(), clientName, 0, 1, true, CachingStrategy.newDefaultStrategy());\r\n        final BlockOpResponseProto reply = BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(pair.in));\r\n        String logInfo = \"trying to read \" + lb.getBlock() + \" from datanode \" + dn;\r\n        DataTransferProtoUtil.checkBlockOpStatus(reply, logInfo);\r\n        return PBHelperClient.convert(reply.getReadOpChecksumInfo().getChecksum().getType());\r\n    } finally {\r\n        IOUtilsClient.cleanupWithLogger(LOG, pair.in, pair.out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setPermission",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setPermission(String src, FsPermission permission) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"setPermission\", src)) {\r\n        namenode.setPermission(src, permission);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, SafeModeException.class, UnresolvedPathException.class, SnapshotAccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setOwner",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setOwner(String src, String username, String groupname) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"setOwner\", src)) {\r\n        namenode.setOwner(src, username, groupname);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, SafeModeException.class, UnresolvedPathException.class, SnapshotAccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStateByIndex",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getStateByIndex(int stateIndex) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"getStats\")) {\r\n        long[] states = namenode.getStats();\r\n        return states.length > stateIndex ? states[stateIndex] : -1;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDiskStatus",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "FsStatus getDiskStatus() throws IOException\n{\r\n    try (TraceScope ignored = tracer.newScope(\"getStats\")) {\r\n        long[] states = namenode.getStats();\r\n        return new FsStatus(getStateAtIndex(states, 0), getStateAtIndex(states, 1), getStateAtIndex(states, 2));\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStateAtIndex",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStateAtIndex(long[] states, int index)\n{\r\n    return states.length > index ? states[index] : -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getMissingBlocksCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getMissingBlocksCount() throws IOException\n{\r\n    return getStateByIndex(ClientProtocol.GET_STATS_MISSING_BLOCKS_IDX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getMissingReplOneBlocksCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getMissingReplOneBlocksCount() throws IOException\n{\r\n    return getStateByIndex(ClientProtocol.GET_STATS_MISSING_REPL_ONE_BLOCKS_IDX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getPendingDeletionBlocksCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getPendingDeletionBlocksCount() throws IOException\n{\r\n    return getStateByIndex(ClientProtocol.GET_STATS_PENDING_DELETION_BLOCKS_IDX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLowRedundancyBlocksCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLowRedundancyBlocksCount() throws IOException\n{\r\n    return getStateByIndex(ClientProtocol.GET_STATS_LOW_REDUNDANCY_IDX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getCorruptBlocksCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCorruptBlocksCount() throws IOException\n{\r\n    return getStateByIndex(ClientProtocol.GET_STATS_CORRUPT_BLOCKS_IDX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBytesInFutureBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBytesInFutureBlocks() throws IOException\n{\r\n    return getStateByIndex(ClientProtocol.GET_STATS_BYTES_IN_FUTURE_BLOCKS_IDX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listCorruptFileBlocks",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "CorruptFileBlocks listCorruptFileBlocks(String path, String cookie) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"listCorruptFileBlocks\", path)) {\r\n        return namenode.listCorruptFileBlocks(path, cookie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "datanodeReport",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DatanodeInfo[] datanodeReport(DatanodeReportType type) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"datanodeReport\")) {\r\n        return namenode.getDatanodeReport(type);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDatanodeStorageReport",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DatanodeStorageReport[] getDatanodeStorageReport(DatanodeReportType type) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"datanodeStorageReport\")) {\r\n        return namenode.getDatanodeStorageReport(type);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setSafeMode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean setSafeMode(SafeModeAction action) throws IOException\n{\r\n    checkOpen();\r\n    return setSafeMode(action, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setSafeMode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean setSafeMode(SafeModeAction action, boolean isChecked) throws IOException\n{\r\n    try (TraceScope ignored = tracer.newScope(\"setSafeMode\")) {\r\n        return namenode.setSafeMode(action, isChecked);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createSnapshot",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String createSnapshot(String snapshotRoot, String snapshotName) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"createSnapshot\")) {\r\n        return namenode.createSnapshot(snapshotRoot, snapshotName);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "deleteSnapshot",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void deleteSnapshot(String snapshotRoot, String snapshotName) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"deleteSnapshot\")) {\r\n        namenode.deleteSnapshot(snapshotRoot, snapshotName);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "renameSnapshot",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void renameSnapshot(String snapshotDir, String snapshotOldName, String snapshotNewName) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"renameSnapshot\")) {\r\n        namenode.renameSnapshot(snapshotDir, snapshotOldName, snapshotNewName);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSnapshottableDirListing",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "SnapshottableDirectoryStatus[] getSnapshottableDirListing() throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"getSnapshottableDirListing\")) {\r\n        return namenode.getSnapshottableDirListing();\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSnapshotListing",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "SnapshotStatus[] getSnapshotListing(String snapshotRoot) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"getSnapshotListing\")) {\r\n        return namenode.getSnapshotListing(snapshotRoot);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "allowSnapshot",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void allowSnapshot(String snapshotRoot) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"allowSnapshot\")) {\r\n        namenode.allowSnapshot(snapshotRoot);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "disallowSnapshot",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void disallowSnapshot(String snapshotRoot) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"disallowSnapshot\")) {\r\n        namenode.disallowSnapshot(snapshotRoot);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSnapshotDiffReport",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "SnapshotDiffReport getSnapshotDiffReport(String snapshotDir, String fromSnapshot, String toSnapshot) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"getSnapshotDiffReport\")) {\r\n        Preconditions.checkArgument(fromSnapshot != null, \"null fromSnapshot\");\r\n        Preconditions.checkArgument(toSnapshot != null, \"null toSnapshot\");\r\n        return namenode.getSnapshotDiffReport(snapshotDir, fromSnapshot, toSnapshot);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSnapshotDiffReportListing",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "SnapshotDiffReportListing getSnapshotDiffReportListing(String snapshotDir, String fromSnapshot, String toSnapshot, byte[] startPath, int index) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"getSnapshotDiffReport\")) {\r\n        return namenode.getSnapshotDiffReportListing(snapshotDir, fromSnapshot, toSnapshot, startPath, index);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addCacheDirective",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long addCacheDirective(CacheDirectiveInfo info, EnumSet<CacheFlag> flags) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"addCacheDirective\")) {\r\n        return namenode.addCacheDirective(info, flags);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "modifyCacheDirective",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void modifyCacheDirective(CacheDirectiveInfo info, EnumSet<CacheFlag> flags) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"modifyCacheDirective\")) {\r\n        namenode.modifyCacheDirective(info, flags);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeCacheDirective",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeCacheDirective(long id) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"removeCacheDirective\")) {\r\n        namenode.removeCacheDirective(id);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listCacheDirectives",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<CacheDirectiveEntry> listCacheDirectives(CacheDirectiveInfo filter) throws IOException\n{\r\n    checkOpen();\r\n    return new CacheDirectiveIterator(namenode, filter, tracer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addCachePool",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addCachePool(CachePoolInfo info) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"addCachePool\")) {\r\n        namenode.addCachePool(info);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "modifyCachePool",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void modifyCachePool(CachePoolInfo info) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"modifyCachePool\")) {\r\n        namenode.modifyCachePool(info);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeCachePool",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeCachePool(String poolName) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"removeCachePool\")) {\r\n        namenode.removeCachePool(poolName);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listCachePools",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<CachePoolEntry> listCachePools() throws IOException\n{\r\n    checkOpen();\r\n    return new CachePoolIterator(namenode, tracer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "saveNamespace",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean saveNamespace(long timeWindow, long txGap) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"saveNamespace\")) {\r\n        return namenode.saveNamespace(timeWindow, txGap);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "rollEdits",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long rollEdits() throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"rollEdits\")) {\r\n        return namenode.rollEdits();\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getPreviousBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExtendedBlock getPreviousBlock(long fileId)\n{\r\n    return filesBeingWritten.get(fileId).getBlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "restoreFailedStorage",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean restoreFailedStorage(String arg) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"restoreFailedStorage\")) {\r\n        return namenode.restoreFailedStorage(arg);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "refreshNodes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void refreshNodes() throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"refreshNodes\")) {\r\n        namenode.refreshNodes();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "metaSave",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void metaSave(String pathname) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"metaSave\")) {\r\n        namenode.metaSave(pathname);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setBalancerBandwidth",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setBalancerBandwidth(long bandwidth) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"setBalancerBandwidth\")) {\r\n        namenode.setBalancerBandwidth(bandwidth);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "finalizeUpgrade",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void finalizeUpgrade() throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"finalizeUpgrade\")) {\r\n        namenode.finalizeUpgrade();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "upgradeStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean upgradeStatus() throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"isUpgradeFinalized\")) {\r\n        return namenode.upgradeStatus();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "rollingUpgrade",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RollingUpgradeInfo rollingUpgrade(RollingUpgradeAction action) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"rollingUpgrade\")) {\r\n        return namenode.rollingUpgrade(action);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean mkdirs(String src) throws IOException\n{\r\n    return mkdirs(src, null, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean mkdirs(String src, FsPermission permission, boolean createParent) throws IOException\n{\r\n    final FsPermission masked = applyUMaskDir(permission);\r\n    return primitiveMkdir(src, masked, createParent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "primitiveMkdir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean primitiveMkdir(String src, FsPermission absPermission) throws IOException\n{\r\n    return primitiveMkdir(src, absPermission, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "primitiveMkdir",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean primitiveMkdir(String src, FsPermission absPermission, boolean createParent) throws IOException\n{\r\n    checkOpen();\r\n    if (absPermission == null) {\r\n        absPermission = applyUMaskDir(null);\r\n    }\r\n    LOG.debug(\"{}: masked={}\", src, absPermission);\r\n    try (TraceScope ignored = tracer.newScope(\"mkdir\")) {\r\n        return namenode.mkdirs(src, absPermission, createParent);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, InvalidPathException.class, FileAlreadyExistsException.class, FileNotFoundException.class, ParentNotDirectoryException.class, SafeModeException.class, NSQuotaExceededException.class, DSQuotaExceededException.class, QuotaByStorageTypeExceededException.class, UnresolvedPathException.class, SnapshotAccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getContentSummary",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ContentSummary getContentSummary(String src) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"getContentSummary\", src)) {\r\n        return namenode.getContentSummary(src);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getQuotaUsage",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "QuotaUsage getQuotaUsage(String src) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"getQuotaUsage\", src)) {\r\n        return namenode.getQuotaUsage(src);\r\n    } catch (RemoteException re) {\r\n        IOException ioe = re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, UnresolvedPathException.class, RpcNoSuchMethodException.class);\r\n        if (ioe instanceof RpcNoSuchMethodException) {\r\n            LOG.debug(\"The version of namenode doesn't support getQuotaUsage API.\" + \" Fall back to use getContentSummary API.\");\r\n            return getContentSummary(src);\r\n        } else {\r\n            throw ioe;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setQuota",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setQuota(String src, long namespaceQuota, long storagespaceQuota) throws IOException\n{\r\n    checkOpen();\r\n    if ((namespaceQuota <= 0 && namespaceQuota != HdfsConstants.QUOTA_DONT_SET && namespaceQuota != HdfsConstants.QUOTA_RESET) || (storagespaceQuota < 0 && storagespaceQuota != HdfsConstants.QUOTA_DONT_SET && storagespaceQuota != HdfsConstants.QUOTA_RESET)) {\r\n        throw new IllegalArgumentException(\"Invalid values for quota : \" + namespaceQuota + \" and \" + storagespaceQuota);\r\n    }\r\n    try (TraceScope ignored = newPathTraceScope(\"setQuota\", src)) {\r\n        namenode.setQuota(src, namespaceQuota, storagespaceQuota, null);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, NSQuotaExceededException.class, DSQuotaExceededException.class, QuotaByStorageTypeExceededException.class, UnresolvedPathException.class, SnapshotAccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setQuotaByStorageType",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setQuotaByStorageType(String src, StorageType type, long quota) throws IOException\n{\r\n    checkOpen();\r\n    if (quota <= 0 && quota != HdfsConstants.QUOTA_DONT_SET && quota != HdfsConstants.QUOTA_RESET) {\r\n        throw new IllegalArgumentException(\"Invalid values for quota :\" + quota);\r\n    }\r\n    if (type == null) {\r\n        throw new IllegalArgumentException(\"Invalid storage type(null)\");\r\n    }\r\n    if (!type.supportTypeQuota()) {\r\n        throw new IllegalArgumentException(\"Don't support Quota for storage type : \" + type.toString());\r\n    }\r\n    try (TraceScope ignored = newPathTraceScope(\"setQuotaByStorageType\", src)) {\r\n        namenode.setQuota(src, HdfsConstants.QUOTA_DONT_SET, quota, type);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, QuotaByStorageTypeExceededException.class, UnresolvedPathException.class, SnapshotAccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setTimes",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setTimes(String src, long mtime, long atime) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"setTimes\", src)) {\r\n        namenode.setTimes(src, mtime, atime);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, UnresolvedPathException.class, SnapshotAccessControlException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "reportChecksumFailure",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void reportChecksumFailure(String file, LocatedBlock[] lblocks)\n{\r\n    try {\r\n        reportBadBlocks(lblocks);\r\n    } catch (IOException ie) {\r\n        LOG.info(\"Found corruption while reading \" + file + \". Error repairing corrupt blocks. Bad blocks remain.\", ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return getClass().getSimpleName() + \"[clientName=\" + clientName + \", ugi=\" + ugi + \"]\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDefaultReadCachingStrategy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CachingStrategy getDefaultReadCachingStrategy()\n{\r\n    return defaultReadCachingStrategy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDefaultWriteCachingStrategy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CachingStrategy getDefaultWriteCachingStrategy()\n{\r\n    return defaultWriteCachingStrategy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getClientContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientContext getClientContext()\n{\r\n    return clientContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "modifyAclEntries",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void modifyAclEntries(String src, List<AclEntry> aclSpec) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"modifyAclEntries\", src)) {\r\n        namenode.modifyAclEntries(src, aclSpec);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, AclException.class, FileNotFoundException.class, NSQuotaExceededException.class, SafeModeException.class, SnapshotAccessControlException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeAclEntries",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeAclEntries(String src, List<AclEntry> aclSpec) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"removeAclEntries\")) {\r\n        namenode.removeAclEntries(src, aclSpec);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, AclException.class, FileNotFoundException.class, NSQuotaExceededException.class, SafeModeException.class, SnapshotAccessControlException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeDefaultAcl",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeDefaultAcl(String src) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"removeDefaultAcl\")) {\r\n        namenode.removeDefaultAcl(src);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, AclException.class, FileNotFoundException.class, NSQuotaExceededException.class, SafeModeException.class, SnapshotAccessControlException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeAcl",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeAcl(String src) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"removeAcl\")) {\r\n        namenode.removeAcl(src);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, AclException.class, FileNotFoundException.class, NSQuotaExceededException.class, SafeModeException.class, SnapshotAccessControlException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setAcl",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setAcl(String src, List<AclEntry> aclSpec) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"setAcl\")) {\r\n        namenode.setAcl(src, aclSpec);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, AclException.class, FileNotFoundException.class, NSQuotaExceededException.class, SafeModeException.class, SnapshotAccessControlException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAclStatus",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AclStatus getAclStatus(String src) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"getAclStatus\", src)) {\r\n        return namenode.getAclStatus(src);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, AclException.class, FileNotFoundException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createEncryptionZone",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createEncryptionZone(String src, String keyName) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"createEncryptionZone\", src)) {\r\n        namenode.createEncryptionZone(src, keyName);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, SafeModeException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getEZForPath",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "EncryptionZone getEZForPath(String src) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"getEZForPath\", src)) {\r\n        return namenode.getEZForPath(src);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listEncryptionZones",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<EncryptionZone> listEncryptionZones() throws IOException\n{\r\n    checkOpen();\r\n    return new EncryptionZoneIterator(namenode, tracer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "reencryptEncryptionZone",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void reencryptEncryptionZone(String zone, ReencryptAction action) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"reencryptEncryptionZone\", zone)) {\r\n        namenode.reencryptEncryptionZone(zone, action);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, SafeModeException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listReencryptionStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<ZoneReencryptionStatus> listReencryptionStatus() throws IOException\n{\r\n    checkOpen();\r\n    return new ReencryptionStatusIterator(namenode, tracer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setErasureCodingPolicy",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setErasureCodingPolicy(String src, String ecPolicyName) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"setErasureCodingPolicy\", src)) {\r\n        namenode.setErasureCodingPolicy(src, ecPolicyName);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, SafeModeException.class, UnresolvedPathException.class, FileNotFoundException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "unsetErasureCodingPolicy",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void unsetErasureCodingPolicy(String src) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"unsetErasureCodingPolicy\", src)) {\r\n        namenode.unsetErasureCodingPolicy(src);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, SafeModeException.class, UnresolvedPathException.class, FileNotFoundException.class, NoECPolicySetException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getECTopologyResultForPolicies",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ECTopologyVerifierResult getECTopologyResultForPolicies(final String... policyNames) throws IOException\n{\r\n    checkOpen();\r\n    try {\r\n        return namenode.getECTopologyResultForPolicies(policyNames);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, SafeModeException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setXAttr",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setXAttr(String src, String name, byte[] value, EnumSet<XAttrSetFlag> flag) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"setXAttr\", src)) {\r\n        namenode.setXAttr(src, XAttrHelper.buildXAttr(name, value), flag);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, NSQuotaExceededException.class, SafeModeException.class, SnapshotAccessControlException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getXAttr",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "byte[] getXAttr(String src, String name) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"getXAttr\", src)) {\r\n        final List<XAttr> xAttrs = XAttrHelper.buildXAttrAsList(name);\r\n        final List<XAttr> result = namenode.getXAttrs(src, xAttrs);\r\n        return XAttrHelper.getFirstXAttrValue(result);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getXAttrs",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(String src) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"getXAttrs\", src)) {\r\n        return XAttrHelper.buildXAttrMap(namenode.getXAttrs(src, null));\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getXAttrs",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(String src, List<String> names) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"getXAttrs\", src)) {\r\n        return XAttrHelper.buildXAttrMap(namenode.getXAttrs(src, XAttrHelper.buildXAttrs(names)));\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listXAttrs",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<String> listXAttrs(String src) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"listXAttrs\", src)) {\r\n        final Map<String, byte[]> xattrs = XAttrHelper.buildXAttrMap(namenode.listXAttrs(src));\r\n        return Lists.newArrayList(xattrs.keySet());\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeXAttr",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeXAttr(String src, String name) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"removeXAttr\", src)) {\r\n        namenode.removeXAttr(src, XAttrHelper.buildXAttr(name));\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, NSQuotaExceededException.class, SafeModeException.class, SnapshotAccessControlException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkAccess",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void checkAccess(String src, FsAction mode) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"checkAccess\", src)) {\r\n        namenode.checkAccess(src, mode);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getErasureCodingPolicies",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ErasureCodingPolicyInfo[] getErasureCodingPolicies() throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"getErasureCodingPolicies\")) {\r\n        return namenode.getErasureCodingPolicies();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getErasureCodingCodecs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<String, String> getErasureCodingCodecs() throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"getErasureCodingCodecs\")) {\r\n        return namenode.getErasureCodingCodecs();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addErasureCodingPolicies",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AddErasureCodingPolicyResponse[] addErasureCodingPolicies(ErasureCodingPolicy[] policies) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"addErasureCodingPolicies\")) {\r\n        return namenode.addErasureCodingPolicies(policies);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, SafeModeException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeErasureCodingPolicy",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"removeErasureCodingPolicy\")) {\r\n        namenode.removeErasureCodingPolicy(ecPolicyName);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, SafeModeException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "enableErasureCodingPolicy",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void enableErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"enableErasureCodingPolicy\")) {\r\n        namenode.enableErasureCodingPolicy(ecPolicyName);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, SafeModeException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "disableErasureCodingPolicy",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void disableErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = tracer.newScope(\"disableErasureCodingPolicy\")) {\r\n        namenode.disableErasureCodingPolicy(ecPolicyName);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, SafeModeException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getInotifyEventStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DFSInotifyEventInputStream getInotifyEventStream() throws IOException\n{\r\n    checkOpen();\r\n    return new DFSInotifyEventInputStream(namenode, tracer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getInotifyEventStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DFSInotifyEventInputStream getInotifyEventStream(long lastReadTxid) throws IOException\n{\r\n    checkOpen();\r\n    return new DFSInotifyEventInputStream(namenode, tracer, lastReadTxid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "newConnectedPeer",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Peer newConnectedPeer(InetSocketAddress addr, Token<BlockTokenIdentifier> blockToken, DatanodeID datanodeId) throws IOException\n{\r\n    Peer peer = null;\r\n    boolean success = false;\r\n    Socket sock = null;\r\n    final int socketTimeout = dfsClientConf.getSocketTimeout();\r\n    try {\r\n        sock = socketFactory.createSocket();\r\n        NetUtils.connect(sock, addr, getRandomLocalInterfaceAddr(), socketTimeout);\r\n        peer = DFSUtilClient.peerFromSocketAndKey(saslClient, sock, this, blockToken, datanodeId, socketTimeout);\r\n        success = true;\r\n        return peer;\r\n    } finally {\r\n        if (!success) {\r\n            IOUtilsClient.cleanupWithLogger(LOG, peer);\r\n            IOUtils.closeSocket(sock);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "updateFileSystemReadStats",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateFileSystemReadStats(int distance, int nRead)\n{\r\n    if (stats != null) {\r\n        stats.incrementBytesRead(nRead);\r\n        stats.incrementBytesReadByDistance(distance, nRead);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "updateFileSystemECReadStats",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateFileSystemECReadStats(int nRead)\n{\r\n    if (stats != null) {\r\n        stats.incrementBytesReadErasureCoded(nRead);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "initThreadsNumForHedgedReads",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void initThreadsNumForHedgedReads(int num)\n{\r\n    if (num <= 0 || HEDGED_READ_THREAD_POOL != null)\r\n        return;\r\n    HEDGED_READ_THREAD_POOL = new ThreadPoolExecutor(1, num, 60, TimeUnit.SECONDS, new SynchronousQueue<Runnable>(), new Daemon.DaemonFactory() {\r\n\r\n        private final AtomicInteger threadIndex = new AtomicInteger(0);\r\n\r\n        @Override\r\n        public Thread newThread(Runnable r) {\r\n            Thread t = super.newThread(r);\r\n            t.setName(\"hedgedRead-\" + threadIndex.getAndIncrement());\r\n            return t;\r\n        }\r\n    }, new ThreadPoolExecutor.CallerRunsPolicy() {\r\n\r\n        @Override\r\n        public void rejectedExecution(Runnable runnable, ThreadPoolExecutor e) {\r\n            LOG.info(\"Execution rejected, Executing in current thread\");\r\n            HEDGED_READ_METRIC.incHedgedReadOpsInCurThread();\r\n            super.rejectedExecution(runnable, e);\r\n        }\r\n    });\r\n    HEDGED_READ_THREAD_POOL.allowCoreThreadTimeOut(true);\r\n    LOG.debug(\"Using hedged reads; pool threads={}\", num);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "initThreadsNumForStripedReads",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initThreadsNumForStripedReads(int numThreads)\n{\r\n    assert numThreads > 0;\r\n    if (STRIPED_READ_THREAD_POOL != null) {\r\n        return;\r\n    }\r\n    synchronized (DFSClient.class) {\r\n        if (STRIPED_READ_THREAD_POOL == null) {\r\n            ThreadPoolExecutor threadPool = DFSUtilClient.getThreadPoolExecutor(1, numThreads, 60, \"StripedRead-\", true);\r\n            threadPool.allowCoreThreadTimeOut(true);\r\n            STRIPED_READ_THREAD_POOL = threadPool;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getHedgedReadsThreadPool",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ThreadPoolExecutor getHedgedReadsThreadPool()\n{\r\n    return HEDGED_READ_THREAD_POOL;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStripedReadsThreadPool",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ThreadPoolExecutor getStripedReadsThreadPool()\n{\r\n    return STRIPED_READ_THREAD_POOL;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isHedgedReadsEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isHedgedReadsEnabled()\n{\r\n    return (HEDGED_READ_THREAD_POOL != null) && HEDGED_READ_THREAD_POOL.getMaximumPoolSize() > 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getHedgedReadMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DFSHedgedReadMetrics getHedgedReadMetrics()\n{\r\n    return HEDGED_READ_METRIC;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getKeyProviderUri",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI getKeyProviderUri() throws IOException\n{\r\n    return HdfsKMSUtil.getKeyProviderUri(ugi, namenodeUri, getServerDefaults().getKeyProviderUri(), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getKeyProvider",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "KeyProvider getKeyProvider() throws IOException\n{\r\n    return clientContext.getKeyProviderCache().get(conf, getKeyProviderUri());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setKeyProvider",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setKeyProvider(KeyProvider provider)\n{\r\n    clientContext.getKeyProviderCache().setKeyProvider(conf, provider);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isHDFSEncryptionEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isHDFSEncryptionEnabled() throws IOException\n{\r\n    return getKeyProviderUri() != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isEZRoot",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isEZRoot(Path p) throws IOException\n{\r\n    EncryptionZone ez = getEZForPath(p.toUri().getPath());\r\n    if (ez == null) {\r\n        return false;\r\n    }\r\n    return ez.getPath().equals(p.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isSnapshotTrashRootEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSnapshotTrashRootEnabled() throws IOException\n{\r\n    return getServerDefaults().getSnapshotTrashRootEnabled();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSnapshotRoot",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getSnapshotRoot(Path path) throws IOException\n{\r\n    SnapshottableDirectoryStatus[] dirStatusList = getSnapshottableDirListing();\r\n    if (dirStatusList == null) {\r\n        return null;\r\n    }\r\n    for (SnapshottableDirectoryStatus dirStatus : dirStatusList) {\r\n        String currDir = dirStatus.getFullPath().toString();\r\n        if (!currDir.endsWith(Path.SEPARATOR)) {\r\n            currDir += Path.SEPARATOR;\r\n        }\r\n        if (path.toUri().getPath().startsWith(currDir)) {\r\n            return currDir;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSaslDataTransferClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SaslDataTransferClient getSaslDataTransferClient()\n{\r\n    return saslClient;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "newPathTraceScope",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TraceScope newPathTraceScope(String description, String path)\n{\r\n    TraceScope scope = tracer.newScope(description);\r\n    if (path != null) {\r\n        scope.addKVAnnotation(\"path\", path);\r\n    }\r\n    return scope;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "newSrcDstTraceScope",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "TraceScope newSrcDstTraceScope(String description, String src, String dst)\n{\r\n    TraceScope scope = tracer.newScope(description);\r\n    if (src != null) {\r\n        scope.addKVAnnotation(\"src\", src);\r\n    }\r\n    if (dst != null) {\r\n        scope.addKVAnnotation(\"dst\", dst);\r\n    }\r\n    return scope;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getErasureCodingPolicy",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ErasureCodingPolicy getErasureCodingPolicy(String src) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"getErasureCodingPolicy\", src)) {\r\n        return namenode.getErasureCodingPolicy(src);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(FileNotFoundException.class, AccessControlException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "satisfyStoragePolicy",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void satisfyStoragePolicy(String src) throws IOException\n{\r\n    checkOpen();\r\n    try (TraceScope ignored = newPathTraceScope(\"satisfyStoragePolicy\", src)) {\r\n        namenode.satisfyStoragePolicy(src);\r\n    } catch (RemoteException re) {\r\n        throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class, SafeModeException.class, UnresolvedPathException.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getTracer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Tracer getTracer()\n{\r\n    return tracer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<OpenFileEntry> listOpenFiles() throws IOException\n{\r\n    checkOpen();\r\n    return listOpenFiles(EnumSet.of(OpenFilesType.ALL_OPEN_FILES), OpenFilesIterator.FILTER_PATH_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<OpenFileEntry> listOpenFiles(String path) throws IOException\n{\r\n    checkOpen();\r\n    return listOpenFiles(EnumSet.of(OpenFilesType.ALL_OPEN_FILES), path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<OpenFileEntry> listOpenFiles(EnumSet<OpenFilesType> openFilesTypes) throws IOException\n{\r\n    checkOpen();\r\n    return listOpenFiles(openFilesTypes, OpenFilesIterator.FILTER_PATH_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<OpenFileEntry> listOpenFiles(EnumSet<OpenFilesType> openFilesTypes, String path) throws IOException\n{\r\n    checkOpen();\r\n    return new OpenFilesIterator(namenode, tracer, openFilesTypes, path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "msync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void msync() throws IOException\n{\r\n    namenode.msync();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getHAServiceState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HAServiceProtocol.HAServiceState getHAServiceState() throws IOException\n{\r\n    return namenode.getHAServiceState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDeadNodes",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ConcurrentHashMap<DatanodeInfo, DatanodeInfo> getDeadNodes(DFSInputStream dfsInputStream)\n{\r\n    if (clientContext.isDeadNodeDetectionEnabled()) {\r\n        ConcurrentHashMap<DatanodeInfo, DatanodeInfo> deadNodes = new ConcurrentHashMap<DatanodeInfo, DatanodeInfo>();\r\n        if (dfsInputStream != null) {\r\n            deadNodes.putAll(dfsInputStream.getLocalDeadNodes());\r\n        }\r\n        Set<DatanodeInfo> detectDeadNodes = clientContext.getDeadNodeDetector().clearAndGetDetectedDeadNodes();\r\n        for (DatanodeInfo detectDeadNode : detectDeadNodes) {\r\n            deadNodes.put(detectDeadNode, detectDeadNode);\r\n        }\r\n        return deadNodes;\r\n    } else {\r\n        return dfsInputStream.getLocalDeadNodes();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isDeadNode",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean isDeadNode(DFSInputStream dfsInputStream, DatanodeInfo datanodeInfo)\n{\r\n    if (isDeadNodeDetectionEnabled()) {\r\n        boolean isDeadNode = clientContext.getDeadNodeDetector().isDeadNode(datanodeInfo);\r\n        if (dfsInputStream != null) {\r\n            isDeadNode = isDeadNode || dfsInputStream.getLocalDeadNodes().contains(datanodeInfo);\r\n        }\r\n        return isDeadNode;\r\n    } else {\r\n        return dfsInputStream.getLocalDeadNodes().contains(datanodeInfo);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addNodeToDeadNodeDetector",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addNodeToDeadNodeDetector(DFSInputStream dfsInputStream, DatanodeInfo datanodeInfo)\n{\r\n    if (!isDeadNodeDetectionEnabled()) {\r\n        LOG.debug(\"DeadNode detection is not enabled, skip to add node {}.\", datanodeInfo);\r\n        return;\r\n    }\r\n    clientContext.getDeadNodeDetector().addNodeToDetect(dfsInputStream, datanodeInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeNodeFromDeadNodeDetector",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeNodeFromDeadNodeDetector(DFSInputStream dfsInputStream, DatanodeInfo datanodeInfo)\n{\r\n    if (!isDeadNodeDetectionEnabled()) {\r\n        LOG.debug(\"DeadNode detection is not enabled, skip to remove node {}.\", datanodeInfo);\r\n        return;\r\n    }\r\n    clientContext.getDeadNodeDetector().removeNodeFromDeadNodeDetector(dfsInputStream, datanodeInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeNodeFromDeadNodeDetector",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void removeNodeFromDeadNodeDetector(DFSInputStream dfsInputStream, LocatedBlocks locatedBlocks)\n{\r\n    if (!isDeadNodeDetectionEnabled() || locatedBlocks == null) {\r\n        LOG.debug(\"DeadNode detection is not enabled or given block {} \" + \"is null, skip to remove node.\", locatedBlocks);\r\n        return;\r\n    }\r\n    for (LocatedBlock locatedBlock : locatedBlocks.getLocatedBlocks()) {\r\n        for (DatanodeInfo datanodeInfo : locatedBlock.getLocations()) {\r\n            removeNodeFromDeadNodeDetector(dfsInputStream, datanodeInfo);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isDeadNodeDetectionEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isDeadNodeDetectionEnabled()\n{\r\n    return clientContext.isDeadNodeDetectionEnabled();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDeadNodeDetector",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DeadNodeDetector getDeadNodeDetector()\n{\r\n    return clientContext.getDeadNodeDetector();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLocatedBlockRefresher",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocatedBlocksRefresher getLocatedBlockRefresher()\n{\r\n    return clientContext.getLocatedBlocksRefresher();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addLocatedBlocksRefresh",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addLocatedBlocksRefresh(DFSInputStream dfsInputStream)\n{\r\n    if (isLocatedBlocksRefresherEnabled()) {\r\n        clientContext.getLocatedBlocksRefresher().addInputStream(dfsInputStream);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeLocatedBlocksRefresh",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void removeLocatedBlocksRefresh(DFSInputStream dfsInputStream)\n{\r\n    if (isLocatedBlocksRefresherEnabled()) {\r\n        clientContext.getLocatedBlocksRefresher().removeInputStream(dfsInputStream);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isLocatedBlocksRefresherEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isLocatedBlocksRefresherEnabled()\n{\r\n    return clientContext.isLocatedBlocksRefresherEnabled();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getInterface",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<T> getInterface()\n{\r\n    return proxyProvider.getInterface();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getProxy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ProxyInfo<T> getProxy()\n{\r\n    return proxyProvider.getProxy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "performFailover",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void performFailover(T currentProxy)\n{\r\n    proxyProvider.performFailover(currentProxy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    proxyProvider.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "useLogicalURI",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean useLogicalURI()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "useLogicalURI",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean useLogicalURI()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "setFallbackToSimpleAuth",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFallbackToSimpleAuth(AtomicBoolean fallbackToSimpleAuth)\n{\r\n    this.fallbackToSimpleAuth = fallbackToSimpleAuth;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getFallbackToSimpleAuth",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AtomicBoolean getFallbackToSimpleAuth()\n{\r\n    return fallbackToSimpleAuth;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getInterface",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<T> getInterface()\n{\r\n    return xface;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "createProxyIfNeeded",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "NNProxyInfo<T> createProxyIfNeeded(NNProxyInfo<T> pi)\n{\r\n    if (pi.proxy == null) {\r\n        assert pi.getAddress() != null : \"Proxy address is null\";\r\n        try {\r\n            pi.proxy = factory.createProxy(conf, pi.getAddress(), xface, ugi, false, getFallbackToSimpleAuth());\r\n        } catch (IOException ioe) {\r\n            LOG.error(\"{} Failed to create RPC proxy to NameNode at {}\", this.getClass().getSimpleName(), pi.address, ioe);\r\n            throw new RuntimeException(ioe);\r\n        }\r\n    }\r\n    return pi;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getProxyAddresses",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "List<NNProxyInfo<T>> getProxyAddresses(URI uri, String addressKey)\n{\r\n    final List<NNProxyInfo<T>> proxies = new ArrayList<NNProxyInfo<T>>();\r\n    Map<String, Map<String, InetSocketAddress>> map = DFSUtilClient.getAddresses(conf, null, addressKey);\r\n    Map<String, InetSocketAddress> addressesInNN = map.get(uri.getHost());\r\n    if (addressesInNN == null || addressesInNN.size() == 0) {\r\n        throw new RuntimeException(\"Could not find any configured addresses \" + \"for URI \" + uri);\r\n    }\r\n    Collection<InetSocketAddress> addressesOfNns = addressesInNN.values();\r\n    try {\r\n        addressesOfNns = getResolvedHostsIfNecessary(addressesOfNns, uri);\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n    for (InetSocketAddress address : addressesOfNns) {\r\n        proxies.add(new NNProxyInfo<T>(address));\r\n    }\r\n    boolean randomized = getRandomOrder(conf, uri);\r\n    if (randomized) {\r\n        Collections.shuffle(proxies);\r\n    }\r\n    HAUtilClient.cloneDelegationTokenForLogicalUri(ugi, uri, addressesOfNns);\r\n    return proxies;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getResolvedHostsIfNecessary",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Collection<InetSocketAddress> getResolvedHostsIfNecessary(Collection<InetSocketAddress> addressesOfNns, URI nameNodeUri) throws IOException\n{\r\n    String host = nameNodeUri.getHost();\r\n    String configKeyWithHost = HdfsClientConfigKeys.Failover.RESOLVE_ADDRESS_NEEDED_KEY + \".\" + host;\r\n    boolean resolveNeeded = conf.getBoolean(configKeyWithHost, HdfsClientConfigKeys.Failover.RESOLVE_ADDRESS_NEEDED_DEFAULT);\r\n    if (!resolveNeeded) {\r\n        return addressesOfNns;\r\n    }\r\n    String useFQDNKeyWithHost = HdfsClientConfigKeys.Failover.RESOLVE_ADDRESS_TO_FQDN + \".\" + host;\r\n    boolean requireFQDN = conf.getBoolean(useFQDNKeyWithHost, HdfsClientConfigKeys.Failover.RESOLVE_ADDRESS_TO_FQDN_DEFAULT);\r\n    Collection<InetSocketAddress> addressOfResolvedNns = new ArrayList<>();\r\n    DomainNameResolver dnr = DomainNameResolverFactory.newInstance(conf, nameNodeUri, HdfsClientConfigKeys.Failover.RESOLVE_SERVICE_KEY);\r\n    LOG.debug(\"Namenode domain name will be resolved with {}\", dnr.getClass().getName());\r\n    for (InetSocketAddress address : addressesOfNns) {\r\n        String[] resolvedHostNames = dnr.getAllResolvedHostnameByDomainName(address.getHostName(), requireFQDN);\r\n        int port = address.getPort();\r\n        for (String hostname : resolvedHostNames) {\r\n            InetSocketAddress resolvedAddress = new InetSocketAddress(hostname, port);\r\n            addressOfResolvedNns.add(resolvedAddress);\r\n        }\r\n    }\r\n    return addressOfResolvedNns;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getRandomOrder",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean getRandomOrder(Configuration conf, URI nameNodeUri)\n{\r\n    String host = nameNodeUri.getHost();\r\n    String configKeyWithHost = HdfsClientConfigKeys.Failover.RANDOM_ORDER + \".\" + host;\r\n    if (conf.get(configKeyWithHost) != null) {\r\n        return conf.getBoolean(configKeyWithHost, HdfsClientConfigKeys.Failover.RANDOM_ORDER_DEFAULT);\r\n    }\r\n    return conf.getBoolean(HdfsClientConfigKeys.Failover.RANDOM_ORDER, HdfsClientConfigKeys.Failover.RANDOM_ORDER_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toExtendedShort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "short toExtendedShort()\n{\r\n    return (short) (toShort() | (aclBit ? ACL_BIT : 0) | (encryptedBit ? ENCRYPTED_BIT : 0) | (erasureCodedBit ? ERASURE_CODED_BIT : 0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getAclBit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getAclBit()\n{\r\n    return aclBit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getEncryptedBit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getEncryptedBit()\n{\r\n    return encryptedBit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getErasureCodedBit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getErasureCodedBit()\n{\r\n    return erasureCodedBit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    return super.equals(o);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return super.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "loadPolicy",
  "errType" : [ "ParserConfigurationException|IOException|SAXException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<ErasureCodingPolicy> loadPolicy(String policyFilePath)\n{\r\n    try {\r\n        File policyFile = getPolicyFile(policyFilePath);\r\n        if (!policyFile.exists()) {\r\n            LOG.warn(\"Not found any EC policy file\");\r\n            return Collections.emptyList();\r\n        }\r\n        return loadECPolicies(policyFile);\r\n    } catch (ParserConfigurationException | IOException | SAXException e) {\r\n        throw new RuntimeException(\"Failed to load EC policy file: \" + policyFilePath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "loadECPolicies",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "List<ErasureCodingPolicy> loadECPolicies(File policyFile) throws ParserConfigurationException, IOException, SAXException\n{\r\n    LOG.info(\"Loading EC policy file \" + policyFile);\r\n    DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n    dbf.setIgnoringComments(true);\r\n    DocumentBuilder builder = dbf.newDocumentBuilder();\r\n    Document doc = builder.parse(policyFile);\r\n    Element root = doc.getDocumentElement();\r\n    if (!\"configuration\".equals(root.getTagName())) {\r\n        throw new RuntimeException(\"Bad EC policy configuration file: \" + \"top-level element not <configuration>\");\r\n    }\r\n    List<ErasureCodingPolicy> policies;\r\n    if (root.getElementsByTagName(\"layoutversion\").getLength() > 0) {\r\n        if (loadLayoutVersion(root) == LAYOUT_VERSION) {\r\n            if (root.getElementsByTagName(\"schemas\").getLength() > 0) {\r\n                Map<String, ECSchema> schemas = loadSchemas(root);\r\n                if (root.getElementsByTagName(\"policies\").getLength() > 0) {\r\n                    policies = loadPolicies(root, schemas);\r\n                } else {\r\n                    throw new RuntimeException(\"Bad EC policy configuration file: \" + \"no <policies> element\");\r\n                }\r\n            } else {\r\n                throw new RuntimeException(\"Bad EC policy configuration file: \" + \"no <schemas> element\");\r\n            }\r\n        } else {\r\n            throw new RuntimeException(\"The parse failed because of \" + \"bad layoutversion value\");\r\n        }\r\n    } else {\r\n        throw new RuntimeException(\"Bad EC policy configuration file: \" + \"no <layoutVersion> element\");\r\n    }\r\n    return policies;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "loadLayoutVersion",
  "errType" : [ "NumberFormatException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int loadLayoutVersion(Element root)\n{\r\n    int layoutVersion;\r\n    Text text = (Text) root.getElementsByTagName(\"layoutversion\").item(0).getFirstChild();\r\n    if (text != null) {\r\n        String value = text.getData().trim();\r\n        try {\r\n            layoutVersion = Integer.parseInt(value);\r\n        } catch (NumberFormatException e) {\r\n            throw new IllegalArgumentException(\"Bad layoutVersion value \" + value + \" is found. It should be an integer\");\r\n        }\r\n    } else {\r\n        throw new IllegalArgumentException(\"Value of <layoutVersion> is null\");\r\n    }\r\n    return layoutVersion;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "loadSchemas",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Map<String, ECSchema> loadSchemas(Element root)\n{\r\n    NodeList elements = root.getElementsByTagName(\"schemas\").item(0).getChildNodes();\r\n    Map<String, ECSchema> schemas = new HashMap<String, ECSchema>();\r\n    for (int i = 0; i < elements.getLength(); i++) {\r\n        Node node = elements.item(i);\r\n        if (node instanceof Element) {\r\n            Element element = (Element) node;\r\n            if (\"schema\".equals(element.getTagName())) {\r\n                String schemaId = element.getAttribute(\"id\");\r\n                ECSchema schema = loadSchema(element);\r\n                if (!schemas.containsValue(schema)) {\r\n                    schemas.put(schemaId, schema);\r\n                } else {\r\n                    throw new RuntimeException(\"Repetitive schemas in EC policy\" + \" configuration file: \" + schemaId);\r\n                }\r\n            } else {\r\n                throw new RuntimeException(\"Bad element in EC policy\" + \" configuration file: \" + element.getTagName());\r\n            }\r\n        }\r\n    }\r\n    return schemas;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "loadPolicies",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "List<ErasureCodingPolicy> loadPolicies(Element root, Map<String, ECSchema> schemas)\n{\r\n    NodeList elements = root.getElementsByTagName(\"policies\").item(0).getChildNodes();\r\n    List<ErasureCodingPolicy> policies = new ArrayList<ErasureCodingPolicy>();\r\n    for (int i = 0; i < elements.getLength(); i++) {\r\n        Node node = elements.item(i);\r\n        if (node instanceof Element) {\r\n            Element element = (Element) node;\r\n            if (\"policy\".equals(element.getTagName())) {\r\n                ErasureCodingPolicy policy = loadPolicy(element, schemas);\r\n                if (!policies.contains(policy)) {\r\n                    policies.add(policy);\r\n                } else {\r\n                    LOG.warn(\"Repetitive policies in EC policy configuration file: \" + policy.toString());\r\n                }\r\n            } else {\r\n                throw new RuntimeException(\"Bad element in EC policy configuration\" + \" file: \" + element.getTagName());\r\n            }\r\n        }\r\n    }\r\n    return policies;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "getPolicyFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "File getPolicyFile(String policyFilePath) throws MalformedURLException\n{\r\n    File policyFile = new File(policyFilePath);\r\n    if (!policyFile.isAbsolute()) {\r\n        URL url = new URL(policyFilePath);\r\n        if (!url.getProtocol().equalsIgnoreCase(\"file\")) {\r\n            throw new RuntimeException(\"EC policy file \" + url + \" found on the classpath is not on the local filesystem.\");\r\n        } else {\r\n            policyFile = new File(url.getPath());\r\n        }\r\n    }\r\n    return policyFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "loadSchema",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "ECSchema loadSchema(Element element)\n{\r\n    Map<String, String> schemaOptions = new HashMap<String, String>();\r\n    NodeList fields = element.getChildNodes();\r\n    for (int i = 0; i < fields.getLength(); i++) {\r\n        Node fieldNode = fields.item(i);\r\n        if (fieldNode instanceof Element) {\r\n            Element field = (Element) fieldNode;\r\n            String tagName = field.getTagName();\r\n            if (\"k\".equals(tagName)) {\r\n                tagName = \"numDataUnits\";\r\n            } else if (\"m\".equals(tagName)) {\r\n                tagName = \"numParityUnits\";\r\n            }\r\n            Text text = (Text) field.getFirstChild();\r\n            if (text != null) {\r\n                String value = text.getData().trim();\r\n                schemaOptions.put(tagName, value);\r\n            } else {\r\n                throw new IllegalArgumentException(\"Value of <\" + tagName + \"> is null\");\r\n            }\r\n        }\r\n    }\r\n    return new ECSchema(schemaOptions);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "loadPolicy",
  "errType" : [ "NumberFormatException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "ErasureCodingPolicy loadPolicy(Element element, Map<String, ECSchema> schemas)\n{\r\n    NodeList fields = element.getChildNodes();\r\n    ECSchema schema = null;\r\n    int cellSize = 0;\r\n    for (int i = 0; i < fields.getLength(); i++) {\r\n        Node fieldNode = fields.item(i);\r\n        if (fieldNode instanceof Element) {\r\n            Element field = (Element) fieldNode;\r\n            String tagName = field.getTagName();\r\n            Text text = (Text) field.getFirstChild();\r\n            if (text != null) {\r\n                if (!text.isElementContentWhitespace()) {\r\n                    String value = text.getData().trim();\r\n                    if (\"schema\".equals(tagName)) {\r\n                        schema = schemas.get(value);\r\n                    } else if (\"cellsize\".equals(tagName)) {\r\n                        try {\r\n                            cellSize = Integer.parseInt(value);\r\n                        } catch (NumberFormatException e) {\r\n                            throw new IllegalArgumentException(\"Bad EC policy cellsize\" + \" value \" + value + \" is found. It should be an integer\");\r\n                        }\r\n                    } else {\r\n                        LOG.warn(\"Invalid tagName: \" + tagName);\r\n                    }\r\n                }\r\n            } else {\r\n                throw new IllegalArgumentException(\"Value of <\" + tagName + \"> is null\");\r\n            }\r\n        }\r\n    }\r\n    if (schema != null && cellSize > 0) {\r\n        return new ErasureCodingPolicy(schema, cellSize);\r\n    } else {\r\n        throw new RuntimeException(\"Bad policy is found in\" + \" EC policy configuration file\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "short getValue(final Configuration conf)\n{\r\n    return getValue() != null ? getValue() : (short) conf.getInt(DFS_REPLICATION_KEY, DFS_REPLICATION_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getSoftwareVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSoftwareVersion()\n{\r\n    return this.softwareVersion;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getConfigVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getConfigVersion()\n{\r\n    return this.configVersion;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getUptime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getUptime()\n{\r\n    return this.uptime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getDatanodeLocalReport",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getDatanodeLocalReport()\n{\r\n    return (\"Uptime: \" + getUptime()) + \", Software version: \" + getSoftwareVersion() + \", Config version: \" + getConfigVersion();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return WebHdfsConstants.SWEBHDFS_SCHEME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getTransportScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTransportScheme()\n{\r\n    return \"https\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getTokenKind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getTokenKind()\n{\r\n    return WebHdfsConstants.SWEBHDFS_TOKEN_KIND;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getDefaultPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getDefaultPort()\n{\r\n    return HdfsClientConfigKeys.DFS_NAMENODE_HTTPS_PORT_DEFAULT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SlowPeerReports create(@Nullable Map<String, Double> slowPeers)\n{\r\n    if (slowPeers == null || slowPeers.isEmpty()) {\r\n        return EMPTY_REPORT;\r\n    }\r\n    return new SlowPeerReports(slowPeers);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "getSlowPeers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, Double> getSlowPeers()\n{\r\n    return slowPeers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "haveSlowPeers",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean haveSlowPeers()\n{\r\n    return slowPeers.size() > 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (this == o) {\r\n        return true;\r\n    }\r\n    if (!(o instanceof SlowPeerReports)) {\r\n        return false;\r\n    }\r\n    SlowPeerReports that = (SlowPeerReports) o;\r\n    return slowPeers.equals(that.slowPeers);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return slowPeers.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HttpURLConnection configure(HttpURLConnection conn)\n{\r\n    if (conn instanceof HttpsURLConnection) {\r\n        HttpsURLConnection c = (HttpsURLConnection) conn;\r\n        c.setSSLSocketFactory(sf);\r\n        c.setHostnameVerifier(hv);\r\n    }\r\n    conn.setConnectTimeout(connectTimeout);\r\n    conn.setReadTimeout(readTimeout);\r\n    return conn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void destroy()\n{\r\n    factory.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "setExpiresIn",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setExpiresIn(String expiresIn)\n{\r\n    this.nextRefreshMSSinceEpoch = convertExpiresIn(timer, expiresIn);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "setExpiresInMSSinceEpoch",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setExpiresInMSSinceEpoch(String expiresInMSSinceEpoch)\n{\r\n    this.nextRefreshMSSinceEpoch = Long.parseLong(expiresInMSSinceEpoch);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "getNextRefreshMSSinceEpoch",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNextRefreshMSSinceEpoch()\n{\r\n    return nextRefreshMSSinceEpoch;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "shouldRefresh",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldRefresh()\n{\r\n    long lowerLimit = nextRefreshMSSinceEpoch - EXPIRE_BUFFER_MS;\r\n    long currTime = timer.now();\r\n    return currTime > lowerLimit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "convertExpiresIn",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Long convertExpiresIn(Timer timer, String expiresInSecs)\n{\r\n    long expiresSecs = Long.parseLong(expiresInSecs);\r\n    long expiresMs = expiresSecs * 1000;\r\n    return timer.now() + expiresMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "newBlockReader",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "BlockReaderLocalLegacy newBlockReader(DfsClientConf conf, UserGroupInformation userGroupInformation, Configuration configuration, String file, ExtendedBlock blk, Token<BlockTokenIdentifier> token, DatanodeInfo node, long startOffset, long length, StorageType storageType) throws IOException\n{\r\n    final ShortCircuitConf scConf = conf.getShortCircuitConf();\r\n    LocalDatanodeInfo localDatanodeInfo = getLocalDatanodeInfo(node.getIpcPort());\r\n    BlockLocalPathInfo pathinfo = localDatanodeInfo.getBlockLocalPathInfo(blk);\r\n    if (pathinfo == null) {\r\n        if (userGroupInformation == null) {\r\n            userGroupInformation = UserGroupInformation.getCurrentUser();\r\n        }\r\n        pathinfo = getBlockPathInfo(userGroupInformation, blk, node, configuration, conf.getSocketTimeout(), token, conf.isConnectToDnViaHostname(), storageType);\r\n    }\r\n    FileInputStream dataIn = null;\r\n    FileInputStream checksumIn = null;\r\n    BlockReaderLocalLegacy localBlockReader = null;\r\n    final boolean skipChecksumCheck = scConf.isSkipShortCircuitChecksums() || storageType.isTransient();\r\n    try {\r\n        File blkfile = new File(pathinfo.getBlockPath());\r\n        dataIn = new FileInputStream(blkfile);\r\n        LOG.debug(\"New BlockReaderLocalLegacy for file {} of size {} startOffset \" + \"{} length {} short circuit checksum {}\", blkfile, blkfile.length(), startOffset, length, !skipChecksumCheck);\r\n        if (!skipChecksumCheck) {\r\n            File metafile = new File(pathinfo.getMetaPath());\r\n            checksumIn = new FileInputStream(metafile);\r\n            final DataChecksum checksum = BlockMetadataHeader.readDataChecksum(new DataInputStream(checksumIn), blk);\r\n            long firstChunkOffset = startOffset - (startOffset % checksum.getBytesPerChecksum());\r\n            localBlockReader = new BlockReaderLocalLegacy(scConf, file, blk, startOffset, checksum, true, dataIn, firstChunkOffset, checksumIn);\r\n        } else {\r\n            localBlockReader = new BlockReaderLocalLegacy(scConf, file, blk, startOffset, dataIn);\r\n        }\r\n    } catch (IOException e) {\r\n        localDatanodeInfo.removeBlockLocalPathInfo(blk);\r\n        LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk + \" from cache because local file \" + pathinfo.getBlockPath() + \" could not be opened.\");\r\n        throw e;\r\n    } finally {\r\n        if (localBlockReader == null) {\r\n            if (dataIn != null) {\r\n                dataIn.close();\r\n            }\r\n            if (checksumIn != null) {\r\n                checksumIn.close();\r\n            }\r\n        }\r\n    }\r\n    return localBlockReader;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getLocalDatanodeInfo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "LocalDatanodeInfo getLocalDatanodeInfo(int port)\n{\r\n    LocalDatanodeInfo ldInfo = localDatanodeInfoMap.get(port);\r\n    if (ldInfo == null) {\r\n        ldInfo = new LocalDatanodeInfo();\r\n        localDatanodeInfoMap.put(port, ldInfo);\r\n    }\r\n    return ldInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getBlockPathInfo",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "BlockLocalPathInfo getBlockPathInfo(UserGroupInformation ugi, ExtendedBlock blk, DatanodeInfo node, Configuration conf, int timeout, Token<BlockTokenIdentifier> token, boolean connectToDnViaHostname, StorageType storageType) throws IOException\n{\r\n    LocalDatanodeInfo localDatanodeInfo = getLocalDatanodeInfo(node.getIpcPort());\r\n    BlockLocalPathInfo pathinfo;\r\n    ClientDatanodeProtocol proxy = localDatanodeInfo.getDatanodeProxy(ugi, node, conf, timeout, connectToDnViaHostname);\r\n    try {\r\n        pathinfo = proxy.getBlockLocalPathInfo(blk, token);\r\n        if (pathinfo != null && !storageType.isTransient()) {\r\n            LOG.debug(\"Cached location of block {} as {}\", blk, pathinfo);\r\n            localDatanodeInfo.setBlockLocalPathInfo(blk, pathinfo);\r\n        }\r\n    } catch (IOException e) {\r\n        localDatanodeInfo.resetDatanodeProxy();\r\n        throw e;\r\n    }\r\n    return pathinfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getSlowReadBufferNumChunks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getSlowReadBufferNumChunks(int bufferSizeBytes, int bytesPerChecksum)\n{\r\n    if (bufferSizeBytes < bytesPerChecksum) {\r\n        throw new IllegalArgumentException(\"Configured BlockReaderLocalLegacy \" + \"buffer size (\" + bufferSizeBytes + \") is not large enough to hold \" + \"a single chunk (\" + bytesPerChecksum + \"). Please configure \" + HdfsClientConfigKeys.Read.ShortCircuit.BUFFER_SIZE_KEY + \" appropriately\");\r\n    }\r\n    return bufferSizeBytes / bytesPerChecksum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "fillBuffer",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int fillBuffer(FileInputStream stream, ByteBuffer buf) throws IOException\n{\r\n    int bytesRead = stream.getChannel().read(buf);\r\n    if (bytesRead < 0) {\r\n        return bytesRead;\r\n    }\r\n    while (buf.remaining() > 0) {\r\n        int n = stream.getChannel().read(buf);\r\n        if (n < 0) {\r\n            return bytesRead;\r\n        }\r\n        bytesRead += n;\r\n    }\r\n    return bytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "writeSlice",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeSlice(ByteBuffer from, ByteBuffer to, int length)\n{\r\n    int oldLimit = from.limit();\r\n    from.limit(from.position() + length);\r\n    try {\r\n        to.put(from);\r\n    } finally {\r\n        from.limit(oldLimit);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "int read(ByteBuffer buf) throws IOException\n{\r\n    int nRead = 0;\r\n    if (verifyChecksum) {\r\n        if (slowReadBuff.hasRemaining()) {\r\n            int fromSlowReadBuff = Math.min(buf.remaining(), slowReadBuff.remaining());\r\n            writeSlice(slowReadBuff, buf, fromSlowReadBuff);\r\n            nRead += fromSlowReadBuff;\r\n        }\r\n        if (buf.remaining() >= bytesPerChecksum && offsetFromChunkBoundary == 0) {\r\n            int len = buf.remaining() - (buf.remaining() % bytesPerChecksum);\r\n            len = Math.min(len, slowReadBuff.capacity());\r\n            int oldlimit = buf.limit();\r\n            buf.limit(buf.position() + len);\r\n            int readResult = 0;\r\n            try {\r\n                readResult = doByteBufferRead(buf);\r\n            } finally {\r\n                buf.limit(oldlimit);\r\n            }\r\n            if (readResult == -1) {\r\n                return nRead;\r\n            } else {\r\n                nRead += readResult;\r\n                buf.position(buf.position() + readResult);\r\n            }\r\n        }\r\n        if ((buf.remaining() > 0 && buf.remaining() < bytesPerChecksum) || offsetFromChunkBoundary > 0) {\r\n            int toRead = Math.min(buf.remaining(), bytesPerChecksum - offsetFromChunkBoundary);\r\n            int readResult = fillSlowReadBuffer(toRead);\r\n            if (readResult == -1) {\r\n                return nRead;\r\n            } else {\r\n                int fromSlowReadBuff = Math.min(readResult, buf.remaining());\r\n                writeSlice(slowReadBuff, buf, fromSlowReadBuff);\r\n                nRead += fromSlowReadBuff;\r\n            }\r\n        }\r\n    } else {\r\n        nRead = doByteBufferRead(buf);\r\n        if (nRead > 0) {\r\n            buf.position(buf.position() + nRead);\r\n        }\r\n    }\r\n    return nRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "doByteBufferRead",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "int doByteBufferRead(ByteBuffer buf) throws IOException\n{\r\n    if (verifyChecksum) {\r\n        assert buf.remaining() % bytesPerChecksum == 0;\r\n    }\r\n    int dataRead;\r\n    int oldpos = buf.position();\r\n    dataRead = fillBuffer(dataIn, buf);\r\n    if (dataRead == -1) {\r\n        return -1;\r\n    }\r\n    if (verifyChecksum) {\r\n        ByteBuffer toChecksum = buf.duplicate();\r\n        toChecksum.position(oldpos);\r\n        toChecksum.limit(oldpos + dataRead);\r\n        checksumBuff.clear();\r\n        int numChunks = (toChecksum.remaining() + bytesPerChecksum - 1) / bytesPerChecksum;\r\n        checksumBuff.limit(checksumSize * numChunks);\r\n        fillBuffer(checksumIn, checksumBuff);\r\n        checksumBuff.flip();\r\n        checksum.verifyChunkedSums(toChecksum, checksumBuff, filename, this.startOffset);\r\n    }\r\n    if (dataRead >= 0) {\r\n        buf.position(oldpos + Math.min(offsetFromChunkBoundary, dataRead));\r\n    }\r\n    if (dataRead < offsetFromChunkBoundary) {\r\n        offsetFromChunkBoundary -= dataRead;\r\n        dataRead = 0;\r\n    } else {\r\n        dataRead -= offsetFromChunkBoundary;\r\n        offsetFromChunkBoundary = 0;\r\n    }\r\n    return dataRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "fillSlowReadBuffer",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int fillSlowReadBuffer(int len) throws IOException\n{\r\n    int nRead;\r\n    if (slowReadBuff.hasRemaining()) {\r\n        nRead = Math.min(len, slowReadBuff.remaining());\r\n    } else {\r\n        int nextChunk = len + offsetFromChunkBoundary + (bytesPerChecksum - ((len + offsetFromChunkBoundary) % bytesPerChecksum));\r\n        int limit = Math.min(nextChunk, slowReadBuff.capacity());\r\n        assert limit % bytesPerChecksum == 0;\r\n        slowReadBuff.clear();\r\n        slowReadBuff.limit(limit);\r\n        nRead = doByteBufferRead(slowReadBuff);\r\n        if (nRead > 0) {\r\n            slowReadBuff.limit(nRead + slowReadBuff.position());\r\n        }\r\n    }\r\n    return nRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int read(byte[] buf, int off, int len) throws IOException\n{\r\n    LOG.trace(\"read off {} len {}\", off, len);\r\n    if (!verifyChecksum) {\r\n        return dataIn.read(buf, off, len);\r\n    }\r\n    int nRead = fillSlowReadBuffer(slowReadBuff.capacity());\r\n    if (nRead > 0) {\r\n        nRead = Math.min(len, nRead);\r\n        slowReadBuff.get(buf, off, nRead);\r\n    }\r\n    return nRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "skip",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "long skip(long n) throws IOException\n{\r\n    LOG.debug(\"skip {}\", n);\r\n    if (n <= 0) {\r\n        return 0;\r\n    }\r\n    if (!verifyChecksum) {\r\n        return dataIn.skip(n);\r\n    }\r\n    int remaining = slowReadBuff.remaining();\r\n    int position = slowReadBuff.position();\r\n    int newPosition = position + (int) n;\r\n    if (n <= remaining) {\r\n        assert offsetFromChunkBoundary == 0;\r\n        slowReadBuff.position(newPosition);\r\n        return n;\r\n    }\r\n    if (n - remaining <= bytesPerChecksum) {\r\n        slowReadBuff.position(position + remaining);\r\n        if (skipBuf == null) {\r\n            skipBuf = new byte[bytesPerChecksum];\r\n        }\r\n        int ret = read(skipBuf, 0, (int) (n - remaining));\r\n        return (remaining + ret);\r\n    }\r\n    int myOffsetFromChunkBoundary = newPosition % bytesPerChecksum;\r\n    long toskip = n - remaining - myOffsetFromChunkBoundary;\r\n    slowReadBuff.position(slowReadBuff.limit());\r\n    checksumBuff.position(checksumBuff.limit());\r\n    IOUtils.skipFully(dataIn, toskip);\r\n    long checkSumOffset = (toskip / bytesPerChecksum) * checksumSize;\r\n    IOUtils.skipFully(checksumIn, checkSumOffset);\r\n    if (skipBuf == null) {\r\n        skipBuf = new byte[bytesPerChecksum];\r\n    }\r\n    assert skipBuf.length == bytesPerChecksum;\r\n    assert myOffsetFromChunkBoundary < bytesPerChecksum;\r\n    int ret = read(skipBuf, 0, myOffsetFromChunkBoundary);\r\n    if (ret == -1) {\r\n        return (toskip + remaining);\r\n    } else {\r\n        return (toskip + remaining + ret);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    IOUtilsClient.cleanupWithLogger(LOG, dataIn, checksumIn);\r\n    if (slowReadBuff != null) {\r\n        bufferPool.returnBuffer(slowReadBuff);\r\n        slowReadBuff = null;\r\n    }\r\n    if (checksumBuff != null) {\r\n        bufferPool.returnBuffer(checksumBuff);\r\n        checksumBuff = null;\r\n    }\r\n    startOffset = -1;\r\n    checksum = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "readAll",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int readAll(byte[] buf, int offset, int len) throws IOException\n{\r\n    return BlockReaderUtil.readAll(this, buf, offset, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "readFully",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readFully(byte[] buf, int off, int len) throws IOException\n{\r\n    BlockReaderUtil.readFully(this, buf, off, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "available",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int available()\n{\r\n    return Integer.MAX_VALUE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "isShortCircuit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isShortCircuit()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getClientMmap",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientMmap getClientMmap(EnumSet<ReadOption> opts)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getDataChecksum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DataChecksum getDataChecksum()\n{\r\n    return checksum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getNetworkDistance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNetworkDistance()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\security\\token\\block",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return keyId + \"/\" + blockPoolId + \"/\" + nonce.length + \"/\" + encryptionKey.length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ExtendedBlock getBlock()\n{\r\n    return block;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DatanodeInfo[] getDatanodes()\n{\r\n    return datanodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockTokens",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Token<BlockTokenIdentifier>[] getBlockTokens()\n{\r\n    return blockTokens;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockIndices",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getBlockIndices()\n{\r\n    return blockIndices;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ErasureCodingPolicy getErasureCodingPolicy()\n{\r\n    return ecPolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getCallsMade",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getCallsMade()\n{\r\n    return callsMade;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "path2String",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String path2String(Path path)\n{\r\n    return path.toUri().getPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "string2Path",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path string2Path(String string)\n{\r\n    return new Path(string);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "loadNext",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void loadNext() throws IOException\n{\r\n    if (files == null || fileIdx >= files.length) {\r\n        CorruptFileBlocks cfb = dfs.listCorruptFileBlocks(path, cookie);\r\n        files = cfb.getFiles();\r\n        cookie = cfb.getCookie();\r\n        fileIdx = 0;\r\n        callsMade++;\r\n    }\r\n    if (fileIdx >= files.length) {\r\n        nextPath = null;\r\n    } else {\r\n        nextPath = string2Path(files[fileIdx]);\r\n        fileIdx++;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "hasNext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasNext()\n{\r\n    return nextPath != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path next() throws IOException\n{\r\n    if (!hasNext()) {\r\n        throw new NoSuchElementException(\"No more corrupt file blocks\");\r\n    }\r\n    Path result = nextPath;\r\n    loadNext();\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client",
  "methodName" : "isHealthy",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "boolean isHealthy(URI uri)\n{\r\n    final String scheme = uri.getScheme();\r\n    if (!HdfsConstants.HDFS_URI_SCHEME.equalsIgnoreCase(scheme)) {\r\n        throw new IllegalArgumentException(\"The scheme is not \" + HdfsConstants.HDFS_URI_SCHEME + \", uri=\" + uri);\r\n    }\r\n    final Configuration conf = new Configuration();\r\n    conf.setBoolean(String.format(\"fs.%s.impl.disable.cache\", scheme), true);\r\n    conf.setBoolean(HdfsClientConfigKeys.Retry.POLICY_ENABLED_KEY, false);\r\n    conf.setInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, 0);\r\n    try (DistributedFileSystem fs = (DistributedFileSystem) FileSystem.get(uri, conf)) {\r\n        final boolean safemode = fs.setSafeMode(SafeModeAction.SAFEMODE_GET);\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"Is namenode in safemode? \" + safemode + \"; uri=\" + uri);\r\n        }\r\n        return !safemode;\r\n    } catch (IOException e) {\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"Got an exception for uri=\" + uri, e);\r\n        }\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int read(byte[] buf, int off, int len) throws IOException\n{\r\n    int nread = accessor.read(pos, buf, off, len);\r\n    if (nread < 0) {\r\n        return nread;\r\n    }\r\n    pos += nread;\r\n    return nread;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int read(ByteBuffer buf) throws IOException\n{\r\n    int nread = accessor.read(pos, buf);\r\n    if (nread < 0) {\r\n        return nread;\r\n    }\r\n    pos += nread;\r\n    return nread;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "skip",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long skip(long n) throws IOException\n{\r\n    if (n <= 0) {\r\n        return 0;\r\n    }\r\n    long oldPos = pos;\r\n    pos += n;\r\n    if (pos > visibleLength) {\r\n        pos = visibleLength;\r\n    }\r\n    return pos - oldPos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "available",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int available()\n{\r\n    long diff = visibleLength - pos;\r\n    if (diff > Integer.MAX_VALUE) {\r\n        return Integer.MAX_VALUE;\r\n    } else {\r\n        return (int) diff;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    accessor.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "readFully",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readFully(byte[] buf, int offset, int len) throws IOException\n{\r\n    BlockReaderUtil.readFully(this, buf, offset, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "readAll",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int readAll(byte[] buf, int offset, int len) throws IOException\n{\r\n    return BlockReaderUtil.readAll(this, buf, offset, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "isShortCircuit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isShortCircuit()\n{\r\n    return accessor.isShortCircuit();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getClientMmap",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientMmap getClientMmap(EnumSet<ReadOption> opts)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getDataChecksum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DataChecksum getDataChecksum()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getNetworkDistance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNetworkDistance()\n{\r\n    return accessor.getNetworkDistance();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "getProxy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NNProxyInfo<T> getProxy()\n{\r\n    return createProxyIfNeeded(nnProxyInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "performFailover",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void performFailover(T currentProxy)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (nnProxyInfo.proxy == null) {\r\n        return;\r\n    }\r\n    if (nnProxyInfo.proxy instanceof Closeable) {\r\n        ((Closeable) nnProxyInfo.proxy).close();\r\n    } else {\r\n        RPC.stopProxy(nnProxyInfo.proxy);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "useLogicalURI",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean useLogicalURI()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "prepareDecodeInputs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void prepareDecodeInputs()\n{\r\n    if (codingBuffer == null) {\r\n        this.decodeInputs = new ECChunk[dataBlkNum + parityBlkNum];\r\n        initDecodeInputs(alignedStripe);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "prepareParityChunk",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean prepareParityChunk(int index)\n{\r\n    Preconditions.checkState(index >= dataBlkNum && alignedStripe.chunks[index] == null);\r\n    int bufLen = (int) alignedStripe.getSpanInBlock();\r\n    decodeInputs[index] = new ECChunk(codingBuffer.duplicate(), index * bufLen, bufLen);\r\n    alignedStripe.chunks[index] = new StripingChunk(decodeInputs[index].getBuffer());\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "decode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void decode() throws IOException\n{\r\n    finalizeDecodeInputs();\r\n    decodeAndFillBuffer(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "initDecodeInputs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void initDecodeInputs(AlignedStripe alignedStripe)\n{\r\n    int bufLen = (int) alignedStripe.getSpanInBlock();\r\n    int bufCount = dataBlkNum + parityBlkNum;\r\n    codingBuffer = dfsStripedInputStream.getBufferPool().getBuffer(useDirectBuffer(), bufLen * bufCount);\r\n    ByteBuffer buffer;\r\n    for (int i = 0; i < dataBlkNum; i++) {\r\n        buffer = codingBuffer.duplicate();\r\n        decodeInputs[i] = new ECChunk(buffer, i * bufLen, bufLen);\r\n    }\r\n    for (int i = 0; i < dataBlkNum; i++) {\r\n        if (alignedStripe.chunks[i] == null) {\r\n            alignedStripe.chunks[i] = new StripingChunk(decodeInputs[i].getBuffer());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close()\n{\r\n    if (decodeInputs != null) {\r\n        for (int i = 0; i < decodeInputs.length; i++) {\r\n            decodeInputs[i] = null;\r\n        }\r\n    }\r\n    if (codingBuffer != null) {\r\n        dfsStripedInputStream.getBufferPool().putBuffer(codingBuffer);\r\n        codingBuffer = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return HdfsConstants.HDFS_URI_SCHEME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI getUri()\n{\r\n    return uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void initialize(URI uri, Configuration conf) throws IOException\n{\r\n    super.initialize(uri, conf);\r\n    setConf(conf);\r\n    String host = uri.getHost();\r\n    if (host == null) {\r\n        throw new IOException(\"Incomplete HDFS URI, no host: \" + uri);\r\n    }\r\n    initDFSClient(uri, conf);\r\n    this.uri = URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\r\n    this.workingDir = getHomeDirectory();\r\n    storageStatistics = (DFSOpsCountStatistics) GlobalStorageStatistics.INSTANCE.put(DFSOpsCountStatistics.NAME, new StorageStatisticsProvider() {\r\n\r\n        @Override\r\n        public StorageStatistics provide() {\r\n            return new DFSOpsCountStatistics();\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "initDFSClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initDFSClient(URI theUri, Configuration conf) throws IOException\n{\r\n    this.dfs = new DFSClient(theUri, conf, statistics);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getWorkingDirectory()\n{\r\n    return workingDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDefaultBlockSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDefaultBlockSize()\n{\r\n    return dfs.getConf().getDefaultBlockSize();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDefaultReplication",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "short getDefaultReplication()\n{\r\n    return dfs.getConf().getDefaultReplication();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setWorkingDirectory(Path dir)\n{\r\n    String result = fixRelativePart(dir).toUri().getPath();\r\n    if (!DFSUtilClient.isValidName(result)) {\r\n        throw new IllegalArgumentException(\"Invalid DFS directory name \" + result);\r\n    }\r\n    workingDir = fixRelativePart(dir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getHomeDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getHomeDirectory()\n{\r\n    return makeQualified(new Path(DFSUtilClient.getHomeDirectory(getConf(), dfs.ugi)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getHedgedReadMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DFSHedgedReadMetrics getHedgedReadMetrics()\n{\r\n    return dfs.getHedgedReadMetrics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getPathName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getPathName(Path file)\n{\r\n    checkPath(file);\r\n    String result = file.toUri().getPath();\r\n    if (!DFSUtilClient.isValidName(result)) {\r\n        throw new IllegalArgumentException(\"Pathname \" + result + \" from \" + file + \" is not a valid DFS filename.\");\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileBlockLocations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockLocation[] getFileBlockLocations(FileStatus file, long start, long len) throws IOException\n{\r\n    if (file == null) {\r\n        return null;\r\n    }\r\n    return getFileBlockLocations(file.getPath(), start, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileBlockLocations",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "BlockLocation[] getFileBlockLocations(Path p, final long start, final long len) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_FILE_BLOCK_LOCATIONS);\r\n    final Path absF = fixRelativePart(p);\r\n    return new FileSystemLinkResolver<BlockLocation[]>() {\r\n\r\n        @Override\r\n        public BlockLocation[] doCall(final Path p) throws IOException {\r\n            return dfs.getBlockLocations(getPathName(p), start, len);\r\n        }\r\n\r\n        @Override\r\n        public BlockLocation[] next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.getFileBlockLocations(p, start, len);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setVerifyChecksum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setVerifyChecksum(boolean verifyChecksum)\n{\r\n    this.verifyChecksum = verifyChecksum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "recoverLease",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean recoverLease(final Path f) throws IOException\n{\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean doCall(final Path p) throws IOException {\r\n            return dfs.recoverLease(getPathName(p));\r\n        }\r\n\r\n        @Override\r\n        public Boolean next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                return myDfs.recoverLease(p);\r\n            }\r\n            throw new UnsupportedOperationException(\"Cannot recoverLease through\" + \" a symlink to a non-DistributedFileSystem: \" + f + \" -> \" + p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "open",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FSDataInputStream open(Path f, final int bufferSize) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.OPEN);\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<FSDataInputStream>() {\r\n\r\n        @Override\r\n        public FSDataInputStream doCall(final Path p) throws IOException {\r\n            final DFSInputStream dfsis = dfs.open(getPathName(p), bufferSize, verifyChecksum);\r\n            try {\r\n                return dfs.createWrappedInputStream(dfsis);\r\n            } catch (IOException ex) {\r\n                dfsis.close();\r\n                throw ex;\r\n            }\r\n        }\r\n\r\n        @Override\r\n        public FSDataInputStream next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.open(p, bufferSize);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "FSDataInputStream open(PathHandle fd, int bufferSize) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.OPEN);\r\n    if (!(fd instanceof HdfsPathHandle)) {\r\n        fd = new HdfsPathHandle(fd.bytes());\r\n    }\r\n    HdfsPathHandle id = (HdfsPathHandle) fd;\r\n    final DFSInputStream dfsis = dfs.open(id, bufferSize, verifyChecksum);\r\n    return dfs.createWrappedInputStream(dfsis);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createPathHandle",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "HdfsPathHandle createPathHandle(FileStatus st, HandleOpt... opts)\n{\r\n    if (!(st instanceof HdfsFileStatus)) {\r\n        throw new IllegalArgumentException(\"Invalid FileStatus \" + st.getClass().getSimpleName());\r\n    }\r\n    if (st.isDirectory() || st.isSymlink()) {\r\n        throw new IllegalArgumentException(\"PathHandle only available for files\");\r\n    }\r\n    if (!getUri().getAuthority().equals(st.getPath().toUri().getAuthority())) {\r\n        throw new IllegalArgumentException(\"Wrong FileSystem: \" + st.getPath());\r\n    }\r\n    HandleOpt.Data data = HandleOpt.getOpt(HandleOpt.Data.class, opts).orElse(HandleOpt.changed(false));\r\n    HandleOpt.Location loc = HandleOpt.getOpt(HandleOpt.Location.class, opts).orElse(HandleOpt.moved(false));\r\n    HdfsFileStatus hst = (HdfsFileStatus) st;\r\n    final Path p;\r\n    final Optional<Long> inodeId;\r\n    if (loc.allowChange()) {\r\n        p = DFSUtilClient.makePathFromFileId(hst.getFileId());\r\n        inodeId = Optional.empty();\r\n    } else {\r\n        p = hst.getPath();\r\n        inodeId = Optional.of(hst.getFileId());\r\n    }\r\n    final Optional<Long> mtime = !data.allowChange() ? Optional.of(hst.getModificationTime()) : Optional.empty();\r\n    return new HdfsPathHandle(getPathName(p), inodeId, mtime);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FSDataOutputStream append(Path f, final int bufferSize, final Progressable progress) throws IOException\n{\r\n    return append(f, EnumSet.of(CreateFlag.APPEND), bufferSize, progress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FSDataOutputStream append(Path f, final EnumSet<CreateFlag> flag, final int bufferSize, final Progressable progress) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.APPEND);\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<FSDataOutputStream>() {\r\n\r\n        @Override\r\n        public FSDataOutputStream doCall(final Path p) throws IOException {\r\n            return dfs.append(getPathName(p), bufferSize, flag, progress, statistics);\r\n        }\r\n\r\n        @Override\r\n        public FSDataOutputStream next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.append(p, bufferSize);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FSDataOutputStream append(Path f, final EnumSet<CreateFlag> flag, final int bufferSize, final Progressable progress, final InetSocketAddress[] favoredNodes) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.APPEND);\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<FSDataOutputStream>() {\r\n\r\n        @Override\r\n        public FSDataOutputStream doCall(final Path p) throws IOException {\r\n            return dfs.append(getPathName(p), bufferSize, flag, progress, statistics, favoredNodes);\r\n        }\r\n\r\n        @Override\r\n        public FSDataOutputStream next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.append(p, bufferSize);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException\n{\r\n    return this.create(f, permission, overwrite ? EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE) : EnumSet.of(CreateFlag.CREATE), bufferSize, replication, blockSize, progress, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HdfsDataOutputStream create(final Path f, final FsPermission permission, final boolean overwrite, final int bufferSize, final short replication, final long blockSize, final Progressable progress, final InetSocketAddress[] favoredNodes) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.CREATE);\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<HdfsDataOutputStream>() {\r\n\r\n        @Override\r\n        public HdfsDataOutputStream doCall(final Path p) throws IOException {\r\n            final DFSOutputStream out = dfs.create(getPathName(f), permission, overwrite ? EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE) : EnumSet.of(CreateFlag.CREATE), true, replication, blockSize, progress, bufferSize, null, favoredNodes);\r\n            return safelyCreateWrappedOutputStream(out);\r\n        }\r\n\r\n        @Override\r\n        public HdfsDataOutputStream next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                return myDfs.create(p, permission, overwrite, bufferSize, replication, blockSize, progress, favoredNodes);\r\n            }\r\n            throw new UnsupportedOperationException(\"Cannot create with\" + \" favoredNodes through a symlink to a non-DistributedFileSystem: \" + f + \" -> \" + p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FSDataOutputStream create(final Path f, final FsPermission permission, final EnumSet<CreateFlag> cflags, final int bufferSize, final short replication, final long blockSize, final Progressable progress, final ChecksumOpt checksumOpt) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.CREATE);\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<FSDataOutputStream>() {\r\n\r\n        @Override\r\n        public FSDataOutputStream doCall(final Path p) throws IOException {\r\n            final DFSOutputStream dfsos = dfs.create(getPathName(p), permission, cflags, replication, blockSize, progress, bufferSize, checksumOpt);\r\n            return safelyCreateWrappedOutputStream(dfsos);\r\n        }\r\n\r\n        @Override\r\n        public FSDataOutputStream next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.create(p, permission, cflags, bufferSize, replication, blockSize, progress, checksumOpt);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HdfsDataOutputStream create(final Path f, final FsPermission permission, final EnumSet<CreateFlag> flag, final int bufferSize, final short replication, final long blockSize, final Progressable progress, final ChecksumOpt checksumOpt, final InetSocketAddress[] favoredNodes, final String ecPolicyName, final String storagePolicy) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.CREATE);\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<HdfsDataOutputStream>() {\r\n\r\n        @Override\r\n        public HdfsDataOutputStream doCall(final Path p) throws IOException {\r\n            final DFSOutputStream out = dfs.create(getPathName(f), permission, flag, true, replication, blockSize, progress, bufferSize, checksumOpt, favoredNodes, ecPolicyName, storagePolicy);\r\n            return safelyCreateWrappedOutputStream(out);\r\n        }\r\n\r\n        @Override\r\n        public HdfsDataOutputStream next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                return myDfs.create(p, permission, flag, bufferSize, replication, blockSize, progress, checksumOpt, favoredNodes, ecPolicyName, storagePolicy);\r\n            }\r\n            throw new UnsupportedOperationException(\"Cannot create with\" + \" favoredNodes through a symlink to a non-DistributedFileSystem: \" + f + \" -> \" + p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "primitiveCreate",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HdfsDataOutputStream primitiveCreate(Path f, FsPermission absolutePermission, EnumSet<CreateFlag> flag, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.PRIMITIVE_CREATE);\r\n    final DFSOutputStream dfsos = dfs.primitiveCreate(getPathName(fixRelativePart(f)), absolutePermission, flag, true, replication, blockSize, progress, bufferSize, checksumOpt);\r\n    return safelyCreateWrappedOutputStream(dfsos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HdfsDataOutputStream createNonRecursive(final Path f, final FsPermission permission, final EnumSet<CreateFlag> flag, final int bufferSize, final short replication, final long blockSize, final Progressable progress, final ChecksumOpt checksumOpt, final InetSocketAddress[] favoredNodes, final String ecPolicyName, final String storagePolicyName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.CREATE);\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<HdfsDataOutputStream>() {\r\n\r\n        @Override\r\n        public HdfsDataOutputStream doCall(final Path p) throws IOException {\r\n            final DFSOutputStream out = dfs.create(getPathName(f), permission, flag, false, replication, blockSize, progress, bufferSize, checksumOpt, favoredNodes, ecPolicyName, storagePolicyName);\r\n            return safelyCreateWrappedOutputStream(out);\r\n        }\r\n\r\n        @Override\r\n        public HdfsDataOutputStream next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                return myDfs.createNonRecursive(p, permission, flag, bufferSize, replication, blockSize, progress, checksumOpt, favoredNodes, ecPolicyName, storagePolicyName);\r\n            }\r\n            throw new UnsupportedOperationException(\"Cannot create with\" + \" favoredNodes through a symlink to a non-DistributedFileSystem: \" + f + \" -> \" + p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "FSDataOutputStream createNonRecursive(final Path f, final FsPermission permission, final EnumSet<CreateFlag> flag, final int bufferSize, final short replication, final long blockSize, final Progressable progress) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.CREATE_NON_RECURSIVE);\r\n    if (flag.contains(CreateFlag.OVERWRITE)) {\r\n        flag.add(CreateFlag.CREATE);\r\n    }\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<FSDataOutputStream>() {\r\n\r\n        @Override\r\n        public FSDataOutputStream doCall(final Path p) throws IOException {\r\n            final DFSOutputStream dfsos = dfs.create(getPathName(p), permission, flag, false, replication, blockSize, progress, bufferSize, null);\r\n            return safelyCreateWrappedOutputStream(dfsos);\r\n        }\r\n\r\n        @Override\r\n        public FSDataOutputStream next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.createNonRecursive(p, permission, flag, bufferSize, replication, blockSize, progress);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "safelyCreateWrappedOutputStream",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "HdfsDataOutputStream safelyCreateWrappedOutputStream(DFSOutputStream dfsos) throws IOException\n{\r\n    try {\r\n        return dfs.createWrappedOutputStream(dfsos, statistics);\r\n    } catch (IOException ex) {\r\n        dfsos.close();\r\n        throw ex;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setReplication",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean setReplication(Path src, final short replication) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_REPLICATION);\r\n    Path absF = fixRelativePart(src);\r\n    return new FileSystemLinkResolver<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean doCall(final Path p) throws IOException {\r\n            return dfs.setReplication(getPathName(p), replication);\r\n        }\r\n\r\n        @Override\r\n        public Boolean next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.setReplication(p, replication);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setStoragePolicy(final Path src, final String policyName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_STORAGE_POLICY);\r\n    Path absF = fixRelativePart(src);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.setStoragePolicy(getPathName(p), policyName);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            fs.setStoragePolicy(p, policyName);\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "unsetStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void unsetStoragePolicy(final Path src) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.UNSET_STORAGE_POLICY);\r\n    Path absF = fixRelativePart(src);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.unsetStoragePolicy(getPathName(p));\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                ((DistributedFileSystem) fs).unsetStoragePolicy(p);\r\n                return null;\r\n            } else {\r\n                throw new UnsupportedOperationException(\"Cannot perform unsetStoragePolicy on a \" + \"non-DistributedFileSystem: \" + src + \" -> \" + p);\r\n            }\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "BlockStoragePolicySpi getStoragePolicy(Path path) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_STORAGE_POLICY);\r\n    Path absF = fixRelativePart(path);\r\n    return new FileSystemLinkResolver<BlockStoragePolicySpi>() {\r\n\r\n        @Override\r\n        public BlockStoragePolicySpi doCall(final Path p) throws IOException {\r\n            return getClient().getStoragePolicy(getPathName(p));\r\n        }\r\n\r\n        @Override\r\n        public BlockStoragePolicySpi next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.getStoragePolicy(p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAllStoragePolicies",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Collection<BlockStoragePolicy> getAllStoragePolicies() throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_STORAGE_POLICIES);\r\n    return Arrays.asList(dfs.getStoragePolicies());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getBytesWithFutureGenerationStamps",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getBytesWithFutureGenerationStamps() throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_BYTES_WITH_FUTURE_GS);\r\n    return dfs.getBytesInFutureBlocks();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStoragePolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockStoragePolicy[] getStoragePolicies() throws IOException\n{\r\n    return getAllStoragePolicies().toArray(new BlockStoragePolicy[0]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "concat",
  "errType" : [ "UnresolvedLinkException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void concat(Path trg, Path[] psrcs) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.CONCAT);\r\n    Path absF = fixRelativePart(trg);\r\n    Path[] srcs = new Path[psrcs.length];\r\n    for (int i = 0; i < psrcs.length; i++) {\r\n        srcs[i] = fixRelativePart(psrcs[i]);\r\n    }\r\n    String[] srcsStr = new String[psrcs.length];\r\n    try {\r\n        for (int i = 0; i < psrcs.length; i++) {\r\n            srcsStr[i] = getPathName(srcs[i]);\r\n        }\r\n        dfs.concat(getPathName(absF), srcsStr);\r\n    } catch (UnresolvedLinkException e) {\r\n        FileStatus stat = getFileLinkStatus(absF);\r\n        if (stat.isSymlink()) {\r\n            throw new IOException(\"Cannot concat with a symlink target: \" + trg + \" -> \" + stat.getPath());\r\n        }\r\n        absF = fixRelativePart(stat.getPath());\r\n        for (int i = 0; i < psrcs.length; i++) {\r\n            stat = getFileLinkStatus(srcs[i]);\r\n            if (stat.isSymlink()) {\r\n                throw new IOException(\"Cannot concat with a symlink src: \" + psrcs[i] + \" -> \" + stat.getPath());\r\n            }\r\n            srcs[i] = fixRelativePart(stat.getPath());\r\n        }\r\n        for (int i = 0; i < psrcs.length; i++) {\r\n            srcsStr[i] = getPathName(srcs[i]);\r\n        }\r\n        dfs.concat(getPathName(absF), srcsStr);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "rename",
  "errType" : [ "UnresolvedLinkException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean rename(Path src, Path dst) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.RENAME);\r\n    final Path absSrc = fixRelativePart(src);\r\n    final Path absDst = fixRelativePart(dst);\r\n    try {\r\n        return dfs.rename(getPathName(absSrc), getPathName(absDst));\r\n    } catch (UnresolvedLinkException e) {\r\n        final Path source = getFileLinkStatus(absSrc).getPath();\r\n        return new FileSystemLinkResolver<Boolean>() {\r\n\r\n            @Override\r\n            public Boolean doCall(final Path p) throws IOException {\r\n                return dfs.rename(getPathName(source), getPathName(p));\r\n            }\r\n\r\n            @Override\r\n            public Boolean next(final FileSystem fs, final Path p) throws IOException {\r\n                return doCall(p);\r\n            }\r\n        }.resolve(this, absDst);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "rename",
  "errType" : [ "UnresolvedLinkException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void rename(Path src, Path dst, final Options.Rename... options) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.RENAME);\r\n    final Path absSrc = fixRelativePart(src);\r\n    final Path absDst = fixRelativePart(dst);\r\n    try {\r\n        dfs.rename(getPathName(absSrc), getPathName(absDst), options);\r\n    } catch (UnresolvedLinkException e) {\r\n        final Path source = getFileLinkStatus(absSrc).getPath();\r\n        new FileSystemLinkResolver<Void>() {\r\n\r\n            @Override\r\n            public Void doCall(final Path p) throws IOException {\r\n                dfs.rename(getPathName(source), getPathName(p), options);\r\n                return null;\r\n            }\r\n\r\n            @Override\r\n            public Void next(final FileSystem fs, final Path p) throws IOException {\r\n                return doCall(p);\r\n            }\r\n        }.resolve(this, absDst);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "truncate",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean truncate(Path f, final long newLength) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.TRUNCATE);\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean doCall(final Path p) throws IOException {\r\n            return dfs.truncate(getPathName(p), newLength);\r\n        }\r\n\r\n        @Override\r\n        public Boolean next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.truncate(p, newLength);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean delete(Path f, final boolean recursive) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.DELETE);\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean doCall(final Path p) throws IOException {\r\n            return dfs.delete(getPathName(p), recursive);\r\n        }\r\n\r\n        @Override\r\n        public Boolean next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.delete(p, recursive);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getContentSummary",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ContentSummary getContentSummary(Path f) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_CONTENT_SUMMARY);\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<ContentSummary>() {\r\n\r\n        @Override\r\n        public ContentSummary doCall(final Path p) throws IOException {\r\n            return dfs.getContentSummary(getPathName(p));\r\n        }\r\n\r\n        @Override\r\n        public ContentSummary next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.getContentSummary(p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getQuotaUsage",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "QuotaUsage getQuotaUsage(Path f) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_QUOTA_USAGE);\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<QuotaUsage>() {\r\n\r\n        @Override\r\n        public QuotaUsage doCall(final Path p) throws IOException, UnresolvedLinkException {\r\n            return dfs.getQuotaUsage(getPathName(p));\r\n        }\r\n\r\n        @Override\r\n        public QuotaUsage next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.getQuotaUsage(p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setQuota",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setQuota(Path src, final long namespaceQuota, final long storagespaceQuota) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_QUOTA_USAGE);\r\n    Path absF = fixRelativePart(src);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.setQuota(getPathName(p), namespaceQuota, storagespaceQuota);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            return doCall(p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setQuotaByStorageType",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setQuotaByStorageType(Path src, final StorageType type, final long quota) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_QUOTA_BYTSTORAGEYPE);\r\n    Path absF = fixRelativePart(src);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.setQuotaByStorageType(getPathName(p), type, quota);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            return doCall(p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listStatusInternal",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "FileStatus[] listStatusInternal(Path p) throws IOException\n{\r\n    String src = getPathName(p);\r\n    DirectoryListing thisListing = dfs.listPaths(src, HdfsFileStatus.EMPTY_NAME);\r\n    if (thisListing == null) {\r\n        throw new FileNotFoundException(\"File \" + p + \" does not exist.\");\r\n    }\r\n    HdfsFileStatus[] partialListing = thisListing.getPartialListing();\r\n    if (!thisListing.hasMore()) {\r\n        FileStatus[] stats = new FileStatus[partialListing.length];\r\n        for (int i = 0; i < partialListing.length; i++) {\r\n            stats[i] = partialListing[i].makeQualified(getUri(), p);\r\n        }\r\n        statistics.incrementReadOps(1);\r\n        storageStatistics.incrementOpCounter(OpType.LIST_STATUS);\r\n        return stats;\r\n    }\r\n    int totalNumEntries = partialListing.length + thisListing.getRemainingEntries();\r\n    ArrayList<FileStatus> listing = new ArrayList<>(totalNumEntries);\r\n    for (HdfsFileStatus fileStatus : partialListing) {\r\n        listing.add(fileStatus.makeQualified(getUri(), p));\r\n    }\r\n    statistics.incrementLargeReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.LIST_STATUS);\r\n    do {\r\n        thisListing = dfs.listPaths(src, thisListing.getLastName());\r\n        if (thisListing == null) {\r\n            throw new FileNotFoundException(\"File \" + p + \" does not exist.\");\r\n        }\r\n        partialListing = thisListing.getPartialListing();\r\n        for (HdfsFileStatus fileStatus : partialListing) {\r\n            listing.add(fileStatus.makeQualified(getUri(), p));\r\n        }\r\n        statistics.incrementLargeReadOps(1);\r\n        storageStatistics.incrementOpCounter(OpType.LIST_STATUS);\r\n    } while (thisListing.hasMore());\r\n    return listing.toArray(new FileStatus[listing.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileStatus[] listStatus(Path p) throws IOException\n{\r\n    Path absF = fixRelativePart(p);\r\n    return new FileSystemLinkResolver<FileStatus[]>() {\r\n\r\n        @Override\r\n        public FileStatus[] doCall(final Path p) throws IOException {\r\n            return listStatusInternal(p);\r\n        }\r\n\r\n        @Override\r\n        public FileStatus[] next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.listStatus(p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listLocatedStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<LocatedFileStatus> listLocatedStatus(final Path p, final PathFilter filter) throws IOException\n{\r\n    Path absF = fixRelativePart(p);\r\n    return new FileSystemLinkResolver<RemoteIterator<LocatedFileStatus>>() {\r\n\r\n        @Override\r\n        public RemoteIterator<LocatedFileStatus> doCall(final Path p) throws IOException {\r\n            return new DirListingIterator<>(p, filter, true);\r\n        }\r\n\r\n        @Override\r\n        public RemoteIterator<LocatedFileStatus> next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                return ((DistributedFileSystem) fs).listLocatedStatus(p, filter);\r\n            }\r\n            throw new IOException(\"Link resolution does not work with multiple \" + \"file systems for listLocatedStatus(): \" + p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<FileStatus> listStatusIterator(final Path p) throws IOException\n{\r\n    Path absF = fixRelativePart(p);\r\n    return new FileSystemLinkResolver<RemoteIterator<FileStatus>>() {\r\n\r\n        @Override\r\n        public RemoteIterator<FileStatus> doCall(final Path p) throws IOException {\r\n            return new DirListingIterator<>(p, false);\r\n        }\r\n\r\n        @Override\r\n        public RemoteIterator<FileStatus> next(final FileSystem fs, final Path p) throws IOException {\r\n            return ((DistributedFileSystem) fs).listStatusIterator(p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "batchedListStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<PartialListing<FileStatus>> batchedListStatusIterator(final List<Path> paths) throws IOException\n{\r\n    List<Path> absPaths = Lists.newArrayListWithCapacity(paths.size());\r\n    for (Path p : paths) {\r\n        absPaths.add(fixRelativePart(p));\r\n    }\r\n    return new PartialListingIterator<>(absPaths, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "batchedListLocatedStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<PartialListing<LocatedFileStatus>> batchedListLocatedStatusIterator(final List<Path> paths) throws IOException\n{\r\n    List<Path> absPaths = Lists.newArrayListWithCapacity(paths.size());\r\n    for (Path p : paths) {\r\n        absPaths.add(fixRelativePart(p));\r\n    }\r\n    return new PartialListingIterator<>(absPaths, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "mkdir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean mkdir(Path f, FsPermission permission) throws IOException\n{\r\n    return mkdirsInternal(f, permission, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean mkdirs(Path f, FsPermission permission) throws IOException\n{\r\n    return mkdirsInternal(f, permission, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "mkdirsInternal",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean mkdirsInternal(Path f, final FsPermission permission, final boolean createParent) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.MKDIRS);\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean doCall(final Path p) throws IOException {\r\n            return dfs.mkdirs(getPathName(p), permission, createParent);\r\n        }\r\n\r\n        @Override\r\n        public Boolean next(final FileSystem fs, final Path p) throws IOException {\r\n            if (!createParent) {\r\n                throw new IOException(\"FileSystem does not support non-recursive\" + \"mkdir\");\r\n            }\r\n            return fs.mkdirs(p, permission);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "primitiveMkdir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean primitiveMkdir(Path f, FsPermission absolutePermission) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.PRIMITIVE_MKDIR);\r\n    return dfs.primitiveMkdir(getPathName(f), absolutePermission);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    try {\r\n        if (dfs != null) {\r\n            dfs.closeOutputStreams(false);\r\n        }\r\n        super.close();\r\n    } finally {\r\n        if (dfs != null) {\r\n            dfs.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"DFS[\" + dfs + \"]\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DFSClient getClient()\n{\r\n    return dfs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FsStatus getStatus(Path p) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_STATUS);\r\n    return dfs.getDiskStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getMissingBlocksCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getMissingBlocksCount() throws IOException\n{\r\n    return dfs.getMissingBlocksCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getPendingDeletionBlocksCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getPendingDeletionBlocksCount() throws IOException\n{\r\n    return dfs.getPendingDeletionBlocksCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getMissingReplOneBlocksCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getMissingReplOneBlocksCount() throws IOException\n{\r\n    return dfs.getMissingReplOneBlocksCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLowRedundancyBlocksCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLowRedundancyBlocksCount() throws IOException\n{\r\n    return dfs.getLowRedundancyBlocksCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getCorruptBlocksCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCorruptBlocksCount() throws IOException\n{\r\n    return dfs.getCorruptBlocksCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listCorruptFileBlocks",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<Path> listCorruptFileBlocks(final Path path) throws IOException\n{\r\n    Path absF = fixRelativePart(path);\r\n    return new FileSystemLinkResolver<RemoteIterator<Path>>() {\r\n\r\n        @Override\r\n        public RemoteIterator<Path> doCall(final Path path) throws IOException, UnresolvedLinkException {\r\n            return new CorruptFileBlockIterator(dfs, path);\r\n        }\r\n\r\n        @Override\r\n        public RemoteIterator<Path> next(final FileSystem fs, final Path path) throws IOException {\r\n            return fs.listCorruptFileBlocks(path);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDataNodeStats",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DatanodeInfo[] getDataNodeStats() throws IOException\n{\r\n    return getDataNodeStats(DatanodeReportType.ALL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDataNodeStats",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DatanodeInfo[] getDataNodeStats(final DatanodeReportType type) throws IOException\n{\r\n    return dfs.datanodeReport(type);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setSafeMode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean setSafeMode(HdfsConstants.SafeModeAction action) throws IOException\n{\r\n    return setSafeMode(action, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setSafeMode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean setSafeMode(HdfsConstants.SafeModeAction action, boolean isChecked) throws IOException\n{\r\n    return dfs.setSafeMode(action, isChecked);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "saveNamespace",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean saveNamespace(long timeWindow, long txGap) throws IOException\n{\r\n    return dfs.saveNamespace(timeWindow, txGap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "saveNamespace",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void saveNamespace() throws IOException\n{\r\n    saveNamespace(0, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "rollEdits",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long rollEdits() throws IOException\n{\r\n    return dfs.rollEdits();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "restoreFailedStorage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean restoreFailedStorage(String arg) throws IOException\n{\r\n    return dfs.restoreFailedStorage(arg);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "refreshNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void refreshNodes() throws IOException\n{\r\n    dfs.refreshNodes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "finalizeUpgrade",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void finalizeUpgrade() throws IOException\n{\r\n    dfs.finalizeUpgrade();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "upgradeStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean upgradeStatus() throws IOException\n{\r\n    return dfs.upgradeStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "rollingUpgrade",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RollingUpgradeInfo rollingUpgrade(RollingUpgradeAction action) throws IOException\n{\r\n    return dfs.rollingUpgrade(action);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "metaSave",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void metaSave(String pathname) throws IOException\n{\r\n    dfs.metaSave(pathname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FsServerDefaults getServerDefaults() throws IOException\n{\r\n    return dfs.getServerDefaults();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FileStatus getFileStatus(Path f) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_FILE_STATUS);\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<FileStatus>() {\r\n\r\n        @Override\r\n        public FileStatus doCall(final Path p) throws IOException {\r\n            HdfsFileStatus fi = dfs.getFileInfo(getPathName(p));\r\n            if (fi != null) {\r\n                return fi.makeQualified(getUri(), p);\r\n            } else {\r\n                throw new FileNotFoundException(\"File does not exist: \" + p);\r\n            }\r\n        }\r\n\r\n        @Override\r\n        public FileStatus next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.getFileStatus(p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "msync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void msync() throws IOException\n{\r\n    dfs.msync();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createSymlink",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createSymlink(final Path target, final Path link, final boolean createParent) throws IOException\n{\r\n    if (!FileSystem.areSymlinksEnabled()) {\r\n        throw new UnsupportedOperationException(\"Symlinks not supported\");\r\n    }\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.CREATE_SYM_LINK);\r\n    final Path absF = fixRelativePart(link);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.createSymlink(target.toString(), getPathName(p), createParent);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            fs.createSymlink(target, p, createParent);\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "supportsSymlinks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean supportsSymlinks()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileLinkStatus",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "FileStatus getFileLinkStatus(final Path f) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_FILE_LINK_STATUS);\r\n    final Path absF = fixRelativePart(f);\r\n    FileStatus status = new FileSystemLinkResolver<FileStatus>() {\r\n\r\n        @Override\r\n        public FileStatus doCall(final Path p) throws IOException {\r\n            HdfsFileStatus fi = dfs.getFileLinkInfo(getPathName(p));\r\n            if (fi != null) {\r\n                return fi.makeQualified(getUri(), p);\r\n            } else {\r\n                throw new FileNotFoundException(\"File does not exist: \" + p);\r\n            }\r\n        }\r\n\r\n        @Override\r\n        public FileStatus next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.getFileLinkStatus(p);\r\n        }\r\n    }.resolve(this, absF);\r\n    if (status.isSymlink()) {\r\n        Path targetQual = FSLinkResolver.qualifySymlinkTarget(this.getUri(), status.getPath(), status.getSymlink());\r\n        status.setSymlink(targetQual);\r\n    }\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getLinkTarget",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path getLinkTarget(final Path f) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_LINK_TARGET);\r\n    final Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<Path>() {\r\n\r\n        @Override\r\n        public Path doCall(final Path p) throws IOException {\r\n            HdfsFileStatus fi = dfs.getFileLinkInfo(getPathName(p));\r\n            if (fi != null) {\r\n                return fi.makeQualified(getUri(), p).getSymlink();\r\n            } else {\r\n                throw new FileNotFoundException(\"File does not exist: \" + p);\r\n            }\r\n        }\r\n\r\n        @Override\r\n        public Path next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.getLinkTarget(p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "resolveLink",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path resolveLink(Path f) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.RESOLVE_LINK);\r\n    String target = dfs.getLinkTarget(getPathName(fixRelativePart(f)));\r\n    if (target == null) {\r\n        throw new FileNotFoundException(\"File does not exist: \" + f.toString());\r\n    }\r\n    return new Path(target);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileChecksum",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FileChecksum getFileChecksum(Path f) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_FILE_CHECKSUM);\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<FileChecksum>() {\r\n\r\n        @Override\r\n        public FileChecksum doCall(final Path p) throws IOException {\r\n            return dfs.getFileChecksumWithCombineMode(getPathName(p), Long.MAX_VALUE);\r\n        }\r\n\r\n        @Override\r\n        public FileChecksum next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.getFileChecksum(p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileChecksum",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FileChecksum getFileChecksum(Path f, final long length) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_FILE_CHECKSUM);\r\n    Path absF = fixRelativePart(f);\r\n    return new FileSystemLinkResolver<FileChecksum>() {\r\n\r\n        @Override\r\n        public FileChecksum doCall(final Path p) throws IOException {\r\n            return dfs.getFileChecksumWithCombineMode(getPathName(p), length);\r\n        }\r\n\r\n        @Override\r\n        public FileChecksum next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                return fs.getFileChecksum(p, length);\r\n            } else {\r\n                throw new UnsupportedFileSystemException(\"getFileChecksum(Path, long) is not supported by \" + fs.getClass().getSimpleName());\r\n            }\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setPermission",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setPermission(Path p, final FsPermission permission) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_PERMISSION);\r\n    Path absF = fixRelativePart(p);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.setPermission(getPathName(p), permission);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            fs.setPermission(p, permission);\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setOwner",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setOwner(Path p, final String username, final String groupname) throws IOException\n{\r\n    if (username == null && groupname == null) {\r\n        throw new IOException(\"username == null && groupname == null\");\r\n    }\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_OWNER);\r\n    Path absF = fixRelativePart(p);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.setOwner(getPathName(p), username, groupname);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            fs.setOwner(p, username, groupname);\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setTimes",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setTimes(Path p, final long mtime, final long atime) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_TIMES);\r\n    Path absF = fixRelativePart(p);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.setTimes(getPathName(p), mtime, atime);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            fs.setTimes(p, mtime, atime);\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDefaultPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getDefaultPort()\n{\r\n    return HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> getDelegationToken(String renewer) throws IOException\n{\r\n    return dfs.getDelegationToken(renewer == null ? null : new Text(renewer));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setBalancerBandwidth",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setBalancerBandwidth(long bandwidth) throws IOException\n{\r\n    dfs.setBalancerBandwidth(bandwidth);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getCanonicalServiceName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getCanonicalServiceName()\n{\r\n    return dfs.getCanonicalServiceName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "canonicalizeUri",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "URI canonicalizeUri(URI uri)\n{\r\n    if (HAUtilClient.isLogicalUri(getConf(), uri)) {\r\n        return uri;\r\n    } else {\r\n        return NetUtils.getCanonicalUri(uri, getDefaultPort());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isInSafeMode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isInSafeMode() throws IOException\n{\r\n    return setSafeMode(SafeModeAction.SAFEMODE_GET, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isSnapshotTrashRootEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSnapshotTrashRootEnabled() throws IOException\n{\r\n    return dfs.isSnapshotTrashRootEnabled();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "allowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void allowSnapshot(final Path path) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.ALLOW_SNAPSHOT);\r\n    Path absF = fixRelativePart(path);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.allowSnapshot(getPathName(p));\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                myDfs.allowSnapshot(p);\r\n            } else {\r\n                throw new UnsupportedOperationException(\"Cannot perform snapshot\" + \" operations on a symlink to a non-DistributedFileSystem: \" + path + \" -> \" + p);\r\n            }\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "disallowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void disallowSnapshot(final Path path) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.DISALLOW_SNAPSHOT);\r\n    Path absF = fixRelativePart(path);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            checkTrashRootAndRemoveIfEmpty(p);\r\n            dfs.disallowSnapshot(getPathName(p));\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                myDfs.checkTrashRootAndRemoveIfEmpty(p);\r\n                myDfs.disallowSnapshot(p);\r\n            } else {\r\n                throw new UnsupportedOperationException(\"Cannot perform snapshot\" + \" operations on a symlink to a non-DistributedFileSystem: \" + path + \" -> \" + p);\r\n            }\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkTrashRootAndRemoveIfEmpty",
  "errType" : [ "FileNotFoundException|AccessControlException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void checkTrashRootAndRemoveIfEmpty(final Path p) throws IOException\n{\r\n    if (dfs.isHDFSEncryptionEnabled() && dfs.isEZRoot(p)) {\r\n        DFSClient.LOG.debug(\"{} is an encryption zone root. \" + \"Skipping empty trash root check.\", p);\r\n        return;\r\n    }\r\n    Path trashRoot = new Path(p, FileSystem.TRASH_PREFIX);\r\n    try {\r\n        FileStatus[] fileStatuses = listStatus(trashRoot);\r\n        if (fileStatuses.length == 0) {\r\n            DFSClient.LOG.debug(\"Removing empty trash root {}\", trashRoot);\r\n            delete(trashRoot, false);\r\n        } else {\r\n            if (fileStatuses.length == 1 && !fileStatuses[0].isDirectory() && fileStatuses[0].getPath().toUri().getPath().equals(trashRoot.toString())) {\r\n                DFSClient.LOG.warn(\"{} is not a directory. Ignored.\", trashRoot);\r\n            } else {\r\n                throw new IOException(\"Found non-empty trash root at \" + trashRoot + \". Rename or delete it, then try again.\");\r\n            }\r\n        }\r\n    } catch (FileNotFoundException | AccessControlException ignored) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createSnapshot",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path createSnapshot(final Path path, final String snapshotName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.CREATE_SNAPSHOT);\r\n    Path absF = fixRelativePart(path);\r\n    return new FileSystemLinkResolver<Path>() {\r\n\r\n        @Override\r\n        public Path doCall(final Path p) throws IOException {\r\n            return new Path(dfs.createSnapshot(getPathName(p), snapshotName));\r\n        }\r\n\r\n        @Override\r\n        public Path next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                return myDfs.createSnapshot(p);\r\n            } else {\r\n                throw new UnsupportedOperationException(\"Cannot perform snapshot\" + \" operations on a symlink to a non-DistributedFileSystem: \" + path + \" -> \" + p);\r\n            }\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "renameSnapshot",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void renameSnapshot(final Path path, final String snapshotOldName, final String snapshotNewName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.RENAME_SNAPSHOT);\r\n    Path absF = fixRelativePart(path);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.renameSnapshot(getPathName(p), snapshotOldName, snapshotNewName);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                myDfs.renameSnapshot(p, snapshotOldName, snapshotNewName);\r\n            } else {\r\n                throw new UnsupportedOperationException(\"Cannot perform snapshot\" + \" operations on a symlink to a non-DistributedFileSystem: \" + path + \" -> \" + p);\r\n            }\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSnapshottableDirListing",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "SnapshottableDirectoryStatus[] getSnapshottableDirListing() throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_SNAPSHOTTABLE_DIRECTORY_LIST);\r\n    return dfs.getSnapshottableDirListing();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSnapshotListing",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "SnapshotStatus[] getSnapshotListing(Path snapshotRoot) throws IOException\n{\r\n    Path absF = fixRelativePart(snapshotRoot);\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_SNAPSHOT_LIST);\r\n    return dfs.getSnapshotListing(getPathName(absF));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "deleteSnapshot",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void deleteSnapshot(final Path snapshotDir, final String snapshotName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.DELETE_SNAPSHOT);\r\n    Path absF = fixRelativePart(snapshotDir);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.deleteSnapshot(getPathName(p), snapshotName);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                myDfs.deleteSnapshot(p, snapshotName);\r\n            } else {\r\n                throw new UnsupportedOperationException(\"Cannot perform snapshot\" + \" operations on a symlink to a non-DistributedFileSystem: \" + snapshotDir + \" -> \" + p);\r\n            }\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "snapshotDiffReportListingRemoteIterator",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<SnapshotDiffReportListing> snapshotDiffReportListingRemoteIterator(final Path snapshotDir, final String fromSnapshot, final String toSnapshot) throws IOException\n{\r\n    Path absF = fixRelativePart(snapshotDir);\r\n    return new FileSystemLinkResolver<RemoteIterator<SnapshotDiffReportListing>>() {\r\n\r\n        @Override\r\n        public RemoteIterator<SnapshotDiffReportListing> doCall(final Path p) throws IOException {\r\n            if (!DFSUtilClient.isValidSnapshotName(fromSnapshot) || !DFSUtilClient.isValidSnapshotName(toSnapshot)) {\r\n                throw new UnsupportedOperationException(\"Remote Iterator is\" + \"supported for snapshotDiffReport between two snapshots\");\r\n            }\r\n            return new SnapshotDiffReportListingIterator(getPathName(p), fromSnapshot, toSnapshot);\r\n        }\r\n\r\n        @Override\r\n        public RemoteIterator<SnapshotDiffReportListing> next(final FileSystem fs, final Path p) throws IOException {\r\n            return ((DistributedFileSystem) fs).snapshotDiffReportListingRemoteIterator(p, fromSnapshot, toSnapshot);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSnapshotDiffReportInternal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SnapshotDiffReport getSnapshotDiffReportInternal(final String snapshotDir, final String fromSnapshot, final String toSnapshot) throws IOException\n{\r\n    return DFSUtilClient.getSnapshotDiffReport(snapshotDir, fromSnapshot, toSnapshot, dfs::getSnapshotDiffReport, dfs::getSnapshotDiffReportListing);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSnapshotDiffReport",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "SnapshotDiffReport getSnapshotDiffReport(final Path snapshotDir, final String fromSnapshot, final String toSnapshot) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_SNAPSHOT_DIFF);\r\n    Path absF = fixRelativePart(snapshotDir);\r\n    return new FileSystemLinkResolver<SnapshotDiffReport>() {\r\n\r\n        @Override\r\n        public SnapshotDiffReport doCall(final Path p) throws IOException {\r\n            return getSnapshotDiffReportInternal(getPathName(p), fromSnapshot, toSnapshot);\r\n        }\r\n\r\n        @Override\r\n        public SnapshotDiffReport next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                myDfs.getSnapshotDiffReport(p, fromSnapshot, toSnapshot);\r\n            } else {\r\n                throw new UnsupportedOperationException(\"Cannot perform snapshot\" + \" operations on a symlink to a non-DistributedFileSystem: \" + snapshotDir + \" -> \" + p);\r\n            }\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getSnapshotDiffReportListing",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "SnapshotDiffReportListing getSnapshotDiffReportListing(Path snapshotDir, String fromSnapshotName, String toSnapshotName, String snapshotDiffStartPath, int snapshotDiffIndex) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_SNAPSHOT_DIFF);\r\n    Path absF = fixRelativePart(snapshotDir);\r\n    return new FileSystemLinkResolver<SnapshotDiffReportListing>() {\r\n\r\n        @Override\r\n        public SnapshotDiffReportListing doCall(final Path p) throws IOException {\r\n            return dfs.getSnapshotDiffReportListing(getPathName(p), fromSnapshotName, toSnapshotName, DFSUtilClient.string2Bytes(snapshotDiffStartPath), snapshotDiffIndex);\r\n        }\r\n\r\n        @Override\r\n        public SnapshotDiffReportListing next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem distributedFileSystem = (DistributedFileSystem) fs;\r\n                distributedFileSystem.getSnapshotDiffReportListing(p, fromSnapshotName, toSnapshotName, snapshotDiffStartPath, snapshotDiffIndex);\r\n            } else {\r\n                throw new UnsupportedOperationException(\"Cannot perform snapshot\" + \" operations on a symlink to a non-DistributedFileSystem: \" + snapshotDir + \" -> \" + p);\r\n            }\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "isFileClosed",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isFileClosed(final Path src) throws IOException\n{\r\n    Path absF = fixRelativePart(src);\r\n    return new FileSystemLinkResolver<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean doCall(final Path p) throws IOException {\r\n            return dfs.isFileClosed(getPathName(p));\r\n        }\r\n\r\n        @Override\r\n        public Boolean next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                return myDfs.isFileClosed(p);\r\n            } else {\r\n                throw new UnsupportedOperationException(\"Cannot call isFileClosed\" + \" on a symlink to a non-DistributedFileSystem: \" + src + \" -> \" + p);\r\n            }\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long addCacheDirective(CacheDirectiveInfo info) throws IOException\n{\r\n    return addCacheDirective(info, EnumSet.noneOf(CacheFlag.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long addCacheDirective(CacheDirectiveInfo info, EnumSet<CacheFlag> flags) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.ADD_CACHE_DIRECTIVE);\r\n    Preconditions.checkNotNull(info.getPath());\r\n    Path path = new Path(getPathName(fixRelativePart(info.getPath()))).makeQualified(getUri(), getWorkingDirectory());\r\n    return dfs.addCacheDirective(new CacheDirectiveInfo.Builder(info).setPath(path).build(), flags);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "modifyCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void modifyCacheDirective(CacheDirectiveInfo info) throws IOException\n{\r\n    modifyCacheDirective(info, EnumSet.noneOf(CacheFlag.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "modifyCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void modifyCacheDirective(CacheDirectiveInfo info, EnumSet<CacheFlag> flags) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.MODIFY_CACHE_DIRECTIVE);\r\n    if (info.getPath() != null) {\r\n        info = new CacheDirectiveInfo.Builder(info).setPath(new Path(getPathName(fixRelativePart(info.getPath()))).makeQualified(getUri(), getWorkingDirectory())).build();\r\n    }\r\n    dfs.modifyCacheDirective(info, flags);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeCacheDirective(long id) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.REMOVE_CACHE_DIRECTIVE);\r\n    dfs.removeCacheDirective(id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listCacheDirectives",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "RemoteIterator<CacheDirectiveEntry> listCacheDirectives(CacheDirectiveInfo filter) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.LIST_CACHE_DIRECTIVE);\r\n    if (filter == null) {\r\n        filter = new CacheDirectiveInfo.Builder().build();\r\n    }\r\n    if (filter.getPath() != null) {\r\n        filter = new CacheDirectiveInfo.Builder(filter).setPath(new Path(getPathName(fixRelativePart(filter.getPath())))).build();\r\n    }\r\n    final RemoteIterator<CacheDirectiveEntry> iter = dfs.listCacheDirectives(filter);\r\n    return new RemoteIterator<CacheDirectiveEntry>() {\r\n\r\n        @Override\r\n        public boolean hasNext() throws IOException {\r\n            return iter.hasNext();\r\n        }\r\n\r\n        @Override\r\n        public CacheDirectiveEntry next() throws IOException {\r\n            CacheDirectiveEntry desc = iter.next();\r\n            CacheDirectiveInfo info = desc.getInfo();\r\n            Path p = info.getPath().makeQualified(getUri(), getWorkingDirectory());\r\n            return new CacheDirectiveEntry(new CacheDirectiveInfo.Builder(info).setPath(p).build(), desc.getStats());\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addCachePool",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addCachePool(CachePoolInfo info) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.ADD_CACHE_POOL);\r\n    CachePoolInfo.validate(info);\r\n    dfs.addCachePool(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "modifyCachePool",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void modifyCachePool(CachePoolInfo info) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.MODIFY_CACHE_POOL);\r\n    CachePoolInfo.validate(info);\r\n    dfs.modifyCachePool(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeCachePool",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeCachePool(String poolName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.REMOVE_CACHE_POOL);\r\n    CachePoolInfo.validateName(poolName);\r\n    dfs.removeCachePool(poolName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listCachePools",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RemoteIterator<CachePoolEntry> listCachePools() throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.LIST_CACHE_POOL);\r\n    return dfs.listCachePools();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "modifyAclEntries",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void modifyAclEntries(Path path, final List<AclEntry> aclSpec) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.MODIFY_ACL_ENTRIES);\r\n    Path absF = fixRelativePart(path);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.modifyAclEntries(getPathName(p), aclSpec);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            fs.modifyAclEntries(p, aclSpec);\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeAclEntries",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeAclEntries(Path path, final List<AclEntry> aclSpec) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.REMOVE_ACL_ENTRIES);\r\n    Path absF = fixRelativePart(path);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.removeAclEntries(getPathName(p), aclSpec);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            fs.removeAclEntries(p, aclSpec);\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeDefaultAcl",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeDefaultAcl(Path path) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.REMOVE_DEFAULT_ACL);\r\n    final Path absF = fixRelativePart(path);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.removeDefaultAcl(getPathName(p));\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            fs.removeDefaultAcl(p);\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeAcl",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeAcl(Path path) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.REMOVE_ACL);\r\n    final Path absF = fixRelativePart(path);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.removeAcl(getPathName(p));\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            fs.removeAcl(p);\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setAcl",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setAcl(Path path, final List<AclEntry> aclSpec) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_ACL);\r\n    Path absF = fixRelativePart(path);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.setAcl(getPathName(p), aclSpec);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            fs.setAcl(p, aclSpec);\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAclStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AclStatus getAclStatus(Path path) throws IOException\n{\r\n    final Path absF = fixRelativePart(path);\r\n    return new FileSystemLinkResolver<AclStatus>() {\r\n\r\n        @Override\r\n        public AclStatus doCall(final Path p) throws IOException {\r\n            return dfs.getAclStatus(getPathName(p));\r\n        }\r\n\r\n        @Override\r\n        public AclStatus next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.getAclStatus(p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createEncryptionZone",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createEncryptionZone(final Path path, final String keyName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.CREATE_ENCRYPTION_ZONE);\r\n    Path absF = fixRelativePart(path);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.createEncryptionZone(getPathName(p), keyName);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                myDfs.createEncryptionZone(p, keyName);\r\n                return null;\r\n            } else {\r\n                throw new UnsupportedOperationException(\"Cannot call createEncryptionZone\" + \" on a symlink to a non-DistributedFileSystem: \" + path + \" -> \" + p);\r\n            }\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getEZForPath",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "EncryptionZone getEZForPath(final Path path) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_ENCRYPTION_ZONE);\r\n    Preconditions.checkNotNull(path);\r\n    Path absF = fixRelativePart(path);\r\n    return new FileSystemLinkResolver<EncryptionZone>() {\r\n\r\n        @Override\r\n        public EncryptionZone doCall(final Path p) throws IOException {\r\n            return dfs.getEZForPath(getPathName(p));\r\n        }\r\n\r\n        @Override\r\n        public EncryptionZone next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                return myDfs.getEZForPath(p);\r\n            } else {\r\n                throw new UnsupportedOperationException(\"Cannot call getEZForPath\" + \" on a symlink to a non-DistributedFileSystem: \" + path + \" -> \" + p);\r\n            }\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listEncryptionZones",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RemoteIterator<EncryptionZone> listEncryptionZones() throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.LIST_ENCRYPTION_ZONE);\r\n    return dfs.listEncryptionZones();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "reencryptEncryptionZone",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void reencryptEncryptionZone(final Path zone, final ReencryptAction action) throws IOException\n{\r\n    final Path absF = fixRelativePart(zone);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.reencryptEncryptionZone(getPathName(p), action);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                myDfs.reencryptEncryptionZone(p, action);\r\n                return null;\r\n            }\r\n            throw new UnsupportedOperationException(\"Cannot call reencryptEncryptionZone\" + \" on a symlink to a non-DistributedFileSystem: \" + zone + \" -> \" + p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listReencryptionStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<ZoneReencryptionStatus> listReencryptionStatus() throws IOException\n{\r\n    return dfs.listReencryptionStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFileEncryptionInfo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileEncryptionInfo getFileEncryptionInfo(final Path path) throws IOException\n{\r\n    Path absF = fixRelativePart(path);\r\n    return new FileSystemLinkResolver<FileEncryptionInfo>() {\r\n\r\n        @Override\r\n        public FileEncryptionInfo doCall(final Path p) throws IOException {\r\n            final HdfsFileStatus fi = dfs.getFileInfo(getPathName(p));\r\n            if (fi == null) {\r\n                throw new FileNotFoundException(\"File does not exist: \" + p);\r\n            }\r\n            return fi.getFileEncryptionInfo();\r\n        }\r\n\r\n        @Override\r\n        public FileEncryptionInfo next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                return myDfs.getFileEncryptionInfo(p);\r\n            }\r\n            throw new UnsupportedOperationException(\"Cannot call getFileEncryptionInfo\" + \" on a symlink to a non-DistributedFileSystem: \" + path + \" -> \" + p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "provisionEZTrash",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void provisionEZTrash(final Path path, final FsPermission trashPermission) throws IOException\n{\r\n    Path absF = fixRelativePart(path);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(Path p) throws IOException {\r\n            provisionEZTrash(getPathName(p), trashPermission);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(FileSystem fs, Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                myDfs.provisionEZTrash(p, trashPermission);\r\n                return null;\r\n            }\r\n            throw new UnsupportedOperationException(\"Cannot provisionEZTrash \" + \"through a symlink to a non-DistributedFileSystem: \" + fs + \" -> \" + p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "provisionEZTrash",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void provisionEZTrash(String path, FsPermission trashPermission) throws IOException\n{\r\n    EncryptionZone ez = dfs.getEZForPath(path);\r\n    if (ez == null) {\r\n        throw new IllegalArgumentException(path + \" is not an encryption zone.\");\r\n    }\r\n    String ezPath = ez.getPath();\r\n    if (!path.toString().equals(ezPath)) {\r\n        throw new IllegalArgumentException(path + \" is not the root of an \" + \"encryption zone. Do you mean \" + ez.getPath() + \"?\");\r\n    }\r\n    Path trashPath = new Path(ez.getPath(), FileSystem.TRASH_PREFIX);\r\n    try {\r\n        FileStatus trashFileStatus = getFileStatus(trashPath);\r\n        String errMessage = \"Will not provision new trash directory for \" + \"encryption zone \" + ez.getPath() + \". Path already exists.\";\r\n        if (!trashFileStatus.isDirectory()) {\r\n            errMessage += \"\\r\\n\" + \"Warning: \" + trashPath.toString() + \" is not a directory\";\r\n        }\r\n        if (!trashFileStatus.getPermission().equals(trashPermission)) {\r\n            errMessage += \"\\r\\n\" + \"Warning: the permission of \" + trashPath.toString() + \" is not \" + trashPermission;\r\n        }\r\n        throw new FileAlreadyExistsException(errMessage);\r\n    } catch (FileNotFoundException ignored) {\r\n    }\r\n    mkdir(trashPath, trashPermission);\r\n    setPermission(trashPath, trashPermission);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "provisionSnapshotTrash",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path provisionSnapshotTrash(final Path path, final FsPermission trashPermission) throws IOException\n{\r\n    Path absF = fixRelativePart(path);\r\n    return new FileSystemLinkResolver<Path>() {\r\n\r\n        @Override\r\n        public Path doCall(Path p) throws IOException {\r\n            return provisionSnapshotTrash(getPathName(p), trashPermission);\r\n        }\r\n\r\n        @Override\r\n        public Path next(FileSystem fs, Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                return myDfs.provisionSnapshotTrash(p, trashPermission);\r\n            }\r\n            throw new UnsupportedOperationException(\"Cannot provisionSnapshotTrash through a symlink to\" + \" a non-DistributedFileSystem: \" + fs + \" -> \" + p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "provisionSnapshotTrash",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "Path provisionSnapshotTrash(String pathStr, FsPermission trashPermission) throws IOException\n{\r\n    Path path = new Path(pathStr);\r\n    FileStatus fileStatus = getFileStatus(path);\r\n    if (!fileStatus.isSnapshotEnabled()) {\r\n        throw new IllegalArgumentException(path + \" is not a snapshottable directory.\");\r\n    }\r\n    Path trashPath = new Path(path, FileSystem.TRASH_PREFIX);\r\n    try {\r\n        FileStatus trashFileStatus = getFileStatus(trashPath);\r\n        boolean throwException = false;\r\n        String errMessage = \"Can't provision trash for snapshottable directory \" + pathStr + \" because trash path \" + trashPath.toString() + \" already exists.\";\r\n        if (!trashFileStatus.isDirectory()) {\r\n            throwException = true;\r\n            errMessage += \"\\r\\n\" + \"WARNING: \" + trashPath.toString() + \" is not a directory.\";\r\n        }\r\n        if (!trashFileStatus.getPermission().equals(trashPermission)) {\r\n            throwException = true;\r\n            errMessage += \"\\r\\n\" + \"WARNING: Permission of \" + trashPath.toString() + \" differs from provided permission \" + trashPermission;\r\n        }\r\n        if (throwException) {\r\n            throw new FileAlreadyExistsException(errMessage);\r\n        }\r\n    } catch (FileNotFoundException ignored) {\r\n    }\r\n    mkdir(trashPath, trashPermission);\r\n    setPermission(trashPath, trashPermission);\r\n    if (!isSnapshotTrashRootEnabled()) {\r\n        DFSClient.LOG.warn(\"New trash is provisioned, but the snapshot trash root\" + \" feature is disabled. This new trash but won't be automatically\" + \" utilized unless the feature is enabled on the NameNode.\");\r\n    }\r\n    return trashPath;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setXAttr",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setXAttr(Path path, final String name, final byte[] value, final EnumSet<XAttrSetFlag> flag) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_XATTR);\r\n    Path absF = fixRelativePart(path);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.setXAttr(getPathName(p), name, value, flag);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            fs.setXAttr(p, name, value, flag);\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getXAttr",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "byte[] getXAttr(Path path, final String name) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_XATTR);\r\n    final Path absF = fixRelativePart(path);\r\n    return new FileSystemLinkResolver<byte[]>() {\r\n\r\n        @Override\r\n        public byte[] doCall(final Path p) throws IOException {\r\n            return dfs.getXAttr(getPathName(p), name);\r\n        }\r\n\r\n        @Override\r\n        public byte[] next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.getXAttr(p, name);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(Path path) throws IOException\n{\r\n    final Path absF = fixRelativePart(path);\r\n    return new FileSystemLinkResolver<Map<String, byte[]>>() {\r\n\r\n        @Override\r\n        public Map<String, byte[]> doCall(final Path p) throws IOException {\r\n            return dfs.getXAttrs(getPathName(p));\r\n        }\r\n\r\n        @Override\r\n        public Map<String, byte[]> next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.getXAttrs(p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(Path path, final List<String> names) throws IOException\n{\r\n    final Path absF = fixRelativePart(path);\r\n    return new FileSystemLinkResolver<Map<String, byte[]>>() {\r\n\r\n        @Override\r\n        public Map<String, byte[]> doCall(final Path p) throws IOException {\r\n            return dfs.getXAttrs(getPathName(p), names);\r\n        }\r\n\r\n        @Override\r\n        public Map<String, byte[]> next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.getXAttrs(p, names);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listXAttrs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<String> listXAttrs(Path path) throws IOException\n{\r\n    final Path absF = fixRelativePart(path);\r\n    return new FileSystemLinkResolver<List<String>>() {\r\n\r\n        @Override\r\n        public List<String> doCall(final Path p) throws IOException {\r\n            return dfs.listXAttrs(getPathName(p));\r\n        }\r\n\r\n        @Override\r\n        public List<String> next(final FileSystem fs, final Path p) throws IOException {\r\n            return fs.listXAttrs(p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeXAttr",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeXAttr(Path path, final String name) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.REMOVE_XATTR);\r\n    Path absF = fixRelativePart(path);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.removeXAttr(getPathName(p), name);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            fs.removeXAttr(p, name);\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "access",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void access(Path path, final FsAction mode) throws IOException\n{\r\n    final Path absF = fixRelativePart(path);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.checkAccess(getPathName(p), mode);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            fs.access(p, mode);\r\n            return null;\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getKeyProviderUri",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI getKeyProviderUri() throws IOException\n{\r\n    return dfs.getKeyProviderUri();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getKeyProvider",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "KeyProvider getKeyProvider() throws IOException\n{\r\n    return dfs.getKeyProvider();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAdditionalTokenIssuers",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DelegationTokenIssuer[] getAdditionalTokenIssuers() throws IOException\n{\r\n    KeyProvider keyProvider = getKeyProvider();\r\n    if (keyProvider instanceof DelegationTokenIssuer) {\r\n        return new DelegationTokenIssuer[] { (DelegationTokenIssuer) keyProvider };\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getInotifyEventStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DFSInotifyEventInputStream getInotifyEventStream() throws IOException\n{\r\n    return dfs.getInotifyEventStream();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getInotifyEventStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DFSInotifyEventInputStream getInotifyEventStream(long lastReadTxid) throws IOException\n{\r\n    return dfs.getInotifyEventStream(lastReadTxid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setErasureCodingPolicy(final Path path, final String ecPolicyName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SET_EC_POLICY);\r\n    Path absF = fixRelativePart(path);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.setErasureCodingPolicy(getPathName(p), ecPolicyName);\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                myDfs.setErasureCodingPolicy(p, ecPolicyName);\r\n                return null;\r\n            }\r\n            throw new UnsupportedOperationException(\"Cannot setErasureCodingPolicy through a symlink to a \" + \"non-DistributedFileSystem: \" + path + \" -> \" + p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "satisfyStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void satisfyStoragePolicy(final Path path) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.SATISFY_STORAGE_POLICY);\r\n    Path absF = fixRelativePart(path);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(Path p) throws IOException {\r\n            dfs.satisfyStoragePolicy(getPathName(p));\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(FileSystem fs, Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                myDfs.satisfyStoragePolicy(p);\r\n                return null;\r\n            }\r\n            throw new UnsupportedOperationException(\"Cannot satisfyStoragePolicy through a symlink to a \" + \"non-DistributedFileSystem: \" + path + \" -> \" + p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ErasureCodingPolicy getErasureCodingPolicy(final Path path) throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_EC_POLICY);\r\n    Path absF = fixRelativePart(path);\r\n    return new FileSystemLinkResolver<ErasureCodingPolicy>() {\r\n\r\n        @Override\r\n        public ErasureCodingPolicy doCall(final Path p) throws IOException {\r\n            return dfs.getErasureCodingPolicy(getPathName(p));\r\n        }\r\n\r\n        @Override\r\n        public ErasureCodingPolicy next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                return myDfs.getErasureCodingPolicy(p);\r\n            }\r\n            throw new UnsupportedOperationException(\"Cannot getErasureCodingPolicy through a symlink to a \" + \"non-DistributedFileSystem: \" + path + \" -> \" + p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAllErasureCodingPolicies",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Collection<ErasureCodingPolicyInfo> getAllErasureCodingPolicies() throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_EC_POLICIES);\r\n    return Arrays.asList(dfs.getErasureCodingPolicies());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getAllErasureCodingCodecs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<String, String> getAllErasureCodingCodecs() throws IOException\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_EC_CODECS);\r\n    return dfs.getErasureCodingCodecs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "addErasureCodingPolicies",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AddErasureCodingPolicyResponse[] addErasureCodingPolicies(ErasureCodingPolicy[] policies) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.ADD_EC_POLICY);\r\n    return dfs.addErasureCodingPolicies(policies);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "removeErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.REMOVE_EC_POLICY);\r\n    dfs.removeErasureCodingPolicy(ecPolicyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "enableErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void enableErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.ENABLE_EC_POLICY);\r\n    dfs.enableErasureCodingPolicy(ecPolicyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "disableErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void disableErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.DISABLE_EC_POLICY);\r\n    dfs.disableErasureCodingPolicy(ecPolicyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "unsetErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void unsetErasureCodingPolicy(final Path path) throws IOException\n{\r\n    statistics.incrementWriteOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.UNSET_EC_POLICY);\r\n    Path absF = fixRelativePart(path);\r\n    new FileSystemLinkResolver<Void>() {\r\n\r\n        @Override\r\n        public Void doCall(final Path p) throws IOException {\r\n            dfs.unsetErasureCodingPolicy(getPathName(p));\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Void next(final FileSystem fs, final Path p) throws IOException {\r\n            if (fs instanceof DistributedFileSystem) {\r\n                DistributedFileSystem myDfs = (DistributedFileSystem) fs;\r\n                myDfs.unsetErasureCodingPolicy(p);\r\n                return null;\r\n            }\r\n            throw new UnsupportedOperationException(\"Cannot unsetErasureCodingPolicy through a symlink to a \" + \"non-DistributedFileSystem: \" + path + \" -> \" + p);\r\n        }\r\n    }.resolve(this, absF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getECTopologyResultForPolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ECTopologyVerifierResult getECTopologyResultForPolicies(final String... policyNames) throws IOException\n{\r\n    return dfs.getECTopologyResultForPolicies(policyNames);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getTrashRoot",
  "errType" : [ "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "Path getTrashRoot(Path path)\n{\r\n    statistics.incrementReadOps(1);\r\n    storageStatistics.incrementOpCounter(OpType.GET_TRASH_ROOT);\r\n    if (path == null) {\r\n        return super.getTrashRoot(null);\r\n    }\r\n    String ssTrashRoot = null;\r\n    try {\r\n        if (dfs.isSnapshotTrashRootEnabled()) {\r\n            String ssRoot = dfs.getSnapshotRoot(path);\r\n            if (ssRoot != null) {\r\n                ssTrashRoot = DFSUtilClient.getSnapshotTrashRoot(ssRoot, dfs.ugi);\r\n            }\r\n        }\r\n    } catch (IOException ioe) {\r\n        DFSClient.LOG.warn(\"Exception while checking whether the path is in a \" + \"snapshottable directory\", ioe);\r\n    }\r\n    try {\r\n        if (!dfs.isHDFSEncryptionEnabled()) {\r\n            if (ssTrashRoot == null) {\r\n                return super.getTrashRoot(path);\r\n            } else {\r\n                return this.makeQualified(new Path(ssTrashRoot));\r\n            }\r\n        }\r\n    } catch (IOException ioe) {\r\n        DFSClient.LOG.warn(\"Exception while checking whether encryption zone is \" + \"supported\", ioe);\r\n    }\r\n    String parentSrc = path.isRoot() ? path.toUri().getPath() : path.getParent().toUri().getPath();\r\n    String ezTrashRoot = null;\r\n    try {\r\n        EncryptionZone ez = dfs.getEZForPath(parentSrc);\r\n        if ((ez != null)) {\r\n            ezTrashRoot = DFSUtilClient.getEZTrashRoot(ez, dfs.ugi);\r\n        }\r\n    } catch (IOException e) {\r\n        DFSClient.LOG.warn(\"Exception in checking the encryption zone for the \" + \"path \" + parentSrc + \". \" + e.getMessage());\r\n    }\r\n    if (ssTrashRoot == null) {\r\n        if (ezTrashRoot == null) {\r\n            return super.getTrashRoot(path);\r\n        } else {\r\n            return this.makeQualified(new Path(ezTrashRoot));\r\n        }\r\n    } else {\r\n        if (ezTrashRoot == null) {\r\n            return this.makeQualified(new Path(ssTrashRoot));\r\n        } else {\r\n            return this.makeQualified(new Path(ssTrashRoot.length() > ezTrashRoot.length() ? ssTrashRoot : ezTrashRoot));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getTrashRoots",
  "errType" : [ "IOException", "FileNotFoundException", "IOException", "FileNotFoundException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "Collection<FileStatus> getTrashRoots(boolean allUsers)\n{\r\n    Set<FileStatus> ret = new HashSet<>();\r\n    ret.addAll(super.getTrashRoots(allUsers));\r\n    try {\r\n        final RemoteIterator<EncryptionZone> it = dfs.listEncryptionZones();\r\n        while (it.hasNext()) {\r\n            EncryptionZone ez = it.next();\r\n            Path ezTrashRoot = new Path(ez.getPath(), FileSystem.TRASH_PREFIX);\r\n            if (!exists(ezTrashRoot)) {\r\n                continue;\r\n            }\r\n            if (allUsers) {\r\n                for (FileStatus candidate : listStatus(ezTrashRoot)) {\r\n                    if (exists(candidate.getPath())) {\r\n                        ret.add(candidate);\r\n                    }\r\n                }\r\n            } else {\r\n                Path userTrash = new Path(DFSUtilClient.getEZTrashRoot(ez, dfs.ugi));\r\n                try {\r\n                    ret.add(getFileStatus(userTrash));\r\n                } catch (FileNotFoundException ignored) {\r\n                }\r\n            }\r\n        }\r\n    } catch (IOException e) {\r\n        DFSClient.LOG.warn(\"Cannot get all encrypted trash roots\", e);\r\n    }\r\n    try {\r\n        if (dfs.isSnapshotTrashRootEnabled()) {\r\n            SnapshottableDirectoryStatus[] lst = dfs.getSnapshottableDirListing();\r\n            if (lst != null) {\r\n                for (SnapshottableDirectoryStatus dirStatus : lst) {\r\n                    String ssDir = dirStatus.getFullPath().toString();\r\n                    Path ssTrashRoot = new Path(ssDir, FileSystem.TRASH_PREFIX);\r\n                    if (!exists(ssTrashRoot)) {\r\n                        continue;\r\n                    }\r\n                    if (allUsers) {\r\n                        for (FileStatus candidate : listStatus(ssTrashRoot)) {\r\n                            if (exists(candidate.getPath())) {\r\n                                ret.add(candidate);\r\n                            }\r\n                        }\r\n                    } else {\r\n                        Path userTrash = new Path(DFSUtilClient.getSnapshotTrashRoot(ssDir, dfs.ugi));\r\n                        try {\r\n                            ret.add(getFileStatus(userTrash));\r\n                        } catch (FileNotFoundException ignored) {\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    } catch (IOException e) {\r\n        DFSClient.LOG.warn(\"Cannot get snapshot trash roots\", e);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "fixRelativePart",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path fixRelativePart(Path p)\n{\r\n    return super.fixRelativePart(p);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getFsStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Statistics getFsStatistics()\n{\r\n    return statistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "getDFSOpsCountStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DFSOpsCountStatistics getDFSOpsCountStatistics()\n{\r\n    return storageStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsDataOutputStreamBuilder createFile(Path path)\n{\r\n    return new HdfsDataOutputStreamBuilder(this, path).create().overwrite(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<OpenFileEntry> listOpenFiles() throws IOException\n{\r\n    return dfs.listOpenFiles();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<OpenFileEntry> listOpenFiles(EnumSet<OpenFilesType> openFilesTypes) throws IOException\n{\r\n    return dfs.listOpenFiles(openFilesTypes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<OpenFileEntry> listOpenFiles(EnumSet<OpenFilesType> openFilesTypes, String path) throws IOException\n{\r\n    Path absF = fixRelativePart(new Path(path));\r\n    return dfs.listOpenFiles(openFilesTypes, getPathName(absF));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "appendFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsDataOutputStreamBuilder appendFile(Path path)\n{\r\n    return new HdfsDataOutputStreamBuilder(this, path).append();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "hasPathCapability",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean hasPathCapability(final Path path, final String capability) throws IOException\n{\r\n    final Path p = makeQualified(path);\r\n    Optional<Boolean> cap = DfsPathCapabilities.hasPathCapability(p, capability);\r\n    if (cap.isPresent()) {\r\n        return cap.get();\r\n    }\r\n    switch(validatePathCapabilityArgs(path, capability)) {\r\n        case CommonPathCapabilities.FS_EXPERIMENTAL_BATCH_LISTING:\r\n            return true;\r\n        default:\r\n    }\r\n    return super.hasPathCapability(p, capability);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createMultipartUploader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MultipartUploaderBuilder createMultipartUploader(final Path basePath) throws IOException\n{\r\n    return new FileSystemMultipartUploaderBuilder(this, basePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "makeRequest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BatchedEntries<CachePoolEntry> makeRequest(String prevKey) throws IOException\n{\r\n    try (TraceScope ignored = tracer.newScope(\"listCachePools\")) {\r\n        return namenode.listCachePools(prevKey);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "elementToPrevKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String elementToPrevKey(CachePoolEntry entry)\n{\r\n    return entry.getInfo().getPoolName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\resources",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getValue(final Configuration conf)\n{\r\n    return getValue() != null ? getValue() : conf.getLongBytes(DFS_BLOCK_SIZE_KEY, DFS_BLOCK_SIZE_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "composePolicyName",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String composePolicyName(ECSchema schema, int cellSize)\n{\r\n    Preconditions.checkNotNull(schema);\r\n    Preconditions.checkArgument(cellSize > 0, \"cellSize must be positive\");\r\n    Preconditions.checkArgument(cellSize % 1024 == 0, \"cellSize must be 1024 aligned\");\r\n    return schema.getCodecName().toUpperCase() + \"-\" + schema.getNumDataUnits() + \"-\" + schema.getNumParityUnits() + \"-\" + cellSize / 1024 + \"k\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getSchema",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ECSchema getSchema()\n{\r\n    return schema;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getCellSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getCellSize()\n{\r\n    return cellSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getNumDataUnits",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumDataUnits()\n{\r\n    return schema.getNumDataUnits();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getNumParityUnits",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumParityUnits()\n{\r\n    return schema.getNumParityUnits();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getCodecName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getCodecName()\n{\r\n    return schema.getCodecName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte getId()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isReplicationPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isReplicationPolicy()\n{\r\n    return (id == ErasureCodeConstants.REPLICATION_POLICY_ID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "isSystemPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSystemPolicy()\n{\r\n    return (this.id < ErasureCodeConstants.USER_DEFINED_POLICY_START_ID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (o == null) {\r\n        return false;\r\n    }\r\n    if (o == this) {\r\n        return true;\r\n    }\r\n    if (o.getClass() != getClass()) {\r\n        return false;\r\n    }\r\n    ErasureCodingPolicy rhs = (ErasureCodingPolicy) o;\r\n    return new EqualsBuilder().append(name, rhs.name).append(schema, rhs.schema).append(cellSize, rhs.cellSize).append(id, rhs.id).isEquals();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return new HashCodeBuilder(303855623, 582626729).append(name).append(schema).append(cellSize).append(id).toHashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"ErasureCodingPolicy=[\" + \"Name=\" + name + \", \" + \"Schema=[\" + schema.toString() + \"], \" + \"CellSize=\" + cellSize + \", \" + \"Id=\" + id + \"]\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockPoolId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockPoolId()\n{\r\n    return poolId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getBlockName()\n{\r\n    return block.getBlockName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getNumBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumBytes()\n{\r\n    return block.getNumBytes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getBlockId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBlockId()\n{\r\n    return block.getBlockId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getGenerationStamp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getGenerationStamp()\n{\r\n    return block.getGenerationStamp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setBlockId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setBlockId(final long bid)\n{\r\n    block.setBlockId(bid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setGenerationStamp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setGenerationStamp(final long genStamp)\n{\r\n    block.setGenerationStamp(genStamp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setNumBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumBytes(final long len)\n{\r\n    block.setNumBytes(len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "set",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void set(String poolId, Block blk)\n{\r\n    this.poolId = poolId != null ? poolId.intern() : null;\r\n    this.block = blk;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLocalBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Block getLocalBlock(final ExtendedBlock b)\n{\r\n    return b == null ? null : b.getLocalBlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLocalBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Block getLocalBlock()\n{\r\n    return block;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (this == o) {\r\n        return true;\r\n    }\r\n    if (!(o instanceof ExtendedBlock)) {\r\n        return false;\r\n    }\r\n    ExtendedBlock b = (ExtendedBlock) o;\r\n    return b.block.equals(block) && (b.poolId != null ? b.poolId.equals(poolId) : poolId == null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return new HashCodeBuilder(31, 17).append(poolId).append(block).toHashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return poolId + \":\" + block;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "getEntry",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RenameEntry getEntry(long inodeId)\n{\r\n    RenameEntry entry = renameMap.get(inodeId);\r\n    if (entry == null) {\r\n        entry = new RenameEntry();\r\n        renameMap.put(inodeId, entry);\r\n    }\r\n    return entry;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "generateReportList",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void generateReportList()\n{\r\n    mlist.sort(INODE_COMPARATOR);\r\n    for (DiffReportListingEntry created : clist) {\r\n        ChildrenDiff entry = dirDiffMap.get(created.getDirId());\r\n        if (entry == null) {\r\n            List<DiffReportListingEntry> createdList = new ChunkedArrayList<>();\r\n            createdList.add(created);\r\n            ChildrenDiff list = new ChildrenDiff(createdList, null);\r\n            dirDiffMap.put(created.getDirId(), list);\r\n        } else {\r\n            dirDiffMap.get(created.getDirId()).getCreatedList().add(created);\r\n        }\r\n        if (created.isReference()) {\r\n            RenameEntry renameEntry = getEntry(created.getFileId());\r\n            if (renameEntry.getTargetPath() != null) {\r\n                renameEntry.setTarget(created.getSourcePath());\r\n            }\r\n        }\r\n    }\r\n    for (DiffReportListingEntry deleted : dlist) {\r\n        ChildrenDiff entry = dirDiffMap.get(deleted.getDirId());\r\n        if (entry == null || (entry.getDeletedList().isEmpty())) {\r\n            ChildrenDiff list;\r\n            List<DiffReportListingEntry> deletedList = new ChunkedArrayList<>();\r\n            deletedList.add(deleted);\r\n            if (entry == null) {\r\n                list = new ChildrenDiff(null, deletedList);\r\n            } else {\r\n                list = new ChildrenDiff(entry.getCreatedList(), deletedList);\r\n            }\r\n            dirDiffMap.put(deleted.getDirId(), list);\r\n        } else {\r\n            entry.getDeletedList().add(deleted);\r\n        }\r\n        if (deleted.isReference()) {\r\n            RenameEntry renameEntry = getEntry(deleted.getFileId());\r\n            renameEntry.setTarget(deleted.getTargetPath());\r\n            renameEntry.setSource(deleted.getSourcePath());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "generateReport",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "SnapshotDiffReport generateReport()\n{\r\n    List<DiffReportEntry> diffReportList = new ChunkedArrayList<>();\r\n    generateReportList();\r\n    for (DiffReportListingEntry modified : mlist) {\r\n        diffReportList.add(new DiffReportEntry(DiffType.MODIFY, modified.getSourcePath(), null));\r\n        if (modified.isReference() && dirDiffMap.get(modified.getDirId()) != null) {\r\n            List<DiffReportEntry> subList = generateReport(modified);\r\n            diffReportList.addAll(subList);\r\n        }\r\n    }\r\n    return new SnapshotDiffReport(snapshotRoot, fromSnapshot, toSnapshot, diffReportList);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "generateReport",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "List<DiffReportEntry> generateReport(DiffReportListingEntry modified)\n{\r\n    List<DiffReportEntry> diffReportList = new ChunkedArrayList<>();\r\n    ChildrenDiff list = dirDiffMap.get(modified.getDirId());\r\n    for (DiffReportListingEntry created : list.getCreatedList()) {\r\n        RenameEntry entry = renameMap.get(created.getFileId());\r\n        if (entry == null || !entry.isRename()) {\r\n            diffReportList.add(new DiffReportEntry(isFromEarlier ? DiffType.CREATE : DiffType.DELETE, created.getSourcePath()));\r\n        }\r\n    }\r\n    for (DiffReportListingEntry deleted : list.getDeletedList()) {\r\n        RenameEntry entry = renameMap.get(deleted.getFileId());\r\n        if (entry != null && entry.isRename()) {\r\n            diffReportList.add(new DiffReportEntry(DiffType.RENAME, isFromEarlier ? entry.getSourcePath() : entry.getTargetPath(), isFromEarlier ? entry.getTargetPath() : entry.getSourcePath()));\r\n        } else {\r\n            diffReportList.add(new DiffReportEntry(isFromEarlier ? DiffType.DELETE : DiffType.CREATE, deleted.getSourcePath()));\r\n        }\r\n    }\r\n    return diffReportList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\inotify",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return eventType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getPoolName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPoolName()\n{\r\n    return poolName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getOwnerName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getOwnerName()\n{\r\n    return ownerName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setOwnerName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CachePoolInfo setOwnerName(String ownerName)\n{\r\n    this.ownerName = ownerName;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getGroupName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getGroupName()\n{\r\n    return groupName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setGroupName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CachePoolInfo setGroupName(String groupName)\n{\r\n    this.groupName = groupName;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getMode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FsPermission getMode()\n{\r\n    return mode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setMode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CachePoolInfo setMode(FsPermission mode)\n{\r\n    this.mode = mode;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getLimit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Long getLimit()\n{\r\n    return limit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setLimit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CachePoolInfo setLimit(Long bytes)\n{\r\n    this.limit = bytes;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getDefaultReplication",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Short getDefaultReplication()\n{\r\n    return defaultReplication;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setDefaultReplication",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CachePoolInfo setDefaultReplication(Short repl)\n{\r\n    this.defaultReplication = repl;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "getMaxRelativeExpiryMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Long getMaxRelativeExpiryMs()\n{\r\n    return maxRelativeExpiryMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "setMaxRelativeExpiryMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CachePoolInfo setMaxRelativeExpiryMs(Long ms)\n{\r\n    this.maxRelativeExpiryMs = ms;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"{\" + \"poolName:\" + poolName + \", ownerName:\" + ownerName + \", groupName:\" + groupName + \", mode:\" + ((mode == null) ? \"null\" : String.format(\"0%03o\", mode.toShort())) + \", limit:\" + limit + \", defaultReplication:\" + defaultReplication + \", maxRelativeExpiryMs:\" + maxRelativeExpiryMs + \"}\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (o == null) {\r\n        return false;\r\n    }\r\n    if (o == this) {\r\n        return true;\r\n    }\r\n    if (o.getClass() != getClass()) {\r\n        return false;\r\n    }\r\n    CachePoolInfo other = (CachePoolInfo) o;\r\n    return new EqualsBuilder().append(poolName, other.poolName).append(ownerName, other.ownerName).append(groupName, other.groupName).append(mode, other.mode).append(limit, other.limit).append(defaultReplication, other.defaultReplication).append(maxRelativeExpiryMs, other.maxRelativeExpiryMs).isEquals();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return new HashCodeBuilder().append(poolName).append(ownerName).append(groupName).append(mode).append(limit).append(defaultReplication).append(maxRelativeExpiryMs).hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void validate(CachePoolInfo info) throws IOException\n{\r\n    if (info == null) {\r\n        throw new InvalidRequestException(\"CachePoolInfo is null\");\r\n    }\r\n    if ((info.getLimit() != null) && (info.getLimit() < 0)) {\r\n        throw new InvalidRequestException(\"Limit is negative.\");\r\n    }\r\n    if ((info.getDefaultReplication() != null) && (info.getDefaultReplication() < 0)) {\r\n        throw new InvalidRequestException(\"Default Replication is negative\");\r\n    }\r\n    if (info.getMaxRelativeExpiryMs() != null) {\r\n        long maxRelativeExpiryMs = info.getMaxRelativeExpiryMs();\r\n        if (maxRelativeExpiryMs < 0l) {\r\n            throw new InvalidRequestException(\"Max relative expiry is negative.\");\r\n        }\r\n        if (maxRelativeExpiryMs > Expiration.MAX_RELATIVE_EXPIRY_MS) {\r\n            throw new InvalidRequestException(\"Max relative expiry is too big.\");\r\n        }\r\n    }\r\n    validateName(info.poolName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "validateName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validateName(String poolName) throws IOException\n{\r\n    if (poolName == null || poolName.isEmpty()) {\r\n        throw new IOException(\"invalid empty cache pool name\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "notNull",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String notNull(Configuration conf, String key)\n{\r\n    String value = conf.get(key);\r\n    if (value == null) {\r\n        throw new IllegalArgumentException(\"No value for \" + key + \" found in conf file.\");\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "postBody",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String postBody(String... kv) throws UnsupportedEncodingException\n{\r\n    if (kv.length % 2 != 0) {\r\n        throw new IllegalArgumentException(\"Arguments must be key value pairs\");\r\n    }\r\n    StringBuilder sb = new StringBuilder();\r\n    int i = 0;\r\n    while (i < kv.length) {\r\n        if (i > 0) {\r\n            sb.append(\"&\");\r\n        }\r\n        sb.append(URLEncoder.encode(kv[i++], \"UTF-8\"));\r\n        sb.append(\"=\");\r\n        sb.append(URLEncoder.encode(kv[i++], \"UTF-8\"));\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getReconfigurationStatus",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "ReconfigurationTaskStatus getReconfigurationStatus(GetReconfigurationStatusResponseProto response)\n{\r\n    Map<PropertyChange, Optional<String>> statusMap = null;\r\n    long startTime;\r\n    long endTime = 0;\r\n    startTime = response.getStartTime();\r\n    if (response.hasEndTime()) {\r\n        endTime = response.getEndTime();\r\n    }\r\n    if (response.getChangesCount() > 0) {\r\n        statusMap = Maps.newHashMap();\r\n        for (GetReconfigurationStatusConfigChangeProto change : response.getChangesList()) {\r\n            PropertyChange pc = new PropertyChange(change.getName(), change.getNewValue(), change.getOldValue());\r\n            String errorMessage = null;\r\n            if (change.hasErrorMessage()) {\r\n                errorMessage = change.getErrorMessage();\r\n            }\r\n            statusMap.put(pc, Optional.ofNullable(errorMessage));\r\n        }\r\n    }\r\n    return new ReconfigurationTaskStatus(startTime, endTime, statusMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "prepareDecodeInputs",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void prepareDecodeInputs()\n{\r\n    final ByteBuffer cur;\r\n    synchronized (dfsStripedInputStream) {\r\n        cur = dfsStripedInputStream.getCurStripeBuf().duplicate();\r\n    }\r\n    if (this.decodeInputs == null) {\r\n        this.decodeInputs = new ECChunk[dataBlkNum + parityBlkNum];\r\n    }\r\n    int bufLen = (int) alignedStripe.getSpanInBlock();\r\n    int bufOff = (int) alignedStripe.getOffsetInBlock();\r\n    for (int i = 0; i < dataBlkNum; i++) {\r\n        cur.limit(cur.capacity());\r\n        int pos = bufOff % cellSize + cellSize * i;\r\n        cur.position(pos);\r\n        cur.limit(pos + bufLen);\r\n        decodeInputs[i] = new ECChunk(cur.slice(), 0, bufLen);\r\n        if (alignedStripe.chunks[i] == null) {\r\n            alignedStripe.chunks[i] = new StripingChunk(decodeInputs[i].getBuffer());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "prepareParityChunk",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean prepareParityChunk(int index)\n{\r\n    Preconditions.checkState(index >= dataBlkNum && alignedStripe.chunks[index] == null);\r\n    final int parityIndex = index - dataBlkNum;\r\n    ByteBuffer buf = dfsStripedInputStream.getParityBuffer().duplicate();\r\n    buf.position(cellSize * parityIndex);\r\n    buf.limit(cellSize * parityIndex + (int) alignedStripe.range.spanInBlock);\r\n    decodeInputs[index] = new ECChunk(buf.slice(), 0, (int) alignedStripe.range.spanInBlock);\r\n    alignedStripe.chunks[index] = new StripingChunk(decodeInputs[index].getBuffer());\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\main\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "decode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void decode() throws IOException\n{\r\n    finalizeDecodeInputs();\r\n    decodeAndFillBuffer(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
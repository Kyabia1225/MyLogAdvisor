[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\lang",
  "methodName" : "call",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Void call() throws Exception\n{\r\n    if (runnable != null) {\r\n        runnable.run();\r\n    } else {\r\n        callable.call();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\lang",
  "methodName" : "run",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void run()\n{\r\n    if (runnable != null) {\r\n        runnable.run();\r\n    } else {\r\n        try {\r\n            callable.call();\r\n        } catch (Exception ex) {\r\n            throw new RuntimeException(ex);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\lang",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return (runnable != null) ? runnable.getClass().getSimpleName() : callable.getClass().getSimpleName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void init(FilterConfig config) throws ServletException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "doFilter",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException\n{\r\n    boolean contentTypeOK = true;\r\n    HttpServletRequest httpReq = (HttpServletRequest) request;\r\n    HttpServletResponse httpRes = (HttpServletResponse) response;\r\n    String method = httpReq.getMethod();\r\n    if (method.equals(\"PUT\") || method.equals(\"POST\")) {\r\n        String op = httpReq.getParameter(HttpFSFileSystem.OP_PARAM);\r\n        if (op != null && UPLOAD_OPERATIONS.contains(StringUtils.toUpperCase(op))) {\r\n            if (\"true\".equalsIgnoreCase(httpReq.getParameter(HttpFSParametersProvider.DataParam.NAME))) {\r\n                String contentType = httpReq.getContentType();\r\n                contentTypeOK = HttpFSFileSystem.UPLOAD_CONTENT_TYPE.equalsIgnoreCase(contentType);\r\n            }\r\n        }\r\n    }\r\n    if (contentTypeOK) {\r\n        chain.doFilter(httpReq, httpRes);\r\n    } else {\r\n        httpRes.sendError(HttpServletResponse.SC_BAD_REQUEST, \"Data upload requests must have content-type set to '\" + HttpFSFileSystem.UPLOAD_CONTENT_TYPE + \"'\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void destroy()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Long parse(String str) throws Exception\n{\r\n    return Long.parseLong(str);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getDomain",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDomain()\n{\r\n    return \"a long\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "setHomeDirForCurrentThread",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setHomeDirForCurrentThread(String homeDir)\n{\r\n    HOME_DIR_TL.set(homeDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "getHomeDir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getHomeDir(String name)\n{\r\n    String homeDir = HOME_DIR_TL.get();\r\n    if (homeDir == null) {\r\n        String sysProp = name + HOME_DIR;\r\n        homeDir = System.getProperty(sysProp);\r\n        if (homeDir == null) {\r\n            throw new IllegalArgumentException(MessageFormat.format(\"System property [{0}] not defined\", sysProp));\r\n        }\r\n    }\r\n    return homeDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "getDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDir(String name, String dirType, String defaultDir)\n{\r\n    String sysProp = name + dirType;\r\n    return System.getProperty(sysProp, defaultDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "contextInitialized",
  "errType" : [ "ServerException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void contextInitialized(ServletContextEvent event)\n{\r\n    try {\r\n        init();\r\n    } catch (ServerException ex) {\r\n        event.getServletContext().log(\"ERROR: \" + ex.getMessage());\r\n        throw new RuntimeException(ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "resolveAuthority",
  "errType" : [ "UnknownHostException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "InetSocketAddress resolveAuthority() throws ServerException\n{\r\n    String hostnameKey = getName() + HTTP_HOSTNAME;\r\n    String portKey = getName() + HTTP_PORT;\r\n    String host = System.getProperty(hostnameKey);\r\n    String port = System.getProperty(portKey);\r\n    if (host == null) {\r\n        throw new ServerException(ServerException.ERROR.S13, hostnameKey);\r\n    }\r\n    if (port == null) {\r\n        throw new ServerException(ServerException.ERROR.S13, portKey);\r\n    }\r\n    try {\r\n        InetAddress add = InetAddress.getByName(host);\r\n        int portNum = Integer.parseInt(port);\r\n        return new InetSocketAddress(add, portNum);\r\n    } catch (UnknownHostException ex) {\r\n        throw new ServerException(ServerException.ERROR.S14, ex.toString(), ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "contextDestroyed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void contextDestroyed(ServletContextEvent event)\n{\r\n    destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "getAuthority",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "InetSocketAddress getAuthority() throws ServerException\n{\r\n    synchronized (this) {\r\n        if (authority == null) {\r\n            authority = resolveAuthority();\r\n        }\r\n    }\r\n    return authority;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "setAuthority",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAuthority(InetSocketAddress authority)\n{\r\n    this.authority = authority;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "isSslEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSslEnabled()\n{\r\n    return Boolean.parseBoolean(System.getProperty(getName() + SSL_ENABLED, \"false\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "EnumSet<E> parse(String str) throws Exception\n{\r\n    final EnumSet<E> set = EnumSet.noneOf(klass);\r\n    if (!str.isEmpty()) {\r\n        for (String sub : str.split(\",\")) {\r\n            set.add(Enum.valueOf(klass, StringUtils.toUpperCase(sub.trim())));\r\n        }\r\n    }\r\n    return set;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getDomain",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDomain()\n{\r\n    return Arrays.asList(klass.getEnumConstants()).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String toString(EnumSet<E> set)\n{\r\n    if (set == null || set.isEmpty()) {\r\n        return \"\";\r\n    } else {\r\n        final StringBuilder b = new StringBuilder();\r\n        final Iterator<E> i = set.iterator();\r\n        b.append(i.next());\r\n        while (i.hasNext()) {\r\n            b.append(',').append(i.next());\r\n        }\r\n        return b.toString();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return getName() + \"=\" + toString(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "toResponse",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Response toResponse(Throwable throwable)\n{\r\n    Response.Status status;\r\n    if (throwable instanceof FileSystemAccessException) {\r\n        throwable = throwable.getCause();\r\n    }\r\n    if (throwable instanceof ContainerException) {\r\n        throwable = throwable.getCause();\r\n    }\r\n    if (throwable instanceof SecurityException) {\r\n        status = Response.Status.UNAUTHORIZED;\r\n    } else if (throwable instanceof FileNotFoundException) {\r\n        status = Response.Status.NOT_FOUND;\r\n    } else if (throwable instanceof IOException) {\r\n        status = Response.Status.INTERNAL_SERVER_ERROR;\r\n        logErrorFully(status, throwable);\r\n    } else if (throwable instanceof UnsupportedOperationException) {\r\n        status = Response.Status.BAD_REQUEST;\r\n        logErrorFully(status, throwable);\r\n    } else if (throwable instanceof IllegalArgumentException) {\r\n        status = Response.Status.BAD_REQUEST;\r\n        logErrorFully(status, throwable);\r\n    } else {\r\n        status = Response.Status.INTERNAL_SERVER_ERROR;\r\n        logErrorFully(status, throwable);\r\n    }\r\n    return createResponse(status, throwable);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "log",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void log(Response.Status status, Throwable throwable)\n{\r\n    String method = MDC.get(\"method\");\r\n    String path = MDC.get(\"path\");\r\n    String message = getOneLineMessage(throwable);\r\n    AUDIT_LOG.warn(\"FAILED [{}:{}] response [{}] {}\", new Object[] { method, path, status, message });\r\n    LOG.warn(\"[{}:{}] response [{}] {}\", method, path, status, message, throwable);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "logErrorFully",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void logErrorFully(Response.Status status, Throwable throwable)\n{\r\n    LOG.debug(\"Failed with {}\", status, throwable);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void init(FilterConfig filterConfig) throws ServletException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "doFilter",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException\n{\r\n    try {\r\n        filterChain.doFilter(servletRequest, servletResponse);\r\n    } finally {\r\n        FileSystem fs = FILE_SYSTEM_TL.get();\r\n        if (fs != null) {\r\n            FILE_SYSTEM_TL.remove();\r\n            getFileSystemAccess().releaseFileSystem(fs);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void destroy()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "setFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFileSystem(FileSystem fs)\n{\r\n    FILE_SYSTEM_TL.set(fs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "getFileSystemAccess",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSystemAccess getFileSystemAccess()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getConfiguration",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "Properties getConfiguration(String configPrefix, FilterConfig filterConfig) throws ServletException\n{\r\n    Configuration conf = HttpFSServerWebApp.get().getConfig();\r\n    Properties props = HttpServer2.getFilterProperties(conf, new ArrayList<>(Arrays.asList(CONF_PREFIXES)));\r\n    String signatureSecretFile = props.getProperty(SIGNATURE_SECRET_FILE, null);\r\n    if (signatureSecretFile == null) {\r\n        throw new RuntimeException(\"Undefined property: \" + SIGNATURE_SECRET_FILE);\r\n    }\r\n    if (!isRandomSecret(filterConfig)) {\r\n        try (Reader reader = new InputStreamReader(Files.newInputStream(Paths.get(signatureSecretFile)), StandardCharsets.UTF_8)) {\r\n            StringBuilder secret = new StringBuilder();\r\n            int c = reader.read();\r\n            while (c > -1) {\r\n                secret.append((char) c);\r\n                c = reader.read();\r\n            }\r\n            String secretString = secret.toString();\r\n            if (secretString.isEmpty()) {\r\n                throw new RuntimeException(\"No secret in HttpFs signature secret file: \" + signatureSecretFile);\r\n            }\r\n            props.setProperty(AuthenticationFilter.SIGNATURE_SECRET, secretString);\r\n        } catch (IOException ex) {\r\n            throw new RuntimeException(\"Could not read HttpFS signature \" + \"secret file: \" + signatureSecretFile);\r\n        }\r\n    }\r\n    setAuthHandlerClass(props);\r\n    String dtkind = WebHdfsConstants.WEBHDFS_TOKEN_KIND.toString();\r\n    if (conf.getBoolean(HttpFSServerWebServer.SSL_ENABLED_KEY, false)) {\r\n        dtkind = WebHdfsConstants.SWEBHDFS_TOKEN_KIND.toString();\r\n    }\r\n    props.setProperty(KerberosDelegationTokenAuthenticationHandler.TOKEN_KIND, dtkind);\r\n    return props;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getProxyuserConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration getProxyuserConfiguration(FilterConfig filterConfig)\n{\r\n    Map<String, String> proxyuserConf = HttpFSServerWebApp.get().getConfig().getValByRegex(\"httpfs\\\\.proxyuser\\\\.\");\r\n    Configuration conf = new Configuration(false);\r\n    for (Map.Entry<String, String> entry : proxyuserConf.entrySet()) {\r\n        conf.set(entry.getKey().substring(\"httpfs.\".length()), entry.getValue());\r\n    }\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "isRandomSecret",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isRandomSecret(FilterConfig filterConfig)\n{\r\n    SignerSecretProvider secretProvider = (SignerSecretProvider) filterConfig.getServletContext().getAttribute(SIGNER_SECRET_PROVIDER_ATTRIBUTE);\r\n    if (secretProvider == null) {\r\n        return false;\r\n    }\r\n    return secretProvider.getClass() == RandomSignerSecretProvider.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "checkAbsolutePath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String checkAbsolutePath(String value, String name)\n{\r\n    if (!new File(value).isAbsolute()) {\r\n        throw new IllegalArgumentException(MessageFormat.format(\"[{0}] must be an absolute path [{1}]\", name, value));\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Status getStatus()\n{\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "setStatus",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setStatus(Status status) throws ServerException\n{\r\n    Check.notNull(status, \"status\");\r\n    if (status.settable) {\r\n        if (status != this.status) {\r\n            Status oldStatus = this.status;\r\n            this.status = status;\r\n            for (Service service : services.values()) {\r\n                try {\r\n                    service.serverStatusChange(oldStatus, status);\r\n                } catch (Exception ex) {\r\n                    log.error(\"Service [{}] exception during status change to [{}] -server shutting down-,  {}\", new Object[] { service.getInterface().getSimpleName(), status, ex.getMessage(), ex });\r\n                    destroy();\r\n                    throw new ServerException(ServerException.ERROR.S11, service.getInterface().getSimpleName(), status, ex.getMessage(), ex);\r\n                }\r\n            }\r\n        }\r\n    } else {\r\n        throw new IllegalArgumentException(\"Status [\" + status + \" is not settable\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "ensureOperational",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void ensureOperational()\n{\r\n    if (!getStatus().isOperational()) {\r\n        throw new IllegalStateException(\"Server is not running\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "getResource",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "InputStream getResource(String name)\n{\r\n    Check.notEmpty(name, \"name\");\r\n    ClassLoader cl = Thread.currentThread().getContextClassLoader();\r\n    if (cl == null) {\r\n        cl = Server.class.getClassLoader();\r\n    }\r\n    return cl.getResourceAsStream(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "init",
  "errType" : [ "IOException", "ServerException" ],
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void init() throws ServerException\n{\r\n    if (status != Status.UNDEF) {\r\n        throw new IllegalStateException(\"Server already initialized\");\r\n    }\r\n    status = Status.BOOTING;\r\n    verifyDir(homeDir);\r\n    verifyDir(tempDir);\r\n    Properties serverInfo = new Properties();\r\n    try {\r\n        InputStream is = getResource(name + \".properties\");\r\n        serverInfo.load(is);\r\n        is.close();\r\n    } catch (IOException ex) {\r\n        throw new RuntimeException(\"Could not load server information file: \" + name + \".properties\");\r\n    }\r\n    initLog();\r\n    log.info(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++\");\r\n    log.info(\"Server [{}] starting\", name);\r\n    log.info(\"  Built information:\");\r\n    log.info(\"    Version           : {}\", serverInfo.getProperty(name + \".version\", \"undef\"));\r\n    log.info(\"    Source Repository : {}\", serverInfo.getProperty(name + \".source.repository\", \"undef\"));\r\n    log.info(\"    Source Revision   : {}\", serverInfo.getProperty(name + \".source.revision\", \"undef\"));\r\n    log.info(\"    Built by          : {}\", serverInfo.getProperty(name + \".build.username\", \"undef\"));\r\n    log.info(\"    Built timestamp   : {}\", serverInfo.getProperty(name + \".build.timestamp\", \"undef\"));\r\n    log.info(\"  Runtime information:\");\r\n    log.info(\"    Home   dir: {}\", homeDir);\r\n    log.info(\"    Config dir: {}\", (config == null) ? configDir : \"-\");\r\n    log.info(\"    Log    dir: {}\", logDir);\r\n    log.info(\"    Temp   dir: {}\", tempDir);\r\n    initConfig();\r\n    log.debug(\"Loading services\");\r\n    List<Service> list = loadServices();\r\n    try {\r\n        log.debug(\"Initializing services\");\r\n        initServices(list);\r\n        log.info(\"Services initialized\");\r\n    } catch (ServerException ex) {\r\n        log.error(\"Services initialization failure, destroying initialized services\");\r\n        destroyServices();\r\n        throw ex;\r\n    }\r\n    Status status = Status.valueOf(getConfig().get(getPrefixedName(CONF_STARTUP_STATUS), Status.NORMAL.toString()));\r\n    setStatus(status);\r\n    log.info(\"Server [{}] started!, status [{}]\", name, status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "verifyDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyDir(String dir) throws ServerException\n{\r\n    File file = new File(dir);\r\n    if (!file.exists()) {\r\n        throw new ServerException(ServerException.ERROR.S01, dir);\r\n    }\r\n    if (!file.isDirectory()) {\r\n        throw new ServerException(ServerException.ERROR.S02, dir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "initLog",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void initLog() throws ServerException\n{\r\n    verifyDir(logDir);\r\n    LogManager.resetConfiguration();\r\n    File log4jFile = new File(configDir, name + \"-log4j.properties\");\r\n    if (log4jFile.exists()) {\r\n        PropertyConfigurator.configureAndWatch(log4jFile.toString(), 10 * 1000);\r\n        log = LoggerFactory.getLogger(Server.class);\r\n    } else {\r\n        Properties props = new Properties();\r\n        try {\r\n            InputStream is = getResource(DEFAULT_LOG4J_PROPERTIES);\r\n            try {\r\n                props.load(is);\r\n            } finally {\r\n                is.close();\r\n            }\r\n        } catch (IOException ex) {\r\n            throw new ServerException(ServerException.ERROR.S03, DEFAULT_LOG4J_PROPERTIES, ex.getMessage(), ex);\r\n        }\r\n        PropertyConfigurator.configure(props);\r\n        log = LoggerFactory.getLogger(Server.class);\r\n        log.warn(\"Log4j [{}] configuration file not found, using default configuration from classpath\", log4jFile);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "initConfig",
  "errType" : [ "Exception", "IOException" ],
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void initConfig() throws ServerException\n{\r\n    verifyDir(configDir);\r\n    File file = new File(configDir);\r\n    Configuration defaultConf;\r\n    String defaultConfig = name + \"-default.xml\";\r\n    ClassLoader classLoader = Thread.currentThread().getContextClassLoader();\r\n    InputStream inputStream = classLoader.getResourceAsStream(defaultConfig);\r\n    if (inputStream == null) {\r\n        log.warn(\"Default configuration file not available in classpath [{}]\", defaultConfig);\r\n        defaultConf = new Configuration(false);\r\n    } else {\r\n        try {\r\n            defaultConf = new Configuration(false);\r\n            ConfigurationUtils.load(defaultConf, inputStream);\r\n        } catch (Exception ex) {\r\n            throw new ServerException(ServerException.ERROR.S03, defaultConfig, ex.getMessage(), ex);\r\n        }\r\n    }\r\n    if (config == null) {\r\n        Configuration siteConf;\r\n        File siteFile = new File(file, name + \"-site.xml\");\r\n        if (!siteFile.exists()) {\r\n            log.warn(\"Site configuration file [{}] not found in config directory\", siteFile);\r\n            siteConf = new Configuration(false);\r\n        } else {\r\n            if (!siteFile.isFile()) {\r\n                throw new ServerException(ServerException.ERROR.S05, siteFile.getAbsolutePath());\r\n            }\r\n            try {\r\n                log.debug(\"Loading site configuration from [{}]\", siteFile);\r\n                inputStream = Files.newInputStream(siteFile.toPath());\r\n                siteConf = new Configuration(false);\r\n                ConfigurationUtils.load(siteConf, inputStream);\r\n            } catch (IOException ex) {\r\n                throw new ServerException(ServerException.ERROR.S06, siteFile, ex.getMessage(), ex);\r\n            }\r\n        }\r\n        config = new Configuration(false);\r\n        ConfigurationUtils.copy(siteConf, config);\r\n    }\r\n    ConfigurationUtils.injectDefaults(defaultConf, config);\r\n    ConfigRedactor redactor = new ConfigRedactor(config);\r\n    for (String name : System.getProperties().stringPropertyNames()) {\r\n        String value = System.getProperty(name);\r\n        if (name.startsWith(getPrefix() + \".\")) {\r\n            config.set(name, value);\r\n            String redacted = redactor.redact(name, value);\r\n            log.info(\"System property sets  {}: {}\", name, redacted);\r\n        }\r\n    }\r\n    log.debug(\"Loaded Configuration:\");\r\n    log.debug(\"------------------------------------------------------\");\r\n    for (Map.Entry<String, String> entry : config) {\r\n        String name = entry.getKey();\r\n        String value = config.get(entry.getKey());\r\n        String redacted = redactor.redact(name, value);\r\n        log.debug(\"  {}: {}\", entry.getKey(), redacted);\r\n    }\r\n    log.debug(\"------------------------------------------------------\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "loadServices",
  "errType" : [ "ServerException", "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void loadServices(Class[] classes, List<Service> list) throws ServerException\n{\r\n    for (Class klass : classes) {\r\n        try {\r\n            Service service = (Service) klass.newInstance();\r\n            log.debug(\"Loading service [{}] implementation [{}]\", service.getInterface(), service.getClass());\r\n            if (!service.getInterface().isInstance(service)) {\r\n                throw new ServerException(ServerException.ERROR.S04, klass, service.getInterface().getName());\r\n            }\r\n            list.add(service);\r\n        } catch (ServerException ex) {\r\n            throw ex;\r\n        } catch (Exception ex) {\r\n            throw new ServerException(ServerException.ERROR.S07, klass, ex.getMessage(), ex);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "loadServices",
  "errType" : [ "RuntimeException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "List<Service> loadServices() throws ServerException\n{\r\n    try {\r\n        Map<Class, Service> map = new LinkedHashMap<Class, Service>();\r\n        Class[] classes = getConfig().getClasses(getPrefixedName(CONF_SERVICES));\r\n        Class[] classesExt = getConfig().getClasses(getPrefixedName(CONF_SERVICES_EXT));\r\n        List<Service> list = new ArrayList<Service>();\r\n        loadServices(classes, list);\r\n        loadServices(classesExt, list);\r\n        for (Service service : list) {\r\n            if (map.containsKey(service.getInterface())) {\r\n                log.debug(\"Replacing service [{}] implementation [{}]\", service.getInterface(), service.getClass());\r\n            }\r\n            map.put(service.getInterface(), service);\r\n        }\r\n        list = new ArrayList<Service>();\r\n        for (Map.Entry<Class, Service> entry : map.entrySet()) {\r\n            list.add(entry.getValue());\r\n        }\r\n        return list;\r\n    } catch (RuntimeException ex) {\r\n        throw new ServerException(ServerException.ERROR.S08, ex.getMessage(), ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "initServices",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void initServices(List<Service> services) throws ServerException\n{\r\n    for (Service service : services) {\r\n        log.debug(\"Initializing service [{}]\", service.getInterface());\r\n        checkServiceDependencies(service);\r\n        service.init(this);\r\n        this.services.put(service.getInterface(), service);\r\n    }\r\n    for (Service service : services) {\r\n        service.postInit();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "checkServiceDependencies",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void checkServiceDependencies(Service service) throws ServerException\n{\r\n    if (service.getServiceDependencies() != null) {\r\n        for (Class dependency : service.getServiceDependencies()) {\r\n            if (services.get(dependency) == null) {\r\n                throw new ServerException(ServerException.ERROR.S10, service.getClass(), dependency);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "destroyServices",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void destroyServices()\n{\r\n    List<Service> list = new ArrayList<Service>(services.values());\r\n    Collections.reverse(list);\r\n    for (Service service : list) {\r\n        try {\r\n            log.debug(\"Destroying service [{}]\", service.getInterface());\r\n            service.destroy();\r\n        } catch (Throwable ex) {\r\n            log.error(\"Could not destroy service [{}], {}\", new Object[] { service.getInterface(), ex.getMessage(), ex });\r\n        }\r\n    }\r\n    log.info(\"Services destroyed\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void destroy()\n{\r\n    ensureOperational();\r\n    destroyServices();\r\n    log.info(\"Server [{}] shutdown!\", name);\r\n    log.info(\"======================================================\");\r\n    if (!Boolean.getBoolean(\"test.circus\")) {\r\n        LogManager.shutdown();\r\n    }\r\n    status = Status.SHUTDOWN;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "getPrefix",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getPrefix()\n{\r\n    return getName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "getPrefixedName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getPrefixedName(String name)\n{\r\n    return getPrefix() + \".\" + Check.notEmpty(name, \"name\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "getHomeDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHomeDir()\n{\r\n    return homeDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "getConfigDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getConfigDir()\n{\r\n    return configDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "getLogDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLogDir()\n{\r\n    return logDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "getTempDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTempDir()\n{\r\n    return tempDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "getConfig",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConfig()\n{\r\n    return config;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "T get(Class<T> serviceKlass)\n{\r\n    ensureOperational();\r\n    Check.notNull(serviceKlass, \"serviceKlass\");\r\n    return (T) services.get(serviceKlass);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "setService",
  "errType" : [ "Exception", "Throwable" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setService(Class<? extends Service> klass) throws ServerException\n{\r\n    ensureOperational();\r\n    Check.notNull(klass, \"serviceKlass\");\r\n    if (getStatus() == Status.SHUTTING_DOWN) {\r\n        throw new IllegalStateException(\"Server shutting down\");\r\n    }\r\n    try {\r\n        Service newService = klass.newInstance();\r\n        Service oldService = services.get(newService.getInterface());\r\n        if (oldService != null) {\r\n            try {\r\n                oldService.destroy();\r\n            } catch (Throwable ex) {\r\n                log.error(\"Could not destroy service [{}], {}\", new Object[] { oldService.getInterface(), ex.getMessage(), ex });\r\n            }\r\n        }\r\n        newService.init(this);\r\n        services.put(newService.getInterface(), newService);\r\n    } catch (Exception ex) {\r\n        log.error(\"Could not set service [{}] programmatically -server shutting down-, {}\", klass, ex);\r\n        destroy();\r\n        throw new ServerException(ServerException.ERROR.S09, klass, ex.getMessage(), ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "createURL",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URL createURL(Path path, Map<String, String> params) throws IOException\n{\r\n    return createURL(path, params, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "createURL",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "URL createURL(Path path, Map<String, String> params, Map<String, List<String>> multiValuedParams) throws IOException\n{\r\n    URI uri = path.toUri();\r\n    String realScheme;\r\n    if (uri.getScheme().equalsIgnoreCase(HttpFSFileSystem.SCHEME)) {\r\n        realScheme = \"http\";\r\n    } else if (uri.getScheme().equalsIgnoreCase(HttpsFSFileSystem.SCHEME)) {\r\n        realScheme = \"https\";\r\n    } else {\r\n        throw new IllegalArgumentException(MessageFormat.format(\"Invalid scheme [{0}] it should be '\" + HttpFSFileSystem.SCHEME + \"' \" + \"or '\" + HttpsFSFileSystem.SCHEME + \"'\", uri));\r\n    }\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(realScheme).append(\"://\").append(uri.getAuthority()).append(SERVICE_PATH).append(uri.getPath());\r\n    String separator = \"?\";\r\n    for (Map.Entry<String, String> entry : params.entrySet()) {\r\n        sb.append(separator).append(entry.getKey()).append(\"=\").append(URLEncoder.encode(entry.getValue(), \"UTF8\"));\r\n        separator = \"&\";\r\n    }\r\n    if (multiValuedParams != null) {\r\n        for (Map.Entry<String, List<String>> multiValuedEntry : multiValuedParams.entrySet()) {\r\n            String name = URLEncoder.encode(multiValuedEntry.getKey(), \"UTF8\");\r\n            List<String> values = multiValuedEntry.getValue();\r\n            for (String value : values) {\r\n                sb.append(separator).append(name).append(\"=\").append(URLEncoder.encode(value, \"UTF8\"));\r\n                separator = \"&\";\r\n            }\r\n        }\r\n    }\r\n    return new URL(sb.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "jsonParse",
  "errType" : [ "ParseException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Object jsonParse(HttpURLConnection conn) throws IOException\n{\r\n    try {\r\n        String contentType = conn.getContentType();\r\n        if (contentType != null) {\r\n            final MediaType parsed = MediaType.valueOf(contentType);\r\n            if (!MediaType.APPLICATION_JSON_TYPE.isCompatible(parsed)) {\r\n                throw new IOException(\"Content-Type \\\"\" + contentType + \"\\\" is incompatible with \\\"\" + MediaType.APPLICATION_JSON + \"\\\" (parsed=\\\"\" + parsed + \"\\\")\");\r\n            }\r\n        }\r\n        JSONParser parser = new JSONParser();\r\n        return parser.parse(new InputStreamReader(conn.getInputStream(), StandardCharsets.UTF_8));\r\n    } catch (ParseException ex) {\r\n        throw new IOException(\"JSON parser error, \" + ex.getMessage(), ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return SCHEME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Boolean parse(String str) throws Exception\n{\r\n    if (str.equalsIgnoreCase(\"true\")) {\r\n        return true;\r\n    } else if (str.equalsIgnoreCase(\"false\")) {\r\n        return false;\r\n    }\r\n    throw new IllegalArgumentException(MessageFormat.format(\"Invalid value [{0}], must be a boolean\", str));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getDomain",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDomain()\n{\r\n    return \"a boolean\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "HttpFSServerMetrics create(Configuration conf, String serverName)\n{\r\n    String sessionId = conf.get(DFSConfigKeys.DFS_METRICS_SESSION_ID_KEY);\r\n    MetricsSystem ms = DefaultMetricsSystem.instance();\r\n    JvmMetrics jm = JvmMetrics.create(\"HttpFSServer\", sessionId, ms);\r\n    String name = \"ServerActivity-\" + (serverName.isEmpty() ? \"UndefinedServer\" + ThreadLocalRandom.current().nextInt() : serverName.replace(':', '-'));\r\n    return ms.register(name, null, new HttpFSServerMetrics(name, sessionId, jm));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "name",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String name()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "getJvmMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JvmMetrics getJvmMetrics()\n{\r\n    return jvmMetrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "incrBytesWritten",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrBytesWritten(long bytes)\n{\r\n    bytesWritten.incr(bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "incrBytesRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrBytesRead(long bytes)\n{\r\n    bytesRead.incr(bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "incrOpsCreate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrOpsCreate()\n{\r\n    opsCreate.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "incrOpsAppend",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrOpsAppend()\n{\r\n    opsAppend.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "incrOpsTruncate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrOpsTruncate()\n{\r\n    opsTruncate.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "incrOpsDelete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrOpsDelete()\n{\r\n    opsDelete.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "incrOpsRename",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrOpsRename()\n{\r\n    opsRename.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "incrOpsMkdir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrOpsMkdir()\n{\r\n    opsMkdir.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "incrOpsOpen",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrOpsOpen()\n{\r\n    opsOpen.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "incrOpsListing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrOpsListing()\n{\r\n    opsListing.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "incrOpsStat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrOpsStat()\n{\r\n    opsStat.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "incrOpsCheckAccess",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrOpsCheckAccess()\n{\r\n    opsCheckAccess.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void shutdown()\n{\r\n    DefaultMetricsSystem.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "getOpsMkdir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getOpsMkdir()\n{\r\n    return opsMkdir.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "getOpsListing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getOpsListing()\n{\r\n    return opsListing.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server\\metrics",
  "methodName" : "getOpsStat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getOpsStat()\n{\r\n    return opsStat.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Short parse(String str) throws Exception\n{\r\n    return Short.parseShort(str, radix);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getDomain",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDomain()\n{\r\n    return \"a short\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void init() throws ServiceException\n{\r\n    timersSize = getServiceConfig().getInt(CONF_TIMERS_SIZE, 10);\r\n    counterLock = new ReentrantLock();\r\n    timerLock = new ReentrantLock();\r\n    variableLock = new ReentrantLock();\r\n    samplerLock = new ReentrantLock();\r\n    Map<String, VariableHolder> jvmVariables = new ConcurrentHashMap<String, VariableHolder>();\r\n    counters = new ConcurrentHashMap<String, Map<String, AtomicLong>>();\r\n    timers = new ConcurrentHashMap<String, Map<String, Timer>>();\r\n    variables = new ConcurrentHashMap<String, Map<String, VariableHolder>>();\r\n    samplers = new ConcurrentHashMap<String, Map<String, Sampler>>();\r\n    samplersList = new ArrayList<Sampler>();\r\n    all = new LinkedHashMap<String, Map<String, ?>>();\r\n    all.put(\"os-env\", System.getenv());\r\n    all.put(\"sys-props\", (Map<String, ?>) (Map) System.getProperties());\r\n    all.put(\"jvm\", jvmVariables);\r\n    all.put(\"counters\", (Map) counters);\r\n    all.put(\"timers\", (Map) timers);\r\n    all.put(\"variables\", (Map) variables);\r\n    all.put(\"samplers\", (Map) samplers);\r\n    jvmVariables.put(\"free.memory\", new VariableHolder<Long>(new Instrumentation.Variable<Long>() {\r\n\r\n        @Override\r\n        public Long getValue() {\r\n            return Runtime.getRuntime().freeMemory();\r\n        }\r\n    }));\r\n    jvmVariables.put(\"max.memory\", new VariableHolder<Long>(new Instrumentation.Variable<Long>() {\r\n\r\n        @Override\r\n        public Long getValue() {\r\n            return Runtime.getRuntime().maxMemory();\r\n        }\r\n    }));\r\n    jvmVariables.put(\"total.memory\", new VariableHolder<Long>(new Instrumentation.Variable<Long>() {\r\n\r\n        @Override\r\n        public Long getValue() {\r\n            return Runtime.getRuntime().totalMemory();\r\n        }\r\n    }));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "postInit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void postInit() throws ServiceException\n{\r\n    Scheduler scheduler = getServer().get(Scheduler.class);\r\n    if (scheduler != null) {\r\n        scheduler.schedule(new SamplersRunnable(), 0, 1, TimeUnit.SECONDS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "getInterface",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class getInterface()\n{\r\n    return Instrumentation.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "getToAdd",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "T getToAdd(String group, String name, Class<T> klass, Lock lock, Map<String, Map<String, T>> map)\n{\r\n    boolean locked = false;\r\n    try {\r\n        Map<String, T> groupMap = map.get(group);\r\n        if (groupMap == null) {\r\n            lock.lock();\r\n            locked = true;\r\n            groupMap = map.get(group);\r\n            if (groupMap == null) {\r\n                groupMap = new ConcurrentHashMap<String, T>();\r\n                map.put(group, groupMap);\r\n            }\r\n        }\r\n        T element = groupMap.get(name);\r\n        if (element == null) {\r\n            if (!locked) {\r\n                lock.lock();\r\n                locked = true;\r\n            }\r\n            element = groupMap.get(name);\r\n            if (element == null) {\r\n                try {\r\n                    if (klass == Timer.class) {\r\n                        element = (T) new Timer(timersSize);\r\n                    } else {\r\n                        element = klass.newInstance();\r\n                    }\r\n                } catch (Exception ex) {\r\n                    throw new RuntimeException(ex);\r\n                }\r\n                groupMap.put(name, element);\r\n            }\r\n        }\r\n        return element;\r\n    } finally {\r\n        if (locked) {\r\n            lock.unlock();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "createCron",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Cron createCron()\n{\r\n    return new Cron();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "incr",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void incr(String group, String name, long count)\n{\r\n    AtomicLong counter = getToAdd(group, name, AtomicLong.class, counterLock, counters);\r\n    counter.addAndGet(count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "addCron",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addCron(String group, String name, Instrumentation.Cron cron)\n{\r\n    Timer timer = getToAdd(group, name, Timer.class, timerLock, timers);\r\n    timer.addCron((Cron) cron);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "addVariable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addVariable(String group, String name, Variable<?> variable)\n{\r\n    VariableHolder holder = getToAdd(group, name, VariableHolder.class, variableLock, variables);\r\n    holder.var = variable;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "addSampler",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void addSampler(String group, String name, int samplingSize, Variable<Long> variable)\n{\r\n    Sampler sampler = getToAdd(group, name, Sampler.class, samplerLock, samplers);\r\n    samplerLock.lock();\r\n    try {\r\n        sampler.init(samplingSize, variable);\r\n        samplersList.add(sampler);\r\n    } finally {\r\n        samplerLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "getSnapshot",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, Map<String, ?>> getSnapshot()\n{\r\n    return all;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "E parse(String str) throws Exception\n{\r\n    return Enum.valueOf(klass, StringUtils.toUpperCase(str));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getDomain",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDomain()\n{\r\n    return StringUtils.join(\",\", Arrays.asList(klass.getEnumConstants()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void init() throws ServerException\n{\r\n    if (SERVER != null) {\r\n        throw new RuntimeException(\"HttpFSServer server already initialized\");\r\n    }\r\n    SERVER = this;\r\n    super.init();\r\n    adminGroup = getConfig().get(getPrefixedName(CONF_ADMIN_GROUP), \"admin\");\r\n    LOG.info(\"Connects to Namenode [{}]\", get().get(FileSystemAccess.class).getFileSystemConfiguration().getTrimmed(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY));\r\n    setMetrics(getConfig());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void destroy()\n{\r\n    SERVER = null;\r\n    if (metrics != null) {\r\n        metrics.shutdown();\r\n    }\r\n    super.destroy();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "setMetrics",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setMetrics(Configuration config)\n{\r\n    LOG.info(\"Initializing HttpFSServerMetrics\");\r\n    metrics = HttpFSServerMetrics.create(config, \"HttpFSServer\");\r\n    JvmPauseMonitor pauseMonitor = new JvmPauseMonitor();\r\n    pauseMonitor.init(config);\r\n    pauseMonitor.start();\r\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\r\n    FSOperations.setBufferSize(config);\r\n    DefaultMetricsSystem.initialize(\"HttpFSServer\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HttpFSServerWebApp get()\n{\r\n    return SERVER;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HttpFSServerMetrics getMetrics()\n{\r\n    return metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getAdminGroup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAdminGroup()\n{\r\n    return adminGroup;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void init(Server server) throws ServiceException\n{\r\n    this.server = server;\r\n    String servicePrefix = getPrefixedName(\"\");\r\n    serviceConfig = new Configuration(false);\r\n    for (Map.Entry<String, String> entry : ConfigurationUtils.resolve(server.getConfig())) {\r\n        String key = entry.getKey();\r\n        if (key.startsWith(servicePrefix)) {\r\n            serviceConfig.set(key.substring(servicePrefix.length()), entry.getValue());\r\n        }\r\n    }\r\n    init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "postInit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void postInit() throws ServiceException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void destroy()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "getServiceDependencies",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class[] getServiceDependencies()\n{\r\n    return new Class[0];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "serverStatusChange",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void serverStatusChange(Server.Status oldStatus, Server.Status newStatus) throws ServiceException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "getPrefix",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPrefix()\n{\r\n    return prefix;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "getServer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Server getServer()\n{\r\n    return server;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "getPrefixedName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getPrefixedName(String name)\n{\r\n    return server.getPrefixedName(prefix + \".\" + name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "getServiceConfig",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getServiceConfig()\n{\r\n    return serviceConfig;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void init() throws ServiceException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\lang",
  "methodName" : "getError",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ERROR getError()\n{\r\n    return error;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\lang",
  "methodName" : "format",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String format(ERROR error, Object... args)\n{\r\n    String template = error.getTemplate();\r\n    if (template == null) {\r\n        StringBuilder sb = new StringBuilder();\r\n        for (int i = 0; i < args.length; i++) {\r\n            sb.append(\" {\").append(i).append(\"}\");\r\n        }\r\n        template = sb.deleteCharAt(0).toString();\r\n    }\r\n    return error + \": \" + MessageFormat.format(template, args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\lang",
  "methodName" : "getCause",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Throwable getCause(Object... params)\n{\r\n    Throwable throwable = null;\r\n    if (params != null && params.length > 0 && params[params.length - 1] instanceof Throwable) {\r\n        throwable = (Throwable) params[params.length - 1];\r\n    }\r\n    return throwable;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "copy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void copy(Configuration source, Configuration target)\n{\r\n    Check.notNull(source, \"source\");\r\n    Check.notNull(target, \"target\");\r\n    for (Map.Entry<String, String> entry : source) {\r\n        target.set(entry.getKey(), entry.getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "injectDefaults",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void injectDefaults(Configuration source, Configuration target)\n{\r\n    Check.notNull(source, \"source\");\r\n    Check.notNull(target, \"target\");\r\n    for (Map.Entry<String, String> entry : source) {\r\n        if (target.get(entry.getKey()) == null) {\r\n            target.set(entry.getKey(), entry.getValue());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "resolve",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration resolve(Configuration conf)\n{\r\n    Configuration resolved = new Configuration(false);\r\n    for (Map.Entry<String, String> entry : conf) {\r\n        resolved.set(entry.getKey(), conf.get(entry.getKey()));\r\n    }\r\n    return resolved;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "load",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void load(Configuration conf, InputStream is) throws IOException\n{\r\n    conf.addResource(is);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void init(FilterConfig config) throws ServletException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "doFilter",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException\n{\r\n    try {\r\n        MDC.clear();\r\n        String hostname = HostnameFilter.get();\r\n        if (hostname != null) {\r\n            MDC.put(\"hostname\", HostnameFilter.get());\r\n        }\r\n        Principal principal = ((HttpServletRequest) request).getUserPrincipal();\r\n        String user = (principal != null) ? principal.getName() : null;\r\n        if (user != null) {\r\n            MDC.put(\"user\", user);\r\n        }\r\n        MDC.put(\"method\", ((HttpServletRequest) request).getMethod());\r\n        if (((HttpServletRequest) request).getPathInfo() != null) {\r\n            MDC.put(\"path\", ((HttpServletRequest) request).getPathInfo());\r\n        }\r\n        chain.doFilter(request, response);\r\n    } finally {\r\n        MDC.clear();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void destroy()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\scheduler",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void init() throws ServiceException\n{\r\n    int threads = getServiceConfig().getInt(CONF_THREADS, 5);\r\n    scheduler = new ScheduledThreadPoolExecutor(threads);\r\n    LOG.debug(\"Scheduler started\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\scheduler",
  "methodName" : "destroy",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void destroy()\n{\r\n    try {\r\n        long limit = Time.now() + 30 * 1000;\r\n        scheduler.shutdownNow();\r\n        while (!scheduler.awaitTermination(1000, TimeUnit.MILLISECONDS)) {\r\n            LOG.debug(\"Waiting for scheduler to shutdown\");\r\n            if (Time.now() > limit) {\r\n                LOG.warn(\"Gave up waiting for scheduler to shutdown\");\r\n                break;\r\n            }\r\n        }\r\n        if (scheduler.isTerminated()) {\r\n            LOG.debug(\"Scheduler shutdown\");\r\n        }\r\n    } catch (InterruptedException ex) {\r\n        LOG.warn(ex.getMessage(), ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\scheduler",
  "methodName" : "getServiceDependencies",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class[] getServiceDependencies()\n{\r\n    return new Class[] { Instrumentation.class };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\scheduler",
  "methodName" : "getInterface",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class getInterface()\n{\r\n    return Scheduler.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\scheduler",
  "methodName" : "schedule",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void schedule(final Callable<?> callable, long delay, long interval, TimeUnit unit)\n{\r\n    Check.notNull(callable, \"callable\");\r\n    if (!scheduler.isShutdown()) {\r\n        LOG.debug(\"Scheduling callable [{}], interval [{}] seconds, delay [{}] in [{}]\", new Object[] { callable, delay, interval, unit });\r\n        Runnable r = new Runnable() {\r\n\r\n            @Override\r\n            public void run() {\r\n                String instrName = callable.getClass().getSimpleName();\r\n                Instrumentation instr = getServer().get(Instrumentation.class);\r\n                if (getServer().getStatus() == Server.Status.HALTED) {\r\n                    LOG.debug(\"Skipping [{}], server status [{}]\", callable, getServer().getStatus());\r\n                    instr.incr(INST_GROUP, instrName + \".skips\", 1);\r\n                } else {\r\n                    LOG.debug(\"Executing [{}]\", callable);\r\n                    instr.incr(INST_GROUP, instrName + \".execs\", 1);\r\n                    Instrumentation.Cron cron = instr.createCron().start();\r\n                    try {\r\n                        callable.call();\r\n                    } catch (Exception ex) {\r\n                        instr.incr(INST_GROUP, instrName + \".fails\", 1);\r\n                        LOG.error(\"Error executing [{}], {}\", new Object[] { callable, ex.getMessage(), ex });\r\n                    } finally {\r\n                        instr.addCron(INST_GROUP, instrName, cron.stop());\r\n                    }\r\n                }\r\n            }\r\n        };\r\n        scheduler.scheduleWithFixedDelay(r, delay, interval, unit);\r\n    } else {\r\n        throw new IllegalStateException(MessageFormat.format(\"Scheduler shutting down, ignoring scheduling of [{}]\", callable));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\scheduler",
  "methodName" : "schedule",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void schedule(Runnable runnable, long delay, long interval, TimeUnit unit)\n{\r\n    schedule((Callable<?>) new RunnableCallable(runnable), delay, interval, unit);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void write(OutputStream os) throws IOException\n{\r\n    IOUtils.skipFully(is, offset);\r\n    long bytes = 0L;\r\n    if (len == -1) {\r\n        bytes = FSOperations.copyBytes(is, os);\r\n    } else {\r\n        bytes = FSOperations.copyBytes(is, os, len);\r\n    }\r\n    HttpFSServerMetrics metrics = HttpFSServerWebApp.get().getMetrics();\r\n    if (metrics != null) {\r\n        metrics.incrBytesRead(bytes);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getFileSystemAccess",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystemAccess getFileSystemAccess()\n{\r\n    return HttpFSServerWebApp.get().get(FileSystemAccess.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "parseParam",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "T parseParam(String str)\n{\r\n    try {\r\n        value = (str != null && str.trim().length() > 0) ? parse(str) : value;\r\n    } catch (Exception ex) {\r\n        throw new IllegalArgumentException(MessageFormat.format(\"Parameter [{0}], invalid value [{1}], value must be [{2}]\", name, str, getDomain()));\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "value",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T value()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getDomain",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDomain()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T parse(String str) throws Exception",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return (value != null) ? value.toString() : \"NULL\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "V get(String name, Class<T> klass)\n{\r\n    List<Param<?>> multiParams = (List<Param<?>>) params.get(name);\r\n    if (multiParams != null && multiParams.size() > 0) {\r\n        return ((T) multiParams.get(0)).value();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getValues",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<V> getValues(String name, Class<T> klass)\n{\r\n    List<Param<?>> multiParams = (List<Param<?>>) params.get(name);\r\n    List<V> values = Lists.newArrayList();\r\n    if (multiParams != null) {\r\n        for (Param<?> param : multiParams) {\r\n            V value = ((T) param).value();\r\n            if (value != null) {\r\n                values.add(value);\r\n            }\r\n        }\r\n    }\r\n    return values;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getHttpUGI",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "UserGroupInformation getHttpUGI(HttpServletRequest request)\n{\r\n    UserGroupInformation user = HttpUserGroupInformation.get();\r\n    if (user != null) {\r\n        return user;\r\n    }\r\n    return UserGroupInformation.createRemoteUser(request.getUserPrincipal().getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "fsExecute",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "T fsExecute(UserGroupInformation ugi, FileSystemAccess.FileSystemExecutor<T> executor) throws IOException, FileSystemAccessException\n{\r\n    FileSystemAccess fsAccess = HttpFSServerWebApp.get().get(FileSystemAccess.class);\r\n    Configuration conf = HttpFSServerWebApp.get().get(FileSystemAccess.class).getFileSystemConfiguration();\r\n    return fsAccess.execute(ugi.getShortUserName(), conf, executor);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createFileSystem",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "FileSystem createFileSystem(UserGroupInformation ugi) throws IOException, FileSystemAccessException\n{\r\n    String hadoopUser = ugi.getShortUserName();\r\n    FileSystemAccess fsAccess = HttpFSServerWebApp.get().get(FileSystemAccess.class);\r\n    Configuration conf = HttpFSServerWebApp.get().get(FileSystemAccess.class).getFileSystemConfiguration();\r\n    FileSystem fs = fsAccess.createFileSystem(hadoopUser, conf);\r\n    FileSystemReleaseFilter.setFileSystem(fs);\r\n    return fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "enforceRootPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void enforceRootPath(HttpFSFileSystem.Operation op, String path)\n{\r\n    if (!path.equals(\"/\")) {\r\n        throw new UnsupportedOperationException(MessageFormat.format(\"Operation [{0}], invalid path [{1}], must be '/'\", op, path));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getRoot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Response getRoot(@Context UriInfo uriInfo, @QueryParam(OperationParam.NAME) OperationParam op, @Context Parameters params, @Context HttpServletRequest request) throws IOException, FileSystemAccessException\n{\r\n    return get(\"\", uriInfo, op, params, request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "makeAbsolute",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String makeAbsolute(String path)\n{\r\n    return \"/\" + ((path != null) ? path : \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "get",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 108,
  "sourceCodeText" : "Response get(@PathParam(\"path\") String path, @Context UriInfo uriInfo, @QueryParam(OperationParam.NAME) OperationParam op, @Context Parameters params, @Context HttpServletRequest request) throws IOException, FileSystemAccessException\n{\r\n    if ((op.value() != HttpFSFileSystem.Operation.GETFILESTATUS) && (op.value() != HttpFSFileSystem.Operation.LISTSTATUS) && accessMode == AccessMode.WRITEONLY) {\r\n        return Response.status(Response.Status.FORBIDDEN).build();\r\n    }\r\n    UserGroupInformation user = HttpUserGroupInformation.get();\r\n    Response response;\r\n    path = makeAbsolute(path);\r\n    MDC.put(HttpFSFileSystem.OP_PARAM, op.value().name());\r\n    MDC.put(\"hostname\", request.getRemoteAddr());\r\n    switch(op.value()) {\r\n        case OPEN:\r\n            {\r\n                Boolean noRedirect = params.get(NoRedirectParam.NAME, NoRedirectParam.class);\r\n                if (noRedirect) {\r\n                    URI redirectURL = createOpenRedirectionURL(uriInfo);\r\n                    final String js = JsonUtil.toJsonString(\"Location\", redirectURL);\r\n                    response = Response.ok(js).type(MediaType.APPLICATION_JSON).build();\r\n                } else {\r\n                    final FSOperations.FSOpen command = new FSOperations.FSOpen(path);\r\n                    final FileSystem fs = createFileSystem(user);\r\n                    InputStream is = null;\r\n                    UserGroupInformation ugi = UserGroupInformation.createProxyUser(user.getShortUserName(), UserGroupInformation.getLoginUser());\r\n                    try {\r\n                        is = ugi.doAs(new PrivilegedExceptionAction<InputStream>() {\r\n\r\n                            @Override\r\n                            public InputStream run() throws Exception {\r\n                                return command.execute(fs);\r\n                            }\r\n                        });\r\n                    } catch (InterruptedException ie) {\r\n                        LOG.warn(\"Open interrupted.\", ie);\r\n                        Thread.currentThread().interrupt();\r\n                    }\r\n                    Long offset = params.get(OffsetParam.NAME, OffsetParam.class);\r\n                    Long len = params.get(LenParam.NAME, LenParam.class);\r\n                    AUDIT_LOG.info(\"[{}] offset [{}] len [{}]\", new Object[] { path, offset, len });\r\n                    InputStreamEntity entity = new InputStreamEntity(is, offset, len);\r\n                    response = Response.ok(entity).type(MediaType.APPLICATION_OCTET_STREAM).build();\r\n                }\r\n                break;\r\n            }\r\n        case GETFILESTATUS:\r\n            {\r\n                FSOperations.FSFileStatus command = new FSOperations.FSFileStatus(path);\r\n                Map json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}]\", path);\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case LISTSTATUS:\r\n            {\r\n                String filter = params.get(FilterParam.NAME, FilterParam.class);\r\n                FSOperations.FSListStatus command = new FSOperations.FSListStatus(path, filter);\r\n                Map json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] filter [{}]\", path, (filter != null) ? filter : \"-\");\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case GETHOMEDIRECTORY:\r\n            {\r\n                enforceRootPath(op.value(), path);\r\n                FSOperations.FSHomeDir command = new FSOperations.FSHomeDir();\r\n                JSONObject json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"Home Directory for [{}]\", user);\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case INSTRUMENTATION:\r\n            {\r\n                enforceRootPath(op.value(), path);\r\n                Groups groups = HttpFSServerWebApp.get().get(Groups.class);\r\n                Set<String> userGroups = groups.getGroupsSet(user.getShortUserName());\r\n                if (!userGroups.contains(HttpFSServerWebApp.get().getAdminGroup())) {\r\n                    throw new AccessControlException(\"User not in HttpFSServer admin group\");\r\n                }\r\n                Instrumentation instrumentation = HttpFSServerWebApp.get().get(Instrumentation.class);\r\n                Map snapshot = instrumentation.getSnapshot();\r\n                response = Response.ok(snapshot).build();\r\n                break;\r\n            }\r\n        case GETCONTENTSUMMARY:\r\n            {\r\n                FSOperations.FSContentSummary command = new FSOperations.FSContentSummary(path);\r\n                Map json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"Content summary for [{}]\", path);\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case GETQUOTAUSAGE:\r\n            {\r\n                FSOperations.FSQuotaUsage command = new FSOperations.FSQuotaUsage(path);\r\n                Map json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"Quota Usage for [{}]\", path);\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case GETFILECHECKSUM:\r\n            {\r\n                FSOperations.FSFileChecksum command = new FSOperations.FSFileChecksum(path);\r\n                Boolean noRedirect = params.get(NoRedirectParam.NAME, NoRedirectParam.class);\r\n                AUDIT_LOG.info(\"[{}]\", path);\r\n                if (noRedirect) {\r\n                    URI redirectURL = createOpenRedirectionURL(uriInfo);\r\n                    final String js = JsonUtil.toJsonString(\"Location\", redirectURL);\r\n                    response = Response.ok(js).type(MediaType.APPLICATION_JSON).build();\r\n                } else {\r\n                    Map json = fsExecute(user, command);\r\n                    response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                }\r\n                break;\r\n            }\r\n        case GETFILEBLOCKLOCATIONS:\r\n            {\r\n                response = Response.status(Response.Status.BAD_REQUEST).build();\r\n                break;\r\n            }\r\n        case GETACLSTATUS:\r\n            {\r\n                FSOperations.FSAclStatus command = new FSOperations.FSAclStatus(path);\r\n                Map json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"ACL status for [{}]\", path);\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case GETXATTRS:\r\n            {\r\n                List<String> xattrNames = params.getValues(XAttrNameParam.NAME, XAttrNameParam.class);\r\n                XAttrCodec encoding = params.get(XAttrEncodingParam.NAME, XAttrEncodingParam.class);\r\n                FSOperations.FSGetXAttrs command = new FSOperations.FSGetXAttrs(path, xattrNames, encoding);\r\n                @SuppressWarnings(\"rawtypes\")\r\n                Map json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"XAttrs for [{}]\", path);\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case LISTXATTRS:\r\n            {\r\n                FSOperations.FSListXAttrs command = new FSOperations.FSListXAttrs(path);\r\n                @SuppressWarnings(\"rawtypes\")\r\n                Map json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"XAttr names for [{}]\", path);\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case LISTSTATUS_BATCH:\r\n            {\r\n                String startAfter = params.get(HttpFSParametersProvider.StartAfterParam.NAME, HttpFSParametersProvider.StartAfterParam.class);\r\n                byte[] token = HttpFSUtils.EMPTY_BYTES;\r\n                if (startAfter != null) {\r\n                    token = startAfter.getBytes(Charsets.UTF_8);\r\n                }\r\n                FSOperations.FSListStatusBatch command = new FSOperations.FSListStatusBatch(path, token);\r\n                @SuppressWarnings(\"rawtypes\")\r\n                Map json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] token [{}]\", path, token);\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case GETTRASHROOT:\r\n            {\r\n                FSOperations.FSTrashRoot command = new FSOperations.FSTrashRoot(path);\r\n                JSONObject json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}]\", path);\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case GETALLSTORAGEPOLICY:\r\n            {\r\n                FSOperations.FSGetAllStoragePolicies command = new FSOperations.FSGetAllStoragePolicies();\r\n                JSONObject json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}]\", path);\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case GETSTORAGEPOLICY:\r\n            {\r\n                FSOperations.FSGetStoragePolicy command = new FSOperations.FSGetStoragePolicy(path);\r\n                JSONObject json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}]\", path);\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case GETSNAPSHOTDIFF:\r\n            {\r\n                String oldSnapshotName = params.get(OldSnapshotNameParam.NAME, OldSnapshotNameParam.class);\r\n                String snapshotName = params.get(SnapshotNameParam.NAME, SnapshotNameParam.class);\r\n                FSOperations.FSGetSnapshotDiff command = new FSOperations.FSGetSnapshotDiff(path, oldSnapshotName, snapshotName);\r\n                String js = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}]\", path);\r\n                response = Response.ok(js).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case GETSNAPSHOTDIFFLISTING:\r\n            {\r\n                String oldSnapshotName = params.get(OldSnapshotNameParam.NAME, OldSnapshotNameParam.class);\r\n                String snapshotName = params.get(SnapshotNameParam.NAME, SnapshotNameParam.class);\r\n                String snapshotDiffStartPath = params.get(HttpFSParametersProvider.SnapshotDiffStartPathParam.NAME, HttpFSParametersProvider.SnapshotDiffStartPathParam.class);\r\n                Integer snapshotDiffIndex = params.get(HttpFSParametersProvider.SnapshotDiffIndexParam.NAME, HttpFSParametersProvider.SnapshotDiffIndexParam.class);\r\n                FSOperations.FSGetSnapshotDiffListing command = new FSOperations.FSGetSnapshotDiffListing(path, oldSnapshotName, snapshotName, snapshotDiffStartPath, snapshotDiffIndex);\r\n                String js = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}]\", path);\r\n                response = Response.ok(js).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case GETSNAPSHOTTABLEDIRECTORYLIST:\r\n            {\r\n                FSOperations.FSGetSnapshottableDirListing command = new FSOperations.FSGetSnapshottableDirListing();\r\n                String js = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}]\", \"/\");\r\n                response = Response.ok(js).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case GETSNAPSHOTLIST:\r\n            {\r\n                FSOperations.FSGetSnapshotListing command = new FSOperations.FSGetSnapshotListing(path);\r\n                String js = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}]\", \"/\");\r\n                response = Response.ok(js).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case GETSERVERDEFAULTS:\r\n            {\r\n                FSOperations.FSGetServerDefaults command = new FSOperations.FSGetServerDefaults();\r\n                String js = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}]\", \"/\");\r\n                response = Response.ok(js).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case CHECKACCESS:\r\n            {\r\n                String mode = params.get(FsActionParam.NAME, FsActionParam.class);\r\n                FsActionParam fsparam = new FsActionParam(mode);\r\n                FSOperations.FSAccess command = new FSOperations.FSAccess(path, FsAction.getFsAction(fsparam.value()));\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}]\", \"/\");\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case GETECPOLICY:\r\n            {\r\n                FSOperations.FSGetErasureCodingPolicy command = new FSOperations.FSGetErasureCodingPolicy(path);\r\n                String js = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}]\", path);\r\n                response = Response.ok(js).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        default:\r\n            {\r\n                throw new IOException(MessageFormat.format(\"Invalid HTTP GET operation [{0}]\", op.value()));\r\n            }\r\n    }\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createOpenRedirectionURL",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "URI createOpenRedirectionURL(UriInfo uriInfo)\n{\r\n    UriBuilder uriBuilder = uriInfo.getRequestUriBuilder();\r\n    uriBuilder.replaceQueryParam(NoRedirectParam.NAME, (Object[]) null);\r\n    return uriBuilder.build((Object[]) null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "Response delete(@PathParam(\"path\") String path, @QueryParam(OperationParam.NAME) OperationParam op, @Context Parameters params, @Context HttpServletRequest request) throws IOException, FileSystemAccessException\n{\r\n    if (accessMode == AccessMode.READONLY) {\r\n        return Response.status(Response.Status.FORBIDDEN).build();\r\n    }\r\n    UserGroupInformation user = HttpUserGroupInformation.get();\r\n    Response response;\r\n    path = makeAbsolute(path);\r\n    MDC.put(HttpFSFileSystem.OP_PARAM, op.value().name());\r\n    MDC.put(\"hostname\", request.getRemoteAddr());\r\n    switch(op.value()) {\r\n        case DELETE:\r\n            {\r\n                Boolean recursive = params.get(RecursiveParam.NAME, RecursiveParam.class);\r\n                AUDIT_LOG.info(\"[{}] recursive [{}]\", path, recursive);\r\n                FSOperations.FSDelete command = new FSOperations.FSDelete(path, recursive);\r\n                JSONObject json = fsExecute(user, command);\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case DELETESNAPSHOT:\r\n            {\r\n                String snapshotName = params.get(SnapshotNameParam.NAME, SnapshotNameParam.class);\r\n                FSOperations.FSDeleteSnapshot command = new FSOperations.FSDeleteSnapshot(path, snapshotName);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] deleted snapshot [{}]\", path, snapshotName);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        default:\r\n            {\r\n                throw new IOException(MessageFormat.format(\"Invalid HTTP DELETE operation [{0}]\", op.value()));\r\n            }\r\n    }\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "postRoot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Response postRoot(InputStream is, @Context UriInfo uriInfo, @QueryParam(OperationParam.NAME) OperationParam op, @Context Parameters params, @Context HttpServletRequest request) throws IOException, FileSystemAccessException\n{\r\n    return post(is, uriInfo, \"/\", op, params, request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "post",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "Response post(InputStream is, @Context UriInfo uriInfo, @PathParam(\"path\") String path, @QueryParam(OperationParam.NAME) OperationParam op, @Context Parameters params, @Context HttpServletRequest request) throws IOException, FileSystemAccessException\n{\r\n    if (accessMode == AccessMode.READONLY) {\r\n        return Response.status(Response.Status.FORBIDDEN).build();\r\n    }\r\n    UserGroupInformation user = HttpUserGroupInformation.get();\r\n    Response response;\r\n    path = makeAbsolute(path);\r\n    MDC.put(HttpFSFileSystem.OP_PARAM, op.value().name());\r\n    MDC.put(\"hostname\", request.getRemoteAddr());\r\n    switch(op.value()) {\r\n        case APPEND:\r\n            {\r\n                Boolean hasData = params.get(DataParam.NAME, DataParam.class);\r\n                URI redirectURL = createUploadRedirectionURL(uriInfo, HttpFSFileSystem.Operation.APPEND);\r\n                Boolean noRedirect = params.get(NoRedirectParam.NAME, NoRedirectParam.class);\r\n                if (noRedirect) {\r\n                    final String js = JsonUtil.toJsonString(\"Location\", redirectURL);\r\n                    response = Response.ok(js).type(MediaType.APPLICATION_JSON).build();\r\n                } else if (hasData) {\r\n                    FSOperations.FSAppend command = new FSOperations.FSAppend(is, path);\r\n                    fsExecute(user, command);\r\n                    AUDIT_LOG.info(\"[{}]\", path);\r\n                    response = Response.ok().type(MediaType.APPLICATION_JSON).build();\r\n                } else {\r\n                    response = Response.temporaryRedirect(redirectURL).build();\r\n                }\r\n                break;\r\n            }\r\n        case CONCAT:\r\n            {\r\n                String sources = params.get(SourcesParam.NAME, SourcesParam.class);\r\n                FSOperations.FSConcat command = new FSOperations.FSConcat(path, sources.split(\",\"));\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}]\", path);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case TRUNCATE:\r\n            {\r\n                Long newLength = params.get(NewLengthParam.NAME, NewLengthParam.class);\r\n                FSOperations.FSTruncate command = new FSOperations.FSTruncate(path, newLength);\r\n                JSONObject json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"Truncate [{}] to length [{}]\", path, newLength);\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case UNSETSTORAGEPOLICY:\r\n            {\r\n                FSOperations.FSUnsetStoragePolicy command = new FSOperations.FSUnsetStoragePolicy(path);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"Unset storage policy [{}]\", path);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case UNSETECPOLICY:\r\n            {\r\n                FSOperations.FSUnSetErasureCodingPolicy command = new FSOperations.FSUnSetErasureCodingPolicy(path);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"Unset ec policy [{}]\", path);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        default:\r\n            {\r\n                throw new IOException(MessageFormat.format(\"Invalid HTTP POST operation [{0}]\", op.value()));\r\n            }\r\n    }\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createUploadRedirectionURL",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "URI createUploadRedirectionURL(UriInfo uriInfo, Enum<?> uploadOperation)\n{\r\n    UriBuilder uriBuilder = uriInfo.getRequestUriBuilder();\r\n    uriBuilder = uriBuilder.replaceQueryParam(OperationParam.NAME, uploadOperation).queryParam(DataParam.NAME, Boolean.TRUE).replaceQueryParam(NoRedirectParam.NAME, (Object[]) null);\r\n    return uriBuilder.build(null);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "putRoot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Response putRoot(InputStream is, @Context UriInfo uriInfo, @QueryParam(OperationParam.NAME) OperationParam op, @Context Parameters params, @Context HttpServletRequest request) throws IOException, FileSystemAccessException\n{\r\n    return put(is, uriInfo, \"/\", op, params, request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "put",
  "errType" : null,
  "containingMethodsNum" : 103,
  "sourceCodeText" : "Response put(InputStream is, @Context UriInfo uriInfo, @PathParam(\"path\") String path, @QueryParam(OperationParam.NAME) OperationParam op, @Context Parameters params, @Context HttpServletRequest request) throws IOException, FileSystemAccessException\n{\r\n    if (accessMode == AccessMode.READONLY) {\r\n        return Response.status(Response.Status.FORBIDDEN).build();\r\n    }\r\n    UserGroupInformation user = HttpUserGroupInformation.get();\r\n    Response response;\r\n    path = makeAbsolute(path);\r\n    MDC.put(HttpFSFileSystem.OP_PARAM, op.value().name());\r\n    MDC.put(\"hostname\", request.getRemoteAddr());\r\n    switch(op.value()) {\r\n        case CREATE:\r\n            {\r\n                Boolean hasData = params.get(DataParam.NAME, DataParam.class);\r\n                URI redirectURL = createUploadRedirectionURL(uriInfo, HttpFSFileSystem.Operation.CREATE);\r\n                Boolean noRedirect = params.get(NoRedirectParam.NAME, NoRedirectParam.class);\r\n                if (noRedirect) {\r\n                    final String js = JsonUtil.toJsonString(\"Location\", redirectURL);\r\n                    response = Response.ok(js).type(MediaType.APPLICATION_JSON).build();\r\n                } else if (hasData) {\r\n                    Short permission = params.get(PermissionParam.NAME, PermissionParam.class);\r\n                    Short unmaskedPermission = params.get(UnmaskedPermissionParam.NAME, UnmaskedPermissionParam.class);\r\n                    Boolean override = params.get(OverwriteParam.NAME, OverwriteParam.class);\r\n                    Short replication = params.get(ReplicationParam.NAME, ReplicationParam.class);\r\n                    Long blockSize = params.get(BlockSizeParam.NAME, BlockSizeParam.class);\r\n                    FSOperations.FSCreate command = new FSOperations.FSCreate(is, path, permission, override, replication, blockSize, unmaskedPermission);\r\n                    fsExecute(user, command);\r\n                    AUDIT_LOG.info(\"[{}] permission [{}] override [{}] \" + \"replication [{}] blockSize [{}] unmaskedpermission [{}]\", new Object[] { path, permission, override, replication, blockSize, unmaskedPermission });\r\n                    final String js = JsonUtil.toJsonString(\"Location\", uriInfo.getAbsolutePath());\r\n                    response = Response.created(uriInfo.getAbsolutePath()).type(MediaType.APPLICATION_JSON).entity(js).build();\r\n                } else {\r\n                    response = Response.temporaryRedirect(redirectURL).build();\r\n                }\r\n                break;\r\n            }\r\n        case ALLOWSNAPSHOT:\r\n            {\r\n                FSOperations.FSAllowSnapshot command = new FSOperations.FSAllowSnapshot(path);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] allowed snapshot\", path);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case DISALLOWSNAPSHOT:\r\n            {\r\n                FSOperations.FSDisallowSnapshot command = new FSOperations.FSDisallowSnapshot(path);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] disallowed snapshot\", path);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case CREATESNAPSHOT:\r\n            {\r\n                String snapshotName = params.get(SnapshotNameParam.NAME, SnapshotNameParam.class);\r\n                FSOperations.FSCreateSnapshot command = new FSOperations.FSCreateSnapshot(path, snapshotName);\r\n                String json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] snapshot created as [{}]\", path, snapshotName);\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case SETXATTR:\r\n            {\r\n                String xattrName = params.get(XAttrNameParam.NAME, XAttrNameParam.class);\r\n                String xattrValue = params.get(XAttrValueParam.NAME, XAttrValueParam.class);\r\n                EnumSet<XAttrSetFlag> flag = params.get(XAttrSetFlagParam.NAME, XAttrSetFlagParam.class);\r\n                FSOperations.FSSetXAttr command = new FSOperations.FSSetXAttr(path, xattrName, xattrValue, flag);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] to xAttr [{}]\", path, xattrName);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case RENAMESNAPSHOT:\r\n            {\r\n                String oldSnapshotName = params.get(OldSnapshotNameParam.NAME, OldSnapshotNameParam.class);\r\n                String snapshotName = params.get(SnapshotNameParam.NAME, SnapshotNameParam.class);\r\n                FSOperations.FSRenameSnapshot command = new FSOperations.FSRenameSnapshot(path, oldSnapshotName, snapshotName);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] renamed snapshot [{}] to [{}]\", path, oldSnapshotName, snapshotName);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case REMOVEXATTR:\r\n            {\r\n                String xattrName = params.get(XAttrNameParam.NAME, XAttrNameParam.class);\r\n                FSOperations.FSRemoveXAttr command = new FSOperations.FSRemoveXAttr(path, xattrName);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] removed xAttr [{}]\", path, xattrName);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case MKDIRS:\r\n            {\r\n                Short permission = params.get(PermissionParam.NAME, PermissionParam.class);\r\n                Short unmaskedPermission = params.get(UnmaskedPermissionParam.NAME, UnmaskedPermissionParam.class);\r\n                FSOperations.FSMkdirs command = new FSOperations.FSMkdirs(path, permission, unmaskedPermission);\r\n                JSONObject json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] permission [{}] unmaskedpermission [{}]\", path, permission, unmaskedPermission);\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case RENAME:\r\n            {\r\n                String toPath = params.get(DestinationParam.NAME, DestinationParam.class);\r\n                FSOperations.FSRename command = new FSOperations.FSRename(path, toPath);\r\n                JSONObject json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] to [{}]\", path, toPath);\r\n                response = Response.ok(json).type(MediaType.APPLICATION_JSON).build();\r\n                break;\r\n            }\r\n        case SETOWNER:\r\n            {\r\n                String owner = params.get(OwnerParam.NAME, OwnerParam.class);\r\n                String group = params.get(GroupParam.NAME, GroupParam.class);\r\n                FSOperations.FSSetOwner command = new FSOperations.FSSetOwner(path, owner, group);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] to (O/G)[{}]\", path, owner + \":\" + group);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case SETPERMISSION:\r\n            {\r\n                Short permission = params.get(PermissionParam.NAME, PermissionParam.class);\r\n                FSOperations.FSSetPermission command = new FSOperations.FSSetPermission(path, permission);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] to [{}]\", path, permission);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case SETREPLICATION:\r\n            {\r\n                Short replication = params.get(ReplicationParam.NAME, ReplicationParam.class);\r\n                FSOperations.FSSetReplication command = new FSOperations.FSSetReplication(path, replication);\r\n                JSONObject json = fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] to [{}]\", path, replication);\r\n                response = Response.ok(json).build();\r\n                break;\r\n            }\r\n        case SETTIMES:\r\n            {\r\n                Long modifiedTime = params.get(ModifiedTimeParam.NAME, ModifiedTimeParam.class);\r\n                Long accessTime = params.get(AccessTimeParam.NAME, AccessTimeParam.class);\r\n                FSOperations.FSSetTimes command = new FSOperations.FSSetTimes(path, modifiedTime, accessTime);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] to (M/A)[{}]\", path, modifiedTime + \":\" + accessTime);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case SETACL:\r\n            {\r\n                String aclSpec = params.get(AclPermissionParam.NAME, AclPermissionParam.class);\r\n                FSOperations.FSSetAcl command = new FSOperations.FSSetAcl(path, aclSpec);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] to acl [{}]\", path, aclSpec);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case REMOVEACL:\r\n            {\r\n                FSOperations.FSRemoveAcl command = new FSOperations.FSRemoveAcl(path);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] removed acl\", path);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case MODIFYACLENTRIES:\r\n            {\r\n                String aclSpec = params.get(AclPermissionParam.NAME, AclPermissionParam.class);\r\n                FSOperations.FSModifyAclEntries command = new FSOperations.FSModifyAclEntries(path, aclSpec);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] modify acl entry with [{}]\", path, aclSpec);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case REMOVEACLENTRIES:\r\n            {\r\n                String aclSpec = params.get(AclPermissionParam.NAME, AclPermissionParam.class);\r\n                FSOperations.FSRemoveAclEntries command = new FSOperations.FSRemoveAclEntries(path, aclSpec);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] remove acl entry [{}]\", path, aclSpec);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case REMOVEDEFAULTACL:\r\n            {\r\n                FSOperations.FSRemoveDefaultAcl command = new FSOperations.FSRemoveDefaultAcl(path);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] remove default acl\", path);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case SETSTORAGEPOLICY:\r\n            {\r\n                String policyName = params.get(PolicyNameParam.NAME, PolicyNameParam.class);\r\n                FSOperations.FSSetStoragePolicy command = new FSOperations.FSSetStoragePolicy(path, policyName);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] to policy [{}]\", path, policyName);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case SETECPOLICY:\r\n            {\r\n                String policyName = params.get(ECPolicyParam.NAME, ECPolicyParam.class);\r\n                FSOperations.FSSetErasureCodingPolicy command = new FSOperations.FSSetErasureCodingPolicy(path, policyName);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"[{}] to policy [{}]\", path, policyName);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        case SATISFYSTORAGEPOLICY:\r\n            {\r\n                FSOperations.FSSatisyStoragePolicy command = new FSOperations.FSSatisyStoragePolicy(path);\r\n                fsExecute(user, command);\r\n                AUDIT_LOG.info(\"satisfy storage policy for [{}]\", path);\r\n                response = Response.ok().build();\r\n                break;\r\n            }\r\n        default:\r\n            {\r\n                throw new IOException(MessageFormat.format(\"Invalid HTTP PUT operation [{0}]\", op.value()));\r\n            }\r\n    }\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "init",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void init() throws ServiceException\n{\r\n    LOG.info(\"Using FileSystemAccess JARs version [{}]\", VersionInfo.getVersion());\r\n    String security = getServiceConfig().get(AUTHENTICATION_TYPE, \"simple\").trim();\r\n    if (security.equals(\"kerberos\")) {\r\n        String defaultName = getServer().getName();\r\n        String keytab = System.getProperty(\"user.home\") + \"/\" + defaultName + \".keytab\";\r\n        keytab = getServiceConfig().get(KERBEROS_KEYTAB, keytab).trim();\r\n        if (keytab.length() == 0) {\r\n            throw new ServiceException(FileSystemAccessException.ERROR.H01, KERBEROS_KEYTAB);\r\n        }\r\n        String principal = defaultName + \"/localhost@LOCALHOST\";\r\n        principal = getServiceConfig().get(KERBEROS_PRINCIPAL, principal).trim();\r\n        if (principal.length() == 0) {\r\n            throw new ServiceException(FileSystemAccessException.ERROR.H01, KERBEROS_PRINCIPAL);\r\n        }\r\n        Configuration conf = new Configuration();\r\n        conf.set(HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\r\n        UserGroupInformation.setConfiguration(conf);\r\n        try {\r\n            UserGroupInformation.loginUserFromKeytab(principal, keytab);\r\n        } catch (IOException ex) {\r\n            throw new ServiceException(FileSystemAccessException.ERROR.H02, ex.getMessage(), ex);\r\n        }\r\n        LOG.info(\"Using FileSystemAccess Kerberos authentication, principal [{}] keytab [{}]\", principal, keytab);\r\n    } else if (security.equals(\"simple\")) {\r\n        Configuration conf = new Configuration();\r\n        conf.set(HADOOP_SECURITY_AUTHENTICATION, \"simple\");\r\n        UserGroupInformation.setConfiguration(conf);\r\n        LOG.info(\"Using FileSystemAccess simple/pseudo authentication, principal [{}]\", System.getProperty(\"user.name\"));\r\n    } else {\r\n        throw new ServiceException(FileSystemAccessException.ERROR.H09, security);\r\n    }\r\n    String hadoopConfDirProp = getServiceConfig().get(HADOOP_CONF_DIR, getServer().getConfigDir());\r\n    File hadoopConfDir = new File(hadoopConfDirProp).getAbsoluteFile();\r\n    if (!hadoopConfDir.exists()) {\r\n        hadoopConfDir = new File(getServer().getConfigDir()).getAbsoluteFile();\r\n    }\r\n    if (!hadoopConfDir.exists()) {\r\n        throw new ServiceException(FileSystemAccessException.ERROR.H10, hadoopConfDir);\r\n    }\r\n    try {\r\n        serviceHadoopConf = loadHadoopConf(hadoopConfDir);\r\n        fileSystemConf = getNewFileSystemConfiguration();\r\n    } catch (IOException ex) {\r\n        throw new ServiceException(FileSystemAccessException.ERROR.H11, ex.toString(), ex);\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"FileSystemAccess FileSystem configuration:\");\r\n        for (Map.Entry entry : serviceHadoopConf) {\r\n            LOG.debug(\"  {} = {}\", entry.getKey(), entry.getValue());\r\n        }\r\n    }\r\n    setRequiredServiceHadoopConf(serviceHadoopConf);\r\n    nameNodeWhitelist = toLowerCase(getServiceConfig().getTrimmedStringCollection(NAME_NODE_WHITELIST));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "loadHadoopConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration loadHadoopConf(File dir) throws IOException\n{\r\n    Configuration hadoopConf = new Configuration(false);\r\n    for (String file : HADOOP_CONF_FILES) {\r\n        File f = new File(dir, file);\r\n        if (f.exists()) {\r\n            hadoopConf.addResource(new Path(f.getAbsolutePath()));\r\n        }\r\n    }\r\n    return hadoopConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "getNewFileSystemConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration getNewFileSystemConfiguration()\n{\r\n    Configuration conf = new Configuration(true);\r\n    ConfigurationUtils.copy(serviceHadoopConf, conf);\r\n    conf.setBoolean(FILE_SYSTEM_SERVICE_CREATED, true);\r\n    conf.set(FsPermission.UMASK_LABEL, \"000\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "postInit",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void postInit() throws ServiceException\n{\r\n    super.postInit();\r\n    Instrumentation instrumentation = getServer().get(Instrumentation.class);\r\n    instrumentation.addVariable(INSTRUMENTATION_GROUP, \"unmanaged.fs\", new Instrumentation.Variable<Integer>() {\r\n\r\n        @Override\r\n        public Integer getValue() {\r\n            return unmanagedFileSystems.get();\r\n        }\r\n    });\r\n    instrumentation.addSampler(INSTRUMENTATION_GROUP, \"unmanaged.fs\", 60, new Instrumentation.Variable<Long>() {\r\n\r\n        @Override\r\n        public Long getValue() {\r\n            return (long) unmanagedFileSystems.get();\r\n        }\r\n    });\r\n    Scheduler scheduler = getServer().get(Scheduler.class);\r\n    int purgeInterval = getServiceConfig().getInt(FS_CACHE_PURGE_FREQUENCY, 60);\r\n    purgeTimeout = getServiceConfig().getLong(FS_CACHE_PURGE_TIMEOUT, 60);\r\n    purgeTimeout = (purgeTimeout > 0) ? purgeTimeout : 0;\r\n    if (purgeTimeout > 0) {\r\n        scheduler.schedule(new FileSystemCachePurger(), purgeInterval, purgeInterval, TimeUnit.SECONDS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "toLowerCase",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<String> toLowerCase(Collection<String> collection)\n{\r\n    Set<String> set = new HashSet<String>();\r\n    for (String value : collection) {\r\n        set.add(StringUtils.toLowerCase(value));\r\n    }\r\n    return set;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "getInterface",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class getInterface()\n{\r\n    return FileSystemAccess.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "getServiceDependencies",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class[] getServiceDependencies()\n{\r\n    return new Class[] { Instrumentation.class, Scheduler.class };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "getUGI",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UserGroupInformation getUGI(String user) throws IOException\n{\r\n    return UserGroupInformation.createProxyUser(user, UserGroupInformation.getLoginUser());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "setRequiredServiceHadoopConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setRequiredServiceHadoopConf(Configuration conf)\n{\r\n    conf.set(\"fs.hdfs.impl.disable.cache\", \"true\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "createFileSystem",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FileSystem createFileSystem(Configuration namenodeConf) throws IOException\n{\r\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    CachedFileSystem newCachedFS = new CachedFileSystem(purgeTimeout);\r\n    CachedFileSystem cachedFS = fsCache.putIfAbsent(user, newCachedFS);\r\n    if (cachedFS == null) {\r\n        cachedFS = newCachedFS;\r\n    }\r\n    Configuration conf = new Configuration(namenodeConf);\r\n    conf.set(HTTPFS_FS_USER, user);\r\n    return cachedFS.getFileSystem(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "closeFileSystem",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void closeFileSystem(FileSystem fs) throws IOException\n{\r\n    if (fsCache.containsKey(fs.getConf().get(HTTPFS_FS_USER))) {\r\n        fsCache.get(fs.getConf().get(HTTPFS_FS_USER)).release();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "validateNamenode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void validateNamenode(String namenode) throws FileSystemAccessException\n{\r\n    if (nameNodeWhitelist.size() > 0 && !nameNodeWhitelist.contains(\"*\")) {\r\n        if (!nameNodeWhitelist.contains(StringUtils.toLowerCase(namenode))) {\r\n            throw new FileSystemAccessException(FileSystemAccessException.ERROR.H05, namenode, \"not in whitelist\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "checkNameNodeHealth",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkNameNodeHealth(FileSystem fileSystem) throws FileSystemAccessException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "execute",
  "errType" : [ "FileSystemAccessException", "Exception" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "T execute(String user, final Configuration conf, final FileSystemExecutor<T> executor) throws FileSystemAccessException\n{\r\n    Check.notEmpty(user, \"user\");\r\n    Check.notNull(conf, \"conf\");\r\n    Check.notNull(executor, \"executor\");\r\n    if (!conf.getBoolean(FILE_SYSTEM_SERVICE_CREATED, false)) {\r\n        throw new FileSystemAccessException(FileSystemAccessException.ERROR.H04);\r\n    }\r\n    if (conf.get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY) == null || conf.getTrimmed(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY).length() == 0) {\r\n        throw new FileSystemAccessException(FileSystemAccessException.ERROR.H06, CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY);\r\n    }\r\n    try {\r\n        validateNamenode(new URI(conf.getTrimmed(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY)).getAuthority());\r\n        UserGroupInformation ugi = getUGI(user);\r\n        return ugi.doAs(new PrivilegedExceptionAction<T>() {\r\n\r\n            @Override\r\n            public T run() throws Exception {\r\n                FileSystem fs = createFileSystem(conf);\r\n                Instrumentation instrumentation = getServer().get(Instrumentation.class);\r\n                Instrumentation.Cron cron = instrumentation.createCron();\r\n                try {\r\n                    checkNameNodeHealth(fs);\r\n                    cron.start();\r\n                    return executor.execute(fs);\r\n                } finally {\r\n                    cron.stop();\r\n                    instrumentation.addCron(INSTRUMENTATION_GROUP, executor.getClass().getSimpleName(), cron);\r\n                    closeFileSystem(fs);\r\n                }\r\n            }\r\n        });\r\n    } catch (FileSystemAccessException ex) {\r\n        throw ex;\r\n    } catch (Exception ex) {\r\n        throw new FileSystemAccessException(FileSystemAccessException.ERROR.H03, ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "createFileSystemInternal",
  "errType" : [ "IOException", "FileSystemAccessException", "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "FileSystem createFileSystemInternal(String user, final Configuration conf) throws IOException, FileSystemAccessException\n{\r\n    Check.notEmpty(user, \"user\");\r\n    Check.notNull(conf, \"conf\");\r\n    if (!conf.getBoolean(FILE_SYSTEM_SERVICE_CREATED, false)) {\r\n        throw new FileSystemAccessException(FileSystemAccessException.ERROR.H04);\r\n    }\r\n    try {\r\n        validateNamenode(new URI(conf.getTrimmed(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY)).getAuthority());\r\n        UserGroupInformation ugi = getUGI(user);\r\n        return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {\r\n\r\n            @Override\r\n            public FileSystem run() throws Exception {\r\n                return createFileSystem(conf);\r\n            }\r\n        });\r\n    } catch (IOException ex) {\r\n        throw ex;\r\n    } catch (FileSystemAccessException ex) {\r\n        throw ex;\r\n    } catch (Exception ex) {\r\n        throw new FileSystemAccessException(FileSystemAccessException.ERROR.H08, ex.getMessage(), ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "createFileSystem",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileSystem createFileSystem(String user, final Configuration conf) throws IOException, FileSystemAccessException\n{\r\n    unmanagedFileSystems.incrementAndGet();\r\n    return createFileSystemInternal(user, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "releaseFileSystem",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void releaseFileSystem(FileSystem fs) throws IOException\n{\r\n    unmanagedFileSystems.decrementAndGet();\r\n    closeFileSystem(fs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "getFileSystemConfiguration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getFileSystemConfiguration()\n{\r\n    return fileSystemConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Byte parse(String str) throws Exception\n{\r\n    return Byte.parseByte(str);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getDomain",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDomain()\n{\r\n    return \"a byte\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "setBufferSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setBufferSize(Configuration conf)\n{\r\n    bufferSize = conf.getInt(HTTPFS_BUFFER_SIZE_KEY, HTTP_BUFFER_SIZE_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "toJson",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, Object> toJson(FileStatus fileStatus)\n{\r\n    Map<String, Object> json = new LinkedHashMap<>();\r\n    json.put(HttpFSFileSystem.FILE_STATUS_JSON, toJsonInner(fileStatus, true));\r\n    return json;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "toJson",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<String, Object> toJson(FileStatus[] fileStatuses, boolean isFile)\n{\r\n    Map<String, Object> json = new LinkedHashMap<>();\r\n    Map<String, Object> inner = new LinkedHashMap<>();\r\n    JSONArray statuses = new JSONArray();\r\n    for (FileStatus f : fileStatuses) {\r\n        statuses.add(toJsonInner(f, isFile));\r\n    }\r\n    inner.put(HttpFSFileSystem.FILE_STATUS_JSON, statuses);\r\n    json.put(HttpFSFileSystem.FILE_STATUSES_JSON, inner);\r\n    return json;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "toJsonInner",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 27,
  "sourceCodeText" : "Map<String, Object> toJsonInner(FileStatus fileStatus, boolean emptyPathSuffix)\n{\r\n    Map<String, Object> json = new LinkedHashMap<String, Object>();\r\n    json.put(HttpFSFileSystem.PATH_SUFFIX_JSON, (emptyPathSuffix) ? \"\" : fileStatus.getPath().getName());\r\n    FILE_TYPE fileType = HttpFSFileSystem.FILE_TYPE.getType(fileStatus);\r\n    json.put(HttpFSFileSystem.TYPE_JSON, fileType.toString());\r\n    if (fileType.equals(FILE_TYPE.SYMLINK)) {\r\n        try {\r\n            json.put(HttpFSFileSystem.SYMLINK_JSON, fileStatus.getSymlink().getName());\r\n        } catch (IOException e) {\r\n        }\r\n    }\r\n    json.put(HttpFSFileSystem.LENGTH_JSON, fileStatus.getLen());\r\n    json.put(HttpFSFileSystem.OWNER_JSON, fileStatus.getOwner());\r\n    json.put(HttpFSFileSystem.GROUP_JSON, fileStatus.getGroup());\r\n    json.put(HttpFSFileSystem.PERMISSION_JSON, HttpFSFileSystem.permissionToString(fileStatus.getPermission()));\r\n    json.put(HttpFSFileSystem.ACCESS_TIME_JSON, fileStatus.getAccessTime());\r\n    json.put(HttpFSFileSystem.MODIFICATION_TIME_JSON, fileStatus.getModificationTime());\r\n    json.put(HttpFSFileSystem.BLOCK_SIZE_JSON, fileStatus.getBlockSize());\r\n    json.put(HttpFSFileSystem.REPLICATION_JSON, fileStatus.getReplication());\r\n    if (fileStatus instanceof HdfsFileStatus) {\r\n        HdfsFileStatus hdfsFileStatus = (HdfsFileStatus) fileStatus;\r\n        json.put(HttpFSFileSystem.CHILDREN_NUM_JSON, hdfsFileStatus.getChildrenNum());\r\n        json.put(HttpFSFileSystem.FILE_ID_JSON, hdfsFileStatus.getFileId());\r\n        json.put(HttpFSFileSystem.STORAGEPOLICY_JSON, hdfsFileStatus.getStoragePolicy());\r\n        if (hdfsFileStatus.getErasureCodingPolicy() != null) {\r\n            json.put(HttpFSFileSystem.ECPOLICYNAME_JSON, hdfsFileStatus.getErasureCodingPolicy().getName());\r\n            json.put(HttpFSFileSystem.ECPOLICY_JSON, JsonUtil.getEcPolicyAsMap(hdfsFileStatus.getErasureCodingPolicy()));\r\n        }\r\n    }\r\n    if (fileStatus.getPermission().getAclBit()) {\r\n        json.put(HttpFSFileSystem.ACL_BIT_JSON, true);\r\n    }\r\n    if (fileStatus.getPermission().getEncryptedBit()) {\r\n        json.put(HttpFSFileSystem.ENC_BIT_JSON, true);\r\n    }\r\n    if (fileStatus.getPermission().getErasureCodedBit()) {\r\n        json.put(HttpFSFileSystem.EC_BIT_JSON, true);\r\n    }\r\n    if (fileStatus.isSnapshotEnabled()) {\r\n        json.put(HttpFSFileSystem.SNAPSHOT_BIT_JSON, true);\r\n    }\r\n    return json;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "toJson",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<String, Object> toJson(FileSystem.DirectoryEntries entries, boolean isFile)\n{\r\n    Map<String, Object> json = new LinkedHashMap<>();\r\n    Map<String, Object> inner = new LinkedHashMap<>();\r\n    Map<String, Object> fileStatuses = toJson(entries.getEntries(), isFile);\r\n    inner.put(HttpFSFileSystem.PARTIAL_LISTING_JSON, fileStatuses);\r\n    inner.put(HttpFSFileSystem.REMAINING_ENTRIES_JSON, entries.hasMore() ? 1 : 0);\r\n    json.put(HttpFSFileSystem.DIRECTORY_LISTING_JSON, inner);\r\n    return json;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "aclStatusToJSON",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Map<String, Object> aclStatusToJSON(AclStatus aclStatus)\n{\r\n    Map<String, Object> json = new LinkedHashMap<String, Object>();\r\n    Map<String, Object> inner = new LinkedHashMap<String, Object>();\r\n    JSONArray entriesArray = new JSONArray();\r\n    inner.put(HttpFSFileSystem.OWNER_JSON, aclStatus.getOwner());\r\n    inner.put(HttpFSFileSystem.GROUP_JSON, aclStatus.getGroup());\r\n    inner.put(HttpFSFileSystem.PERMISSION_JSON, HttpFSFileSystem.permissionToString(aclStatus.getPermission()));\r\n    inner.put(HttpFSFileSystem.ACL_STICKY_BIT_JSON, aclStatus.isStickyBit());\r\n    for (AclEntry e : aclStatus.getEntries()) {\r\n        entriesArray.add(e.toString());\r\n    }\r\n    inner.put(HttpFSFileSystem.ACL_ENTRIES_JSON, entriesArray);\r\n    json.put(HttpFSFileSystem.ACL_STATUS_JSON, inner);\r\n    return json;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "fileChecksumToJSON",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map fileChecksumToJSON(FileChecksum checksum)\n{\r\n    Map json = new LinkedHashMap();\r\n    json.put(HttpFSFileSystem.CHECKSUM_ALGORITHM_JSON, checksum.getAlgorithmName());\r\n    json.put(HttpFSFileSystem.CHECKSUM_BYTES_JSON, org.apache.hadoop.util.StringUtils.byteToHexString(checksum.getBytes()));\r\n    json.put(HttpFSFileSystem.CHECKSUM_LENGTH_JSON, checksum.getLength());\r\n    Map response = new LinkedHashMap();\r\n    response.put(HttpFSFileSystem.FILE_CHECKSUM_JSON, json);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "xAttrsToJSON",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Map xAttrsToJSON(Map<String, byte[]> xAttrs, XAttrCodec encoding) throws IOException\n{\r\n    Map jsonMap = new LinkedHashMap();\r\n    JSONArray jsonArray = new JSONArray();\r\n    if (xAttrs != null) {\r\n        for (Entry<String, byte[]> e : xAttrs.entrySet()) {\r\n            Map json = new LinkedHashMap();\r\n            json.put(HttpFSFileSystem.XATTR_NAME_JSON, e.getKey());\r\n            if (e.getValue() != null) {\r\n                json.put(HttpFSFileSystem.XATTR_VALUE_JSON, XAttrCodec.encodeValue(e.getValue(), encoding));\r\n            }\r\n            jsonArray.add(json);\r\n        }\r\n    }\r\n    jsonMap.put(HttpFSFileSystem.XATTRS_JSON, jsonArray);\r\n    return jsonMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "xAttrNamesToJSON",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map xAttrNamesToJSON(List<String> names) throws IOException\n{\r\n    Map jsonMap = new LinkedHashMap();\r\n    jsonMap.put(HttpFSFileSystem.XATTRNAMES_JSON, JSONArray.toJSONString(names));\r\n    return jsonMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "contentSummaryToJSON",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Map contentSummaryToJSON(ContentSummary contentSummary)\n{\r\n    Map json = new LinkedHashMap();\r\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_DIRECTORY_COUNT_JSON, contentSummary.getDirectoryCount());\r\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_ECPOLICY_JSON, contentSummary.getErasureCodingPolicy());\r\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_FILE_COUNT_JSON, contentSummary.getFileCount());\r\n    json.put(HttpFSFileSystem.CONTENT_SUMMARY_LENGTH_JSON, contentSummary.getLength());\r\n    Map<String, Object> quotaUsageMap = quotaUsageToMap(contentSummary);\r\n    for (Map.Entry<String, Object> e : quotaUsageMap.entrySet()) {\r\n        if (!e.getKey().equals(HttpFSFileSystem.QUOTA_USAGE_FILE_AND_DIRECTORY_COUNT_JSON)) {\r\n            json.put(e.getKey(), e.getValue());\r\n        }\r\n    }\r\n    Map response = new LinkedHashMap();\r\n    response.put(HttpFSFileSystem.CONTENT_SUMMARY_JSON, json);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "quotaUsageToJSON",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Map quotaUsageToJSON(QuotaUsage quotaUsage)\n{\r\n    Map response = new LinkedHashMap();\r\n    Map quotaUsageMap = quotaUsageToMap(quotaUsage);\r\n    response.put(HttpFSFileSystem.QUOTA_USAGE_JSON, quotaUsageMap);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "quotaUsageToMap",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "Map<String, Object> quotaUsageToMap(QuotaUsage quotaUsage)\n{\r\n    Map<String, Object> result = new LinkedHashMap<>();\r\n    result.put(HttpFSFileSystem.QUOTA_USAGE_FILE_AND_DIRECTORY_COUNT_JSON, quotaUsage.getFileAndDirectoryCount());\r\n    result.put(HttpFSFileSystem.QUOTA_USAGE_QUOTA_JSON, quotaUsage.getQuota());\r\n    result.put(HttpFSFileSystem.QUOTA_USAGE_SPACE_CONSUMED_JSON, quotaUsage.getSpaceConsumed());\r\n    result.put(HttpFSFileSystem.QUOTA_USAGE_SPACE_QUOTA_JSON, quotaUsage.getSpaceQuota());\r\n    Map<String, Map<String, Long>> typeQuota = new TreeMap<>();\r\n    for (StorageType t : StorageType.getTypesSupportingQuota()) {\r\n        long tQuota = quotaUsage.getTypeQuota(t);\r\n        if (tQuota != HdfsConstants.QUOTA_RESET) {\r\n            Map<String, Long> type = typeQuota.get(t.toString());\r\n            if (type == null) {\r\n                type = new TreeMap<>();\r\n                typeQuota.put(t.toString(), type);\r\n            }\r\n            type.put(HttpFSFileSystem.QUOTA_USAGE_QUOTA_JSON, quotaUsage.getTypeQuota(t));\r\n            type.put(HttpFSFileSystem.QUOTA_USAGE_CONSUMED_JSON, quotaUsage.getTypeConsumed(t));\r\n        }\r\n    }\r\n    result.put(HttpFSFileSystem.QUOTA_USAGE_TYPE_QUOTA_JSON, typeQuota);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "toJSON",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JSONObject toJSON(String name, Object value)\n{\r\n    JSONObject json = new JSONObject();\r\n    json.put(name, value);\r\n    return json;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "storagePolicyToJSON",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "JSONObject storagePolicyToJSON(BlockStoragePolicySpi policy)\n{\r\n    BlockStoragePolicy p = (BlockStoragePolicy) policy;\r\n    JSONObject policyJson = new JSONObject();\r\n    policyJson.put(\"id\", p.getId());\r\n    policyJson.put(\"name\", p.getName());\r\n    policyJson.put(\"storageTypes\", toJsonArray(p.getStorageTypes()));\r\n    policyJson.put(\"creationFallbacks\", toJsonArray(p.getCreationFallbacks()));\r\n    policyJson.put(\"replicationFallbacks\", toJsonArray(p.getReplicationFallbacks()));\r\n    policyJson.put(\"copyOnCreateFile\", p.isCopyOnCreateFile());\r\n    return policyJson;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "toJsonArray",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JSONArray toJsonArray(StorageType[] storageTypes)\n{\r\n    JSONArray jsonArray = new JSONArray();\r\n    for (StorageType type : storageTypes) {\r\n        jsonArray.add(type.toString());\r\n    }\r\n    return jsonArray;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "storagePoliciesToJSON",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "JSONObject storagePoliciesToJSON(Collection<? extends BlockStoragePolicySpi> storagePolicies)\n{\r\n    JSONObject json = new JSONObject();\r\n    JSONArray jsonArray = new JSONArray();\r\n    JSONObject policies = new JSONObject();\r\n    if (storagePolicies != null) {\r\n        for (BlockStoragePolicySpi policy : storagePolicies) {\r\n            JSONObject policyMap = storagePolicyToJSON(policy);\r\n            jsonArray.add(policyMap);\r\n        }\r\n    }\r\n    policies.put(HttpFSFileSystem.STORAGE_POLICY_JSON, jsonArray);\r\n    json.put(HttpFSFileSystem.STORAGE_POLICIES_JSON, policies);\r\n    return json;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "copyBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long copyBytes(InputStream in, OutputStream out) throws IOException\n{\r\n    return copyBytes(in, out, Long.MAX_VALUE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "copyBytes",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long copyBytes(InputStream in, OutputStream out, long count) throws IOException\n{\r\n    long totalBytes = 0;\r\n    byte[] buf = new byte[bufferSize];\r\n    long bytesRemaining = count;\r\n    int bytesRead;\r\n    try {\r\n        while (bytesRemaining > 0) {\r\n            int bytesToRead = (int) (bytesRemaining < buf.length ? bytesRemaining : buf.length);\r\n            bytesRead = in.read(buf, 0, bytesToRead);\r\n            if (bytesRead == -1) {\r\n                break;\r\n            }\r\n            out.write(buf, 0, bytesRead);\r\n            bytesRemaining -= bytesRead;\r\n            totalBytes += bytesRead;\r\n        }\r\n        return totalBytes;\r\n    } finally {\r\n        try {\r\n            in.close();\r\n        } finally {\r\n            out.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "deprecateEnv",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void deprecateEnv(String varName, Configuration conf, String propName, String confFile)\n{\r\n    String value = System.getenv(varName);\r\n    if (value == null) {\r\n        return;\r\n    }\r\n    LOG.warn(\"Environment variable {} is deprecated and overriding\" + \" property {}', please set the property in {} instead.\", varName, propName, confFile);\r\n    conf.set(propName, value, \"environment variable \" + varName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "start",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void start() throws IOException\n{\r\n    DefaultMetricsSystem.initialize(\"httpfs\");\r\n    httpServer.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "join",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void join() throws InterruptedException\n{\r\n    httpServer.join();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stop() throws Exception\n{\r\n    httpServer.stop();\r\n    DefaultMetricsSystem.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getUrl",
  "errType" : [ "MalformedURLException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "URL getUrl()\n{\r\n    InetSocketAddress addr = httpServer.getConnectorAddress(0);\r\n    if (null == addr) {\r\n        return null;\r\n    }\r\n    try {\r\n        return new URL(scheme, addr.getHostName(), addr.getPort(), SERVLET_PATH);\r\n    } catch (MalformedURLException ex) {\r\n        throw new RuntimeException(\"It should never happen: \" + ex.getMessage(), ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getHttpServer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HttpServer2 getHttpServer()\n{\r\n    return httpServer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    startupShutdownMessage(HttpFSServerWebServer.class, args, LOG);\r\n    Configuration conf = new Configuration(true);\r\n    Configuration sslConf = SSLFactory.readSSLConfiguration(conf, SSLFactory.Mode.SERVER);\r\n    HttpFSServerWebServer webServer = new HttpFSServerWebServer(conf, sslConf);\r\n    webServer.start();\r\n    webServer.join();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "addDeprecatedKeys",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addDeprecatedKeys()\n{\r\n    Configuration.addDeprecations(new Configuration.DeprecationDelta[] { new Configuration.DeprecationDelta(CONF_PREFIX + KEYTAB, HADOOP_HTTP_CONF_PREFIX + KEYTAB), new Configuration.DeprecationDelta(CONF_PREFIX + PRINCIPAL, HADOOP_HTTP_CONF_PREFIX + PRINCIPAL), new Configuration.DeprecationDelta(CONF_PREFIX + SIGNATURE_SECRET_FILE, HADOOP_HTTP_CONF_PREFIX + SIGNATURE_SECRET_FILE), new Configuration.DeprecationDelta(CONF_PREFIX + AUTH_TYPE, HADOOP_HTTP_CONF_PREFIX + AUTH_TYPE) });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "createResponse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Response createResponse(Response.Status status, Throwable throwable)\n{\r\n    return HttpExceptionUtils.createJerseyExceptionResponse(status, throwable);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getOneLineMessage",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getOneLineMessage(Throwable throwable)\n{\r\n    String message = throwable.getMessage();\r\n    if (message != null) {\r\n        int i = message.indexOf(ENTER);\r\n        if (i > -1) {\r\n            message = message.substring(0, i);\r\n        }\r\n    }\r\n    return message;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "log",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void log(Response.Status status, Throwable throwable)\n{\r\n    LOG.debug(\"{}\", throwable.getMessage(), throwable);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "toResponse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Response toResponse(Throwable throwable)\n{\r\n    return createResponse(Response.Status.BAD_REQUEST, throwable);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getConnection",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HttpURLConnection getConnection(final String method, Map<String, String> params, Path path, boolean makeQualified) throws IOException\n{\r\n    return getConnection(method, params, null, path, makeQualified);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getConnection",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "HttpURLConnection getConnection(final String method, Map<String, String> params, Map<String, List<String>> multiValuedParams, Path path, boolean makeQualified) throws IOException\n{\r\n    if (makeQualified) {\r\n        path = makeQualified(path);\r\n    }\r\n    final URL url = HttpFSUtils.createURL(path, params, multiValuedParams);\r\n    try {\r\n        return UserGroupInformation.getCurrentUser().doAs(new PrivilegedExceptionAction<HttpURLConnection>() {\r\n\r\n            @Override\r\n            public HttpURLConnection run() throws Exception {\r\n                return getConnection(url, method);\r\n            }\r\n        });\r\n    } catch (Exception ex) {\r\n        if (ex instanceof IOException) {\r\n            throw (IOException) ex;\r\n        } else {\r\n            throw new IOException(ex);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getConnection",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "HttpURLConnection getConnection(URL url, String method) throws IOException\n{\r\n    try {\r\n        HttpURLConnection conn = authURL.openConnection(url, authToken);\r\n        conn.setRequestMethod(method);\r\n        if (method.equals(HTTP_POST) || method.equals(HTTP_PUT)) {\r\n            conn.setDoOutput(true);\r\n        }\r\n        return conn;\r\n    } catch (Exception ex) {\r\n        throw new IOException(ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "initialize",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void initialize(URI name, Configuration conf) throws IOException\n{\r\n    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n    realUser = ugi.getRealUser();\r\n    if (realUser == null) {\r\n        realUser = UserGroupInformation.getLoginUser();\r\n    }\r\n    super.initialize(name, conf);\r\n    try {\r\n        uri = new URI(name.getScheme() + \"://\" + name.getAuthority());\r\n    } catch (URISyntaxException ex) {\r\n        throw new IOException(ex);\r\n    }\r\n    Class<? extends DelegationTokenAuthenticator> klass = getConf().getClass(\"httpfs.authenticator.class\", KerberosDelegationTokenAuthenticator.class, DelegationTokenAuthenticator.class);\r\n    DelegationTokenAuthenticator authenticator = ReflectionUtils.newInstance(klass, getConf());\r\n    authURL = new DelegationTokenAuthenticatedURL(authenticator);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return SCHEME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI getUri()\n{\r\n    return uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getDefaultPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getDefaultPort()\n{\r\n    return DFSConfigKeys.DFS_NAMENODE_HTTP_PORT_DEFAULT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FSDataInputStream open(Path f, int bufferSize) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.OPEN.toString());\r\n    HttpURLConnection conn = getConnection(Operation.OPEN.getMethod(), params, f, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    return new FSDataInputStream(new HttpFSDataInputStream(conn.getInputStream(), bufferSize));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "permissionToString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String permissionToString(FsPermission p)\n{\r\n    return Integer.toString((p == null) ? DEFAULT_PERMISSION : p.toShort(), 8);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "uploadData",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "FSDataOutputStream uploadData(String method, Path f, Map<String, String> params, int bufferSize, int expectedStatus) throws IOException\n{\r\n    HttpURLConnection conn = getConnection(method, params, f, true);\r\n    conn.setInstanceFollowRedirects(false);\r\n    boolean exceptionAlreadyHandled = false;\r\n    try {\r\n        if (conn.getResponseCode() == HTTP_TEMPORARY_REDIRECT) {\r\n            exceptionAlreadyHandled = true;\r\n            String location = conn.getHeaderField(\"Location\");\r\n            if (location != null) {\r\n                conn = getConnection(new URL(location), method);\r\n                conn.setRequestProperty(\"Content-Type\", UPLOAD_CONTENT_TYPE);\r\n                try {\r\n                    OutputStream os = new BufferedOutputStream(conn.getOutputStream(), bufferSize);\r\n                    return new HttpFSDataOutputStream(conn, os, expectedStatus, statistics);\r\n                } catch (IOException ex) {\r\n                    HttpExceptionUtils.validateResponse(conn, expectedStatus);\r\n                    throw ex;\r\n                }\r\n            } else {\r\n                HttpExceptionUtils.validateResponse(conn, HTTP_TEMPORARY_REDIRECT);\r\n                throw new IOException(\"Missing HTTP 'Location' header for [\" + conn.getURL() + \"]\");\r\n            }\r\n        } else {\r\n            throw new IOException(MessageFormat.format(\"Expected HTTP status was [307], received [{0}]\", conn.getResponseCode()));\r\n        }\r\n    } catch (IOException ex) {\r\n        if (exceptionAlreadyHandled) {\r\n            throw ex;\r\n        } else {\r\n            HttpExceptionUtils.validateResponse(conn, HTTP_TEMPORARY_REDIRECT);\r\n            throw ex;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.CREATE.toString());\r\n    params.put(OVERWRITE_PARAM, Boolean.toString(overwrite));\r\n    params.put(REPLICATION_PARAM, Short.toString(replication));\r\n    params.put(BLOCKSIZE_PARAM, Long.toString(blockSize));\r\n    params.put(PERMISSION_PARAM, permissionToString(permission));\r\n    return uploadData(Operation.CREATE.getMethod(), f, params, bufferSize, HttpURLConnection.HTTP_CREATED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataOutputStream append(Path f, int bufferSize, Progressable progress) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.APPEND.toString());\r\n    return uploadData(Operation.APPEND.getMethod(), f, params, bufferSize, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "truncate",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean truncate(Path f, long newLength) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.TRUNCATE.toString());\r\n    params.put(NEW_LENGTH_PARAM, Long.toString(newLength));\r\n    HttpURLConnection conn = getConnection(Operation.TRUNCATE.getMethod(), params, f, true);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return (Boolean) json.get(TRUNCATE_JSON);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "concat",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void concat(Path f, Path[] psrcs) throws IOException\n{\r\n    List<String> strPaths = new ArrayList<String>(psrcs.length);\r\n    for (Path psrc : psrcs) {\r\n        strPaths.add(psrc.toUri().getPath());\r\n    }\r\n    String srcs = StringUtils.join(\",\", strPaths);\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.CONCAT.toString());\r\n    params.put(SOURCES_PARAM, srcs);\r\n    HttpURLConnection conn = getConnection(Operation.CONCAT.getMethod(), params, f, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "rename",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean rename(Path src, Path dst) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.RENAME.toString());\r\n    params.put(DESTINATION_PARAM, dst.toString());\r\n    HttpURLConnection conn = getConnection(Operation.RENAME.getMethod(), params, src, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return (Boolean) json.get(RENAME_JSON);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean delete(Path f) throws IOException\n{\r\n    return delete(f, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean delete(Path f, boolean recursive) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.DELETE.toString());\r\n    params.put(RECURSIVE_PARAM, Boolean.toString(recursive));\r\n    HttpURLConnection conn = getConnection(Operation.DELETE.getMethod(), params, f, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return (Boolean) json.get(DELETE_JSON);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "toFileStatuses",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "FileStatus[] toFileStatuses(JSONObject json, Path f)\n{\r\n    json = (JSONObject) json.get(FILE_STATUSES_JSON);\r\n    JSONArray jsonArray = (JSONArray) json.get(FILE_STATUS_JSON);\r\n    FileStatus[] array = new FileStatus[jsonArray.size()];\r\n    f = makeQualified(f);\r\n    for (int i = 0; i < jsonArray.size(); i++) {\r\n        array[i] = createFileStatus(f, (JSONObject) jsonArray.get(i));\r\n    }\r\n    return array;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "listStatus",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "FileStatus[] listStatus(Path f) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.LISTSTATUS.toString());\r\n    HttpURLConnection conn = getConnection(Operation.LISTSTATUS.getMethod(), params, f, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return toFileStatuses(json, f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "listStatusBatch",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "DirectoryEntries listStatusBatch(Path f, byte[] token) throws FileNotFoundException, IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.LISTSTATUS_BATCH.toString());\r\n    if (token != null) {\r\n        params.put(START_AFTER_PARAM, new String(token, Charsets.UTF_8));\r\n    }\r\n    HttpURLConnection conn = getConnection(Operation.LISTSTATUS_BATCH.getMethod(), params, f, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    JSONObject listing = (JSONObject) json.get(DIRECTORY_LISTING_JSON);\r\n    FileStatus[] statuses = toFileStatuses((JSONObject) listing.get(PARTIAL_LISTING_JSON), f);\r\n    byte[] newToken = null;\r\n    if (statuses.length > 0) {\r\n        newToken = statuses[statuses.length - 1].getPath().getName().toString().getBytes(Charsets.UTF_8);\r\n    }\r\n    final long remainingEntries = (Long) listing.get(REMAINING_ENTRIES_JSON);\r\n    final boolean hasMore = remainingEntries > 0 ? true : false;\r\n    return new DirectoryEntries(statuses, newToken, hasMore);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "setWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setWorkingDirectory(Path newDir)\n{\r\n    String result = newDir.toUri().getPath();\r\n    if (!DFSUtilClient.isValidName(result)) {\r\n        throw new IllegalArgumentException(\"Invalid DFS directory name \" + result);\r\n    }\r\n    workingDir = newDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getWorkingDirectory()\n{\r\n    if (workingDir == null) {\r\n        workingDir = getHomeDirectory();\r\n    }\r\n    return workingDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean mkdirs(Path f, FsPermission permission) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.MKDIRS.toString());\r\n    params.put(PERMISSION_PARAM, permissionToString(permission));\r\n    HttpURLConnection conn = getConnection(Operation.MKDIRS.getMethod(), params, f, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return (Boolean) json.get(MKDIRS_JSON);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "FileStatus getFileStatus(Path f) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.GETFILESTATUS.toString());\r\n    HttpURLConnection conn = getConnection(Operation.GETFILESTATUS.getMethod(), params, f, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    HdfsFileStatus status = JsonUtilClient.toFileStatus(json, true);\r\n    return status.makeQualified(getUri(), f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getHomeDirectory",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Path getHomeDirectory()\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.GETHOMEDIRECTORY.toString());\r\n    try {\r\n        HttpURLConnection conn = getConnection(Operation.GETHOMEDIRECTORY.getMethod(), params, new Path(getUri().toString(), \"/\"), false);\r\n        HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n        JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n        return new Path((String) json.get(HOME_DIR_JSON));\r\n    } catch (IOException ex) {\r\n        throw new RuntimeException(ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getTrashRoot",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Path getTrashRoot(Path fullPath)\n{\r\n    Map<String, String> params = new HashMap<>();\r\n    params.put(OP_PARAM, Operation.GETTRASHROOT.toString());\r\n    try {\r\n        HttpURLConnection conn = getConnection(Operation.GETTRASHROOT.getMethod(), params, fullPath, true);\r\n        HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n        JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n        return new Path((String) json.get(TRASH_DIR_JSON));\r\n    } catch (IOException ex) {\r\n        LOG.warn(\"Cannot find trash root of \" + fullPath, ex);\r\n        return super.getTrashRoot(fullPath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "setOwner",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setOwner(Path p, String username, String groupname) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.SETOWNER.toString());\r\n    params.put(OWNER_PARAM, username);\r\n    params.put(GROUP_PARAM, groupname);\r\n    HttpURLConnection conn = getConnection(Operation.SETOWNER.getMethod(), params, p, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "setPermission",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setPermission(Path p, FsPermission permission) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.SETPERMISSION.toString());\r\n    params.put(PERMISSION_PARAM, permissionToString(permission));\r\n    HttpURLConnection conn = getConnection(Operation.SETPERMISSION.getMethod(), params, p, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "setTimes",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setTimes(Path p, long mtime, long atime) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.SETTIMES.toString());\r\n    params.put(MODIFICATION_TIME_PARAM, Long.toString(mtime));\r\n    params.put(ACCESS_TIME_PARAM, Long.toString(atime));\r\n    HttpURLConnection conn = getConnection(Operation.SETTIMES.getMethod(), params, p, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "setReplication",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean setReplication(Path src, short replication) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.SETREPLICATION.toString());\r\n    params.put(REPLICATION_PARAM, Short.toString(replication));\r\n    HttpURLConnection conn = getConnection(Operation.SETREPLICATION.getMethod(), params, src, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return (Boolean) json.get(SET_REPLICATION_JSON);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "modifyAclEntries",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void modifyAclEntries(Path path, List<AclEntry> aclSpec) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.MODIFYACLENTRIES.toString());\r\n    params.put(ACLSPEC_PARAM, AclEntry.aclSpecToString(aclSpec));\r\n    HttpURLConnection conn = getConnection(Operation.MODIFYACLENTRIES.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "removeAclEntries",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeAclEntries(Path path, List<AclEntry> aclSpec) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.REMOVEACLENTRIES.toString());\r\n    params.put(ACLSPEC_PARAM, AclEntry.aclSpecToString(aclSpec));\r\n    HttpURLConnection conn = getConnection(Operation.REMOVEACLENTRIES.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "removeDefaultAcl",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeDefaultAcl(Path path) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.REMOVEDEFAULTACL.toString());\r\n    HttpURLConnection conn = getConnection(Operation.REMOVEDEFAULTACL.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "removeAcl",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeAcl(Path path) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.REMOVEACL.toString());\r\n    HttpURLConnection conn = getConnection(Operation.REMOVEACL.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "setAcl",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setAcl(Path path, List<AclEntry> aclSpec) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.SETACL.toString());\r\n    params.put(ACLSPEC_PARAM, AclEntry.aclSpecToString(aclSpec));\r\n    HttpURLConnection conn = getConnection(Operation.SETACL.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getAclStatus",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "AclStatus getAclStatus(Path path) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.GETACLSTATUS.toString());\r\n    HttpURLConnection conn = getConnection(Operation.GETACLSTATUS.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    json = (JSONObject) json.get(ACL_STATUS_JSON);\r\n    return createAclStatus(json);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "toFsPermission",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FsPermission toFsPermission(JSONObject json)\n{\r\n    final String s = (String) json.get(PERMISSION_JSON);\r\n    return new FsPermission(Short.parseShort(s, 8));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "createFileStatus",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "FileStatus createFileStatus(Path parent, JSONObject json)\n{\r\n    String pathSuffix = (String) json.get(PATH_SUFFIX_JSON);\r\n    Path path = (pathSuffix.equals(\"\")) ? parent : new Path(parent, pathSuffix);\r\n    FILE_TYPE type = FILE_TYPE.valueOf((String) json.get(TYPE_JSON));\r\n    String symLinkValue = type == FILE_TYPE.SYMLINK ? (String) json.get(SYMLINK_JSON) : null;\r\n    Path symLink = symLinkValue == null ? null : new Path(symLinkValue);\r\n    long len = (Long) json.get(LENGTH_JSON);\r\n    String owner = (String) json.get(OWNER_JSON);\r\n    String group = (String) json.get(GROUP_JSON);\r\n    final FsPermission permission = toFsPermission(json);\r\n    long aTime = (Long) json.get(ACCESS_TIME_JSON);\r\n    long mTime = (Long) json.get(MODIFICATION_TIME_JSON);\r\n    long blockSize = (Long) json.get(BLOCK_SIZE_JSON);\r\n    short replication = ((Long) json.get(REPLICATION_JSON)).shortValue();\r\n    final Boolean aclBit = (Boolean) json.get(ACL_BIT_JSON);\r\n    final Boolean encBit = (Boolean) json.get(ENC_BIT_JSON);\r\n    final Boolean erasureBit = (Boolean) json.get(EC_BIT_JSON);\r\n    final Boolean snapshotEnabledBit = (Boolean) json.get(SNAPSHOT_BIT_JSON);\r\n    final boolean aBit = (aclBit != null) ? aclBit : false;\r\n    final boolean eBit = (encBit != null) ? encBit : false;\r\n    final boolean ecBit = (erasureBit != null) ? erasureBit : false;\r\n    final boolean seBit = (snapshotEnabledBit != null) ? snapshotEnabledBit : false;\r\n    if (aBit || eBit || ecBit || seBit) {\r\n        FsPermissionExtension deprecatedPerm = new FsPermissionExtension(permission, aBit, eBit, ecBit);\r\n        FileStatus fileStatus = new FileStatus(len, FILE_TYPE.DIRECTORY == type, replication, blockSize, mTime, aTime, deprecatedPerm, owner, group, symLink, path, FileStatus.attributes(aBit, eBit, ecBit, seBit));\r\n        return fileStatus;\r\n    } else {\r\n        return new FileStatus(len, FILE_TYPE.DIRECTORY == type, replication, blockSize, mTime, aTime, permission, owner, group, symLink, path);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "createAclStatus",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "AclStatus createAclStatus(JSONObject json)\n{\r\n    AclStatus.Builder aclStatusBuilder = new AclStatus.Builder().owner((String) json.get(OWNER_JSON)).group((String) json.get(GROUP_JSON)).stickyBit((Boolean) json.get(ACL_STICKY_BIT_JSON));\r\n    final FsPermission permission = toFsPermission(json);\r\n    aclStatusBuilder.setPermission(permission);\r\n    JSONArray entries = (JSONArray) json.get(ACL_ENTRIES_JSON);\r\n    for (Object e : entries) {\r\n        aclStatusBuilder.addEntry(AclEntry.parseAclEntry(e.toString(), true));\r\n    }\r\n    return aclStatusBuilder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getContentSummary",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "ContentSummary getContentSummary(Path f) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.GETCONTENTSUMMARY.toString());\r\n    HttpURLConnection conn = getConnection(Operation.GETCONTENTSUMMARY.getMethod(), params, f, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) ((JSONObject) HttpFSUtils.jsonParse(conn)).get(CONTENT_SUMMARY_JSON);\r\n    ContentSummary.Builder builder = new ContentSummary.Builder().length((Long) json.get(CONTENT_SUMMARY_LENGTH_JSON)).fileCount((Long) json.get(CONTENT_SUMMARY_FILE_COUNT_JSON)).directoryCount((Long) json.get(CONTENT_SUMMARY_DIRECTORY_COUNT_JSON)).erasureCodingPolicy((String) json.get(CONTENT_SUMMARY_ECPOLICY_JSON));\r\n    builder = buildQuotaUsage(builder, json, ContentSummary.Builder.class);\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getQuotaUsage",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "QuotaUsage getQuotaUsage(Path f) throws IOException\n{\r\n    Map<String, String> params = new HashMap<>();\r\n    params.put(OP_PARAM, Operation.GETQUOTAUSAGE.toString());\r\n    HttpURLConnection conn = getConnection(Operation.GETQUOTAUSAGE.getMethod(), params, f, true);\r\n    JSONObject json = (JSONObject) ((JSONObject) HttpFSUtils.jsonParse(conn)).get(QUOTA_USAGE_JSON);\r\n    QuotaUsage.Builder builder = new QuotaUsage.Builder();\r\n    builder = buildQuotaUsage(builder, json, QuotaUsage.Builder.class);\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "buildQuotaUsage",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "T buildQuotaUsage(T builder, JSONObject json, Class<T> type)\n{\r\n    long quota = (Long) json.get(QUOTA_USAGE_QUOTA_JSON);\r\n    long spaceConsumed = (Long) json.get(QUOTA_USAGE_SPACE_CONSUMED_JSON);\r\n    long spaceQuota = (Long) json.get(QUOTA_USAGE_SPACE_QUOTA_JSON);\r\n    JSONObject typeJson = (JSONObject) json.get(QUOTA_USAGE_TYPE_QUOTA_JSON);\r\n    builder = type.cast(builder.quota(quota).spaceConsumed(spaceConsumed).spaceQuota(spaceQuota));\r\n    if (json.get(QUOTA_USAGE_FILE_AND_DIRECTORY_COUNT_JSON) != null) {\r\n        long fileAndDirectoryCount = (Long) json.get(QUOTA_USAGE_FILE_AND_DIRECTORY_COUNT_JSON);\r\n        builder = type.cast(builder.fileAndDirectoryCount(fileAndDirectoryCount));\r\n    }\r\n    if (typeJson != null) {\r\n        for (StorageType t : StorageType.getTypesSupportingQuota()) {\r\n            JSONObject typeQuota = (JSONObject) typeJson.get(t.toString());\r\n            if (typeQuota != null) {\r\n                builder = type.cast(builder.typeQuota(t, ((Long) typeQuota.get(QUOTA_USAGE_QUOTA_JSON))).typeConsumed(t, ((Long) typeQuota.get(QUOTA_USAGE_CONSUMED_JSON))));\r\n            }\r\n        }\r\n    }\r\n    return builder;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getFileChecksum",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "FileChecksum getFileChecksum(Path f) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.GETFILECHECKSUM.toString());\r\n    HttpURLConnection conn = getConnection(Operation.GETFILECHECKSUM.getMethod(), params, f, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    final JSONObject json = (JSONObject) ((JSONObject) HttpFSUtils.jsonParse(conn)).get(FILE_CHECKSUM_JSON);\r\n    return new FileChecksum() {\r\n\r\n        @Override\r\n        public String getAlgorithmName() {\r\n            return (String) json.get(CHECKSUM_ALGORITHM_JSON);\r\n        }\r\n\r\n        @Override\r\n        public int getLength() {\r\n            return ((Long) json.get(CHECKSUM_LENGTH_JSON)).intValue();\r\n        }\r\n\r\n        @Override\r\n        public byte[] getBytes() {\r\n            return StringUtils.hexStringToByte((String) json.get(CHECKSUM_BYTES_JSON));\r\n        }\r\n\r\n        @Override\r\n        public void write(DataOutput out) throws IOException {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n\r\n        @Override\r\n        public void readFields(DataInput in) throws IOException {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getDelegationToken",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<?> getDelegationToken(final String renewer) throws IOException\n{\r\n    try {\r\n        return UserGroupInformation.getCurrentUser().doAs(new PrivilegedExceptionAction<Token<?>>() {\r\n\r\n            @Override\r\n            public Token<?> run() throws Exception {\r\n                return authURL.getDelegationToken(uri.toURL(), authToken, renewer);\r\n            }\r\n        });\r\n    } catch (Exception ex) {\r\n        if (ex instanceof IOException) {\r\n            throw (IOException) ex;\r\n        } else {\r\n            throw new IOException(ex);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "renewDelegationToken",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long renewDelegationToken(final Token<?> token) throws IOException\n{\r\n    try {\r\n        return UserGroupInformation.getCurrentUser().doAs(new PrivilegedExceptionAction<Long>() {\r\n\r\n            @Override\r\n            public Long run() throws Exception {\r\n                return authURL.renewDelegationToken(uri.toURL(), authToken);\r\n            }\r\n        });\r\n    } catch (Exception ex) {\r\n        if (ex instanceof IOException) {\r\n            throw (IOException) ex;\r\n        } else {\r\n            throw new IOException(ex);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "cancelDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cancelDelegationToken(final Token<?> token) throws IOException\n{\r\n    authURL.cancelDelegationToken(uri.toURL(), authToken);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getRenewToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Token<?> getRenewToken()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "setDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDelegationToken(Token<T> token)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "setXAttr",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setXAttr(Path f, String name, byte[] value, EnumSet<XAttrSetFlag> flag) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.SETXATTR.toString());\r\n    params.put(XATTR_NAME_PARAM, name);\r\n    if (value != null) {\r\n        params.put(XATTR_VALUE_PARAM, XAttrCodec.encodeValue(value, XAttrCodec.HEX));\r\n    }\r\n    params.put(XATTR_SET_FLAG_PARAM, EnumSetParam.toString(flag));\r\n    HttpURLConnection conn = getConnection(Operation.SETXATTR.getMethod(), params, f, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getXAttr",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "byte[] getXAttr(Path f, String name) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.GETXATTRS.toString());\r\n    params.put(XATTR_NAME_PARAM, name);\r\n    HttpURLConnection conn = getConnection(Operation.GETXATTRS.getMethod(), params, f, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    Map<String, byte[]> xAttrs = createXAttrMap((JSONArray) json.get(XATTRS_JSON));\r\n    return xAttrs != null ? xAttrs.get(name) : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "createXAttrMap",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<String, byte[]> createXAttrMap(JSONArray jsonArray) throws IOException\n{\r\n    Map<String, byte[]> xAttrs = Maps.newHashMap();\r\n    for (Object obj : jsonArray) {\r\n        JSONObject jsonObj = (JSONObject) obj;\r\n        final String name = (String) jsonObj.get(XATTR_NAME_JSON);\r\n        final byte[] value = XAttrCodec.decodeValue((String) jsonObj.get(XATTR_VALUE_JSON));\r\n        xAttrs.put(name, value);\r\n    }\r\n    return xAttrs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "createXAttrNames",
  "errType" : [ "ParseException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<String> createXAttrNames(String xattrNamesStr) throws IOException\n{\r\n    JSONParser parser = new JSONParser();\r\n    JSONArray jsonArray;\r\n    try {\r\n        jsonArray = (JSONArray) parser.parse(xattrNamesStr);\r\n        List<String> names = Lists.newArrayListWithCapacity(jsonArray.size());\r\n        for (Object name : jsonArray) {\r\n            names.add((String) name);\r\n        }\r\n        return names;\r\n    } catch (ParseException e) {\r\n        throw new IOException(\"JSON parser error, \" + e.getMessage(), e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(Path f) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.GETXATTRS.toString());\r\n    HttpURLConnection conn = getConnection(Operation.GETXATTRS.getMethod(), params, f, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return createXAttrMap((JSONArray) json.get(XATTRS_JSON));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(Path f, List<String> names) throws IOException\n{\r\n    Preconditions.checkArgument(names != null && !names.isEmpty(), \"XAttr names cannot be null or empty.\");\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.GETXATTRS.toString());\r\n    Map<String, List<String>> multiValuedParams = Maps.newHashMap();\r\n    multiValuedParams.put(XATTR_NAME_PARAM, names);\r\n    HttpURLConnection conn = getConnection(Operation.GETXATTRS.getMethod(), params, multiValuedParams, f, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return createXAttrMap((JSONArray) json.get(XATTRS_JSON));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "listXAttrs",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<String> listXAttrs(Path f) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.LISTXATTRS.toString());\r\n    HttpURLConnection conn = getConnection(Operation.LISTXATTRS.getMethod(), params, f, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return createXAttrNames((String) json.get(XATTRNAMES_JSON));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "removeXAttr",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeXAttr(Path f, String name) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.REMOVEXATTR.toString());\r\n    params.put(XATTR_NAME_PARAM, name);\r\n    HttpURLConnection conn = getConnection(Operation.REMOVEXATTR.getMethod(), params, f, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getAllStoragePolicies",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Collection<BlockStoragePolicy> getAllStoragePolicies() throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.GETALLSTORAGEPOLICY.toString());\r\n    HttpURLConnection conn = getConnection(Operation.GETALLSTORAGEPOLICY.getMethod(), params, new Path(getUri().toString(), \"/\"), false);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return createStoragePolicies((JSONObject) json.get(STORAGE_POLICIES_JSON));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "createStoragePolicies",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Collection<BlockStoragePolicy> createStoragePolicies(JSONObject map) throws IOException\n{\r\n    JSONArray jsonArray = (JSONArray) map.get(STORAGE_POLICY_JSON);\r\n    BlockStoragePolicy[] policies = new BlockStoragePolicy[jsonArray.size()];\r\n    for (int i = 0; i < jsonArray.size(); i++) {\r\n        policies[i] = createStoragePolicy((JSONObject) jsonArray.get(i));\r\n    }\r\n    return Arrays.asList(policies);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "BlockStoragePolicy getStoragePolicy(Path src) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.GETSTORAGEPOLICY.toString());\r\n    HttpURLConnection conn = getConnection(Operation.GETSTORAGEPOLICY.getMethod(), params, src, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return createStoragePolicy((JSONObject) json.get(STORAGE_POLICY_JSON));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "createStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "BlockStoragePolicy createStoragePolicy(JSONObject policyJson) throws IOException\n{\r\n    byte id = ((Number) policyJson.get(\"id\")).byteValue();\r\n    String name = (String) policyJson.get(\"name\");\r\n    StorageType[] storageTypes = toStorageTypes((JSONArray) policyJson.get(\"storageTypes\"));\r\n    StorageType[] creationFallbacks = toStorageTypes((JSONArray) policyJson.get(\"creationFallbacks\"));\r\n    StorageType[] replicationFallbacks = toStorageTypes((JSONArray) policyJson.get(\"replicationFallbacks\"));\r\n    Boolean copyOnCreateFile = (Boolean) policyJson.get(\"copyOnCreateFile\");\r\n    return new BlockStoragePolicy(id, name, storageTypes, creationFallbacks, replicationFallbacks, copyOnCreateFile.booleanValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "toStorageTypes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "StorageType[] toStorageTypes(JSONArray array) throws IOException\n{\r\n    if (array == null) {\r\n        return null;\r\n    } else {\r\n        List<StorageType> storageTypes = new ArrayList<StorageType>(array.size());\r\n        for (Object name : array) {\r\n            storageTypes.add(StorageType.parseStorageType((String) name));\r\n        }\r\n        return storageTypes.toArray(new StorageType[storageTypes.size()]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "setStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setStoragePolicy(Path src, String policyName) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.SETSTORAGEPOLICY.toString());\r\n    params.put(POLICY_NAME_PARAM, policyName);\r\n    HttpURLConnection conn = getConnection(Operation.SETSTORAGEPOLICY.getMethod(), params, src, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "unsetStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void unsetStoragePolicy(Path src) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.UNSETSTORAGEPOLICY.toString());\r\n    HttpURLConnection conn = getConnection(Operation.UNSETSTORAGEPOLICY.getMethod(), params, src, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "allowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void allowSnapshot(Path path) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.ALLOWSNAPSHOT.toString());\r\n    HttpURLConnection conn = getConnection(Operation.ALLOWSNAPSHOT.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "disallowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void disallowSnapshot(Path path) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.DISALLOWSNAPSHOT.toString());\r\n    HttpURLConnection conn = getConnection(Operation.DISALLOWSNAPSHOT.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "createSnapshot",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Path createSnapshot(Path path, String snapshotName) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.CREATESNAPSHOT.toString());\r\n    if (snapshotName != null) {\r\n        params.put(SNAPSHOT_NAME_PARAM, snapshotName);\r\n    }\r\n    HttpURLConnection conn = getConnection(Operation.CREATESNAPSHOT.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return new Path((String) json.get(SNAPSHOT_JSON));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "renameSnapshot",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void renameSnapshot(Path path, String snapshotOldName, String snapshotNewName) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.RENAMESNAPSHOT.toString());\r\n    params.put(SNAPSHOT_NAME_PARAM, snapshotNewName);\r\n    params.put(OLD_SNAPSHOT_NAME_PARAM, snapshotOldName);\r\n    HttpURLConnection conn = getConnection(Operation.RENAMESNAPSHOT.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "deleteSnapshot",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void deleteSnapshot(Path path, String snapshotName) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.DELETESNAPSHOT.toString());\r\n    params.put(SNAPSHOT_NAME_PARAM, snapshotName);\r\n    HttpURLConnection conn = getConnection(Operation.DELETESNAPSHOT.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getSnapshotDiffReport",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "SnapshotDiffReport getSnapshotDiffReport(Path path, String snapshotOldName, String snapshotNewName) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.GETSNAPSHOTDIFF.toString());\r\n    params.put(SNAPSHOT_NAME_PARAM, snapshotNewName);\r\n    params.put(OLD_SNAPSHOT_NAME_PARAM, snapshotOldName);\r\n    HttpURLConnection conn = getConnection(Operation.GETSNAPSHOTDIFF.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return JsonUtilClient.toSnapshotDiffReport(json);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getSnapshotDiffReportListing",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "SnapshotDiffReportListing getSnapshotDiffReportListing(Path path, String snapshotOldName, String snapshotNewName, byte[] snapshotDiffStartPath, Integer snapshotDiffIndex) throws IOException\n{\r\n    Map<String, String> params = new HashMap<>();\r\n    params.put(OP_PARAM, Operation.GETSNAPSHOTDIFFLISTING.toString());\r\n    params.put(SNAPSHOT_NAME_PARAM, snapshotNewName);\r\n    params.put(OLD_SNAPSHOT_NAME_PARAM, snapshotOldName);\r\n    params.put(SNAPSHOT_DIFF_START_PATH, DFSUtilClient.bytes2String(snapshotDiffStartPath));\r\n    params.put(SNAPSHOT_DIFF_INDEX, snapshotDiffIndex.toString());\r\n    HttpURLConnection conn = getConnection(Operation.GETSNAPSHOTDIFFLISTING.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return JsonUtilClient.toSnapshotDiffReportListing(json);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getSnapshottableDirectoryList",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "SnapshottableDirectoryStatus[] getSnapshottableDirectoryList() throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.GETSNAPSHOTTABLEDIRECTORYLIST.toString());\r\n    HttpURLConnection conn = getConnection(Operation.GETSNAPSHOTTABLEDIRECTORYLIST.getMethod(), params, new Path(getUri().toString(), \"/\"), true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return JsonUtilClient.toSnapshottableDirectoryList(json);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getSnapshotListing",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "SnapshotStatus[] getSnapshotListing(Path snapshotRoot) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.GETSNAPSHOTLIST.toString());\r\n    HttpURLConnection conn = getConnection(Operation.GETSNAPSHOTLIST.getMethod(), params, snapshotRoot, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return JsonUtilClient.toSnapshotList(json);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "hasPathCapability",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean hasPathCapability(final Path path, final String capability) throws IOException\n{\r\n    final Path p = makeQualified(path);\r\n    switch(validatePathCapabilityArgs(p, capability)) {\r\n        case CommonPathCapabilities.FS_ACLS:\r\n        case CommonPathCapabilities.FS_APPEND:\r\n        case CommonPathCapabilities.FS_CONCAT:\r\n        case CommonPathCapabilities.FS_PERMISSIONS:\r\n        case CommonPathCapabilities.FS_SNAPSHOTS:\r\n        case CommonPathCapabilities.FS_STORAGEPOLICY:\r\n        case CommonPathCapabilities.FS_XATTRS:\r\n            return true;\r\n        case CommonPathCapabilities.FS_SYMLINKS:\r\n            return false;\r\n        default:\r\n            return super.hasPathCapability(p, capability);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "FsServerDefaults getServerDefaults() throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.GETSERVERDEFAULTS.toString());\r\n    HttpURLConnection conn = getConnection(Operation.GETSERVERDEFAULTS.getMethod(), params, new Path(getUri().toString(), \"/\"), true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return JsonUtilClient.toFsServerDefaults(json);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FsServerDefaults getServerDefaults(Path p) throws IOException\n{\r\n    return getServerDefaults();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "access",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void access(final Path path, final FsAction mode) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.CHECKACCESS.toString());\r\n    params.put(FSACTION_MODE_PARAM, mode.SYMBOL);\r\n    HttpURLConnection conn = getConnection(Operation.CHECKACCESS.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "setErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setErasureCodingPolicy(final Path path, String policyName) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.SETECPOLICY.toString());\r\n    params.put(EC_POLICY_NAME_PARAM, policyName);\r\n    HttpURLConnection conn = getConnection(Operation.SETECPOLICY.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ErasureCodingPolicy getErasureCodingPolicy(final Path path) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.GETECPOLICY.toString());\r\n    HttpURLConnection conn = getConnection(Operation.GETECPOLICY.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n    JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\r\n    return JsonUtilClient.toECPolicy(json);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "unsetErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void unsetErasureCodingPolicy(final Path path) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.UNSETECPOLICY.toString());\r\n    HttpURLConnection conn = getConnection(Operation.UNSETECPOLICY.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "satisfyStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void satisfyStoragePolicy(final Path path) throws IOException\n{\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(OP_PARAM, Operation.SATISFYSTORAGEPOLICY.toString());\r\n    HttpURLConnection conn = getConnection(Operation.SATISFYSTORAGEPOLICY.getMethod(), params, path, true);\r\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "isWriteable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isWriteable(Class<?> aClass, Type type, Annotation[] annotations, MediaType mediaType)\n{\r\n    return Map.class.isAssignableFrom(aClass);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSize(Map map, Class<?> aClass, Type type, Annotation[] annotations, MediaType mediaType)\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "writeTo",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writeTo(Map map, Class<?> aClass, Type type, Annotation[] annotations, MediaType mediaType, MultivaluedMap<String, Object> stringObjectMultivaluedMap, OutputStream outputStream) throws IOException, WebApplicationException\n{\r\n    Writer writer = new OutputStreamWriter(outputStream, StandardCharsets.UTF_8);\r\n    JSONObject.writeJSONString(map, writer);\r\n    writer.write(ENTER);\r\n    writer.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "parseParam",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String parseParam(String str)\n{\r\n    try {\r\n        if (str != null) {\r\n            str = str.trim();\r\n            if (str.length() > 0) {\r\n                value = parse(str);\r\n            }\r\n        }\r\n    } catch (Exception ex) {\r\n        throw new IllegalArgumentException(MessageFormat.format(\"Parameter [{0}], invalid value [{1}], value must be [{2}]\", getName(), str, getDomain()));\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String parse(String str) throws Exception\n{\r\n    if (pattern != null) {\r\n        if (!pattern.matcher(str).matches()) {\r\n            throw new IllegalArgumentException(\"Invalid value\");\r\n        }\r\n    }\r\n    return str;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getDomain",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDomain()\n{\r\n    return (pattern == null) ? \"a string\" : pattern.pattern();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Integer parse(String str) throws Exception\n{\r\n    return Integer.parseInt(str);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getDomain",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDomain()\n{\r\n    return \"an integer\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getValue",
  "errType" : [ "IllegalArgumentException", "Exception" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "Parameters getValue(HttpContext httpContext)\n{\r\n    Map<String, List<Param<?>>> map = new HashMap<String, List<Param<?>>>();\r\n    Map<String, List<String>> queryString = httpContext.getRequest().getQueryParameters();\r\n    String str = ((MultivaluedMap<String, String>) queryString).getFirst(driverParam);\r\n    if (str == null) {\r\n        throw new IllegalArgumentException(MessageFormat.format(\"Missing Operation parameter [{0}]\", driverParam));\r\n    }\r\n    Enum op;\r\n    try {\r\n        op = Enum.valueOf(enumClass, StringUtils.toUpperCase(str));\r\n    } catch (IllegalArgumentException ex) {\r\n        throw new IllegalArgumentException(MessageFormat.format(\"Invalid Operation [{0}]\", str));\r\n    }\r\n    if (!paramsDef.containsKey(op)) {\r\n        throw new IllegalArgumentException(MessageFormat.format(\"Unsupported Operation [{0}]\", op));\r\n    }\r\n    for (Class<Param<?>> paramClass : paramsDef.get(op)) {\r\n        Param<?> param = newParam(paramClass);\r\n        List<Param<?>> paramList = Lists.newArrayList();\r\n        List<String> ps = queryString.get(param.getName());\r\n        if (ps != null) {\r\n            for (String p : ps) {\r\n                try {\r\n                    param.parseParam(p);\r\n                } catch (Exception ex) {\r\n                    throw new IllegalArgumentException(ex.toString(), ex);\r\n                }\r\n                paramList.add(param);\r\n                param = newParam(paramClass);\r\n            }\r\n        } else {\r\n            paramList.add(param);\r\n        }\r\n        map.put(param.getName(), paramList);\r\n    }\r\n    return new Parameters(map);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "newParam",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Param<?> newParam(Class<Param<?>> paramClass)\n{\r\n    try {\r\n        return paramClass.newInstance();\r\n    } catch (Exception ex) {\r\n        throw new UnsupportedOperationException(MessageFormat.format(\"Param class [{0}] does not have default constructor\", paramClass.getName()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getScope",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ComponentScope getScope()\n{\r\n    return ComponentScope.PerRequest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getInjectable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Injectable getInjectable(ComponentContext componentContext, Context context, Type type)\n{\r\n    return (type.equals(Parameters.class)) ? this : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void init(FilterConfig config) throws ServletException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "doFilter",
  "errType" : [ "UnknownHostException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException\n{\r\n    try {\r\n        String hostname;\r\n        try {\r\n            String address = request.getRemoteAddr();\r\n            if (address != null) {\r\n                hostname = InetAddress.getByName(address).getCanonicalHostName();\r\n            } else {\r\n                log.warn(\"Request remote address is NULL\");\r\n                hostname = \"???\";\r\n            }\r\n        } catch (UnknownHostException ex) {\r\n            log.warn(\"Request remote address could not be resolved, {0}\", ex.toString(), ex);\r\n            hostname = \"???\";\r\n        }\r\n        HOSTNAME_TL.set(hostname);\r\n        chain.doFilter(request, response);\r\n    } finally {\r\n        HOSTNAME_TL.remove();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String get()\n{\r\n    return HOSTNAME_TL.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void destroy()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\security",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void init() throws ServiceException\n{\r\n    Configuration hConf = new Configuration(false);\r\n    ConfigurationUtils.copy(getServiceConfig(), hConf);\r\n    hGroups = new org.apache.hadoop.security.Groups(hConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\security",
  "methodName" : "getInterface",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class getInterface()\n{\r\n    return Groups.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\security",
  "methodName" : "getGroups",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> getGroups(String user) throws IOException\n{\r\n    return hGroups.getGroups(user);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\service\\security",
  "methodName" : "getGroupsSet",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<String> getGroupsSet(String user) throws IOException\n{\r\n    return hGroups.getGroupsSet(user);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "notNull",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T notNull(T obj, String name)\n{\r\n    if (obj == null) {\r\n        throw new IllegalArgumentException(name + \" cannot be null\");\r\n    }\r\n    return obj;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "notNullElements",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<T> notNullElements(List<T> list, String name)\n{\r\n    notNull(list, name);\r\n    for (int i = 0; i < list.size(); i++) {\r\n        notNull(list.get(i), MessageFormat.format(\"list [{0}] element [{1}]\", name, i));\r\n    }\r\n    return list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "notEmpty",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String notEmpty(String str, String name)\n{\r\n    if (str == null) {\r\n        throw new IllegalArgumentException(name + \" cannot be null\");\r\n    }\r\n    if (str.length() == 0) {\r\n        throw new IllegalArgumentException(name + \" cannot be empty\");\r\n    }\r\n    return str;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "notEmptyElements",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<String> notEmptyElements(List<String> list, String name)\n{\r\n    notNull(list, name);\r\n    for (int i = 0; i < list.size(); i++) {\r\n        notEmpty(list.get(i), MessageFormat.format(\"list [{0}] element [{1}]\", name, i));\r\n    }\r\n    return list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "validIdentifier",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String validIdentifier(String value, int maxLen, String name)\n{\r\n    Check.notEmpty(value, name);\r\n    if (value.length() > maxLen) {\r\n        throw new IllegalArgumentException(MessageFormat.format(\"[{0}] = [{1}] exceeds max len [{2}]\", name, value, maxLen));\r\n    }\r\n    if (!IDENTIFIER_PATTERN.matcher(value).find()) {\r\n        throw new IllegalArgumentException(MessageFormat.format(\"[{0}] = [{1}] must be '{2}'\", name, value, IDENTIFIER_PATTERN_STR));\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "gt0",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int gt0(int value, String name)\n{\r\n    return (int) gt0((long) value, name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "gt0",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long gt0(long value, String name)\n{\r\n    if (value <= 0) {\r\n        throw new IllegalArgumentException(MessageFormat.format(\"parameter [{0}] = [{1}] must be greater than zero\", name, value));\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "ge0",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int ge0(int value, String name)\n{\r\n    return (int) ge0((long) value, name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "ge0",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long ge0(long value, String name)\n{\r\n    if (value < 0) {\r\n        throw new IllegalArgumentException(MessageFormat.format(\"parameter [{0}] = [{1}] must be greater than or equals zero\", name, value));\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "isWriteable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isWriteable(Class<?> aClass, Type type, Annotation[] annotations, MediaType mediaType)\n{\r\n    return JSONStreamAware.class.isAssignableFrom(aClass);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "getSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSize(JSONStreamAware jsonStreamAware, Class<?> aClass, Type type, Annotation[] annotations, MediaType mediaType)\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\main\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "writeTo",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writeTo(JSONStreamAware jsonStreamAware, Class<?> aClass, Type type, Annotation[] annotations, MediaType mediaType, MultivaluedMap<String, Object> stringObjectMultivaluedMap, OutputStream outputStream) throws IOException, WebApplicationException\n{\r\n    Writer writer = new OutputStreamWriter(outputStream, StandardCharsets.UTF_8);\r\n    jsonStreamAware.writeJSONString(writer);\r\n    writer.write(ENTER);\r\n    writer.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
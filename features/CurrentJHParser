[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "canParse",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean canParse(InputStream input) throws IOException\n{\r\n    final DataInputStream in = new ForkedDataInputStream(input);\r\n    try {\r\n        final EventReader reader = new EventReader(in);\r\n        try {\r\n            reader.getNextEvent();\r\n        } catch (IOException e) {\r\n            return false;\r\n        } finally {\r\n            reader.close();\r\n        }\r\n    } catch (IOException e) {\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "nextEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HistoryEvent nextEvent() throws IOException\n{\r\n    return reader.getNextEvent();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    reader.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    return super.equals(obj);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return super.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMemory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMemory()\n{\r\n    return memory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMapSlots",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMapSlots()\n{\r\n    return mapSlots;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getReduceSlots",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReduceSlots()\n{\r\n    return reduceSlots;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMemoryPerMapSlot",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMemoryPerMapSlot()\n{\r\n    return memoryPerMapSlot;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMemoryPerReduceSlot",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMemoryPerReduceSlot()\n{\r\n    return memoryPerReduceSlot;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getNumCores",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumCores()\n{\r\n    return numCores;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getRackNode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RackNode getRackNode()\n{\r\n    return (RackNode) getParent();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "addChild",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean addChild(Node child)\n{\r\n    throw new IllegalStateException(\"Cannot add child to MachineNode\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getValue()\n{\r\n    return userName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getPrefix",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPrefix()\n{\r\n    return \"user\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "parseDuration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long parseDuration(String durationString)\n{\r\n    String numeral = durationString.substring(0, durationString.length() - 1);\r\n    char durationCode = durationString.charAt(durationString.length() - 1);\r\n    long result = Integer.parseInt(numeral);\r\n    if (result <= 0) {\r\n        throw new IllegalArgumentException(\"Negative durations are not allowed\");\r\n    }\r\n    switch(durationCode) {\r\n        case 'D':\r\n        case 'd':\r\n            return 24L * 60L * 60L * TICKS_PER_SECOND * result;\r\n        case 'H':\r\n        case 'h':\r\n            return 60L * 60L * TICKS_PER_SECOND * result;\r\n        case 'M':\r\n        case 'm':\r\n            return 60L * TICKS_PER_SECOND * result;\r\n        case 'S':\r\n        case 's':\r\n            return TICKS_PER_SECOND * result;\r\n        default:\r\n            throw new IllegalArgumentException(\"Missing or invalid duration code\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "initialize",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 26,
  "sourceCodeText" : "int initialize(String[] args) throws IllegalArgumentException\n{\r\n    String tempDirName = null;\r\n    String inputPathName = null;\r\n    String outputPathName = null;\r\n    for (int i = 0; i < args.length; ++i) {\r\n        String thisArg = args[i];\r\n        if (thisArg.equalsIgnoreCase(\"-starts-after\")) {\r\n            startsAfter = parseDuration(args[++i]);\r\n        } else if (thisArg.equalsIgnoreCase(\"-output-duration\")) {\r\n            outputDuration = parseDuration(args[++i]);\r\n        } else if (thisArg.equalsIgnoreCase(\"-input-cycle\")) {\r\n            inputCycle = parseDuration(args[++i]);\r\n        } else if (thisArg.equalsIgnoreCase(\"-concentration\")) {\r\n            concentration = Double.parseDouble(args[++i]);\r\n        } else if (thisArg.equalsIgnoreCase(\"-debug\")) {\r\n            debug = true;\r\n        } else if (thisArg.equalsIgnoreCase(\"-allow-missorting\")) {\r\n            allowMissorting = true;\r\n        } else if (thisArg.equalsIgnoreCase(\"-seed\")) {\r\n            seeded = true;\r\n            randomSeed = Long.parseLong(args[++i]);\r\n        } else if (thisArg.equalsIgnoreCase(\"-skew-buffer-length\")) {\r\n            skewBufferLength = Integer.parseInt(args[++i]);\r\n        } else if (thisArg.equalsIgnoreCase(\"-temp-directory\")) {\r\n            tempDirName = args[++i];\r\n        } else if (thisArg.equals(\"\") || thisArg.startsWith(\"-\")) {\r\n            throw new IllegalArgumentException(\"Illegal switch argument, \" + thisArg + \" at position \" + i);\r\n        } else {\r\n            inputPathName = thisArg;\r\n            outputPathName = args[++i];\r\n            if (i != args.length - 1) {\r\n                throw new IllegalArgumentException(\"Too many non-switch arguments\");\r\n            }\r\n        }\r\n    }\r\n    try {\r\n        Configuration conf = getConf();\r\n        Path inPath = new Path(inputPathName);\r\n        reader = new DeskewedJobTraceReader(new JobTraceReader(inPath, conf), skewBufferLength, !allowMissorting);\r\n        Path outPath = new Path(outputPathName);\r\n        outGen = new DefaultOutputter<LoggedJob>();\r\n        outGen.init(outPath, conf);\r\n        tempDir = tempDirName == null ? outPath.getParent() : new Path(tempDirName);\r\n        FileSystem fs = tempDir.getFileSystem(getConf());\r\n        if (!fs.getFileStatus(tempDir).isDirectory()) {\r\n            throw new IOException(\"Your temp directory is not a directory\");\r\n        }\r\n        if (inputCycle <= 0) {\r\n            LOG.error(\"You must have an input cycle length.\");\r\n            return NO_INPUT_CYCLE_LENGTH;\r\n        }\r\n        if (outputDuration <= 0) {\r\n            outputDuration = 60L * 60L * TICKS_PER_SECOND;\r\n        }\r\n        if (inputCycle <= 0) {\r\n            inputCycle = outputDuration;\r\n        }\r\n        timeDilation = (double) outputDuration / (double) inputCycle;\r\n        random = seeded ? new Random(randomSeed) : new Random();\r\n        if (debug) {\r\n            randomSeed = random.nextLong();\r\n            LOG.warn(\"This run effectively has a -seed of \" + randomSeed);\r\n            random = new Random(randomSeed);\r\n            seeded = true;\r\n        }\r\n    } catch (IOException e) {\r\n        e.printStackTrace(System.err);\r\n        return NON_EXISTENT_FILES;\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int run(String[] args) throws IOException\n{\r\n    int result = initialize(args);\r\n    if (result != 0) {\r\n        return result;\r\n    }\r\n    return run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "run",
  "errType" : [ "IOException", "DeskewedJobTraceReader.OutOfOrderException", "IOException", "IOException" ],
  "containingMethodsNum" : 69,
  "sourceCodeText" : "int run() throws IOException\n{\r\n    class JobEntryComparator implements Comparator<Pair<LoggedJob, JobTraceReader>> {\r\n\r\n        public int compare(Pair<LoggedJob, JobTraceReader> p1, Pair<LoggedJob, JobTraceReader> p2) {\r\n            LoggedJob j1 = p1.first();\r\n            LoggedJob j2 = p2.first();\r\n            return (j1.getSubmitTime() < j2.getSubmitTime()) ? -1 : (j1.getSubmitTime() == j2.getSubmitTime()) ? 0 : 1;\r\n        }\r\n    }\r\n    Queue<Pair<LoggedJob, JobTraceReader>> heap = new PriorityQueue<Pair<LoggedJob, JobTraceReader>>();\r\n    try {\r\n        LoggedJob job = reader.nextJob();\r\n        if (job == null) {\r\n            LOG.error(\"The job trace is empty\");\r\n            return EMPTY_JOB_TRACE;\r\n        }\r\n        if (startsAfter > 0) {\r\n            LOG.info(\"starts-after time is specified. Initial job submit time : \" + job.getSubmitTime());\r\n            long approximateTime = job.getSubmitTime() + startsAfter;\r\n            job = reader.nextJob();\r\n            long skippedCount = 0;\r\n            while (job != null && job.getSubmitTime() < approximateTime) {\r\n                job = reader.nextJob();\r\n                skippedCount++;\r\n            }\r\n            LOG.debug(\"Considering jobs with submit time greater than \" + startsAfter + \" ms. Skipped \" + skippedCount + \" jobs.\");\r\n            if (job == null) {\r\n                LOG.error(\"No more jobs to process in the trace with 'starts-after'\" + \" set to \" + startsAfter + \"ms.\");\r\n                return EMPTY_JOB_TRACE;\r\n            }\r\n            LOG.info(\"The first job has a submit time of \" + job.getSubmitTime());\r\n        }\r\n        firstJobSubmitTime = job.getSubmitTime();\r\n        long lastJobSubmitTime = firstJobSubmitTime;\r\n        int numberJobs = 0;\r\n        long currentIntervalEnd = Long.MIN_VALUE;\r\n        Path nextSegment = null;\r\n        Outputter<LoggedJob> tempGen = null;\r\n        if (debug) {\r\n            LOG.debug(\"The first job has a submit time of \" + firstJobSubmitTime);\r\n        }\r\n        final Configuration conf = getConf();\r\n        try {\r\n            while (job != null) {\r\n                final Random tempNameGenerator = new Random();\r\n                lastJobSubmitTime = job.getSubmitTime();\r\n                ++numberJobs;\r\n                if (job.getSubmitTime() >= currentIntervalEnd) {\r\n                    if (tempGen != null) {\r\n                        tempGen.close();\r\n                    }\r\n                    nextSegment = null;\r\n                    for (int i = 0; i < 3 && nextSegment == null; ++i) {\r\n                        try {\r\n                            nextSegment = new Path(tempDir, \"segment-\" + tempNameGenerator.nextLong() + \".json.gz\");\r\n                            if (debug) {\r\n                                LOG.debug(\"The next segment name is \" + nextSegment);\r\n                            }\r\n                            FileSystem fs = nextSegment.getFileSystem(conf);\r\n                            try {\r\n                                if (!fs.exists(nextSegment)) {\r\n                                    break;\r\n                                }\r\n                                continue;\r\n                            } catch (IOException e) {\r\n                            }\r\n                        } catch (IOException e) {\r\n                        }\r\n                    }\r\n                    if (nextSegment == null) {\r\n                        throw new RuntimeException(\"Failed to create a new file!\");\r\n                    }\r\n                    if (debug) {\r\n                        LOG.debug(\"Creating \" + nextSegment + \" for a job with a submit time of \" + job.getSubmitTime());\r\n                    }\r\n                    deletees.add(nextSegment);\r\n                    tempPaths.add(nextSegment);\r\n                    tempGen = new DefaultOutputter<LoggedJob>();\r\n                    tempGen.init(nextSegment, conf);\r\n                    long currentIntervalNumber = (job.getSubmitTime() - firstJobSubmitTime) / inputCycle;\r\n                    currentIntervalEnd = firstJobSubmitTime + ((currentIntervalNumber + 1) * inputCycle);\r\n                }\r\n                if (tempGen != null) {\r\n                    tempGen.output(job);\r\n                }\r\n                job = reader.nextJob();\r\n            }\r\n        } catch (DeskewedJobTraceReader.OutOfOrderException e) {\r\n            return OUT_OF_ORDER_JOBS;\r\n        } finally {\r\n            if (tempGen != null) {\r\n                tempGen.close();\r\n            }\r\n        }\r\n        if (lastJobSubmitTime <= firstJobSubmitTime) {\r\n            LOG.error(\"All of your job[s] have the same submit time.\" + \"  Please just use your input file.\");\r\n            return ALL_JOBS_SIMULTANEOUS;\r\n        }\r\n        double submitTimeSpan = lastJobSubmitTime - firstJobSubmitTime;\r\n        LOG.warn(\"Your input trace spans \" + (lastJobSubmitTime - firstJobSubmitTime) + \" ticks.\");\r\n        double foldingRatio = submitTimeSpan * (numberJobs + 1) / numberJobs / inputCycle;\r\n        if (debug) {\r\n            LOG.warn(\"run: submitTimeSpan = \" + submitTimeSpan + \", numberJobs = \" + numberJobs + \", inputCycle = \" + inputCycle);\r\n        }\r\n        if (reader.neededSkewBufferSize() > 0) {\r\n            LOG.warn(\"You needed a -skew-buffer-length of \" + reader.neededSkewBufferSize() + \" but no more, for this input.\");\r\n        }\r\n        double tProbability = timeDilation * concentration / foldingRatio;\r\n        if (debug) {\r\n            LOG.warn(\"run: timeDilation = \" + timeDilation + \", concentration = \" + concentration + \", foldingRatio = \" + foldingRatio);\r\n            LOG.warn(\"The transcription probability is \" + tProbability);\r\n        }\r\n        transcriptionRateInteger = (int) Math.floor(tProbability);\r\n        transcriptionRateFraction = tProbability - Math.floor(tProbability);\r\n        heap = new PriorityQueue<Pair<LoggedJob, JobTraceReader>>(tempPaths.size(), new JobEntryComparator());\r\n        for (Path tempPath : tempPaths) {\r\n            JobTraceReader thisReader = new JobTraceReader(tempPath, conf);\r\n            closees.add(thisReader);\r\n            LoggedJob streamFirstJob = thisReader.getNext();\r\n            long thisIndex = (streamFirstJob.getSubmitTime() - firstJobSubmitTime) / inputCycle;\r\n            if (debug) {\r\n                LOG.debug(\"A job with submit time of \" + streamFirstJob.getSubmitTime() + \" is in interval # \" + thisIndex);\r\n            }\r\n            adjustJobTimes(streamFirstJob);\r\n            if (debug) {\r\n                LOG.debug(\"That job's submit time is adjusted to \" + streamFirstJob.getSubmitTime());\r\n            }\r\n            heap.add(new Pair<LoggedJob, JobTraceReader>(streamFirstJob, thisReader));\r\n        }\r\n        Pair<LoggedJob, JobTraceReader> next = heap.poll();\r\n        while (next != null) {\r\n            maybeOutput(next.first());\r\n            if (debug) {\r\n                LOG.debug(\"The most recent job has an adjusted submit time of \" + next.first().getSubmitTime());\r\n                LOG.debug(\" Its replacement in the heap will come from input engine \" + next.second());\r\n            }\r\n            LoggedJob replacement = next.second().getNext();\r\n            if (replacement == null) {\r\n                next.second().close();\r\n                if (debug) {\r\n                    LOG.debug(\"That input engine is depleted.\");\r\n                }\r\n            } else {\r\n                adjustJobTimes(replacement);\r\n                if (debug) {\r\n                    LOG.debug(\"The replacement has an adjusted submit time of \" + replacement.getSubmitTime());\r\n                }\r\n                heap.add(new Pair<LoggedJob, JobTraceReader>(replacement, next.second()));\r\n            }\r\n            next = heap.poll();\r\n        }\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(null, reader);\r\n        if (outGen != null) {\r\n            outGen.close();\r\n        }\r\n        for (Pair<LoggedJob, JobTraceReader> heapEntry : heap) {\r\n            heapEntry.second().close();\r\n        }\r\n        for (Closeable closee : closees) {\r\n            closee.close();\r\n        }\r\n        if (!debug) {\r\n            Configuration conf = getConf();\r\n            for (Path deletee : deletees) {\r\n                FileSystem fs = deletee.getFileSystem(conf);\r\n                try {\r\n                    fs.delete(deletee, false);\r\n                } catch (IOException e) {\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 5,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "maybeOutput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void maybeOutput(LoggedJob job) throws IOException\n{\r\n    for (int i = 0; i < transcriptionRateInteger; ++i) {\r\n        outGen.output(job);\r\n    }\r\n    if (random.nextDouble() < transcriptionRateFraction) {\r\n        outGen.output(job);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "adjustJobTimes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void adjustJobTimes(LoggedJob adjustee)\n{\r\n    long offsetInCycle = (adjustee.getSubmitTime() - firstJobSubmitTime) % inputCycle;\r\n    long outputOffset = (long) ((double) offsetInCycle * timeDilation);\r\n    long adjustment = firstJobSubmitTime + outputOffset - adjustee.getSubmitTime();\r\n    adjustee.adjustTimes(adjustment);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "main",
  "errType" : [ "IOException", "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    Folder instance = new Folder();\r\n    int result = 0;\r\n    try {\r\n        result = ToolRunner.run(instance, args);\r\n    } catch (IOException e) {\r\n        e.printStackTrace(System.err);\r\n        System.exit(IO_ERROR);\r\n    } catch (Exception e) {\r\n        e.printStackTrace(System.err);\r\n        System.exit(OTHER_ERROR);\r\n    }\r\n    if (result != 0) {\r\n        System.exit(result);\r\n    }\r\n    return;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getInputBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getInputBytes()\n{\r\n    return bytesIn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getInputRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getInputRecords()\n{\r\n    return recsIn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getOutputBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getOutputBytes()\n{\r\n    return bytesOut;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getOutputRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getOutputRecords()\n{\r\n    return recsOut;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getTaskMemory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTaskMemory()\n{\r\n    return maxMemory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getTaskVCores",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTaskVCores()\n{\r\n    return maxVcores;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getResourceUsageMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ResourceUsageMetrics getResourceUsageMetrics()\n{\r\n    return metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(T object) throws IOException\n{\r\n    writer.writeObject(object);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    writer.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "first",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CarType first()\n{\r\n    return car;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "second",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CdrType second()\n{\r\n    return cdr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "incorporateCounters",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void incorporateCounters(JhCounters counters)\n{\r\n    Map<String, Long> countersMap = JobHistoryUtils.extractCounters(counters);\r\n    putCounters(countersMap);\r\n    super.incorporateCounters(counters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "putCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void putCounters(Map<String, Long> counters)\n{\r\n    this.countersMap = counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, Long> obtainCounters()\n{\r\n    return countersMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "putDiagnosticInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void putDiagnosticInfo(String msg)\n{\r\n    diagnosticInfo = msg;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainDiagnosticInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String obtainDiagnosticInfo()\n{\r\n    return diagnosticInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "putFailedDueToAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void putFailedDueToAttemptId(String attempt)\n{\r\n    failedDueToAttempt = attempt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainFailedDueToAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String obtainFailedDueToAttemptId()\n{\r\n    return failedDueToAttempt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<ParsedTaskAttempt> obtainTaskAttempts()\n{\r\n    List<LoggedTaskAttempt> attempts = getAttempts();\r\n    return convertTaskAttempts(attempts);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "convertTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<ParsedTaskAttempt> convertTaskAttempts(List<LoggedTaskAttempt> attempts)\n{\r\n    List<ParsedTaskAttempt> result = new ArrayList<ParsedTaskAttempt>();\r\n    for (LoggedTaskAttempt t : attempts) {\r\n        if (t instanceof ParsedTaskAttempt) {\r\n            result.add((ParsedTaskAttempt) t);\r\n        } else {\r\n            throw new RuntimeException(\"Unexpected type of taskAttempts in the list...\");\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "dumpParsedTask",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void dumpParsedTask()\n{\r\n    LOG.info(\"ParsedTask details:\" + obtainCounters() + \"\\n\" + obtainFailedDueToAttemptId() + \"\\nPreferred Locations are:\");\r\n    List<LoggedLocation> loc = getPreferredLocations();\r\n    for (LoggedLocation l : loc) {\r\n        LOG.info(l.getLayers() + \";\" + l.toString());\r\n    }\r\n    List<ParsedTaskAttempt> attempts = obtainTaskAttempts();\r\n    for (ParsedTaskAttempt attempt : attempts) {\r\n        attempt.dumpParsedTaskAttempt();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\serializers",
  "methodName" : "serialize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serialize(T object, JsonGenerator jGen, SerializerProvider sProvider) throws IOException, JsonProcessingException\n{\r\n    jGen.writeString(object.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "incorporateCounters",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void incorporateCounters(JhCounters counters)\n{\r\n    Map<String, Long> countersMap = JobHistoryUtils.extractCounters(counters);\r\n    putCounters(countersMap);\r\n    super.incorporateCounters(counters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "putCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void putCounters(Map<String, Long> counters)\n{\r\n    this.countersMap = counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, Long> obtainCounters()\n{\r\n    return countersMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "putDiagnosticInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void putDiagnosticInfo(String msg)\n{\r\n    diagnosticInfo = msg;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainDiagnosticInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String obtainDiagnosticInfo()\n{\r\n    return diagnosticInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "putTrackerName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void putTrackerName(String trackerName)\n{\r\n    this.trackerName = trackerName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainTrackerName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String obtainTrackerName()\n{\r\n    return trackerName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "putHttpPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void putHttpPort(int port)\n{\r\n    httpPort = port;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainHttpPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Integer obtainHttpPort()\n{\r\n    return httpPort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "putShufflePort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void putShufflePort(int port)\n{\r\n    shufflePort = port;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainShufflePort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Integer obtainShufflePort()\n{\r\n    return shufflePort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "dumpParsedTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void dumpParsedTaskAttempt()\n{\r\n    LOG.info(\"ParsedTaskAttempt details:\" + obtainCounters() + \";DiagnosticInfo=\" + obtainDiagnosticInfo() + \"\\n\" + obtainTrackerName() + \";\" + obtainHttpPort() + \";\" + obtainShufflePort() + \";rack=\" + getHostName().getRackName() + \";host=\" + getHostName().getHostName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getCumulativeCpuUsage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCumulativeCpuUsage()\n{\r\n    return cumulativeCpuUsage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setCumulativeCpuUsage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCumulativeCpuUsage(long usage)\n{\r\n    cumulativeCpuUsage = usage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getVirtualMemoryUsage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getVirtualMemoryUsage()\n{\r\n    return virtualMemoryUsage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setVirtualMemoryUsage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setVirtualMemoryUsage(long usage)\n{\r\n    virtualMemoryUsage = usage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getPhysicalMemoryUsage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPhysicalMemoryUsage()\n{\r\n    return physicalMemoryUsage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setPhysicalMemoryUsage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setPhysicalMemoryUsage(long usage)\n{\r\n    physicalMemoryUsage = usage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getHeapUsage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getHeapUsage()\n{\r\n    return heapUsage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setHeapUsage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHeapUsage(long usage)\n{\r\n    heapUsage = usage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "size",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int size()\n{\r\n    int size = 0;\r\n    size += WritableUtils.getVIntSize(cumulativeCpuUsage);\r\n    size += WritableUtils.getVIntSize(virtualMemoryUsage);\r\n    size += WritableUtils.getVIntSize(physicalMemoryUsage);\r\n    size += WritableUtils.getVIntSize(heapUsage);\r\n    return size;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    cumulativeCpuUsage = WritableUtils.readVLong(in);\r\n    virtualMemoryUsage = WritableUtils.readVLong(in);\r\n    physicalMemoryUsage = WritableUtils.readVLong(in);\r\n    heapUsage = WritableUtils.readVLong(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    WritableUtils.writeVLong(out, cumulativeCpuUsage);\r\n    WritableUtils.writeVLong(out, virtualMemoryUsage);\r\n    WritableUtils.writeVLong(out, physicalMemoryUsage);\r\n    WritableUtils.writeVLong(out, heapUsage);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compareMetric",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void compareMetric(long m1, long m2, TreePath loc) throws DeepInequalityException\n{\r\n    if (m1 != m2) {\r\n        throw new DeepInequalityException(\"Value miscompared:\" + loc.toString(), loc);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compareSize",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void compareSize(ResourceUsageMetrics m1, ResourceUsageMetrics m2, TreePath loc) throws DeepInequalityException\n{\r\n    if (m1.size() != m2.size()) {\r\n        throw new DeepInequalityException(\"Size miscompared: \" + loc.toString(), loc);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "deepCompare",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void deepCompare(DeepCompare other, TreePath loc) throws DeepInequalityException\n{\r\n    if (!(other instanceof ResourceUsageMetrics)) {\r\n        throw new DeepInequalityException(\"Comparand has wrong type\", loc);\r\n    }\r\n    ResourceUsageMetrics metrics2 = (ResourceUsageMetrics) other;\r\n    compareMetric(getCumulativeCpuUsage(), metrics2.getCumulativeCpuUsage(), new TreePath(loc, \"cumulativeCpu\"));\r\n    compareMetric(getVirtualMemoryUsage(), metrics2.getVirtualMemoryUsage(), new TreePath(loc, \"virtualMemory\"));\r\n    compareMetric(getPhysicalMemoryUsage(), metrics2.getPhysicalMemoryUsage(), new TreePath(loc, \"physicalMemory\"));\r\n    compareMetric(getHeapUsage(), metrics2.getHeapUsage(), new TreePath(loc, \"heapUsage\"));\r\n    compareSize(this, metrics2, new TreePath(loc, \"size\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "newDistributionBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Histogram[][] newDistributionBlock()\n{\r\n    return newDistributionBlock(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "newDistributionBlock",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Histogram[][] newDistributionBlock(String blockname)\n{\r\n    Histogram[][] result = new Histogram[JobOutcome.values().length][];\r\n    for (int i = 0; i < JobOutcome.values().length; ++i) {\r\n        result[i] = new Histogram[LoggedJob.JobType.values().length];\r\n        for (int j = 0; j < LoggedJob.JobType.values().length; ++j) {\r\n            result[i][j] = blockname == null ? new Histogram() : new Histogram(blockname);\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getDistribution",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Histogram getDistribution(Histogram[][] block, JobOutcome outcome, LoggedJob.JobType type)\n{\r\n    return block[outcome.ordinal()][type.ordinal()];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "usage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void usage()\n{\r\n    statusOutput.print(\"Usage: \\n\" + \"administrative subcommands:\\n\" + \"-v1                  specify version 1 of the jt logs\\n\" + \"-h or -help          print this message\\n\" + \"-d or -debug         print voluminous debug info during processing\\n\" + \"-collect-prefixes    collect the prefixes of log lines\\n\\n\" + \"  job trace subcommands\\n\" + \"-write-job-trace     takes a filename.\\n\" + \"                     writes job trace in JSON to that filename\\n\" + \"-single-line-job-traces  omit prettyprinting of job trace\\n\" + \"-omit-task-details   leave out info about each task and attempt,\\n\" + \"                     so only statistical info is added to each job\\n\" + \"-write-topology      takes a filename.\\n\" + \"                     writes JSON file giving network topology\\n\" + \"-job-digest-spectra  takes a list of percentile points\\n\" + \"                     writes CDFs with min, max, and those percentiles\\n\\n\" + \"subcommands for task statistical info\\n\" + \"-spreads             we have a mode where, for each job, we can\\n\" + \"                     develop the ratio of percentile B to percentile A\\n\" + \"                     of task run times.  Having developed that ratio,\\n\" + \"                     we can consider it to be a datum and we can\\n\" + \"                     build a CDF of those ratios.  -spreads turns\\n\" + \"                     this option on, and takes A and B\\n\" + \"-delays              tells us to gather and print CDFs for delays\\n\" + \"                     from job submit to job start\\n\" + \"-runtimes            prints CDFs of job wallclock times [launch\\n\" + \"                     to finish]\\n\" + \"-tasktimes           prints CDFs of job wallclock times [launch\\n\" + \"                     to finish]\\n\\n\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "pathIsDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean pathIsDirectory(Path p) throws IOException\n{\r\n    FileSystem fs = p.getFileSystem(getConf());\r\n    return fs.getFileStatus(p).isDirectory();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "initializeHadoopLogsAnalyzer",
  "errType" : null,
  "containingMethodsNum" : 50,
  "sourceCodeText" : "int initializeHadoopLogsAnalyzer(String[] args) throws FileNotFoundException, IOException\n{\r\n    Path jobTraceFilename = null;\r\n    Path topologyFilename = null;\r\n    if (args.length == 0 || args[args.length - 1].charAt(0) == '-') {\r\n        throw new IllegalArgumentException(\"No input specified.\");\r\n    } else {\r\n        inputFilename = args[args.length - 1];\r\n    }\r\n    for (int i = 0; i < args.length - (inputFilename == null ? 0 : 1); ++i) {\r\n        if (StringUtils.equalsIgnoreCase(\"-h\", args[i]) || StringUtils.equalsIgnoreCase(\"-help\", args[i])) {\r\n            usage();\r\n            return 0;\r\n        }\r\n        if (StringUtils.equalsIgnoreCase(\"-c\", args[i]) || StringUtils.equalsIgnoreCase(\"-collect-prefixes\", args[i])) {\r\n            collecting = true;\r\n            continue;\r\n        }\r\n        if (StringUtils.equalsIgnoreCase(\"-write-job-trace\", args[i])) {\r\n            ++i;\r\n            jobTraceFilename = new Path(args[i]);\r\n            continue;\r\n        }\r\n        if (StringUtils.equalsIgnoreCase(\"-single-line-job-traces\", args[i])) {\r\n            prettyprintTrace = false;\r\n            continue;\r\n        }\r\n        if (StringUtils.equalsIgnoreCase(\"-omit-task-details\", args[i])) {\r\n            omitTaskDetails = true;\r\n            continue;\r\n        }\r\n        if (StringUtils.equalsIgnoreCase(\"-write-topology\", args[i])) {\r\n            ++i;\r\n            topologyFilename = new Path(args[i]);\r\n            continue;\r\n        }\r\n        if (StringUtils.equalsIgnoreCase(\"-job-digest-spectra\", args[i])) {\r\n            ArrayList<Integer> values = new ArrayList<Integer>();\r\n            ++i;\r\n            while (i < args.length && Character.isDigit(args[i].charAt(0))) {\r\n                values.add(Integer.parseInt(args[i]));\r\n                ++i;\r\n            }\r\n            if (values.size() == 0) {\r\n                throw new IllegalArgumentException(\"Empty -job-digest-spectra list\");\r\n            }\r\n            attemptTimesPercentiles = new int[values.size()];\r\n            int lastValue = 0;\r\n            for (int j = 0; j < attemptTimesPercentiles.length; ++j) {\r\n                if (values.get(j) <= lastValue || values.get(j) >= 100) {\r\n                    throw new IllegalArgumentException(\"Bad -job-digest-spectra percentiles list\");\r\n                }\r\n                attemptTimesPercentiles[j] = values.get(j);\r\n            }\r\n            --i;\r\n            continue;\r\n        }\r\n        if (StringUtils.equalsIgnoreCase(\"-d\", args[i]) || StringUtils.equalsIgnoreCase(\"-debug\", args[i])) {\r\n            debug = true;\r\n            continue;\r\n        }\r\n        if (StringUtils.equalsIgnoreCase(\"-spreads\", args[i])) {\r\n            int min = Integer.parseInt(args[i + 1]);\r\n            int max = Integer.parseInt(args[i + 2]);\r\n            if (min < max && min < 1000 && max < 1000) {\r\n                spreadMin = min;\r\n                spreadMax = max;\r\n                spreading = true;\r\n                i += 2;\r\n            }\r\n            continue;\r\n        }\r\n        if (StringUtils.equalsIgnoreCase(\"-delays\", args[i])) {\r\n            delays = true;\r\n            continue;\r\n        }\r\n        if (StringUtils.equalsIgnoreCase(\"-runtimes\", args[i])) {\r\n            runtimes = true;\r\n            continue;\r\n        }\r\n        if (StringUtils.equalsIgnoreCase(\"-tasktimes\", args[i])) {\r\n            collectTaskTimes = true;\r\n            continue;\r\n        }\r\n        if (StringUtils.equalsIgnoreCase(\"-v1\", args[i])) {\r\n            version = 1;\r\n            continue;\r\n        }\r\n        throw new IllegalArgumentException(\"Unrecognized argument: \" + args[i]);\r\n    }\r\n    runTimeDists = newDistributionBlock();\r\n    delayTimeDists = newDistributionBlock();\r\n    mapTimeSpreadDists = newDistributionBlock(\"map-time-spreads\");\r\n    shuffleTimeSpreadDists = newDistributionBlock();\r\n    sortTimeSpreadDists = newDistributionBlock();\r\n    reduceTimeSpreadDists = newDistributionBlock();\r\n    mapTimeDists = newDistributionBlock();\r\n    shuffleTimeDists = newDistributionBlock();\r\n    sortTimeDists = newDistributionBlock();\r\n    reduceTimeDists = newDistributionBlock();\r\n    taskAttemptStartTimes = new HashMap<String, Long>();\r\n    taskReduceAttemptShuffleEndTimes = new HashMap<String, Long>();\r\n    taskReduceAttemptSortEndTimes = new HashMap<String, Long>();\r\n    taskMapAttemptFinishTimes = new HashMap<String, Long>();\r\n    taskReduceAttemptFinishTimes = new HashMap<String, Long>();\r\n    final Path inputPath = new Path(inputFilename);\r\n    inputIsDirectory = pathIsDirectory(inputPath);\r\n    if (jobTraceFilename != null && attemptTimesPercentiles == null) {\r\n        attemptTimesPercentiles = new int[19];\r\n        for (int i = 0; i < 19; ++i) {\r\n            attemptTimesPercentiles[i] = (i + 1) * 5;\r\n        }\r\n    }\r\n    if (!inputIsDirectory) {\r\n        input = maybeUncompressedPath(inputPath);\r\n    } else {\r\n        inputDirectoryPath = inputPath;\r\n        FileSystem fs = inputPath.getFileSystem(getConf());\r\n        FileStatus[] statuses = fs.listStatus(inputPath);\r\n        inputDirectoryFiles = new String[statuses.length];\r\n        for (int i = 0; i < statuses.length; ++i) {\r\n            inputDirectoryFiles[i] = statuses[i].getPath().getName();\r\n        }\r\n        int dropPoint = 0;\r\n        for (int i = 0; i < inputDirectoryFiles.length; ++i) {\r\n            String name = inputDirectoryFiles[i];\r\n            if (!(name.length() >= 4 && \".crc\".equals(name.substring(name.length() - 4)))) {\r\n                inputDirectoryFiles[dropPoint++] = name;\r\n            }\r\n        }\r\n        LOG.info(\"We dropped \" + (inputDirectoryFiles.length - dropPoint) + \" crc files.\");\r\n        String[] new_inputDirectoryFiles = new String[dropPoint];\r\n        System.arraycopy(inputDirectoryFiles, 0, new_inputDirectoryFiles, 0, dropPoint);\r\n        inputDirectoryFiles = new_inputDirectoryFiles;\r\n        Arrays.sort(inputDirectoryFiles);\r\n        if (!setNextDirectoryInputStream()) {\r\n            throw new FileNotFoundException(\"Empty directory specified.\");\r\n        }\r\n    }\r\n    if (jobTraceFilename != null) {\r\n        jobTraceGen = new DefaultOutputter<LoggedJob>();\r\n        jobTraceGen.init(jobTraceFilename, getConf());\r\n        if (topologyFilename != null) {\r\n            topologyGen = new DefaultOutputter<LoggedNetworkTopology>();\r\n            topologyGen.init(topologyFilename, getConf());\r\n        }\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "maybeUncompressedPath",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "LineReader maybeUncompressedPath(Path p) throws FileNotFoundException, IOException\n{\r\n    CompressionCodecFactory codecs = new CompressionCodecFactory(getConf());\r\n    inputCodec = codecs.getCodec(p);\r\n    FileSystem fs = p.getFileSystem(getConf());\r\n    FSDataInputStream fileIn = fs.open(p);\r\n    if (inputCodec == null) {\r\n        return new LineReader(fileIn, getConf());\r\n    } else {\r\n        inputDecompressor = CodecPool.getDecompressor(inputCodec);\r\n        return new LineReader(inputCodec.createInputStream(fileIn, inputDecompressor), getConf());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setNextDirectoryInputStream",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean setNextDirectoryInputStream() throws FileNotFoundException, IOException\n{\r\n    if (input != null) {\r\n        input.close();\r\n        LOG.info(\"File closed: \" + currentFileName);\r\n        input = null;\r\n    }\r\n    if (inputCodec != null) {\r\n        CodecPool.returnDecompressor(inputDecompressor);\r\n        inputDecompressor = null;\r\n        inputCodec = null;\r\n    }\r\n    ++inputDirectoryCursor;\r\n    if (inputDirectoryCursor >= inputDirectoryFiles.length) {\r\n        return false;\r\n    }\r\n    fileFirstLine = true;\r\n    currentFileName = inputDirectoryFiles[inputDirectoryCursor];\r\n    LOG.info(\"\\nOpening file \" + currentFileName + \"  *************************** .\");\r\n    LOG.info(\"This file, \" + (inputDirectoryCursor + 1) + \"/\" + inputDirectoryFiles.length + \", starts with line \" + lineNumber + \".\");\r\n    input = maybeUncompressedPath(new Path(inputDirectoryPath, currentFileName));\r\n    return true;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "readInputLine",
  "errType" : [ "EOFException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String readInputLine() throws IOException\n{\r\n    try {\r\n        if (input == null) {\r\n            return null;\r\n        }\r\n        inputLineText.clear();\r\n        if (input.readLine(inputLineText) == 0) {\r\n            return null;\r\n        }\r\n        return inputLineText.toString();\r\n    } catch (EOFException e) {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "readCountedLine",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String readCountedLine() throws IOException\n{\r\n    if (rereadableLine != null) {\r\n        String result = rereadableLine;\r\n        rereadableLine = null;\r\n        return result;\r\n    }\r\n    String result = readInputLine();\r\n    if (result != null) {\r\n        if (fileFirstLine && (result.equals(\"\") || result.charAt(0) != '\\f')) {\r\n            fileFirstLine = false;\r\n            rereadableLine = result;\r\n            return \"\\f!!FILE \" + currentFileName + \"!!\\n\";\r\n        }\r\n        fileFirstLine = false;\r\n        ++lineNumber;\r\n    } else if (inputIsDirectory && setNextDirectoryInputStream()) {\r\n        result = readCountedLine();\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "unreadCountedLine",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void unreadCountedLine(String unreadee)\n{\r\n    if (rereadableLine == null) {\r\n        rereadableLine = unreadee;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "apparentConfFileHeader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean apparentConfFileHeader(String header)\n{\r\n    return confFileHeader.matcher(header).find();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "apparentXMLFileStart",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean apparentXMLFileStart(String line)\n{\r\n    return xmlFilePrefix.matcher(line).lookingAt();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "readBalancedLine",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "Pair<String, String> readBalancedLine() throws IOException\n{\r\n    String line = readCountedLine();\r\n    if (line == null) {\r\n        return null;\r\n    }\r\n    while (line.indexOf('\\f') > 0) {\r\n        line = line.substring(line.indexOf('\\f'));\r\n    }\r\n    if (line.length() != 0 && line.charAt(0) == '\\f') {\r\n        String subjectLine = readCountedLine();\r\n        if (subjectLine != null && subjectLine.length() != 0 && apparentConfFileHeader(line) && apparentXMLFileStart(subjectLine)) {\r\n            StringBuilder sb = new StringBuilder();\r\n            while (subjectLine != null && subjectLine.indexOf('\\f') > 0) {\r\n                subjectLine = subjectLine.substring(subjectLine.indexOf('\\f'));\r\n            }\r\n            while (subjectLine != null && (subjectLine.length() == 0 || subjectLine.charAt(0) != '\\f')) {\r\n                sb.append(subjectLine);\r\n                subjectLine = readCountedLine();\r\n            }\r\n            if (subjectLine != null) {\r\n                unreadCountedLine(subjectLine);\r\n            }\r\n            return new Pair<String, String>(line, sb.toString());\r\n        }\r\n        return readBalancedLine();\r\n    }\r\n    String endlineString = (version == 0 ? \" \" : \" .\");\r\n    if (line.length() < endlineString.length()) {\r\n        return new Pair<String, String>(null, line);\r\n    }\r\n    if (!endlineString.equals(line.substring(line.length() - endlineString.length()))) {\r\n        StringBuilder sb = new StringBuilder(line);\r\n        String addedLine;\r\n        do {\r\n            addedLine = readCountedLine();\r\n            if (addedLine == null) {\r\n                return new Pair<String, String>(null, sb.toString());\r\n            }\r\n            while (addedLine.indexOf('\\f') > 0) {\r\n                addedLine = addedLine.substring(addedLine.indexOf('\\f'));\r\n            }\r\n            if (addedLine.length() > 0 && addedLine.charAt(0) == '\\f') {\r\n                unreadCountedLine(addedLine);\r\n                return new Pair<String, String>(null, sb.toString());\r\n            }\r\n            sb.append(\"\\n\");\r\n            sb.append(addedLine);\r\n        } while (!endlineString.equals(addedLine.substring(addedLine.length() - endlineString.length())));\r\n        line = sb.toString();\r\n    }\r\n    return new Pair<String, String>(null, line);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "incorporateSpread",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void incorporateSpread(Histogram taskTimes, Histogram[][] spreadTo, JobOutcome outcome, LoggedJob.JobType jtype)\n{\r\n    if (!spreading) {\r\n        return;\r\n    }\r\n    if (taskTimes.getTotalCount() <= 1) {\r\n        return;\r\n    }\r\n    int[] endpoints = new int[2];\r\n    endpoints[0] = spreadMin;\r\n    endpoints[1] = spreadMax;\r\n    long[] endpointKeys = taskTimes.getCDF(1000, endpoints);\r\n    int smallResultOffset = (taskTimes.getTotalCount() < SMALL_SPREAD_COMPENSATION_THRESHOLD ? 1 : 0);\r\n    Histogram myTotal = spreadTo[outcome.ordinal()][jtype.ordinal()];\r\n    long dividend = endpointKeys[2 + smallResultOffset];\r\n    long divisor = endpointKeys[1 - smallResultOffset];\r\n    if (divisor > 0) {\r\n        long mytotalRatio = dividend * 1000000L / divisor;\r\n        myTotal.enter(mytotalRatio);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "canonicalDistributionsEnter",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void canonicalDistributionsEnter(Histogram[][] block, JobOutcome outcome, LoggedJob.JobType type, long value)\n{\r\n    getDistribution(block, outcome, type).enter(value);\r\n    getDistribution(block, JobOutcome.OVERALL, type).enter(value);\r\n    getDistribution(block, outcome, LoggedJob.JobType.OVERALL).enter(value);\r\n    getDistribution(block, JobOutcome.OVERALL, LoggedJob.JobType.OVERALL).enter(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processJobLine",
  "errType" : [ "NumberFormatException" ],
  "containingMethodsNum" : 55,
  "sourceCodeText" : "void processJobLine(ParsedLine line) throws JsonProcessingException, IOException\n{\r\n    try {\r\n        if (version == 0 || version == 1) {\r\n            String jobID = line.get(\"JOBID\");\r\n            String user = line.get(\"USER\");\r\n            String jobPriority = line.get(\"JOB_PRIORITY\");\r\n            String submitTime = line.get(\"SUBMIT_TIME\");\r\n            String jobName = line.get(\"JOBNAME\");\r\n            String launchTime = line.get(\"LAUNCH_TIME\");\r\n            String finishTime = line.get(\"FINISH_TIME\");\r\n            String status = line.get(\"JOB_STATUS\");\r\n            String totalMaps = line.get(\"TOTAL_MAPS\");\r\n            String totalReduces = line.get(\"TOTAL_REDUCES\");\r\n            if (jobID != null && jobTraceGen != null && (jobBeingTraced == null || !jobID.equals(jobBeingTraced.getJobID().toString()))) {\r\n                finalizeJob();\r\n                jobBeingTraced = new LoggedJob(jobID);\r\n                tasksInCurrentJob = new HashMap<String, LoggedTask>();\r\n                attemptsInCurrentJob = new HashMap<String, LoggedTaskAttempt>();\r\n                successfulMapAttemptTimes = new Histogram[ParsedHost.numberOfDistances() + 1];\r\n                for (int i = 0; i < successfulMapAttemptTimes.length; ++i) {\r\n                    successfulMapAttemptTimes[i] = new Histogram();\r\n                }\r\n                successfulReduceAttemptTimes = new Histogram();\r\n                failedMapAttemptTimes = new Histogram[ParsedHost.numberOfDistances() + 1];\r\n                for (int i = 0; i < failedMapAttemptTimes.length; ++i) {\r\n                    failedMapAttemptTimes[i] = new Histogram();\r\n                }\r\n                failedReduceAttemptTimes = new Histogram();\r\n                successfulNthMapperAttempts = new Histogram();\r\n                successfulNthReducerAttempts = new Histogram();\r\n                mapperLocality = new Histogram();\r\n            }\r\n            if (jobBeingTraced != null) {\r\n                if (user != null) {\r\n                    jobBeingTraced.setUser(user);\r\n                }\r\n                if (jobPriority != null) {\r\n                    jobBeingTraced.setPriority(LoggedJob.JobPriority.valueOf(jobPriority));\r\n                }\r\n                if (totalMaps != null) {\r\n                    jobBeingTraced.setTotalMaps(Integer.parseInt(totalMaps));\r\n                }\r\n                if (totalReduces != null) {\r\n                    jobBeingTraced.setTotalReduces(Integer.parseInt(totalReduces));\r\n                }\r\n                if (submitTime != null) {\r\n                    jobBeingTraced.setSubmitTime(Long.parseLong(submitTime));\r\n                }\r\n                if (launchTime != null) {\r\n                    jobBeingTraced.setLaunchTime(Long.parseLong(launchTime));\r\n                }\r\n                if (finishTime != null) {\r\n                    jobBeingTraced.setFinishTime(Long.parseLong(finishTime));\r\n                    if (status != null) {\r\n                        jobBeingTraced.setOutcome(Pre21JobHistoryConstants.Values.valueOf(status));\r\n                    }\r\n                    maybeMateJobAndConf();\r\n                }\r\n            }\r\n            if (jobName != null) {\r\n                Matcher m = streamingJobnamePattern.matcher(jobName);\r\n                thisJobType = LoggedJob.JobType.JAVA;\r\n                if (m.matches()) {\r\n                    thisJobType = LoggedJob.JobType.STREAMING;\r\n                }\r\n            }\r\n            if (submitTime != null) {\r\n                submitTimeCurrentJob = Long.parseLong(submitTime);\r\n                currentJobID = jobID;\r\n                taskAttemptStartTimes = new HashMap<String, Long>();\r\n                taskReduceAttemptShuffleEndTimes = new HashMap<String, Long>();\r\n                taskReduceAttemptSortEndTimes = new HashMap<String, Long>();\r\n                taskMapAttemptFinishTimes = new HashMap<String, Long>();\r\n                taskReduceAttemptFinishTimes = new HashMap<String, Long>();\r\n                launchTimeCurrentJob = 0L;\r\n            } else if (launchTime != null && jobID != null && currentJobID.equals(jobID)) {\r\n                launchTimeCurrentJob = Long.parseLong(launchTime);\r\n            } else if (finishTime != null && jobID != null && currentJobID.equals(jobID)) {\r\n                long endTime = Long.parseLong(finishTime);\r\n                if (launchTimeCurrentJob != 0) {\r\n                    String jobResultText = line.get(\"JOB_STATUS\");\r\n                    JobOutcome thisOutcome = ((jobResultText != null && \"SUCCESS\".equals(jobResultText)) ? JobOutcome.SUCCESS : JobOutcome.FAILURE);\r\n                    if (submitTimeCurrentJob != 0L) {\r\n                        canonicalDistributionsEnter(delayTimeDists, thisOutcome, thisJobType, launchTimeCurrentJob - submitTimeCurrentJob);\r\n                    }\r\n                    if (launchTimeCurrentJob != 0L) {\r\n                        canonicalDistributionsEnter(runTimeDists, thisOutcome, thisJobType, endTime - launchTimeCurrentJob);\r\n                    }\r\n                    Histogram currentJobMapTimes = new Histogram();\r\n                    Histogram currentJobShuffleTimes = new Histogram();\r\n                    Histogram currentJobSortTimes = new Histogram();\r\n                    Histogram currentJobReduceTimes = new Histogram();\r\n                    Iterator<Map.Entry<String, Long>> taskIter = taskAttemptStartTimes.entrySet().iterator();\r\n                    while (taskIter.hasNext()) {\r\n                        Map.Entry<String, Long> entry = taskIter.next();\r\n                        long startTime = entry.getValue();\r\n                        Long mapEndTime = taskMapAttemptFinishTimes.get(entry.getKey());\r\n                        if (mapEndTime != null) {\r\n                            currentJobMapTimes.enter(mapEndTime - startTime);\r\n                            canonicalDistributionsEnter(mapTimeDists, thisOutcome, thisJobType, mapEndTime - startTime);\r\n                        }\r\n                        Long shuffleEnd = taskReduceAttemptShuffleEndTimes.get(entry.getKey());\r\n                        Long sortEnd = taskReduceAttemptSortEndTimes.get(entry.getKey());\r\n                        Long reduceEnd = taskReduceAttemptFinishTimes.get(entry.getKey());\r\n                        if (shuffleEnd != null && sortEnd != null && reduceEnd != null) {\r\n                            currentJobShuffleTimes.enter(shuffleEnd - startTime);\r\n                            currentJobSortTimes.enter(sortEnd - shuffleEnd);\r\n                            currentJobReduceTimes.enter(reduceEnd - sortEnd);\r\n                            canonicalDistributionsEnter(shuffleTimeDists, thisOutcome, thisJobType, shuffleEnd - startTime);\r\n                            canonicalDistributionsEnter(sortTimeDists, thisOutcome, thisJobType, sortEnd - shuffleEnd);\r\n                            canonicalDistributionsEnter(reduceTimeDists, thisOutcome, thisJobType, reduceEnd - sortEnd);\r\n                        }\r\n                    }\r\n                    incorporateSpread(currentJobMapTimes, mapTimeSpreadDists, thisOutcome, thisJobType);\r\n                    incorporateSpread(currentJobShuffleTimes, shuffleTimeSpreadDists, thisOutcome, thisJobType);\r\n                    incorporateSpread(currentJobSortTimes, sortTimeSpreadDists, thisOutcome, thisJobType);\r\n                    incorporateSpread(currentJobReduceTimes, reduceTimeSpreadDists, thisOutcome, thisJobType);\r\n                }\r\n            }\r\n        }\r\n    } catch (NumberFormatException e) {\r\n        LOG.warn(\"HadoopLogsAnalyzer.processJobLine: bad numerical format, at line \" + lineNumber + \".\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processTaskLine",
  "errType" : [ "IllegalArgumentException", "IllegalArgumentException" ],
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void processTaskLine(ParsedLine line)\n{\r\n    if (jobBeingTraced != null) {\r\n        String taskID = line.get(\"TASKID\");\r\n        String taskType = line.get(\"TASK_TYPE\");\r\n        String startTime = line.get(\"START_TIME\");\r\n        String status = line.get(\"TASK_STATUS\");\r\n        String finishTime = line.get(\"FINISH_TIME\");\r\n        String splits = line.get(\"SPLITS\");\r\n        LoggedTask task = tasksInCurrentJob.get(taskID);\r\n        boolean taskAlreadyLogged = task != null;\r\n        if (task == null) {\r\n            task = new LoggedTask();\r\n        }\r\n        if (splits != null) {\r\n            ArrayList<LoggedLocation> locations = null;\r\n            StringTokenizer tok = new StringTokenizer(splits, \",\", false);\r\n            if (tok.countTokens() <= MAXIMUM_PREFERRED_LOCATIONS) {\r\n                locations = new ArrayList<LoggedLocation>();\r\n            }\r\n            while (tok.hasMoreTokens()) {\r\n                String nextSplit = tok.nextToken();\r\n                ParsedHost node = getAndRecordParsedHost(nextSplit);\r\n                if (locations != null && node != null) {\r\n                    locations.add(node.makeLoggedLocation());\r\n                }\r\n            }\r\n            task.setPreferredLocations(locations);\r\n        }\r\n        task.setTaskID(taskID);\r\n        if (startTime != null) {\r\n            task.setStartTime(Long.parseLong(startTime));\r\n        }\r\n        if (finishTime != null) {\r\n            task.setFinishTime(Long.parseLong(finishTime));\r\n        }\r\n        Pre21JobHistoryConstants.Values typ;\r\n        Pre21JobHistoryConstants.Values stat;\r\n        try {\r\n            stat = status == null ? null : Pre21JobHistoryConstants.Values.valueOf(status);\r\n        } catch (IllegalArgumentException e) {\r\n            LOG.error(\"A task status you don't know about is \\\"\" + status + \"\\\".\", e);\r\n            stat = null;\r\n        }\r\n        task.setTaskStatus(stat);\r\n        try {\r\n            typ = taskType == null ? null : Pre21JobHistoryConstants.Values.valueOf(taskType);\r\n        } catch (IllegalArgumentException e) {\r\n            LOG.error(\"A task type you don't know about is \\\"\" + taskType + \"\\\".\", e);\r\n            typ = null;\r\n        }\r\n        if (typ == null) {\r\n            return;\r\n        }\r\n        task.setTaskType(typ);\r\n        List<LoggedTask> vec = typ == Pre21JobHistoryConstants.Values.MAP ? jobBeingTraced.getMapTasks() : typ == Pre21JobHistoryConstants.Values.REDUCE ? jobBeingTraced.getReduceTasks() : jobBeingTraced.getOtherTasks();\r\n        if (!taskAlreadyLogged) {\r\n            vec.add(task);\r\n            tasksInCurrentJob.put(taskID, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "counterPattern",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Pattern counterPattern(String counterName)\n{\r\n    Pattern result = counterPatterns.get(counterName);\r\n    if (result == null) {\r\n        String namePatternRegex = \"\\\\[\\\\(\" + counterName + \"\\\\)\\\\([^)]+\\\\)\\\\(([0-9]+)\\\\)\\\\]\";\r\n        result = Pattern.compile(namePatternRegex);\r\n        counterPatterns.put(counterName, result);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "parseCounter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String parseCounter(String counterString, String counterName)\n{\r\n    if (counterString == null) {\r\n        return null;\r\n    }\r\n    Matcher mat = counterPattern(counterName).matcher(counterString);\r\n    if (mat.find()) {\r\n        return mat.group(1);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "incorporateCounter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void incorporateCounter(SetField thunk, String counterString, String counterName)\n{\r\n    String valueString = parseCounter(counterString, counterName);\r\n    if (valueString != null) {\r\n        thunk.set(Long.parseLong(valueString));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "incorporateCounters",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void incorporateCounters(LoggedTaskAttempt attempt2, String counterString)\n{\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.hdfsBytesRead = val;\r\n        }\r\n    }, counterString, \"HDFS_BYTES_READ\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.hdfsBytesWritten = val;\r\n        }\r\n    }, counterString, \"HDFS_BYTES_WRITTEN\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.fileBytesRead = val;\r\n        }\r\n    }, counterString, \"FILE_BYTES_READ\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.fileBytesWritten = val;\r\n        }\r\n    }, counterString, \"FILE_BYTES_WRITTEN\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.mapInputBytes = val;\r\n        }\r\n    }, counterString, \"MAP_INPUT_BYTES\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.mapInputRecords = val;\r\n        }\r\n    }, counterString, \"MAP_INPUT_RECORDS\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.mapOutputBytes = val;\r\n        }\r\n    }, counterString, \"MAP_OUTPUT_BYTES\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.mapOutputRecords = val;\r\n        }\r\n    }, counterString, \"MAP_OUTPUT_RECORDS\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.combineInputRecords = val;\r\n        }\r\n    }, counterString, \"COMBINE_INPUT_RECORDS\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.reduceInputGroups = val;\r\n        }\r\n    }, counterString, \"REDUCE_INPUT_GROUPS\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.reduceInputRecords = val;\r\n        }\r\n    }, counterString, \"REDUCE_INPUT_RECORDS\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.reduceShuffleBytes = val;\r\n        }\r\n    }, counterString, \"REDUCE_SHUFFLE_BYTES\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.reduceOutputRecords = val;\r\n        }\r\n    }, counterString, \"REDUCE_OUTPUT_RECORDS\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.spilledRecords = val;\r\n        }\r\n    }, counterString, \"SPILLED_RECORDS\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.getResourceUsageMetrics().setCumulativeCpuUsage(val);\r\n        }\r\n    }, counterString, \"CPU_MILLISECONDS\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.getResourceUsageMetrics().setVirtualMemoryUsage(val);\r\n        }\r\n    }, counterString, \"VIRTUAL_MEMORY_BYTES\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.getResourceUsageMetrics().setPhysicalMemoryUsage(val);\r\n        }\r\n    }, counterString, \"PHYSICAL_MEMORY_BYTES\");\r\n    incorporateCounter(new SetField(attempt2) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.getResourceUsageMetrics().setHeapUsage(val);\r\n        }\r\n    }, counterString, \"COMMITTED_HEAP_BYTES\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getAndRecordParsedHost",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ParsedHost getAndRecordParsedHost(String hostName)\n{\r\n    ParsedHost result = ParsedHost.parse(hostName);\r\n    if (result != null && !allHosts.contains(result)) {\r\n        allHosts.add(result);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processMapAttemptLine",
  "errType" : [ "IllegalArgumentException", "NumberFormatException" ],
  "containingMethodsNum" : 48,
  "sourceCodeText" : "void processMapAttemptLine(ParsedLine line)\n{\r\n    String attemptID = line.get(\"TASK_ATTEMPT_ID\");\r\n    String taskID = line.get(\"TASKID\");\r\n    String status = line.get(\"TASK_STATUS\");\r\n    String attemptStartTime = line.get(\"START_TIME\");\r\n    String attemptFinishTime = line.get(\"FINISH_TIME\");\r\n    String hostName = line.get(\"HOSTNAME\");\r\n    String counters = line.get(\"COUNTERS\");\r\n    if (jobBeingTraced != null && taskID != null) {\r\n        LoggedTask task = tasksInCurrentJob.get(taskID);\r\n        if (task == null) {\r\n            task = new LoggedTask();\r\n            task.setTaskID(taskID);\r\n            jobBeingTraced.getMapTasks().add(task);\r\n            tasksInCurrentJob.put(taskID, task);\r\n        }\r\n        task.setTaskID(taskID);\r\n        LoggedTaskAttempt attempt = attemptsInCurrentJob.get(attemptID);\r\n        boolean attemptAlreadyExists = attempt != null;\r\n        if (attempt == null) {\r\n            attempt = new LoggedTaskAttempt();\r\n            attempt.setAttemptID(attemptID);\r\n        }\r\n        if (!attemptAlreadyExists) {\r\n            attemptsInCurrentJob.put(attemptID, attempt);\r\n            task.getAttempts().add(attempt);\r\n        }\r\n        Pre21JobHistoryConstants.Values stat = null;\r\n        try {\r\n            stat = status == null ? null : Pre21JobHistoryConstants.Values.valueOf(status);\r\n        } catch (IllegalArgumentException e) {\r\n            LOG.error(\"A map attempt status you don't know about is \\\"\" + status + \"\\\".\", e);\r\n            stat = null;\r\n        }\r\n        incorporateCounters(attempt, counters);\r\n        attempt.setResult(stat);\r\n        if (attemptStartTime != null) {\r\n            attempt.setStartTime(Long.parseLong(attemptStartTime));\r\n        }\r\n        if (attemptFinishTime != null) {\r\n            attempt.setFinishTime(Long.parseLong(attemptFinishTime));\r\n        }\r\n        int distance = Integer.MAX_VALUE;\r\n        if (hostName != null) {\r\n            ParsedHost host = getAndRecordParsedHost(hostName);\r\n            if (host != null) {\r\n                attempt.setHostName(host.getNodeName(), host.getRackName());\r\n                attempt.setLocation(host.makeLoggedLocation());\r\n            } else {\r\n                attempt.setHostName(hostName, null);\r\n            }\r\n            List<LoggedLocation> locs = task.getPreferredLocations();\r\n            if (host != null && locs != null) {\r\n                for (LoggedLocation loc : locs) {\r\n                    ParsedHost preferedLoc = new ParsedHost(loc);\r\n                    distance = Math.min(distance, preferedLoc.distance(host));\r\n                }\r\n            }\r\n            mapperLocality.enter(distance);\r\n        }\r\n        distance = Math.min(distance, successfulMapAttemptTimes.length - 1);\r\n        if (attempt.getStartTime() > 0 && attempt.getFinishTime() > 0) {\r\n            long runtime = attempt.getFinishTime() - attempt.getStartTime();\r\n            if (stat == Pre21JobHistoryConstants.Values.SUCCESS) {\r\n                successfulMapAttemptTimes[distance].enter(runtime);\r\n            }\r\n            if (stat == Pre21JobHistoryConstants.Values.FAILED) {\r\n                failedMapAttemptTimes[distance].enter(runtime);\r\n            }\r\n        }\r\n        if (attemptID != null) {\r\n            Matcher matcher = taskAttemptIDPattern.matcher(attemptID);\r\n            if (matcher.matches()) {\r\n                String attemptNumberString = matcher.group(1);\r\n                if (attemptNumberString != null) {\r\n                    int attemptNumber = Integer.parseInt(attemptNumberString);\r\n                    successfulNthMapperAttempts.enter(attemptNumber);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    try {\r\n        if (attemptStartTime != null) {\r\n            long startTimeValue = Long.parseLong(attemptStartTime);\r\n            if (startTimeValue != 0 && startTimeValue + MAXIMUM_CLOCK_SKEW >= launchTimeCurrentJob) {\r\n                taskAttemptStartTimes.put(attemptID, startTimeValue);\r\n            } else {\r\n                taskAttemptStartTimes.remove(attemptID);\r\n            }\r\n        } else if (status != null && attemptFinishTime != null) {\r\n            long finishTime = Long.parseLong(attemptFinishTime);\r\n            if (status.equals(\"SUCCESS\")) {\r\n                taskMapAttemptFinishTimes.put(attemptID, finishTime);\r\n            }\r\n        }\r\n    } catch (NumberFormatException e) {\r\n        LOG.warn(\"HadoopLogsAnalyzer.processMapAttemptLine: bad numerical format, at line\" + lineNumber + \".\", e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processReduceAttemptLine",
  "errType" : [ "IllegalArgumentException", "NumberFormatException" ],
  "containingMethodsNum" : 50,
  "sourceCodeText" : "void processReduceAttemptLine(ParsedLine line)\n{\r\n    String attemptID = line.get(\"TASK_ATTEMPT_ID\");\r\n    String taskID = line.get(\"TASKID\");\r\n    String status = line.get(\"TASK_STATUS\");\r\n    String attemptStartTime = line.get(\"START_TIME\");\r\n    String attemptFinishTime = line.get(\"FINISH_TIME\");\r\n    String attemptShuffleFinished = line.get(\"SHUFFLE_FINISHED\");\r\n    String attemptSortFinished = line.get(\"SORT_FINISHED\");\r\n    String counters = line.get(\"COUNTERS\");\r\n    String hostName = line.get(\"HOSTNAME\");\r\n    if (hostName != null && !hostNames.contains(hostName)) {\r\n        hostNames.add(hostName);\r\n    }\r\n    if (jobBeingTraced != null && taskID != null) {\r\n        LoggedTask task = tasksInCurrentJob.get(taskID);\r\n        if (task == null) {\r\n            task = new LoggedTask();\r\n            task.setTaskID(taskID);\r\n            jobBeingTraced.getReduceTasks().add(task);\r\n            tasksInCurrentJob.put(taskID, task);\r\n        }\r\n        task.setTaskID(taskID);\r\n        LoggedTaskAttempt attempt = attemptsInCurrentJob.get(attemptID);\r\n        boolean attemptAlreadyExists = attempt != null;\r\n        if (attempt == null) {\r\n            attempt = new LoggedTaskAttempt();\r\n            attempt.setAttemptID(attemptID);\r\n        }\r\n        if (!attemptAlreadyExists) {\r\n            attemptsInCurrentJob.put(attemptID, attempt);\r\n            task.getAttempts().add(attempt);\r\n        }\r\n        Pre21JobHistoryConstants.Values stat = null;\r\n        try {\r\n            stat = status == null ? null : Pre21JobHistoryConstants.Values.valueOf(status);\r\n        } catch (IllegalArgumentException e) {\r\n            LOG.warn(\"A map attempt status you don't know about is \\\"\" + status + \"\\\".\", e);\r\n            stat = null;\r\n        }\r\n        incorporateCounters(attempt, counters);\r\n        attempt.setResult(stat);\r\n        if (attemptStartTime != null) {\r\n            attempt.setStartTime(Long.parseLong(attemptStartTime));\r\n        }\r\n        if (attemptFinishTime != null) {\r\n            attempt.setFinishTime(Long.parseLong(attemptFinishTime));\r\n        }\r\n        if (attemptShuffleFinished != null) {\r\n            attempt.setShuffleFinished(Long.parseLong(attemptShuffleFinished));\r\n        }\r\n        if (attemptSortFinished != null) {\r\n            attempt.setSortFinished(Long.parseLong(attemptSortFinished));\r\n        }\r\n        if (attempt.getStartTime() > 0 && attempt.getFinishTime() > 0) {\r\n            long runtime = attempt.getFinishTime() - attempt.getStartTime();\r\n            if (stat == Pre21JobHistoryConstants.Values.SUCCESS) {\r\n                successfulReduceAttemptTimes.enter(runtime);\r\n            }\r\n            if (stat == Pre21JobHistoryConstants.Values.FAILED) {\r\n                failedReduceAttemptTimes.enter(runtime);\r\n            }\r\n        }\r\n        if (hostName != null) {\r\n            ParsedHost host = getAndRecordParsedHost(hostName);\r\n            if (host != null) {\r\n                attempt.setHostName(host.getNodeName(), host.getRackName());\r\n            } else {\r\n                attempt.setHostName(hostName, null);\r\n            }\r\n        }\r\n        if (attemptID != null) {\r\n            Matcher matcher = taskAttemptIDPattern.matcher(attemptID);\r\n            if (matcher.matches()) {\r\n                String attemptNumberString = matcher.group(1);\r\n                if (attemptNumberString != null) {\r\n                    int attemptNumber = Integer.parseInt(attemptNumberString);\r\n                    successfulNthReducerAttempts.enter(attemptNumber);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    try {\r\n        if (attemptStartTime != null) {\r\n            long startTimeValue = Long.parseLong(attemptStartTime);\r\n            if (startTimeValue != 0 && startTimeValue + MAXIMUM_CLOCK_SKEW >= launchTimeCurrentJob) {\r\n                taskAttemptStartTimes.put(attemptID, startTimeValue);\r\n            }\r\n        } else if (status != null && status.equals(\"SUCCESS\") && attemptFinishTime != null) {\r\n            long finishTime = Long.parseLong(attemptFinishTime);\r\n            taskReduceAttemptFinishTimes.put(attemptID, finishTime);\r\n            if (attemptShuffleFinished != null) {\r\n                taskReduceAttemptShuffleEndTimes.put(attemptID, Long.parseLong(attemptShuffleFinished));\r\n            }\r\n            if (attemptSortFinished != null) {\r\n                taskReduceAttemptSortEndTimes.put(attemptID, Long.parseLong(attemptSortFinished));\r\n            }\r\n        }\r\n    } catch (NumberFormatException e) {\r\n        LOG.error(\"HadoopLogsAnalyzer.processReduceAttemptLine: bad numerical format, at line\" + lineNumber + \".\", e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processParsedLine",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void processParsedLine(ParsedLine line) throws JsonProcessingException, IOException\n{\r\n    if (!collecting) {\r\n        LogRecordType myType = line.getType();\r\n        if (myType == canonicalJob) {\r\n            processJobLine(line);\r\n        } else if (myType == canonicalTask) {\r\n            processTaskLine(line);\r\n        } else if (myType == canonicalMapAttempt) {\r\n            processMapAttemptLine(line);\r\n        } else if (myType == canonicalReduceAttempt) {\r\n            processReduceAttemptLine(line);\r\n        } else {\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "printDistributionSet",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void printDistributionSet(String title, Histogram[][] distSet)\n{\r\n    statisticalOutput.print(title + \"\\n\\n\");\r\n    for (int i = 0; i < JobOutcome.values().length; ++i) {\r\n        for (int j = 0; j < LoggedJob.JobType.values().length; ++j) {\r\n            JobOutcome thisOutcome = JobOutcome.values()[i];\r\n            LoggedJob.JobType thisType = LoggedJob.JobType.values()[j];\r\n            statisticalOutput.print(\"outcome = \");\r\n            statisticalOutput.print(thisOutcome.toString());\r\n            statisticalOutput.print(\", and type = \");\r\n            statisticalOutput.print(thisType.toString());\r\n            statisticalOutput.print(\".\\n\\n\");\r\n            Histogram dist = distSet[i][j];\r\n            printSingleDistributionData(dist);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "printSingleDistributionData",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void printSingleDistributionData(Histogram dist)\n{\r\n    int[] percentiles = new int[numberBuckets];\r\n    for (int k = 0; k < numberBuckets; ++k) {\r\n        percentiles[k] = k + 1;\r\n    }\r\n    long[] cdf = dist.getCDF(numberBuckets + 1, percentiles);\r\n    if (cdf == null) {\r\n        statisticalOutput.print(\"(No data)\\n\");\r\n    } else {\r\n        statisticalOutput.print(\"min:  \");\r\n        statisticalOutput.print(cdf[0]);\r\n        statisticalOutput.print(\"\\n\");\r\n        for (int k = 0; k < numberBuckets; ++k) {\r\n            statisticalOutput.print(percentiles[k]);\r\n            statisticalOutput.print(\"%   \");\r\n            statisticalOutput.print(cdf[k + 1]);\r\n            statisticalOutput.print(\"\\n\");\r\n        }\r\n        statisticalOutput.print(\"max:  \");\r\n        statisticalOutput.print(cdf[numberBuckets + 1]);\r\n        statisticalOutput.print(\"\\n\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "maybeMateJobAndConf",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void maybeMateJobAndConf() throws IOException\n{\r\n    if (jobBeingTraced != null && jobconf != null && jobBeingTraced.getJobID().toString().equals(jobconf.jobID)) {\r\n        jobBeingTraced.setHeapMegabytes(jobconf.heapMegabytes);\r\n        jobBeingTraced.setQueue(jobconf.queue);\r\n        jobBeingTraced.setJobName(jobconf.jobName);\r\n        jobBeingTraced.setClusterMapMB(jobconf.clusterMapMB);\r\n        jobBeingTraced.setClusterReduceMB(jobconf.clusterReduceMB);\r\n        jobBeingTraced.setJobMapMB(jobconf.jobMapMB);\r\n        jobBeingTraced.setJobReduceMB(jobconf.jobReduceMB);\r\n        jobBeingTraced.setJobProperties(jobconf.properties);\r\n        jobconf = null;\r\n        finalizeJob();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "mapCDFArrayList",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ArrayList<LoggedDiscreteCDF> mapCDFArrayList(Histogram[] data)\n{\r\n    ArrayList<LoggedDiscreteCDF> result = new ArrayList<LoggedDiscreteCDF>();\r\n    for (Histogram hist : data) {\r\n        LoggedDiscreteCDF discCDF = new LoggedDiscreteCDF();\r\n        discCDF.setCDF(hist, attemptTimesPercentiles, 100);\r\n        result.add(discCDF);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "finalizeJob",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void finalizeJob() throws IOException\n{\r\n    if (jobBeingTraced != null) {\r\n        if (omitTaskDetails) {\r\n            jobBeingTraced.setMapTasks(null);\r\n            jobBeingTraced.setReduceTasks(null);\r\n            jobBeingTraced.setOtherTasks(null);\r\n        }\r\n        jobBeingTraced.setSuccessfulMapAttemptCDFs(mapCDFArrayList(successfulMapAttemptTimes));\r\n        jobBeingTraced.setFailedMapAttemptCDFs(mapCDFArrayList(failedMapAttemptTimes));\r\n        LoggedDiscreteCDF discCDF = new LoggedDiscreteCDF();\r\n        discCDF.setCDF(successfulReduceAttemptTimes, attemptTimesPercentiles, 100);\r\n        jobBeingTraced.setSuccessfulReduceAttemptCDF(discCDF);\r\n        discCDF = new LoggedDiscreteCDF();\r\n        discCDF.setCDF(failedReduceAttemptTimes, attemptTimesPercentiles, 100);\r\n        jobBeingTraced.setFailedReduceAttemptCDF(discCDF);\r\n        long totalSuccessfulAttempts = 0L;\r\n        long maxTriesToSucceed = 0L;\r\n        for (Map.Entry<Long, Long> ent : successfulNthMapperAttempts) {\r\n            totalSuccessfulAttempts += ent.getValue();\r\n            maxTriesToSucceed = Math.max(maxTriesToSucceed, ent.getKey());\r\n        }\r\n        if (totalSuccessfulAttempts > 0L) {\r\n            double[] successAfterI = new double[(int) maxTriesToSucceed + 1];\r\n            for (int i = 0; i < successAfterI.length; ++i) {\r\n                successAfterI[i] = 0.0D;\r\n            }\r\n            for (Map.Entry<Long, Long> ent : successfulNthMapperAttempts) {\r\n                successAfterI[ent.getKey().intValue()] = ((double) ent.getValue()) / totalSuccessfulAttempts;\r\n            }\r\n            jobBeingTraced.setMapperTriesToSucceed(successAfterI);\r\n        } else {\r\n            jobBeingTraced.setMapperTriesToSucceed(null);\r\n        }\r\n        jobTraceGen.output(jobBeingTraced);\r\n        jobBeingTraced = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int run(String[] args) throws IOException\n{\r\n    int result = initializeHadoopLogsAnalyzer(args);\r\n    if (result != 0) {\r\n        return result;\r\n    }\r\n    return run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "run",
  "errType" : [ "StringIndexOutOfBoundsException" ],
  "containingMethodsNum" : 28,
  "sourceCodeText" : "int run() throws IOException\n{\r\n    Pair<String, String> line = readBalancedLine();\r\n    while (line != null) {\r\n        if (debug && (lineNumber < 1000000L && lineNumber % 1000L == 0 || lineNumber % 1000000L == 0)) {\r\n            LOG.debug(\"\" + lineNumber + \" \" + line.second());\r\n        }\r\n        if (line.first() == null) {\r\n            try {\r\n                processParsedLine(new ParsedLine(line.second(), version));\r\n            } catch (StringIndexOutOfBoundsException e) {\r\n                LOG.warn(\"anomalous line #\" + lineNumber + \":\" + line, e);\r\n            }\r\n        } else {\r\n            jobconf = new ParsedConfigFile(line.first(), line.second());\r\n            if (jobconf.valid == false) {\r\n                jobconf = null;\r\n            }\r\n            maybeMateJobAndConf();\r\n        }\r\n        line = readBalancedLine();\r\n    }\r\n    finalizeJob();\r\n    if (collecting) {\r\n        String[] typeNames = LogRecordType.lineTypes();\r\n        for (int i = 0; i < typeNames.length; ++i) {\r\n            statisticalOutput.print(typeNames[i]);\r\n            statisticalOutput.print('\\n');\r\n        }\r\n    } else {\r\n        if (delays) {\r\n            printDistributionSet(\"Job start delay spectrum:\", delayTimeDists);\r\n        }\r\n        if (runtimes) {\r\n            printDistributionSet(\"Job run time spectrum:\", runTimeDists);\r\n        }\r\n        if (spreading) {\r\n            String ratioDescription = \"(\" + spreadMax + \"/1000 %ile) to (\" + spreadMin + \"/1000 %ile) scaled by 1000000\";\r\n            printDistributionSet(\"Map task success times \" + ratioDescription + \":\", mapTimeSpreadDists);\r\n            printDistributionSet(\"Shuffle success times \" + ratioDescription + \":\", shuffleTimeSpreadDists);\r\n            printDistributionSet(\"Sort success times \" + ratioDescription + \":\", sortTimeSpreadDists);\r\n            printDistributionSet(\"Reduce success times \" + ratioDescription + \":\", reduceTimeSpreadDists);\r\n        }\r\n        if (collectTaskTimes) {\r\n            printDistributionSet(\"Global map task success times:\", mapTimeDists);\r\n            printDistributionSet(\"Global shuffle task success times:\", shuffleTimeDists);\r\n            printDistributionSet(\"Global sort task success times:\", sortTimeDists);\r\n            printDistributionSet(\"Global reduce task success times:\", reduceTimeDists);\r\n        }\r\n    }\r\n    if (topologyGen != null) {\r\n        LoggedNetworkTopology topo = new LoggedNetworkTopology(allHosts, \"<root>\", 0);\r\n        topologyGen.output(topo);\r\n        topologyGen.close();\r\n    }\r\n    if (jobTraceGen != null) {\r\n        jobTraceGen.close();\r\n    }\r\n    if (input != null) {\r\n        input.close();\r\n        input = null;\r\n    }\r\n    if (inputCodec != null) {\r\n        CodecPool.returnDecompressor(inputDecompressor);\r\n        inputDecompressor = null;\r\n        inputCodec = null;\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "main",
  "errType" : [ "FileNotFoundException", "IOException", "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    try {\r\n        HadoopLogsAnalyzer analyzer = new HadoopLogsAnalyzer();\r\n        int result = ToolRunner.run(analyzer, args);\r\n        if (result == 0) {\r\n            return;\r\n        }\r\n        System.exit(result);\r\n    } catch (FileNotFoundException e) {\r\n        LOG.error(\"\", e);\r\n        e.printStackTrace(staticDebugOutput);\r\n        System.exit(1);\r\n    } catch (IOException e) {\r\n        LOG.error(\"\", e);\r\n        e.printStackTrace(staticDebugOutput);\r\n        System.exit(2);\r\n    } catch (Exception e) {\r\n        LOG.error(\"\", e);\r\n        e.printStackTrace(staticDebugOutput);\r\n        System.exit(3);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setUnknownAttribute",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUnknownAttribute(String attributeName, Object ignored)\n{\r\n    if (!alreadySeenAnySetterAttributes.contains(attributeName)) {\r\n        alreadySeenAnySetterAttributes.add(attributeName);\r\n        System.err.println(\"In LoggedJob, we saw the unknown attribute \" + attributeName + \".\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NodeName getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setName(String name)\n{\r\n    this.name = new NodeName(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getChildren",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<LoggedNetworkTopology> getChildren()\n{\r\n    return children;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setChildren",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setChildren(List<LoggedNetworkTopology> children)\n{\r\n    this.children = children;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void compare1(List<LoggedNetworkTopology> c1, List<LoggedNetworkTopology> c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    if (c1 == null || c2 == null || c1.size() != c2.size()) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n    Collections.sort(c1, new TopoSort());\r\n    Collections.sort(c2, new TopoSort());\r\n    for (int i = 0; i < c1.size(); ++i) {\r\n        c1.get(i).deepCompare(c2.get(i), new TreePath(loc, eltname, i));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "deepCompare",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deepCompare(DeepCompare comparand, TreePath loc) throws DeepInequalityException\n{\r\n    if (!(comparand instanceof LoggedNetworkTopology)) {\r\n        throw new DeepInequalityException(\"comparand has wrong type\", loc);\r\n    }\r\n    LoggedNetworkTopology other = (LoggedNetworkTopology) comparand;\r\n    compare1(children, other.children, loc, \"children\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "finalSEEs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<SingleEventEmitter> finalSEEs()\n{\r\n    return finals;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "nonFinalSEEs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<SingleEventEmitter> nonFinalSEEs()\n{\r\n    return nonFinals;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getRunState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "State getRunState()\n{\r\n    return state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getRuntime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getRuntime()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getTaskInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskInfo getTaskInfo()\n{\r\n    return taskInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getSplitVector",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<Integer> getSplitVector(LoggedTaskAttempt.SplitVectorKind kind)\n{\r\n    return kind.get(allSplits);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "rawNextJob",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "LoggedJob rawNextJob() throws IOException\n{\r\n    LoggedJob result = reader.getNext();\r\n    if ((!abortOnUnfixableSkew || skewBufferLength > 0) && result != null) {\r\n        long thisTime = result.getSubmitTime();\r\n        if (submitTimesSoFar.contains(thisTime)) {\r\n            Integer myCount = countedRepeatedSubmitTimesSoFar.get(thisTime);\r\n            countedRepeatedSubmitTimesSoFar.put(thisTime, myCount == null ? 2 : myCount + 1);\r\n        } else {\r\n            submitTimesSoFar.add(thisTime);\r\n        }\r\n        if (thisTime < skewMeasurementLatestSubmitTime) {\r\n            Iterator<Long> endCursor = submitTimesSoFar.descendingIterator();\r\n            int thisJobNeedsSkew = 0;\r\n            Long keyNeedingSkew;\r\n            while (endCursor.hasNext() && (keyNeedingSkew = endCursor.next()) > thisTime) {\r\n                Integer keyNeedsSkewAmount = countedRepeatedSubmitTimesSoFar.get(keyNeedingSkew);\r\n                thisJobNeedsSkew += keyNeedsSkewAmount == null ? 1 : keyNeedsSkewAmount;\r\n            }\r\n            maxSkewBufferNeeded = Math.max(maxSkewBufferNeeded, thisJobNeedsSkew);\r\n        }\r\n        skewMeasurementLatestSubmitTime = Math.max(thisTime, skewMeasurementLatestSubmitTime);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "nextJob",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "LoggedJob nextJob() throws IOException, OutOfOrderException\n{\r\n    LoggedJob newJob = rawNextJob();\r\n    if (newJob != null) {\r\n        skewBuffer.add(newJob);\r\n    }\r\n    LoggedJob result = skewBuffer.poll();\r\n    while (result != null && result.getSubmitTime() < returnedLatestSubmitTime) {\r\n        LOG.error(\"The current job was submitted earlier than the previous one\");\r\n        LOG.error(\"Its jobID is \" + result.getJobID());\r\n        LOG.error(\"Its submit time is \" + result.getSubmitTime() + \",but the previous one was \" + returnedLatestSubmitTime);\r\n        if (abortOnUnfixableSkew) {\r\n            throw new OutOfOrderException(\"Job submit time is \" + result.getSubmitTime() + \",but the previous one was \" + returnedLatestSubmitTime);\r\n        }\r\n        result = rawNextJob();\r\n    }\r\n    if (result != null) {\r\n        returnedLatestSubmitTime = result.getSubmitTime();\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "fillSkewBuffer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void fillSkewBuffer() throws IOException\n{\r\n    for (int i = 0; i < skewBufferLength; ++i) {\r\n        LoggedJob newJob = rawNextJob();\r\n        if (newJob == null) {\r\n            return;\r\n        }\r\n        skewBuffer.add(newJob);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "neededSkewBufferSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int neededSkewBufferSize()\n{\r\n    return maxSkewBufferNeeded;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    reader.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getReduceRuntime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getReduceRuntime()\n{\r\n    return reduceTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getShuffleRuntime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getShuffleRuntime()\n{\r\n    return shuffleTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMergeRuntime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMergeRuntime()\n{\r\n    return mergeTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getRuntime",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getRuntime()\n{\r\n    return (getShuffleRuntime() + getMergeRuntime() + getReduceRuntime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getValue()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "allSplitVectors",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<List<Integer>> allSplitVectors()\n{\r\n    List<List<Integer>> result = new ArrayList<List<Integer>>(SplitVectorKind.values().length);\r\n    for (SplitVectorKind kind : SplitVectorKind.values()) {\r\n        result.add(kind.get(this));\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setUnknownAttribute",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUnknownAttribute(String attributeName, Object ignored)\n{\r\n    if (!alreadySeenAnySetterAttributes.contains(attributeName)) {\r\n        alreadySeenAnySetterAttributes.add(attributeName);\r\n        System.err.println(\"In LoggedJob, we saw the unknown attribute \" + attributeName + \".\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getClockSplits",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<Integer> getClockSplits()\n{\r\n    return clockSplits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setClockSplits",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setClockSplits(List<Integer> clockSplits)\n{\r\n    this.clockSplits = clockSplits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "arraySetClockSplits",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void arraySetClockSplits(int[] clockSplits)\n{\r\n    List<Integer> result = new ArrayList<Integer>();\r\n    for (int i = 0; i < clockSplits.length; ++i) {\r\n        result.add(clockSplits[i]);\r\n    }\r\n    this.clockSplits = result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getCpuUsages",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<Integer> getCpuUsages()\n{\r\n    return cpuUsages;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setCpuUsages",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCpuUsages(List<Integer> cpuUsages)\n{\r\n    this.cpuUsages = cpuUsages;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "arraySetCpuUsages",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void arraySetCpuUsages(int[] cpuUsages)\n{\r\n    List<Integer> result = new ArrayList<Integer>();\r\n    for (int i = 0; i < cpuUsages.length; ++i) {\r\n        result.add(cpuUsages[i]);\r\n    }\r\n    this.cpuUsages = result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getVMemKbytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<Integer> getVMemKbytes()\n{\r\n    return vMemKbytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setVMemKbytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setVMemKbytes(List<Integer> vMemKbytes)\n{\r\n    this.vMemKbytes = vMemKbytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "arraySetVMemKbytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void arraySetVMemKbytes(int[] vMemKbytes)\n{\r\n    List<Integer> result = new ArrayList<Integer>();\r\n    for (int i = 0; i < vMemKbytes.length; ++i) {\r\n        result.add(vMemKbytes[i]);\r\n    }\r\n    this.vMemKbytes = result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getPhysMemKbytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<Integer> getPhysMemKbytes()\n{\r\n    return physMemKbytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setPhysMemKbytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setPhysMemKbytes(List<Integer> physMemKbytes)\n{\r\n    this.physMemKbytes = physMemKbytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "arraySetPhysMemKbytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void arraySetPhysMemKbytes(int[] physMemKbytes)\n{\r\n    List<Integer> result = new ArrayList<Integer>();\r\n    for (int i = 0; i < physMemKbytes.length; ++i) {\r\n        result.add(physMemKbytes[i]);\r\n    }\r\n    this.physMemKbytes = result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "adjustTimes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void adjustTimes(long adjustment)\n{\r\n    startTime += adjustment;\r\n    finishTime += adjustment;\r\n    if (sortFinished >= 0) {\r\n        shuffleFinished += adjustment;\r\n        sortFinished += adjustment;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getShuffleFinished",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getShuffleFinished()\n{\r\n    return shuffleFinished;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setShuffleFinished",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setShuffleFinished(long shuffleFinished)\n{\r\n    this.shuffleFinished = shuffleFinished;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getSortFinished",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSortFinished()\n{\r\n    return sortFinished;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setSortFinished",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSortFinished(long sortFinished)\n{\r\n    this.sortFinished = sortFinished;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getAttemptID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getAttemptID()\n{\r\n    return attemptID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setAttemptID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setAttemptID(String attemptID)\n{\r\n    this.attemptID = TaskAttemptID.forName(attemptID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Pre21JobHistoryConstants.Values getResult()\n{\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setResult(Pre21JobHistoryConstants.Values result)\n{\r\n    this.result = result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStartTime(long startTime)\n{\r\n    this.startTime = startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFinishTime(long finishTime)\n{\r\n    this.finishTime = finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getHostName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NodeName getHostName()\n{\r\n    return hostName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setHostName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHostName(String hostName)\n{\r\n    this.hostName = hostName == null ? null : new NodeName(hostName);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setHostName",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setHostName(String hostName, String rackName)\n{\r\n    if (hostName == null || hostName.length() == 0) {\r\n        throw new RuntimeException(\"Invalid entry! Missing hostname\");\r\n    } else if (rackName == null || rackName.length() == 0) {\r\n        setHostName(hostName);\r\n    } else {\r\n        if (!rackName.startsWith(\"/\")) {\r\n            rackName = \"/\" + rackName;\r\n        }\r\n        if (!hostName.startsWith(\"/\")) {\r\n            hostName = \"/\" + hostName;\r\n        }\r\n        setHostName(rackName.intern() + hostName.intern());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getHdfsBytesRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getHdfsBytesRead()\n{\r\n    return hdfsBytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setHdfsBytesRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHdfsBytesRead(long hdfsBytesRead)\n{\r\n    this.hdfsBytesRead = hdfsBytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getHdfsBytesWritten",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getHdfsBytesWritten()\n{\r\n    return hdfsBytesWritten;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setHdfsBytesWritten",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHdfsBytesWritten(long hdfsBytesWritten)\n{\r\n    this.hdfsBytesWritten = hdfsBytesWritten;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getFileBytesRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFileBytesRead()\n{\r\n    return fileBytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setFileBytesRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFileBytesRead(long fileBytesRead)\n{\r\n    this.fileBytesRead = fileBytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getFileBytesWritten",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFileBytesWritten()\n{\r\n    return fileBytesWritten;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setFileBytesWritten",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFileBytesWritten(long fileBytesWritten)\n{\r\n    this.fileBytesWritten = fileBytesWritten;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMapInputRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMapInputRecords()\n{\r\n    return mapInputRecords;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setMapInputRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMapInputRecords(long mapInputRecords)\n{\r\n    this.mapInputRecords = mapInputRecords;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMapOutputBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMapOutputBytes()\n{\r\n    return mapOutputBytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setMapOutputBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMapOutputBytes(long mapOutputBytes)\n{\r\n    this.mapOutputBytes = mapOutputBytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMapOutputRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMapOutputRecords()\n{\r\n    return mapOutputRecords;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setMapOutputRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMapOutputRecords(long mapOutputRecords)\n{\r\n    this.mapOutputRecords = mapOutputRecords;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getCombineInputRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCombineInputRecords()\n{\r\n    return combineInputRecords;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setCombineInputRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCombineInputRecords(long combineInputRecords)\n{\r\n    this.combineInputRecords = combineInputRecords;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getReduceInputGroups",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getReduceInputGroups()\n{\r\n    return reduceInputGroups;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setReduceInputGroups",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setReduceInputGroups(long reduceInputGroups)\n{\r\n    this.reduceInputGroups = reduceInputGroups;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getReduceInputRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getReduceInputRecords()\n{\r\n    return reduceInputRecords;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setReduceInputRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setReduceInputRecords(long reduceInputRecords)\n{\r\n    this.reduceInputRecords = reduceInputRecords;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getReduceShuffleBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getReduceShuffleBytes()\n{\r\n    return reduceShuffleBytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setReduceShuffleBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setReduceShuffleBytes(long reduceShuffleBytes)\n{\r\n    this.reduceShuffleBytes = reduceShuffleBytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getReduceOutputRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getReduceOutputRecords()\n{\r\n    return reduceOutputRecords;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setReduceOutputRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setReduceOutputRecords(long reduceOutputRecords)\n{\r\n    this.reduceOutputRecords = reduceOutputRecords;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getSpilledRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSpilledRecords()\n{\r\n    return spilledRecords;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setSpilledRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSpilledRecords(long spilledRecords)\n{\r\n    this.spilledRecords = spilledRecords;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getLocation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LoggedLocation getLocation()\n{\r\n    return location;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setLocation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLocation(LoggedLocation location)\n{\r\n    this.location = location;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMapInputBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMapInputBytes()\n{\r\n    return mapInputBytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setMapInputBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMapInputBytes(long mapInputBytes)\n{\r\n    this.mapInputBytes = mapInputBytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "incorporateCounters",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void incorporateCounters(JhCounters counters)\n{\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.hdfsBytesRead = val;\r\n        }\r\n    }, counters, \"HDFS_BYTES_READ\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.hdfsBytesWritten = val;\r\n        }\r\n    }, counters, \"HDFS_BYTES_WRITTEN\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.fileBytesRead = val;\r\n        }\r\n    }, counters, \"FILE_BYTES_READ\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.fileBytesWritten = val;\r\n        }\r\n    }, counters, \"FILE_BYTES_WRITTEN\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.mapInputBytes = val;\r\n        }\r\n    }, counters, \"MAP_INPUT_BYTES\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.mapInputRecords = val;\r\n        }\r\n    }, counters, \"MAP_INPUT_RECORDS\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.mapOutputBytes = val;\r\n        }\r\n    }, counters, \"MAP_OUTPUT_BYTES\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.mapOutputRecords = val;\r\n        }\r\n    }, counters, \"MAP_OUTPUT_RECORDS\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.combineInputRecords = val;\r\n        }\r\n    }, counters, \"COMBINE_INPUT_RECORDS\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.reduceInputGroups = val;\r\n        }\r\n    }, counters, \"REDUCE_INPUT_GROUPS\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.reduceInputRecords = val;\r\n        }\r\n    }, counters, \"REDUCE_INPUT_RECORDS\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.reduceShuffleBytes = val;\r\n        }\r\n    }, counters, \"REDUCE_SHUFFLE_BYTES\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.reduceOutputRecords = val;\r\n        }\r\n    }, counters, \"REDUCE_OUTPUT_RECORDS\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            attempt.spilledRecords = val;\r\n        }\r\n    }, counters, \"SPILLED_RECORDS\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            metrics.setCumulativeCpuUsage(val);\r\n        }\r\n    }, counters, \"CPU_MILLISECONDS\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            metrics.setVirtualMemoryUsage(val);\r\n        }\r\n    }, counters, \"VIRTUAL_MEMORY_BYTES\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            metrics.setPhysicalMemoryUsage(val);\r\n        }\r\n    }, counters, \"PHYSICAL_MEMORY_BYTES\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            metrics.setHeapUsage(val);\r\n        }\r\n    }, counters, \"COMMITTED_HEAP_BYTES\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getResourceUsageMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ResourceUsageMetrics getResourceUsageMetrics()\n{\r\n    return metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setResourceUsageMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setResourceUsageMetrics(ResourceUsageMetrics metrics)\n{\r\n    this.metrics = metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "canonicalizeCounterName",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String canonicalizeCounterName(String nonCanonicalName)\n{\r\n    String result = StringUtils.toLowerCase(nonCanonicalName);\r\n    result = result.replace(' ', '|');\r\n    result = result.replace('-', '|');\r\n    result = result.replace('_', '|');\r\n    result = result.replace('.', '|');\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "incorporateCounter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void incorporateCounter(SetField thunk, JhCounters counters, String counterName)\n{\r\n    counterName = canonicalizeCounterName(counterName);\r\n    for (JhCounterGroup group : counters.groups) {\r\n        for (JhCounter counter : group.counts) {\r\n            if (counterName.equals(canonicalizeCounterName(counter.name.toString()))) {\r\n                thunk.set(counter.value);\r\n                return;\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void compare1(String c1, String c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    if (c1 == null || c2 == null || !c1.equals(c2)) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void compare1(NodeName c1, NodeName c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    if (c1 == null || c2 == null) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n    compare1(c1.getValue(), c2.getValue(), new TreePath(loc, eltname), \"value\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void compare1(long c1, long c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 != c2) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void compare1(Pre21JobHistoryConstants.Values c1, Pre21JobHistoryConstants.Values c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 != c2) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void compare1(LoggedLocation c1, LoggedLocation c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    TreePath recurse = new TreePath(loc, eltname);\r\n    if (c1 == null || c2 == null) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", recurse);\r\n    }\r\n    c1.deepCompare(c2, recurse);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void compare1(List<Integer> c1, List<Integer> c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    if (c1 == null || c2 == null || c1.size() != c2.size()) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n    for (int i = 0; i < c1.size(); ++i) {\r\n        if (!c1.get(i).equals(c2.get(i))) {\r\n            throw new DeepInequalityException(\"\" + c1.get(i) + \" != \" + c2.get(i), new TreePath(loc, eltname, i));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "deepCompare",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void deepCompare(DeepCompare comparand, TreePath loc) throws DeepInequalityException\n{\r\n    if (!(comparand instanceof LoggedTaskAttempt)) {\r\n        throw new DeepInequalityException(\"comparand has wrong type\", loc);\r\n    }\r\n    LoggedTaskAttempt other = (LoggedTaskAttempt) comparand;\r\n    compare1(attemptID.toString(), other.attemptID.toString(), loc, \"attemptID\");\r\n    compare1(result, other.result, loc, \"result\");\r\n    compare1(startTime, other.startTime, loc, \"startTime\");\r\n    compare1(finishTime, other.finishTime, loc, \"finishTime\");\r\n    compare1(hostName, other.hostName, loc, \"hostName\");\r\n    compare1(hdfsBytesRead, other.hdfsBytesRead, loc, \"hdfsBytesRead\");\r\n    compare1(hdfsBytesWritten, other.hdfsBytesWritten, loc, \"hdfsBytesWritten\");\r\n    compare1(fileBytesRead, other.fileBytesRead, loc, \"fileBytesRead\");\r\n    compare1(fileBytesWritten, other.fileBytesWritten, loc, \"fileBytesWritten\");\r\n    compare1(mapInputBytes, other.mapInputBytes, loc, \"mapInputBytes\");\r\n    compare1(mapInputRecords, other.mapInputRecords, loc, \"mapInputRecords\");\r\n    compare1(mapOutputBytes, other.mapOutputBytes, loc, \"mapOutputBytes\");\r\n    compare1(mapOutputRecords, other.mapOutputRecords, loc, \"mapOutputRecords\");\r\n    compare1(combineInputRecords, other.combineInputRecords, loc, \"combineInputRecords\");\r\n    compare1(reduceInputGroups, other.reduceInputGroups, loc, \"reduceInputGroups\");\r\n    compare1(reduceInputRecords, other.reduceInputRecords, loc, \"reduceInputRecords\");\r\n    compare1(reduceShuffleBytes, other.reduceShuffleBytes, loc, \"reduceShuffleBytes\");\r\n    compare1(reduceOutputRecords, other.reduceOutputRecords, loc, \"reduceOutputRecords\");\r\n    compare1(spilledRecords, other.spilledRecords, loc, \"spilledRecords\");\r\n    compare1(shuffleFinished, other.shuffleFinished, loc, \"shuffleFinished\");\r\n    compare1(sortFinished, other.sortFinished, loc, \"sortFinished\");\r\n    compare1(location, other.location, loc, \"location\");\r\n    compare1(clockSplits, other.clockSplits, loc, \"clockSplits\");\r\n    compare1(cpuUsages, other.cpuUsages, loc, \"cpuUsages\");\r\n    compare1(vMemKbytes, other.vMemKbytes, loc, \"vMemKbytes\");\r\n    compare1(physMemKbytes, other.physMemKbytes, loc, \"physMemKbytes\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getRuntime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getRuntime()\n{\r\n    return getMapRuntime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMapRuntime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMapRuntime()\n{\r\n    return runtime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getValue()\n{\r\n    return fileName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getAnonymizedValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getAnonymizedValue(StatePool statePool, Configuration conf)\n{\r\n    if (anonymizedFileName == null) {\r\n        anonymize(statePool, conf);\r\n    }\r\n    return anonymizedFileName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "anonymize",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void anonymize(StatePool statePool, Configuration conf)\n{\r\n    FileNameState fState = (FileNameState) statePool.getState(getClass());\r\n    if (fState == null) {\r\n        fState = new FileNameState();\r\n        statePool.addState(getClass(), fState);\r\n    }\r\n    String[] files = StringUtils.split(fileName);\r\n    String[] anonymizedFileNames = new String[files.length];\r\n    int i = 0;\r\n    for (String f : files) {\r\n        anonymizedFileNames[i++] = anonymize(statePool, conf, fState, f);\r\n    }\r\n    anonymizedFileName = StringUtils.arrayToString(anonymizedFileNames);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "anonymize",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String anonymize(StatePool statePool, Configuration conf, FileNameState fState, String fileName)\n{\r\n    String ret = null;\r\n    try {\r\n        URI uri = new URI(fileName);\r\n        ret = anonymizePath(uri.getPath(), fState.getDirectoryState(), fState.getFileNameState());\r\n        String authority = uri.getAuthority();\r\n        String scheme = uri.getScheme();\r\n        if (scheme != null) {\r\n            String anonymizedAuthority = \"\";\r\n            if (authority != null) {\r\n                NodeName hostName = new NodeName(null, uri.getHost());\r\n                anonymizedAuthority = hostName.getAnonymizedValue(statePool, conf);\r\n            }\r\n            ret = scheme + \"://\" + anonymizedAuthority + ret;\r\n        }\r\n    } catch (URISyntaxException use) {\r\n        throw new RuntimeException(use);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "anonymizePath",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "String anonymizePath(String path, WordList dState, WordList fState)\n{\r\n    StringBuilder buffer = new StringBuilder();\r\n    StringTokenizer tokenizer = new StringTokenizer(path, Path.SEPARATOR, true);\r\n    while (tokenizer.hasMoreTokens()) {\r\n        String token = tokenizer.nextToken();\r\n        if (Path.SEPARATOR.equals(token)) {\r\n            buffer.append(token);\r\n        } else if (Path.CUR_DIR.equals(token)) {\r\n            buffer.append(token);\r\n        } else if (PREV_DIR.equals(token)) {\r\n            buffer.append(token);\r\n        } else if (tokenizer.hasMoreTokens()) {\r\n            buffer.append(anonymize(token, dState));\r\n        } else {\r\n            buffer.append(anonymize(token, fState));\r\n        }\r\n    }\r\n    return buffer.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "anonymize",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String anonymize(String data, WordList wordList)\n{\r\n    if (data == null) {\r\n        return null;\r\n    }\r\n    if (WordListAnonymizerUtility.needsAnonymization(data)) {\r\n        String suffix = \"\";\r\n        String coreData = data;\r\n        if (WordListAnonymizerUtility.hasSuffix(data, KNOWN_SUFFIXES)) {\r\n            String[] split = WordListAnonymizerUtility.extractSuffix(data, KNOWN_SUFFIXES);\r\n            suffix = split[1];\r\n            coreData = split[0];\r\n        }\r\n        String anonymizedData = coreData;\r\n        if (!WordListAnonymizerUtility.isKnownData(coreData)) {\r\n            if (!wordList.contains(coreData)) {\r\n                wordList.add(coreData);\r\n            }\r\n            anonymizedData = wordList.getName() + wordList.indexOf(coreData);\r\n        }\r\n        return anonymizedData + suffix;\r\n    } else {\r\n        return data;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\anonymization",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\anonymization",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void add(String word)\n{\r\n    if (!contains(word)) {\r\n        int index = getSize();\r\n        list.put(word, index);\r\n        isUpdated = true;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\anonymization",
  "methodName" : "contains",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean contains(String word)\n{\r\n    return list.containsKey(word);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\anonymization",
  "methodName" : "indexOf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int indexOf(String word)\n{\r\n    return list.get(word);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\anonymization",
  "methodName" : "getSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getSize()\n{\r\n    return list.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\anonymization",
  "methodName" : "isUpdated",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isUpdated()\n{\r\n    return isUpdated;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\anonymization",
  "methodName" : "setSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSize(int size)\n{\r\n    list = new HashMap<String, Integer>(size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\anonymization",
  "methodName" : "setName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setName(String name)\n{\r\n    this.name = name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\anonymization",
  "methodName" : "getWords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, Integer> getWords()\n{\r\n    return list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\anonymization",
  "methodName" : "setWords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setWords(Map<String, Integer> list)\n{\r\n    this.list = list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "maybeGetIntValue",
  "errType" : [ "NumberFormatException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int maybeGetIntValue(String propName, String attr, String value, int oldValue)\n{\r\n    if (propName.equals(attr) && value != null) {\r\n        try {\r\n            return Integer.parseInt(value);\r\n        } catch (NumberFormatException e) {\r\n            return oldValue;\r\n        }\r\n    }\r\n    return oldValue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setCDF",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setCDF(Histogram data, int[] steps, int modulus)\n{\r\n    numberValues = data.getTotalCount();\r\n    long[] CDF = data.getCDF(modulus, steps);\r\n    if (CDF != null) {\r\n        minimum = CDF[0];\r\n        maximum = CDF[CDF.length - 1];\r\n        rankings = new ArrayList<LoggedSingleRelativeRanking>();\r\n        for (int i = 1; i < CDF.length - 1; ++i) {\r\n            LoggedSingleRelativeRanking srr = new LoggedSingleRelativeRanking();\r\n            srr.setRelativeRanking(((double) steps[i - 1]) / modulus);\r\n            srr.setDatum(CDF[i]);\r\n            rankings.add(srr);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMinimum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMinimum()\n{\r\n    return minimum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setMinimum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMinimum(long minimum)\n{\r\n    this.minimum = minimum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getRankings",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<LoggedSingleRelativeRanking> getRankings()\n{\r\n    return rankings;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setRankings",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRankings(List<LoggedSingleRelativeRanking> rankings)\n{\r\n    this.rankings = rankings;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMaximum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMaximum()\n{\r\n    return maximum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setMaximum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMaximum(long maximum)\n{\r\n    this.maximum = maximum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getNumberValues",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumberValues()\n{\r\n    return numberValues;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setNumberValues",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumberValues(long numberValues)\n{\r\n    this.numberValues = numberValues;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void compare1(long c1, long c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 != c2) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void compare1(List<LoggedSingleRelativeRanking> c1, List<LoggedSingleRelativeRanking> c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    if (c1 == null || c2 == null || c1.size() != c2.size()) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n    for (int i = 0; i < c1.size(); ++i) {\r\n        c1.get(i).deepCompare(c2.get(i), new TreePath(loc, eltname, i));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "deepCompare",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void deepCompare(DeepCompare comparand, TreePath loc) throws DeepInequalityException\n{\r\n    if (!(comparand instanceof LoggedDiscreteCDF)) {\r\n        throw new DeepInequalityException(\"comparand has wrong type\", loc);\r\n    }\r\n    LoggedDiscreteCDF other = (LoggedDiscreteCDF) comparand;\r\n    compare1(numberValues, other.numberValues, loc, \"numberValues\");\r\n    compare1(minimum, other.minimum, loc, \"minimum\");\r\n    compare1(maximum, other.maximum, loc, \"maximum\");\r\n    compare1(rankings, other.rankings, loc, \"rankings\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getSeed",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getSeed(String streamId, long masterSeed)\n{\r\n    MessageDigest md5 = md5Holder.get();\r\n    md5.reset();\r\n    String str = streamId + '/' + masterSeed;\r\n    byte[] digest = md5.digest(str.getBytes(UTF_8));\r\n    long seed = 0;\r\n    for (int i = 0; i < 8; i++) {\r\n        seed = (seed << 8) + ((int) digest[i] + 128);\r\n    }\r\n    return seed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getValue()\n{\r\n    return queueName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getPrefix",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPrefix()\n{\r\n    return \"queue\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "maybeEmitEvent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HistoryEvent maybeEmitEvent(ParsedLine line, String name, HistoryEventEmitter that)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "putTotalCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void putTotalCounters(Map<String, Long> totalCounters)\n{\r\n    this.totalCountersMap = totalCounters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainTotalCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, Long> obtainTotalCounters()\n{\r\n    return totalCountersMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "putMapCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void putMapCounters(Map<String, Long> mapCounters)\n{\r\n    this.mapCountersMap = mapCounters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainMapCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, Long> obtainMapCounters()\n{\r\n    return mapCountersMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "putReduceCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void putReduceCounters(Map<String, Long> reduceCounters)\n{\r\n    this.reduceCountersMap = reduceCounters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainReduceCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, Long> obtainReduceCounters()\n{\r\n    return reduceCountersMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "putJobConfPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void putJobConfPath(String confPath)\n{\r\n    jobConfPath = confPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainJobConfpath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String obtainJobConfpath()\n{\r\n    return jobConfPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "putJobAcls",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void putJobAcls(Map<JobACL, AccessControlList> acls)\n{\r\n    jobAcls = acls;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainJobAcls",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<JobACL, AccessControlList> obtainJobAcls()\n{\r\n    return jobAcls;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainMapTasks",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<ParsedTask> obtainMapTasks()\n{\r\n    List<LoggedTask> tasks = super.getMapTasks();\r\n    return convertTasks(tasks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainReduceTasks",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<ParsedTask> obtainReduceTasks()\n{\r\n    List<LoggedTask> tasks = super.getReduceTasks();\r\n    return convertTasks(tasks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "obtainOtherTasks",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<ParsedTask> obtainOtherTasks()\n{\r\n    List<LoggedTask> tasks = super.getOtherTasks();\r\n    return convertTasks(tasks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "convertTasks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<ParsedTask> convertTasks(List<LoggedTask> tasks)\n{\r\n    List<ParsedTask> result = new ArrayList<ParsedTask>();\r\n    for (LoggedTask t : tasks) {\r\n        if (t instanceof ParsedTask) {\r\n            result.add((ParsedTask) t);\r\n        } else {\r\n            throw new RuntimeException(\"Unexpected type of tasks in the list...\");\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "dumpParsedJob",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void dumpParsedJob()\n{\r\n    LOG.info(\"ParsedJob details:\" + obtainTotalCounters() + \";\" + obtainMapCounters() + \";\" + obtainReduceCounters() + \"\\n\" + obtainJobConfpath() + \"\\n\" + obtainJobAcls() + \";Q=\" + (getQueue() == null ? \"null\" : getQueue().getValue()));\r\n    List<ParsedTask> maps = obtainMapTasks();\r\n    for (ParsedTask task : maps) {\r\n        task.dumpParsedTask();\r\n    }\r\n    List<ParsedTask> reduces = obtainReduceTasks();\r\n    for (ParsedTask task : reduces) {\r\n        task.dumpParsedTask();\r\n    }\r\n    List<ParsedTask> others = obtainOtherTasks();\r\n    for (ParsedTask task : others) {\r\n        task.dumpParsedTask();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LogRecordType getType()\n{\r\n    return type;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String get(String key)\n{\r\n    return content.getProperty(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getLong",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getLong(String key)\n{\r\n    String val = get(key);\r\n    return Long.parseLong(val);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "numberOfDistances",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int numberOfDistances()\n{\r\n    return 3;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "nameComponent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String nameComponent(int i) throws IllegalArgumentException\n{\r\n    switch(i) {\r\n        case 0:\r\n            return rackName;\r\n        case 1:\r\n            return nodeName;\r\n        default:\r\n            throw new IllegalArgumentException(\"Host location component index out of range.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int hashCode()\n{\r\n    return rackName.hashCode() * 17 + nodeName.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ParsedHost parse(String name)\n{\r\n    Matcher matcher = splitPattern.matcher(name);\r\n    if (!matcher.matches())\r\n        return null;\r\n    return new ParsedHost(matcher.group(1), matcher.group(2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "process",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String process(String name)\n{\r\n    return name == null ? null : name.startsWith(\"/\") ? name.substring(1) : name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "makeLoggedLocation",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "LoggedLocation makeLoggedLocation()\n{\r\n    LoggedLocation result = new LoggedLocation();\r\n    List<String> coordinates = new ArrayList<String>();\r\n    coordinates.add(rackName);\r\n    coordinates.add(nodeName);\r\n    result.setLayers(coordinates);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getNodeName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNodeName()\n{\r\n    return nodeName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getRackName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRackName()\n{\r\n    return rackName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean equals(Object other)\n{\r\n    if (!(other instanceof ParsedHost)) {\r\n        return false;\r\n    }\r\n    ParsedHost host = (ParsedHost) other;\r\n    return (nodeName.equals(host.nodeName) && rackName.equals(host.rackName));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "distance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int distance(ParsedHost other)\n{\r\n    if (nodeName.equals(other.nodeName)) {\r\n        return 0;\r\n    }\r\n    if (rackName.equals(other.rackName)) {\r\n        return 1;\r\n    }\r\n    return 2;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "addChild",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean addChild(Node child)\n{\r\n    if (!(child instanceof MachineNode)) {\r\n        throw new IllegalArgumentException(\"Only MachineNode can be added to RackNode\");\r\n    }\r\n    return super.addChild(child);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMachinesInRack",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<MachineNode> getMachinesInRack()\n{\r\n    return (Set<MachineNode>) (Set) getChildren();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\serializers",
  "methodName" : "serialize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serialize(String object, JsonGenerator jGen, SerializerProvider sProvider) throws IOException, JsonProcessingException\n{\r\n    jGen.writeNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "main",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    TraceBuilder builder = new TraceBuilder();\r\n    int result = RUN_METHOD_FAILED_EXIT_CODE;\r\n    try {\r\n        result = ToolRunner.run(builder, args);\r\n    } catch (Throwable t) {\r\n        t.printStackTrace(System.err);\r\n    } finally {\r\n        try {\r\n            builder.finish();\r\n        } finally {\r\n            if (result == 0) {\r\n                return;\r\n            }\r\n            System.exit(result);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "run",
  "errType" : [ "IOException", "Throwable" ],
  "containingMethodsNum" : 27,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    MyOptions options = new MyOptions(args, getConf());\r\n    traceWriter = options.clazzTraceOutputter.newInstance();\r\n    traceWriter.init(options.traceOutput, getConf());\r\n    topologyWriter = new DefaultOutputter<LoggedNetworkTopology>();\r\n    topologyWriter.init(options.topologyOutput, getConf());\r\n    try {\r\n        JobBuilder jobBuilder = null;\r\n        for (Path p : options.inputs) {\r\n            InputDemuxer inputDemuxer = options.inputDemuxerClass.newInstance();\r\n            try {\r\n                inputDemuxer.bindTo(p, getConf());\r\n            } catch (IOException e) {\r\n                LOG.warn(\"Unable to bind Path \" + p + \" .  Skipping...\", e);\r\n                continue;\r\n            }\r\n            Pair<String, InputStream> filePair = null;\r\n            try {\r\n                while ((filePair = inputDemuxer.getNext()) != null) {\r\n                    RewindableInputStream ris = new RewindableInputStream(filePair.second());\r\n                    JobHistoryParser parser = null;\r\n                    try {\r\n                        String jobID = JobHistoryUtils.extractJobID(filePair.first());\r\n                        if (jobID == null) {\r\n                            LOG.warn(\"File skipped: Invalid file name: \" + filePair.first());\r\n                            continue;\r\n                        }\r\n                        if ((jobBuilder == null) || (!jobBuilder.getJobID().equals(jobID))) {\r\n                            if (jobBuilder != null) {\r\n                                traceWriter.output(jobBuilder.build());\r\n                            }\r\n                            jobBuilder = new JobBuilder(jobID);\r\n                        }\r\n                        if (JobHistoryUtils.isJobConfXml(filePair.first())) {\r\n                            processJobConf(JobConfigurationParser.parse(ris.rewind()), jobBuilder);\r\n                        } else {\r\n                            parser = JobHistoryParserFactory.getParser(ris);\r\n                            if (parser == null) {\r\n                                LOG.warn(\"File skipped: Cannot find suitable parser: \" + filePair.first());\r\n                            } else {\r\n                                processJobHistory(parser, jobBuilder);\r\n                            }\r\n                        }\r\n                    } finally {\r\n                        if (parser == null) {\r\n                            ris.close();\r\n                        } else {\r\n                            parser.close();\r\n                            parser = null;\r\n                        }\r\n                    }\r\n                }\r\n            } catch (Throwable t) {\r\n                if (filePair != null) {\r\n                    LOG.warn(\"TraceBuilder got an error while processing the [possibly virtual] file \" + filePair.first() + \" within Path \" + p, t);\r\n                }\r\n            } finally {\r\n                inputDemuxer.close();\r\n            }\r\n        }\r\n        if (jobBuilder != null) {\r\n            traceWriter.output(jobBuilder.build());\r\n            jobBuilder = null;\r\n        } else {\r\n            LOG.warn(\"No job found in traces: \");\r\n        }\r\n        topologyWriter.output(topologyBuilder.build());\r\n    } finally {\r\n        traceWriter.close();\r\n        topologyWriter.close();\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processJobConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void processJobConf(Properties properties, JobBuilder jobBuilder)\n{\r\n    jobBuilder.process(properties);\r\n    topologyBuilder.process(properties);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processJobHistory",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void processJobHistory(JobHistoryParser parser, JobBuilder jobBuilder) throws IOException\n{\r\n    HistoryEvent e;\r\n    while ((e = parser.nextEvent()) != null) {\r\n        jobBuilder.process(e);\r\n        topologyBuilder.process(e);\r\n    }\r\n    parser.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "finish",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void finish()\n{\r\n    IOUtils.cleanupWithLogger(LOG, traceWriter, topologyWriter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void init(Path path, Configuration conf) throws IOException\n{\r\n    FileSystem fs = path.getFileSystem(conf);\r\n    CompressionCodec codec = new CompressionCodecFactory(conf).getCodec(path);\r\n    OutputStream output;\r\n    if (codec != null) {\r\n        compressor = CodecPool.getCompressor(codec);\r\n        output = codec.createOutputStream(fs.create(path), compressor);\r\n    } else {\r\n        output = fs.create(path);\r\n    }\r\n    writer = new JsonObjectMapperWriter<T>(output, conf.getBoolean(\"rumen.output.pretty.print\", true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "output",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void output(T object) throws IOException\n{\r\n    writer.write(object);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    try {\r\n        writer.close();\r\n    } finally {\r\n        if (compressor != null) {\r\n            CodecPool.returnCompressor(compressor);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\serializers",
  "methodName" : "serialize",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serialize(AnonymizableDataType object, JsonGenerator jGen, SerializerProvider sProvider) throws IOException, JsonProcessingException\n{\r\n    Object val = object.getAnonymizedValue(statePool, conf);\r\n    if (val instanceof String) {\r\n        jGen.writeString(val.toString());\r\n    } else {\r\n        jGen.writeObject(val);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "process",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void process(HistoryEvent event)\n{\r\n    if (event instanceof TaskAttemptFinishedEvent) {\r\n        processTaskAttemptFinishedEvent((TaskAttemptFinishedEvent) event);\r\n    } else if (event instanceof TaskAttemptUnsuccessfulCompletionEvent) {\r\n        processTaskAttemptUnsuccessfulCompletionEvent((TaskAttemptUnsuccessfulCompletionEvent) event);\r\n    } else if (event instanceof TaskStartedEvent) {\r\n        processTaskStartedEvent((TaskStartedEvent) event);\r\n    } else if (event instanceof MapAttemptFinishedEvent) {\r\n        processMapAttemptFinishedEvent((MapAttemptFinishedEvent) event);\r\n    } else if (event instanceof ReduceAttemptFinishedEvent) {\r\n        processReduceAttemptFinishedEvent((ReduceAttemptFinishedEvent) event);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "process",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void process(Properties conf)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "build",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LoggedNetworkTopology build()\n{\r\n    return new LoggedNetworkTopology(allHosts);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processTaskStartedEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void processTaskStartedEvent(TaskStartedEvent event)\n{\r\n    preferredLocationForSplits(event.getSplitLocations());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processTaskAttemptUnsuccessfulCompletionEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void processTaskAttemptUnsuccessfulCompletionEvent(TaskAttemptUnsuccessfulCompletionEvent event)\n{\r\n    recordParsedHost(event.getHostname(), event.getRackName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processTaskAttemptFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void processTaskAttemptFinishedEvent(TaskAttemptFinishedEvent event)\n{\r\n    recordParsedHost(event.getHostname(), event.getRackName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processMapAttemptFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void processMapAttemptFinishedEvent(MapAttemptFinishedEvent event)\n{\r\n    recordParsedHost(event.getHostname(), event.getRackName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processReduceAttemptFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void processReduceAttemptFinishedEvent(ReduceAttemptFinishedEvent event)\n{\r\n    recordParsedHost(event.getHostname(), event.getRackName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "recordParsedHost",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void recordParsedHost(String hostName, String rackName)\n{\r\n    if (hostName == null) {\r\n        return;\r\n    }\r\n    ParsedHost result = null;\r\n    if (rackName == null) {\r\n        result = ParsedHost.parse(hostName);\r\n    } else {\r\n        result = new ParsedHost(rackName, hostName);\r\n    }\r\n    if (result != null && !allHosts.contains(result)) {\r\n        allHosts.add(result);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "recordParsedHost",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void recordParsedHost(String nodeName)\n{\r\n    ParsedHost result = ParsedHost.parse(nodeName);\r\n    if (result != null && !allHosts.contains(result)) {\r\n        allHosts.add(result);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "preferredLocationForSplits",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void preferredLocationForSplits(String splits)\n{\r\n    if (splits != null) {\r\n        StringTokenizer tok = new StringTokenizer(splits, \",\", false);\r\n        while (tok.hasMoreTokens()) {\r\n            String nextSplit = tok.nextToken();\r\n            recordParsedHost(nextSplit);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "finalSEEs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<SingleEventEmitter> finalSEEs()\n{\r\n    return finals;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "nonFinalSEEs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<SingleEventEmitter> nonFinalSEEs()\n{\r\n    return nonFinals;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    String mySegment = fieldName + (index == -1 ? \"\" : (\"[\" + index + \"]\"));\r\n    return ((parent == null) ? \"\" : parent.toString() + \"-->\") + mySegment;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes\\util",
  "methodName" : "accept",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean accept(String key)\n{\r\n    return getLatestKeyName(key) != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes\\util",
  "methodName" : "getLatestKeyName",
  "errType" : [ "IllegalAccessException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getLatestKeyName(String key)\n{\r\n    configuration.set(key, key);\r\n    try {\r\n        for (Field f : mrFields) {\r\n            String mrKey = f.get(f.getName()).toString();\r\n            if (configuration.get(mrKey) != null) {\r\n                return mrKey;\r\n            }\r\n        }\r\n        return null;\r\n    } catch (IllegalAccessException iae) {\r\n        throw new RuntimeException(iae);\r\n    } finally {\r\n        configuration.clear();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes\\util",
  "methodName" : "parseJobProperty",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DataType<?> parseJobProperty(String key, String value)\n{\r\n    if (accept(key)) {\r\n        return fromString(key, value);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes\\util",
  "methodName" : "extractMaxHeapOpts",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void extractMaxHeapOpts(final String javaOptions, List<String> heapOpts, List<String> others)\n{\r\n    for (String opt : javaOptions.split(\" \")) {\r\n        Matcher matcher = MAX_HEAP_PATTERN.matcher(opt);\r\n        if (matcher.find()) {\r\n            heapOpts.add(opt);\r\n        } else {\r\n            others.add(opt);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes\\util",
  "methodName" : "extractMinHeapOpts",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void extractMinHeapOpts(String javaOptions, List<String> heapOpts, List<String> others)\n{\r\n    for (String opt : javaOptions.split(\" \")) {\r\n        Matcher matcher = MIN_HEAP_PATTERN.matcher(opt);\r\n        if (matcher.find()) {\r\n            heapOpts.add(opt);\r\n        } else {\r\n            others.add(opt);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes\\util",
  "methodName" : "fromString",
  "errType" : [ "ParseException", "Exception" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "DataType<?> fromString(String key, String value)\n{\r\n    DefaultDataType defaultValue = new DefaultDataType(value);\r\n    if (value != null) {\r\n        String latestKey = getLatestKeyName(key);\r\n        if (MRJobConfig.JOB_NAME.equals(latestKey)) {\r\n            return new JobName(value);\r\n        }\r\n        if (MRJobConfig.USER_NAME.equals(latestKey)) {\r\n            return new UserName(value);\r\n        }\r\n        if (MRJobConfig.QUEUE_NAME.equals(latestKey)) {\r\n            return new QueueName(value);\r\n        }\r\n        if (MRJobConfig.MAP_JAVA_OPTS.equals(latestKey) || MRJobConfig.REDUCE_JAVA_OPTS.equals(latestKey)) {\r\n            List<String> heapOptions = new ArrayList<String>();\r\n            extractMaxHeapOpts(value, heapOptions, new ArrayList<String>());\r\n            extractMinHeapOpts(value, heapOptions, new ArrayList<String>());\r\n            return new DefaultDataType(StringUtils.join(heapOptions, ' '));\r\n        }\r\n        try {\r\n            format.parse(value);\r\n            return defaultValue;\r\n        } catch (ParseException pe) {\r\n        }\r\n        if (\"true\".equals(value) || \"false\".equals(value)) {\r\n            return defaultValue;\r\n        }\r\n        if (latestKey.endsWith(\".class\") || latestKey.endsWith(\".codec\")) {\r\n            return new ClassName(value);\r\n        }\r\n        if (latestKey.endsWith(\"sizes\") || latestKey.endsWith(\".timestamps\")) {\r\n            return defaultValue;\r\n        }\r\n        if (latestKey.endsWith(\".dir\") || latestKey.endsWith(\".location\") || latestKey.endsWith(\".jar\") || latestKey.endsWith(\".path\") || latestKey.endsWith(\".logfile\") || latestKey.endsWith(\".file\") || latestKey.endsWith(\".files\") || latestKey.endsWith(\".archives\")) {\r\n            try {\r\n                return new FileName(value);\r\n            } catch (Exception ioe) {\r\n            }\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getNextJob",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ZombieJob getNextJob() throws IOException\n{\r\n    LoggedJob job = reader.getNext();\r\n    if (job == null) {\r\n        return null;\r\n    } else if (hasRandomSeed) {\r\n        long subRandomSeed = RandomSeedGenerator.getSeed(\"forZombieJob\" + job.getJobID(), randomSeed);\r\n        return new ZombieJob(job, cluster, subRandomSeed);\r\n    } else {\r\n        return new ZombieJob(job, cluster);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    reader.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getLayers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<NodeName> getLayers()\n{\r\n    return layers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setLayers",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void setLayers(List<String> layers)\n{\r\n    if (layers == null || layers.isEmpty()) {\r\n        this.layers = Collections.emptyList();\r\n    } else {\r\n        synchronized (layersCache) {\r\n            List<NodeName> found = layersCache.get(layers);\r\n            if (found == null) {\r\n                List<NodeName> clone = new ArrayList<NodeName>(layers.size());\r\n                clone.add(new NodeName(layers.get(0).intern(), null));\r\n                clone.add(new NodeName(null, layers.get(1).intern()));\r\n                List<NodeName> readonlyLayers = Collections.unmodifiableList(clone);\r\n                List<String> readonlyLayersKey = Collections.unmodifiableList(layers);\r\n                layersCache.put(readonlyLayersKey, readonlyLayers);\r\n                this.layers = readonlyLayers;\r\n            } else {\r\n                this.layers = found;\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setUnknownAttribute",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUnknownAttribute(String attributeName, Object ignored)\n{\r\n    if (!alreadySeenAnySetterAttributes.contains(attributeName)) {\r\n        alreadySeenAnySetterAttributes.add(attributeName);\r\n        System.err.println(\"In LoggedJob, we saw the unknown attribute \" + attributeName + \".\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compareStrings",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void compareStrings(List<NodeName> c1, List<NodeName> c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    TreePath recursePath = new TreePath(loc, eltname);\r\n    if (c1 == null || c2 == null || (c1.size() != c2.size())) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", recursePath);\r\n    }\r\n    for (NodeName n1 : c1) {\r\n        boolean found = false;\r\n        for (NodeName n2 : c2) {\r\n            if (n1.getValue().equals(n2.getValue())) {\r\n                found = true;\r\n                break;\r\n            }\r\n        }\r\n        if (!found) {\r\n            throw new DeepInequalityException(eltname + \" miscompared [\" + n1.getValue() + \"]\", recursePath);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "deepCompare",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deepCompare(DeepCompare comparand, TreePath loc) throws DeepInequalityException\n{\r\n    if (!(comparand instanceof LoggedLocation)) {\r\n        throw new DeepInequalityException(\"comparand has wrong type\", loc);\r\n    }\r\n    LoggedLocation other = (LoggedLocation) comparand;\r\n    compareStrings(layers, other.layers, loc, \"layers\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "finalSEEs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<SingleEventEmitter> finalSEEs()\n{\r\n    return finals;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "nonFinalSEEs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<SingleEventEmitter> nonFinalSEEs()\n{\r\n    return nonFinals;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\serializers",
  "methodName" : "serialize",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serialize(DataType object, JsonGenerator jGen, SerializerProvider sProvider) throws IOException, JsonProcessingException\n{\r\n    Object data = object.getValue();\r\n    if (data instanceof String) {\r\n        jGen.writeString(data.toString());\r\n    } else {\r\n        jGen.writeObject(data);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getClusterTopology",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Node getClusterTopology()\n{\r\n    return root;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "buildCluster",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void buildCluster(LoggedNetworkTopology topology, MachineNode defaultNode)\n{\r\n    Map<LoggedNetworkTopology, Integer> levelMapping = new IdentityHashMap<LoggedNetworkTopology, Integer>();\r\n    Deque<LoggedNetworkTopology> unvisited = new ArrayDeque<LoggedNetworkTopology>();\r\n    unvisited.add(topology);\r\n    levelMapping.put(topology, 0);\r\n    int leafLevel = -1;\r\n    for (LoggedNetworkTopology n = unvisited.poll(); n != null; n = unvisited.poll()) {\r\n        int level = levelMapping.get(n);\r\n        List<LoggedNetworkTopology> children = n.getChildren();\r\n        if (children == null || children.isEmpty()) {\r\n            if (leafLevel == -1) {\r\n                leafLevel = level;\r\n            } else if (leafLevel != level) {\r\n                throw new IllegalArgumentException(\"Leaf nodes are not on the same level\");\r\n            }\r\n        } else {\r\n            for (LoggedNetworkTopology child : children) {\r\n                levelMapping.put(child, level + 1);\r\n                unvisited.addFirst(child);\r\n            }\r\n        }\r\n    }\r\n    Node[] path = new Node[leafLevel];\r\n    unvisited.add(topology);\r\n    for (LoggedNetworkTopology n = unvisited.poll(); n != null; n = unvisited.poll()) {\r\n        int level = levelMapping.get(n);\r\n        Node current;\r\n        if (level == leafLevel) {\r\n            MachineNode.Builder builder = new MachineNode.Builder(n.getName().getValue(), level);\r\n            if (defaultNode != null) {\r\n                builder.cloneFrom(defaultNode);\r\n            }\r\n            current = builder.build();\r\n        } else {\r\n            current = (level == leafLevel - 1) ? new RackNode(n.getName().getValue(), level) : new Node(n.getName().getValue(), level);\r\n            path[level] = current;\r\n            for (LoggedNetworkTopology child : n.getChildren()) {\r\n                unvisited.addFirst(child);\r\n            }\r\n        }\r\n        if (level != 0) {\r\n            path[level - 1].addChild(current);\r\n        }\r\n    }\r\n    root = path[0];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Properties getValue()\n{\r\n    return jobProperties;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getAnonymizedValue",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "Properties getAnonymizedValue(StatePool statePool, Configuration conf)\n{\r\n    Properties filteredProperties = null;\r\n    List<JobPropertyParser> pList = new ArrayList<JobPropertyParser>(1);\r\n    String config = conf.get(PARSERS_CONFIG_KEY);\r\n    if (config != null) {\r\n        @SuppressWarnings(\"unchecked\")\r\n        Class<JobPropertyParser>[] parsers = (Class[]) conf.getClasses(PARSERS_CONFIG_KEY);\r\n        for (Class<JobPropertyParser> c : parsers) {\r\n            JobPropertyParser parser = ReflectionUtils.newInstance(c, conf);\r\n            pList.add(parser);\r\n        }\r\n    } else {\r\n        JobPropertyParser parser = new MapReduceJobPropertiesParser();\r\n        pList.add(parser);\r\n    }\r\n    if (jobProperties != null) {\r\n        filteredProperties = new Properties();\r\n        for (Map.Entry<Object, Object> entry : jobProperties.entrySet()) {\r\n            String key = entry.getKey().toString();\r\n            String value = entry.getValue().toString();\r\n            for (JobPropertyParser p : pList) {\r\n                DataType<?> pValue = p.parseJobProperty(key, value);\r\n                if (pValue != null) {\r\n                    filteredProperties.put(key, pValue);\r\n                    break;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return filteredProperties;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setUnknownAttribute",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUnknownAttribute(String attributeName, Object ignored)\n{\r\n    if (!alreadySeenAnySetterAttributes.contains(attributeName)) {\r\n        alreadySeenAnySetterAttributes.add(attributeName);\r\n        System.err.println(\"In LoggedJob, we saw the unknown attribute \" + attributeName + \".\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getRelativeRanking",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getRelativeRanking()\n{\r\n    return relativeRanking;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setRelativeRanking",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRelativeRanking(double relativeRanking)\n{\r\n    this.relativeRanking = relativeRanking;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDatum()\n{\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDatum(long datum)\n{\r\n    this.datum = datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void compare1(long c1, long c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 != c2) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void compare1(double c1, double c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 != c2) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "deepCompare",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void deepCompare(DeepCompare comparand, TreePath loc) throws DeepInequalityException\n{\r\n    if (!(comparand instanceof LoggedSingleRelativeRanking)) {\r\n        throw new DeepInequalityException(\"comparand has wrong type\", loc);\r\n    }\r\n    LoggedSingleRelativeRanking other = (LoggedSingleRelativeRanking) comparand;\r\n    compare1(relativeRanking, other.relativeRanking, loc, \"relativeRanking\");\r\n    compare1(datum, other.datum, loc, \"datum\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\anonymization",
  "methodName" : "needsAnonymization",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean needsAnonymization(String data)\n{\r\n    if (StringUtils.isNumeric(data)) {\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\anonymization",
  "methodName" : "hasSuffix",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasSuffix(String data, String[] suffixes)\n{\r\n    for (String ks : suffixes) {\r\n        if (data.endsWith(ks)) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\anonymization",
  "methodName" : "extractSuffix",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String[] extractSuffix(String data, String[] suffixes)\n{\r\n    String suffix = \"\";\r\n    for (String ks : suffixes) {\r\n        if (data.endsWith(ks)) {\r\n            suffix = ks;\r\n            data = data.substring(0, data.length() - suffix.length());\r\n            return new String[] { data, suffix };\r\n        }\r\n    }\r\n    throw new RuntimeException(\"Data [\" + data + \"] doesn't have a suffix from\" + \" known suffixes [\" + StringUtils.join(suffixes, ',') + \"]\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\anonymization",
  "methodName" : "isKnownData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isKnownData(String data)\n{\r\n    return isKnownData(data, KNOWN_WORDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\anonymization",
  "methodName" : "isKnownData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isKnownData(String data, String[] knownWords)\n{\r\n    for (String kd : knownWords) {\r\n        if (data.equals(kd)) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int read() throws IOException\n{\r\n    return input.read();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int read(byte[] buffer, int offset, int length) throws IOException\n{\r\n    return input.read(buffer, offset, length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    input.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "rewind",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "InputStream rewind() throws IOException\n{\r\n    try {\r\n        input.reset();\r\n        return this;\r\n    } catch (IOException e) {\r\n        throw new IOException(\"Unable to rewind the stream\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "finalSEEs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<SingleEventEmitter> finalSEEs()\n{\r\n    return finals;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "nonFinalSEEs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<SingleEventEmitter> nonFinalSEEs()\n{\r\n    return nonFinals;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "applyParser",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String applyParser(String fileName, Pattern pattern)\n{\r\n    Matcher matcher = pattern.matcher(fileName);\r\n    if (!matcher.matches()) {\r\n        return null;\r\n    }\r\n    return matcher.group(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "extractJobID",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String extractJobID(String fileName)\n{\r\n    String jobId = extractJobIDFromConfFileName(fileName);\r\n    if (jobId == null) {\r\n        jobId = extractJobIDFromHistoryFileName(fileName);\r\n    }\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "extractJobIDFromCurrentHistoryFile",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String extractJobIDFromCurrentHistoryFile(String fileName)\n{\r\n    JobID id = null;\r\n    if (org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils.isValidJobHistoryFileName(fileName)) {\r\n        try {\r\n            id = org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils.getJobIDFromHistoryFilePath(fileName);\r\n        } catch (IOException e) {\r\n        }\r\n    }\r\n    if (id != null) {\r\n        return id.toString();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "extractJobIDFromHistoryFileName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String extractJobIDFromHistoryFileName(String fileName)\n{\r\n    String jobID = extractJobIDFromCurrentHistoryFile(fileName);\r\n    if (jobID != null) {\r\n        return jobID;\r\n    }\r\n    String pre21JobID = applyParser(fileName, Pre21JobHistoryConstants.JOBHISTORY_FILENAME_REGEX_V1);\r\n    if (pre21JobID == null) {\r\n        pre21JobID = applyParser(fileName, Pre21JobHistoryConstants.JOBHISTORY_FILENAME_REGEX_V2);\r\n    }\r\n    return pre21JobID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "extractJobIDFromConfFileName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String extractJobIDFromConfFileName(String fileName)\n{\r\n    String pre21JobID = applyParser(fileName, Pre21JobHistoryConstants.CONF_FILENAME_REGEX_V1);\r\n    if (pre21JobID == null) {\r\n        pre21JobID = applyParser(fileName, Pre21JobHistoryConstants.CONF_FILENAME_REGEX_V2);\r\n    }\r\n    if (pre21JobID != null) {\r\n        return pre21JobID;\r\n    }\r\n    return applyParser(fileName, JobHistory.CONF_FILENAME_REGEX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "isJobConfXml",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isJobConfXml(String fileName)\n{\r\n    String jobId = extractJobIDFromConfFileName(fileName);\r\n    return jobId != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "extractCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, Long> extractCounters(JhCounters counters)\n{\r\n    Map<String, Long> countersMap = new HashMap<String, Long>();\r\n    if (counters != null) {\r\n        for (JhCounterGroup group : counters.groups) {\r\n            for (JhCounter counter : group.counts) {\r\n                countersMap.put(counter.name.toString(), counter.value);\r\n            }\r\n        }\r\n    }\r\n    return countersMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "intern",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "LogRecordType intern(String typeName)\n{\r\n    LogRecordType result = internees.get(typeName);\r\n    if (result == null) {\r\n        result = new LogRecordType(typeName);\r\n        internees.put(typeName, result);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "internSoft",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LogRecordType internSoft(String typeName)\n{\r\n    return internees.get(typeName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "lineTypes",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String[] lineTypes()\n{\r\n    Iterator<Map.Entry<String, LogRecordType>> iter = internees.entrySet().iterator();\r\n    String[] result = new String[internees.size()];\r\n    for (int i = 0; i < internees.size(); ++i) {\r\n        result[i] = iter.next().getKey();\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "get20TaskType",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "TaskType get20TaskType(String taskType)\n{\r\n    try {\r\n        return TaskType.valueOf(taskType);\r\n    } catch (IllegalArgumentException e) {\r\n        if (\"CLEANUP\".equals(taskType)) {\r\n            return TaskType.JOB_CLEANUP;\r\n        }\r\n        if (\"SETUP\".equals(taskType)) {\r\n            return TaskType.JOB_SETUP;\r\n        }\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\state",
  "methodName" : "deserialize",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "StatePair deserialize(JsonParser parser, DeserializationContext context) throws IOException, JsonProcessingException\n{\r\n    ObjectMapper mapper = (ObjectMapper) parser.getCodec();\r\n    ObjectNode statePairObject = (ObjectNode) mapper.readTree(parser);\r\n    Class<?> stateClass = null;\r\n    try {\r\n        stateClass = Class.forName(statePairObject.get(\"className\").textValue().trim());\r\n    } catch (ClassNotFoundException cnfe) {\r\n        throw new RuntimeException(\"Invalid classname!\", cnfe);\r\n    }\r\n    String stateJsonString = statePairObject.get(\"state\").toString();\r\n    State state = (State) mapper.readValue(stateJsonString, stateClass);\r\n    return new StatePair(state);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "convertState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "State convertState(Values status)\n{\r\n    if (status == Values.SUCCESS) {\r\n        return State.SUCCEEDED;\r\n    } else if (status == Values.FAILED) {\r\n        return State.FAILED;\r\n    } else if (status == Values.KILLED) {\r\n        return State.KILLED;\r\n    } else {\r\n        throw new IllegalArgumentException(\"unknown status \" + status);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getJobConf",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "JobConf getJobConf()\n{\r\n    if (jobConf == null) {\r\n        jobConf = new JobConf();\r\n        for (Map.Entry<Object, Object> entry : job.getJobProperties().getValue().entrySet()) {\r\n            jobConf.set(entry.getKey().toString(), entry.getValue().toString());\r\n        }\r\n        jobConf.setJobName(getName());\r\n        jobConf.setUser(getUser());\r\n        jobConf.setNumMapTasks(getNumberMaps());\r\n        jobConf.setNumReduceTasks(getNumberReduces());\r\n        jobConf.setQueueName(getQueueName());\r\n    }\r\n    return jobConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getInputSplits",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "InputSplit[] getInputSplits()\n{\r\n    if (splits == null) {\r\n        List<InputSplit> splitsList = new ArrayList<InputSplit>();\r\n        Path emptyPath = new Path(\"/\");\r\n        int totalHosts = 0;\r\n        for (LoggedTask mapTask : job.getMapTasks()) {\r\n            Pre21JobHistoryConstants.Values taskType = mapTask.getTaskType();\r\n            if (taskType != Pre21JobHistoryConstants.Values.MAP) {\r\n                LOG.warn(\"TaskType for a MapTask is not Map. task=\" + mapTask.getTaskID() + \" type=\" + ((taskType == null) ? \"null\" : taskType.toString()));\r\n                continue;\r\n            }\r\n            List<LoggedLocation> locations = mapTask.getPreferredLocations();\r\n            List<String> hostList = new ArrayList<String>();\r\n            if (locations != null) {\r\n                for (LoggedLocation location : locations) {\r\n                    List<NodeName> layers = location.getLayers();\r\n                    if (layers.size() == 0) {\r\n                        LOG.warn(\"Bad location layer format for task \" + mapTask.getTaskID());\r\n                        continue;\r\n                    }\r\n                    String host = layers.get(layers.size() - 1).getValue();\r\n                    if (host == null) {\r\n                        LOG.warn(\"Bad location layer format for task \" + mapTask.getTaskID() + \": \" + layers);\r\n                        continue;\r\n                    }\r\n                    hostList.add(host);\r\n                }\r\n            }\r\n            String[] hosts = hostList.toArray(new String[hostList.size()]);\r\n            totalHosts += hosts.length;\r\n            long mapInputBytes = getTaskInfo(mapTask).getInputBytes();\r\n            if (mapInputBytes < 0) {\r\n                LOG.warn(\"InputBytes for task \" + mapTask.getTaskID() + \" is not defined.\");\r\n                mapInputBytes = 0;\r\n            }\r\n            splitsList.add(new FileSplit(emptyPath, 0, mapInputBytes, hosts));\r\n        }\r\n        int totalMaps = job.getTotalMaps();\r\n        if (totalMaps < splitsList.size()) {\r\n            LOG.warn(\"TotalMaps for job \" + job.getJobID() + \" is less than the total number of map task descriptions (\" + totalMaps + \"<\" + splitsList.size() + \").\");\r\n        }\r\n        int avgHostPerSplit;\r\n        if (splitsList.size() == 0) {\r\n            avgHostPerSplit = 3;\r\n        } else {\r\n            avgHostPerSplit = totalHosts / splitsList.size();\r\n            if (avgHostPerSplit == 0) {\r\n                avgHostPerSplit = 3;\r\n            }\r\n        }\r\n        for (int i = splitsList.size(); i < totalMaps; i++) {\r\n            if (cluster == null) {\r\n                splitsList.add(new FileSplit(emptyPath, 0, 0, new String[0]));\r\n            } else {\r\n                MachineNode[] mNodes = cluster.getRandomMachines(avgHostPerSplit, random);\r\n                String[] hosts = new String[mNodes.length];\r\n                for (int j = 0; j < hosts.length; ++j) {\r\n                    hosts[j] = mNodes[j].getName();\r\n                }\r\n                splitsList.add(new FileSplit(emptyPath, 0, 0, hosts));\r\n            }\r\n        }\r\n        splits = splitsList.toArray(new InputSplit[splitsList.size()]);\r\n    }\r\n    return splits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getName()\n{\r\n    JobName jobName = job.getJobName();\r\n    if (jobName == null || jobName.getValue() == null) {\r\n        return \"(name unknown)\";\r\n    } else {\r\n        return jobName.getValue();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID getJobID()\n{\r\n    return getLoggedJob().getJobID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "sanitizeValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int sanitizeValue(int oldVal, int defaultVal, String name, JobID id)\n{\r\n    if (oldVal == -1) {\r\n        LOG.warn(name + \" not defined for \" + id);\r\n        return defaultVal;\r\n    }\r\n    return oldVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getNumberMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumberMaps()\n{\r\n    return sanitizeValue(job.getTotalMaps(), 0, \"NumberMaps\", job.getJobID());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getNumberReduces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumberReduces()\n{\r\n    return sanitizeValue(job.getTotalReduces(), 0, \"NumberReduces\", job.getJobID());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getOutcome",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Values getOutcome()\n{\r\n    return job.getOutcome();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getSubmissionTime",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getSubmissionTime()\n{\r\n    return job.getSubmitTime() - job.getRelativeTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getQueueName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getQueueName()\n{\r\n    QueueName queue = job.getQueue();\r\n    return (queue == null || queue.getValue() == null) ? JobConf.DEFAULT_QUEUE_NAME : queue.getValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getNumLoggedMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumLoggedMaps()\n{\r\n    return job.getMapTasks().size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getNumLoggedReduces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumLoggedReduces()\n{\r\n    return job.getReduceTasks().size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "maskTaskID",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskID maskTaskID(TaskID taskId)\n{\r\n    JobID jobId = new JobID();\r\n    TaskType taskType = taskId.getTaskType();\r\n    return new TaskID(jobId, taskType, taskId.getId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "maskAttemptID",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "TaskAttemptID maskAttemptID(TaskAttemptID attemptId)\n{\r\n    JobID jobId = new JobID();\r\n    TaskType taskType = attemptId.getTaskType();\r\n    TaskID taskId = attemptId.getTaskID();\r\n    return new TaskAttemptID(jobId.getJtIdentifier(), jobId.getId(), taskType, taskId.getId(), attemptId.getId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "sanitizeLoggedTask",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "LoggedTask sanitizeLoggedTask(LoggedTask task)\n{\r\n    if (task == null) {\r\n        return null;\r\n    }\r\n    if (task.getTaskType() == null) {\r\n        LOG.warn(\"Task \" + task.getTaskID() + \" has nulll TaskType\");\r\n        return null;\r\n    }\r\n    if (task.getTaskStatus() == null) {\r\n        LOG.warn(\"Task \" + task.getTaskID() + \" has nulll TaskStatus\");\r\n        return null;\r\n    }\r\n    return task;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "sanitizeLoggedTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "LoggedTaskAttempt sanitizeLoggedTaskAttempt(LoggedTaskAttempt attempt)\n{\r\n    if (attempt == null) {\r\n        return null;\r\n    }\r\n    if (attempt.getResult() == null) {\r\n        LOG.warn(\"TaskAttempt \" + attempt.getResult() + \" has nulll Result\");\r\n        return null;\r\n    }\r\n    return attempt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "buildMaps",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void buildMaps()\n{\r\n    if (loggedTaskMap == null) {\r\n        loggedTaskMap = new HashMap<TaskID, LoggedTask>();\r\n        loggedTaskAttemptMap = new HashMap<TaskAttemptID, LoggedTaskAttempt>();\r\n        for (LoggedTask map : job.getMapTasks()) {\r\n            map = sanitizeLoggedTask(map);\r\n            if (map != null) {\r\n                loggedTaskMap.put(maskTaskID(map.taskID), map);\r\n                for (LoggedTaskAttempt mapAttempt : map.getAttempts()) {\r\n                    mapAttempt = sanitizeLoggedTaskAttempt(mapAttempt);\r\n                    if (mapAttempt != null) {\r\n                        TaskAttemptID id = mapAttempt.getAttemptID();\r\n                        loggedTaskAttemptMap.put(maskAttemptID(id), mapAttempt);\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        for (LoggedTask reduce : job.getReduceTasks()) {\r\n            reduce = sanitizeLoggedTask(reduce);\r\n            if (reduce != null) {\r\n                loggedTaskMap.put(maskTaskID(reduce.taskID), reduce);\r\n                for (LoggedTaskAttempt reduceAttempt : reduce.getAttempts()) {\r\n                    reduceAttempt = sanitizeLoggedTaskAttempt(reduceAttempt);\r\n                    if (reduceAttempt != null) {\r\n                        TaskAttemptID id = reduceAttempt.getAttemptID();\r\n                        loggedTaskAttemptMap.put(maskAttemptID(id), reduceAttempt);\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getUser()\n{\r\n    UserName retval = job.getUser();\r\n    return (retval == null || retval.getValue() == null) ? \"(unknown)\" : retval.getValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getLoggedJob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LoggedJob getLoggedJob()\n{\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getTaskAttemptInfo",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "TaskAttemptInfo getTaskAttemptInfo(TaskType taskType, int taskNumber, int taskAttemptNumber)\n{\r\n    int locality = 0;\r\n    LoggedTask loggedTask = getLoggedTask(taskType, taskNumber);\r\n    if (loggedTask == null) {\r\n        TaskInfo taskInfo = new TaskInfo(0, 0, 0, 0, 0, 0);\r\n        return makeUpTaskAttemptInfo(taskType, taskInfo, taskAttemptNumber, taskNumber, locality);\r\n    }\r\n    LoggedTaskAttempt loggedAttempt = getLoggedTaskAttempt(taskType, taskNumber, taskAttemptNumber);\r\n    if (loggedAttempt == null) {\r\n        TaskInfo taskInfo = getTaskInfo(loggedTask);\r\n        return makeUpTaskAttemptInfo(taskType, taskInfo, taskAttemptNumber, taskNumber, locality);\r\n    } else {\r\n        if (loggedAttempt.getResult() == Values.KILLED) {\r\n            TaskInfo taskInfo = getTaskInfo(loggedTask);\r\n            return makeUpTaskAttemptInfo(taskType, taskInfo, taskAttemptNumber, taskNumber, locality);\r\n        } else {\r\n            return getTaskAttemptInfo(loggedTask, loggedAttempt);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getTaskInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskInfo getTaskInfo(TaskType taskType, int taskNumber)\n{\r\n    return getTaskInfo(getLoggedTask(taskType, taskNumber));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMapTaskAttemptInfoAdjusted",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "TaskAttemptInfo getMapTaskAttemptInfoAdjusted(int taskNumber, int taskAttemptNumber, int locality)\n{\r\n    TaskType taskType = TaskType.MAP;\r\n    LoggedTask loggedTask = getLoggedTask(taskType, taskNumber);\r\n    if (loggedTask == null) {\r\n        TaskInfo taskInfo = new TaskInfo(0, 0, 0, 0, 0, 0);\r\n        return makeUpTaskAttemptInfo(taskType, taskInfo, taskAttemptNumber, taskNumber, locality);\r\n    }\r\n    LoggedTaskAttempt loggedAttempt = getLoggedTaskAttempt(taskType, taskNumber, taskAttemptNumber);\r\n    if (loggedAttempt == null) {\r\n        TaskInfo taskInfo = getTaskInfo(loggedTask);\r\n        return makeUpTaskAttemptInfo(taskType, taskInfo, taskAttemptNumber, taskNumber, locality);\r\n    } else {\r\n        if (loggedAttempt.getResult() == Values.KILLED) {\r\n            TaskInfo taskInfo = getTaskInfo(loggedTask);\r\n            return makeUpTaskAttemptInfo(taskType, taskInfo, taskAttemptNumber, taskNumber, locality);\r\n        } else if (loggedAttempt.getResult() == Values.FAILED) {\r\n            return getTaskAttemptInfo(loggedTask, loggedAttempt);\r\n        } else if (loggedAttempt.getResult() == Values.SUCCESS) {\r\n            int loggedLocality = getLocality(loggedTask, loggedAttempt);\r\n            if (locality == loggedLocality) {\r\n                return getTaskAttemptInfo(loggedTask, loggedAttempt);\r\n            } else {\r\n                return scaleInfo(loggedTask, loggedAttempt, locality, loggedLocality, rackLocalOverNodeLocal, rackRemoteOverNodeLocal);\r\n            }\r\n        } else {\r\n            throw new IllegalArgumentException(\"attempt result is not SUCCEEDED, FAILED or KILLED: \" + loggedAttempt.getResult());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "sanitizeTaskRuntime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long sanitizeTaskRuntime(long time, ID id)\n{\r\n    if (time < 0) {\r\n        LOG.warn(\"Negative running time for task \" + id + \": \" + time);\r\n        return 100L;\r\n    }\r\n    return time;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "scaleInfo",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "TaskAttemptInfo scaleInfo(LoggedTask loggedTask, LoggedTaskAttempt loggedAttempt, int locality, int loggedLocality, double rackLocalOverNodeLocal, double rackRemoteOverNodeLocal)\n{\r\n    TaskInfo taskInfo = getTaskInfo(loggedTask);\r\n    double[] factors = new double[] { 1.0, rackLocalOverNodeLocal, rackRemoteOverNodeLocal };\r\n    double scaleFactor = factors[locality] / factors[loggedLocality];\r\n    State state = convertState(loggedAttempt.getResult());\r\n    if (loggedTask.getTaskType() == Values.MAP) {\r\n        long taskTime = 0;\r\n        if (loggedAttempt.getStartTime() == 0) {\r\n            taskTime = makeUpMapRuntime(state, locality);\r\n        } else {\r\n            taskTime = loggedAttempt.getFinishTime() - loggedAttempt.getStartTime();\r\n        }\r\n        taskTime = sanitizeTaskRuntime(taskTime, loggedAttempt.getAttemptID());\r\n        taskTime *= scaleFactor;\r\n        return new MapTaskAttemptInfo(state, taskInfo, taskTime, loggedAttempt.allSplitVectors());\r\n    } else {\r\n        throw new IllegalArgumentException(\"taskType can only be MAP: \" + loggedTask.getTaskType());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getLocality",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "int getLocality(LoggedTask loggedTask, LoggedTaskAttempt loggedAttempt)\n{\r\n    int distance = cluster.getMaximumDistance();\r\n    String rackHostName = loggedAttempt.getHostName().getValue();\r\n    if (rackHostName == null) {\r\n        return distance;\r\n    }\r\n    MachineNode mn = getMachineNode(rackHostName);\r\n    if (mn == null) {\r\n        return distance;\r\n    }\r\n    List<LoggedLocation> locations = loggedTask.getPreferredLocations();\r\n    if (locations != null) {\r\n        for (LoggedLocation location : locations) {\r\n            List<NodeName> layers = location.getLayers();\r\n            if ((layers == null) || (layers.isEmpty())) {\r\n                continue;\r\n            }\r\n            String dataNodeName = layers.get(layers.size() - 1).getValue();\r\n            MachineNode dataNode = cluster.getMachineByName(dataNodeName);\r\n            if (dataNode != null) {\r\n                distance = Math.min(distance, cluster.distance(mn, dataNode));\r\n            }\r\n        }\r\n    }\r\n    return distance;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMachineNode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "MachineNode getMachineNode(String rackHostName)\n{\r\n    ParsedHost parsedHost = ParsedHost.parse(rackHostName);\r\n    String hostName = (parsedHost == null) ? rackHostName : parsedHost.getNodeName();\r\n    if (hostName == null) {\r\n        return null;\r\n    }\r\n    return (cluster == null) ? null : cluster.getMachineByName(hostName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getTaskAttemptInfo",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "TaskAttemptInfo getTaskAttemptInfo(LoggedTask loggedTask, LoggedTaskAttempt loggedAttempt)\n{\r\n    TaskInfo taskInfo = getTaskInfo(loggedTask);\r\n    List<List<Integer>> allSplitVectors = loggedAttempt.allSplitVectors();\r\n    State state = convertState(loggedAttempt.getResult());\r\n    if (loggedTask.getTaskType() == Values.MAP) {\r\n        long taskTime;\r\n        if (loggedAttempt.getStartTime() == 0) {\r\n            int locality = getLocality(loggedTask, loggedAttempt);\r\n            taskTime = makeUpMapRuntime(state, locality);\r\n        } else {\r\n            taskTime = loggedAttempt.getFinishTime() - loggedAttempt.getStartTime();\r\n        }\r\n        taskTime = sanitizeTaskRuntime(taskTime, loggedAttempt.getAttemptID());\r\n        return new MapTaskAttemptInfo(state, taskInfo, taskTime, allSplitVectors);\r\n    } else if (loggedTask.getTaskType() == Values.REDUCE) {\r\n        long startTime = loggedAttempt.getStartTime();\r\n        long mergeDone = loggedAttempt.getSortFinished();\r\n        long shuffleDone = loggedAttempt.getShuffleFinished();\r\n        long finishTime = loggedAttempt.getFinishTime();\r\n        if (startTime <= 0 || startTime >= finishTime) {\r\n            long reduceTime = makeUpReduceRuntime(state);\r\n            return new ReduceTaskAttemptInfo(state, taskInfo, 0, 0, reduceTime, allSplitVectors);\r\n        } else {\r\n            if (shuffleDone <= 0) {\r\n                shuffleDone = startTime;\r\n            }\r\n            if (mergeDone <= 0) {\r\n                mergeDone = finishTime;\r\n            }\r\n            long shuffleTime = shuffleDone - startTime;\r\n            long mergeTime = mergeDone - shuffleDone;\r\n            long reduceTime = finishTime - mergeDone;\r\n            reduceTime = sanitizeTaskRuntime(reduceTime, loggedAttempt.getAttemptID());\r\n            return new ReduceTaskAttemptInfo(state, taskInfo, shuffleTime, mergeTime, reduceTime, allSplitVectors);\r\n        }\r\n    } else {\r\n        throw new IllegalArgumentException(\"taskType for \" + loggedTask.getTaskID() + \" is neither MAP nor REDUCE: \" + loggedTask.getTaskType());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getTaskInfo",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "TaskInfo getTaskInfo(LoggedTask loggedTask)\n{\r\n    if (loggedTask == null) {\r\n        return new TaskInfo(0, 0, 0, 0, 0, 0);\r\n    }\r\n    List<LoggedTaskAttempt> attempts = loggedTask.getAttempts();\r\n    long inputBytes = -1;\r\n    long inputRecords = -1;\r\n    long outputBytes = -1;\r\n    long outputRecords = -1;\r\n    long heapMegabytes = -1;\r\n    ResourceUsageMetrics metrics = new ResourceUsageMetrics();\r\n    Values type = loggedTask.getTaskType();\r\n    if ((type != Values.MAP) && (type != Values.REDUCE)) {\r\n        throw new IllegalArgumentException(\"getTaskInfo only supports MAP or REDUCE tasks: \" + type.toString() + \" for task = \" + loggedTask.getTaskID());\r\n    }\r\n    for (LoggedTaskAttempt attempt : attempts) {\r\n        attempt = sanitizeLoggedTaskAttempt(attempt);\r\n        if ((attempt == null) || (attempt.getResult() != Values.SUCCESS)) {\r\n            continue;\r\n        }\r\n        if (type == Values.MAP) {\r\n            inputBytes = attempt.getHdfsBytesRead();\r\n            inputRecords = attempt.getMapInputRecords();\r\n            outputBytes = (job.getTotalReduces() > 0) ? attempt.getMapOutputBytes() : attempt.getHdfsBytesWritten();\r\n            outputRecords = attempt.getMapOutputRecords();\r\n            heapMegabytes = (job.getJobMapMB() > 0) ? job.getJobMapMB() : job.getHeapMegabytes();\r\n        } else {\r\n            inputBytes = attempt.getReduceShuffleBytes();\r\n            inputRecords = attempt.getReduceInputRecords();\r\n            outputBytes = attempt.getHdfsBytesWritten();\r\n            outputRecords = attempt.getReduceOutputRecords();\r\n            heapMegabytes = (job.getJobReduceMB() > 0) ? job.getJobReduceMB() : job.getHeapMegabytes();\r\n        }\r\n        metrics = attempt.getResourceUsageMetrics();\r\n        break;\r\n    }\r\n    TaskInfo taskInfo = new TaskInfo(inputBytes, (int) inputRecords, outputBytes, (int) outputRecords, (int) heapMegabytes, 1, metrics);\r\n    return taskInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "makeTaskAttemptID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptID makeTaskAttemptID(TaskType taskType, int taskNumber, int taskAttemptNumber)\n{\r\n    return new TaskAttemptID(new TaskID(job.getJobID(), taskType, taskNumber), taskAttemptNumber);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "makeUpTaskAttemptInfo",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "TaskAttemptInfo makeUpTaskAttemptInfo(TaskType taskType, TaskInfo taskInfo, int taskAttemptNumber, int taskNumber, int locality)\n{\r\n    if (taskType == TaskType.MAP) {\r\n        State state = State.SUCCEEDED;\r\n        long runtime = 0;\r\n        state = makeUpState(taskAttemptNumber, job.getMapperTriesToSucceed());\r\n        runtime = makeUpMapRuntime(state, locality);\r\n        runtime = sanitizeTaskRuntime(runtime, makeTaskAttemptID(taskType, taskNumber, taskAttemptNumber));\r\n        TaskAttemptInfo tai = new MapTaskAttemptInfo(state, taskInfo, runtime, null);\r\n        return tai;\r\n    } else if (taskType == TaskType.REDUCE) {\r\n        State state = State.SUCCEEDED;\r\n        long shuffleTime = 0;\r\n        long sortTime = 0;\r\n        long reduceTime = 0;\r\n        reduceTime = makeUpReduceRuntime(state);\r\n        TaskAttemptInfo tai = new ReduceTaskAttemptInfo(state, taskInfo, shuffleTime, sortTime, reduceTime, null);\r\n        return tai;\r\n    }\r\n    throw new IllegalArgumentException(\"taskType is neither MAP nor REDUCE: \" + taskType);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "makeUpReduceRuntime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long makeUpReduceRuntime(State state)\n{\r\n    long reduceTime = 0;\r\n    for (int i = 0; i < 5; i++) {\r\n        reduceTime = doMakeUpReduceRuntime(state);\r\n        if (reduceTime >= 0) {\r\n            return reduceTime;\r\n        }\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "doMakeUpReduceRuntime",
  "errType" : [ "NoValueToMakeUpRuntime" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long doMakeUpReduceRuntime(State state)\n{\r\n    long reduceTime;\r\n    try {\r\n        if (state == State.SUCCEEDED) {\r\n            reduceTime = makeUpRuntime(job.getSuccessfulReduceAttemptCDF());\r\n        } else if (state == State.FAILED) {\r\n            reduceTime = makeUpRuntime(job.getFailedReduceAttemptCDF());\r\n        } else {\r\n            throw new IllegalArgumentException(\"state is neither SUCCEEDED nor FAILED: \" + state);\r\n        }\r\n        return reduceTime;\r\n    } catch (NoValueToMakeUpRuntime e) {\r\n        return 0;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "makeUpMapRuntime",
  "errType" : [ "NoValueToMakeUpRuntime" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long makeUpMapRuntime(State state, int locality)\n{\r\n    long runtime;\r\n    if (state == State.SUCCEEDED || state == State.FAILED) {\r\n        List<LoggedDiscreteCDF> cdfList = state == State.SUCCEEDED ? job.getSuccessfulMapAttemptCDFs() : job.getFailedMapAttemptCDFs();\r\n        if (cdfList == null) {\r\n            runtime = -1;\r\n            return runtime;\r\n        }\r\n        try {\r\n            runtime = makeUpRuntime(cdfList.get(locality));\r\n        } catch (NoValueToMakeUpRuntime e) {\r\n            runtime = makeUpRuntime(cdfList);\r\n        }\r\n    } else {\r\n        throw new IllegalArgumentException(\"state is neither SUCCEEDED nor FAILED: \" + state);\r\n    }\r\n    return runtime;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "makeUpRuntime",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long makeUpRuntime(List<LoggedDiscreteCDF> mapAttemptCDFs)\n{\r\n    int total = 0;\r\n    if (mapAttemptCDFs == null) {\r\n        return -1;\r\n    }\r\n    for (LoggedDiscreteCDF cdf : mapAttemptCDFs) {\r\n        total += cdf.getNumberValues();\r\n    }\r\n    if (total == 0) {\r\n        return -1;\r\n    }\r\n    int index = random.nextInt(total);\r\n    for (LoggedDiscreteCDF cdf : mapAttemptCDFs) {\r\n        if (index >= cdf.getNumberValues()) {\r\n            index -= cdf.getNumberValues();\r\n        } else {\r\n            if (index < 0) {\r\n                throw new IllegalStateException(\"application error\");\r\n            }\r\n            return makeUpRuntime(cdf);\r\n        }\r\n    }\r\n    throw new IllegalStateException(\"not possible to get here\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "makeUpRuntime",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long makeUpRuntime(LoggedDiscreteCDF loggedDiscreteCDF)\n{\r\n    if (hasRandomSeed) {\r\n        synchronized (interpolatorMap) {\r\n            return makeUpRuntimeCore(loggedDiscreteCDF);\r\n        }\r\n    }\r\n    return makeUpRuntimeCore(loggedDiscreteCDF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getNextRandomSeed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNextRandomSeed()\n{\r\n    numRandomSeeds++;\r\n    return RandomSeedGenerator.getSeed(\"forZombieJob\" + job.getJobID(), numRandomSeeds);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "makeUpRuntimeCore",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long makeUpRuntimeCore(LoggedDiscreteCDF loggedDiscreteCDF)\n{\r\n    CDFRandomGenerator interpolator;\r\n    synchronized (interpolatorMap) {\r\n        interpolator = interpolatorMap.get(loggedDiscreteCDF);\r\n    }\r\n    if (interpolator == null) {\r\n        if (loggedDiscreteCDF.getNumberValues() == 0) {\r\n            throw new NoValueToMakeUpRuntime(\"no value to use to make up runtime\");\r\n        }\r\n        interpolator = hasRandomSeed ? new CDFPiecewiseLinearRandomGenerator(loggedDiscreteCDF, getNextRandomSeed()) : new CDFPiecewiseLinearRandomGenerator(loggedDiscreteCDF);\r\n        synchronized (interpolatorMap) {\r\n            interpolatorMap.put(loggedDiscreteCDF, interpolator);\r\n        }\r\n    }\r\n    return interpolator.randomValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "makeUpState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "State makeUpState(int taskAttemptNumber, double[] numAttempts)\n{\r\n    if (numAttempts == null) {\r\n        return State.FAILED;\r\n    }\r\n    if (taskAttemptNumber >= numAttempts.length - 1) {\r\n        return State.SUCCEEDED;\r\n    } else {\r\n        double pSucceed = numAttempts[taskAttemptNumber];\r\n        double pFail = 0;\r\n        for (int i = taskAttemptNumber + 1; i < numAttempts.length; i++) {\r\n            pFail += numAttempts[i];\r\n        }\r\n        return (random.nextDouble() < pSucceed / (pSucceed + pFail)) ? State.SUCCEEDED : State.FAILED;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMaskedTaskID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskID getMaskedTaskID(TaskType taskType, int taskNumber)\n{\r\n    return new TaskID(new JobID(), taskType, taskNumber);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getLoggedTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "LoggedTask getLoggedTask(TaskType taskType, int taskNumber)\n{\r\n    buildMaps();\r\n    return loggedTaskMap.get(getMaskedTaskID(taskType, taskNumber));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getLoggedTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "LoggedTaskAttempt getLoggedTaskAttempt(TaskType taskType, int taskNumber, int taskAttemptNumber)\n{\r\n    buildMaps();\r\n    TaskAttemptID id = new TaskAttemptID(getMaskedTaskID(taskType, taskNumber), taskAttemptNumber);\r\n    return loggedTaskAttemptMap.get(id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getPrefix",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPrefix()\n{\r\n    return DEFAULT_PREFIX;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "needsAnonymization",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean needsAnonymization(Configuration conf)\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getAnonymizedValue",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getAnonymizedValue(StatePool statePool, Configuration conf)\n{\r\n    if (needsAnonymization(conf)) {\r\n        WordList state = (WordList) statePool.getState(getClass());\r\n        if (state == null) {\r\n            state = new WordList(getPrefix());\r\n            statePool.addState(getClass(), state);\r\n        }\r\n        return anonymize(getValue(), state);\r\n    } else {\r\n        return getValue();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "anonymize",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String anonymize(String data, WordList wordList)\n{\r\n    if (data == null) {\r\n        return null;\r\n    }\r\n    if (!wordList.contains(data)) {\r\n        wordList.add(data);\r\n    }\r\n    return wordList.getName() + wordList.indexOf(data);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "canParse",
  "errType" : [ "EOFException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean canParse(InputStream input) throws IOException\n{\r\n    try {\r\n        LineReader reader = new LineReader(input);\r\n        Text buffer = new Text();\r\n        return reader.readLine(buffer) != 0 && buffer.toString().equals(\"Meta VERSION=\\\"1\\\" .\");\r\n    } catch (EOFException e) {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "nextEvent",
  "errType" : [ "EOFException", "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "HistoryEvent nextEvent()\n{\r\n    try {\r\n        while (remainingEvents.isEmpty()) {\r\n            ParsedLine line = new ParsedLine(getFullLine(), internalVersion);\r\n            LineType type = LineType.findLineType(line.getType());\r\n            if (type == null) {\r\n                continue;\r\n            }\r\n            String name = type.getName(line);\r\n            HistoryEventEmitter emitter = findOrMakeEmitter(name, type);\r\n            Pair<Queue<HistoryEvent>, HistoryEventEmitter.PostEmitAction> pair = emitter.emitterCore(line, name);\r\n            if (pair.second() == HistoryEventEmitter.PostEmitAction.REMOVE_HEE) {\r\n                liveEmitters.remove(name);\r\n            }\r\n            remainingEvents = pair.first();\r\n        }\r\n        return remainingEvents.poll();\r\n    } catch (EOFException e) {\r\n        return null;\r\n    } catch (IOException e) {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "findOrMakeEmitter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "HistoryEventEmitter findOrMakeEmitter(String name, LineType type)\n{\r\n    HistoryEventEmitter result = liveEmitters.get(name);\r\n    if (result == null) {\r\n        result = type.createEmitter();\r\n        liveEmitters.put(name, result);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getOneLine",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getOneLine() throws IOException\n{\r\n    Text resultText = new Text();\r\n    if (reader.readLine(resultText) == 0) {\r\n        throw new EOFException(\"apparent bad line\");\r\n    }\r\n    return resultText.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getFullLine",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "String getFullLine() throws IOException\n{\r\n    String line = getOneLine();\r\n    while (line.length() < endLineString.length()) {\r\n        line = getOneLine();\r\n    }\r\n    if (line.endsWith(endLineString)) {\r\n        return line;\r\n    }\r\n    StringBuilder sb = new StringBuilder(line);\r\n    String addedLine;\r\n    do {\r\n        addedLine = getOneLine();\r\n        sb.append(\"\\n\");\r\n        sb.append(addedLine);\r\n    } while (addedLine.length() < endLineString.length() || !endLineString.equals(addedLine.substring(addedLine.length() - endLineString.length())));\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (reader != null) {\r\n        reader.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "nonFinalSEEs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<SingleEventEmitter> nonFinalSEEs()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "finalSEEs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<SingleEventEmitter> finalSEEs()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "emitterCore",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Pair<Queue<HistoryEvent>, PostEmitAction> emitterCore(ParsedLine line, String name)\n{\r\n    Queue<HistoryEvent> results = new LinkedList<HistoryEvent>();\r\n    PostEmitAction removeEmitter = PostEmitAction.NONE;\r\n    for (SingleEventEmitter see : nonFinalSEEs()) {\r\n        HistoryEvent event = see.maybeEmitEvent(line, name, this);\r\n        if (event != null) {\r\n            results.add(event);\r\n        }\r\n    }\r\n    for (SingleEventEmitter see : finalSEEs()) {\r\n        HistoryEvent event = see.maybeEmitEvent(line, name, this);\r\n        if (event != null) {\r\n            results.add(event);\r\n            removeEmitter = PostEmitAction.REMOVE_HEE;\r\n            break;\r\n        }\r\n    }\r\n    return new Pair<Queue<HistoryEvent>, PostEmitAction>(results, removeEmitter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "maybeParseCounters",
  "errType" : [ "ParseException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Counters maybeParseCounters(String counters)\n{\r\n    try {\r\n        return parseCounters(counters);\r\n    } catch (ParseException e) {\r\n        LOG.warn(\"The counter string, \\\"\" + counters + \"\\\" is badly formatted.\");\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "parseCounters",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Counters parseCounters(String counters) throws ParseException\n{\r\n    if (counters == null) {\r\n        LOG.warn(\"HistoryEventEmitters: null counter detected:\");\r\n        return null;\r\n    }\r\n    counters = counters.replace(\"\\\\.\", \"\\\\\\\\.\");\r\n    counters = counters.replace(\"\\\\\\\\{\", \"\\\\{\");\r\n    counters = counters.replace(\"\\\\\\\\}\", \"\\\\}\");\r\n    counters = counters.replace(\"\\\\\\\\(\", \"\\\\(\");\r\n    counters = counters.replace(\"\\\\\\\\)\", \"\\\\)\");\r\n    counters = counters.replace(\"\\\\\\\\[\", \"\\\\[\");\r\n    counters = counters.replace(\"\\\\\\\\]\", \"\\\\]\");\r\n    org.apache.hadoop.mapred.Counters depForm = org.apache.hadoop.mapred.Counters.fromEscapedCompactString(counters);\r\n    return new Counters(depForm);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getValue()\n{\r\n    return className;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getPrefix",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPrefix()\n{\r\n    return \"class\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "needsAnonymization",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean needsAnonymization(Configuration conf)\n{\r\n    String[] preserves = conf.getStrings(CLASSNAME_PRESERVE_CONFIG);\r\n    if (preserves != null) {\r\n        for (String p : preserves) {\r\n            if (className.startsWith(p)) {\r\n                return false;\r\n            }\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getHostName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHostName()\n{\r\n    return hostName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getRackName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRackName()\n{\r\n    return rackName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getValue()\n{\r\n    return nodeName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getAnonymizedValue",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getAnonymizedValue(StatePool statePool, Configuration conf)\n{\r\n    if (this.getValue().equals(ROOT.getValue())) {\r\n        return getValue();\r\n    }\r\n    if (anonymizedNodeName == null) {\r\n        anonymize(statePool);\r\n    }\r\n    return anonymizedNodeName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "anonymize",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void anonymize(StatePool pool)\n{\r\n    StringBuffer buf = new StringBuffer();\r\n    NodeNameState state = (NodeNameState) pool.getState(getClass());\r\n    if (state == null) {\r\n        state = new NodeNameState();\r\n        pool.addState(getClass(), state);\r\n    }\r\n    if (rackName != null && hostName != null) {\r\n        buf.append('/');\r\n        buf.append(anonymize(rackName, state.getRackNameState()));\r\n        buf.append('/');\r\n        buf.append(anonymize(hostName, state.getHostNameState()));\r\n    } else {\r\n        if (state.getRackNameState().contains(nodeName) || rackName != null) {\r\n            buf.append(anonymize(nodeName, state.getRackNameState()));\r\n        } else {\r\n            buf.append(anonymize(nodeName, state.getHostNameState()));\r\n        }\r\n    }\r\n    anonymizedNodeName = buf.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "anonymize",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String anonymize(String data, WordList wordList)\n{\r\n    if (data == null) {\r\n        return null;\r\n    }\r\n    if (!wordList.contains(data)) {\r\n        wordList.add(data);\r\n    }\r\n    return wordList.getName() + wordList.indexOf(data);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "bindTo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void bindTo(Path path, Configuration conf) throws IOException\n{\r\n    if (name != null) {\r\n        close();\r\n    }\r\n    name = path.getName();\r\n    input = new PossiblyDecompressedInputStream(path, conf);\r\n    return;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getNext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Pair<String, InputStream> getNext() throws IOException\n{\r\n    if (name != null) {\r\n        Pair<String, InputStream> ret = new Pair<String, InputStream>(name, input);\r\n        name = null;\r\n        input = null;\r\n        return ret;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    try {\r\n        if (input != null) {\r\n            input.close();\r\n        }\r\n    } finally {\r\n        name = null;\r\n        input = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getParser",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "JobHistoryParser getParser(RewindableInputStream ris) throws IOException\n{\r\n    for (VersionDetector vd : VersionDetector.values()) {\r\n        boolean canParse = vd.canParse(ris);\r\n        ris.rewind();\r\n        if (canParse) {\r\n            return vd.newInstance(ris);\r\n        }\r\n    }\r\n    throw new IOException(\"No suitable parser.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getNext",
  "errType" : [ "JsonMappingException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T getNext() throws IOException\n{\r\n    try {\r\n        return mapper.readValue(jsonParser, clazz);\r\n    } catch (JsonMappingException e) {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    jsonParser.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "valueAt",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long valueAt(double probability)\n{\r\n    int rangeFloor = floorIndex(probability);\r\n    double segmentProbMin = getRankingAt(rangeFloor);\r\n    double segmentProbMax = getRankingAt(rangeFloor + 1);\r\n    long segmentMinValue = getDatumAt(rangeFloor);\r\n    long segmentMaxValue = getDatumAt(rangeFloor + 1);\r\n    double segmentProbRange = segmentProbMax - segmentProbMin;\r\n    long segmentDatumRange = segmentMaxValue - segmentMinValue;\r\n    long result = (long) ((probability - segmentProbMin) / segmentProbRange * segmentDatumRange) + segmentMinValue;\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "readTopology",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readTopology(JsonObjectMapperParser<LoggedNetworkTopology> parser) throws IOException\n{\r\n    try {\r\n        topology = parser.getNext();\r\n        if (topology == null) {\r\n            throw new IOException(\"Input file does not contain valid topology data.\");\r\n        }\r\n    } finally {\r\n        parser.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LoggedNetworkTopology get()\n{\r\n    return topology;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "initializeTables",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void initializeTables(LoggedDiscreteCDF cdf)\n{\r\n    rankings[0] = 0.0;\r\n    values[0] = cdf.getMinimum();\r\n    rankings[rankings.length - 1] = 1.0;\r\n    values[rankings.length - 1] = cdf.getMaximum();\r\n    List<LoggedSingleRelativeRanking> subjects = cdf.getRankings();\r\n    for (int i = 0; i < subjects.size(); ++i) {\r\n        rankings[i + 1] = subjects.get(i).getRelativeRanking();\r\n        values[i + 1] = subjects.get(i).getDatum();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "floorIndex",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int floorIndex(double probe)\n{\r\n    int result = Arrays.binarySearch(rankings, probe);\r\n    return Math.abs(result + 1) - 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getRankingAt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getRankingAt(int index)\n{\r\n    return rankings[index];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getDatumAt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDatumAt(int index)\n{\r\n    return values[index];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "randomValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long randomValue()\n{\r\n    return valueAt(random.nextDouble());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "valueAt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long valueAt(double probability)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "initialize",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void initialize(String[] args) throws Exception\n{\r\n    try {\r\n        for (int i = 0; i < args.length; ++i) {\r\n            if (\"-trace\".equals(args[i])) {\r\n                anonymizeTrace = true;\r\n                inputTracePath = new Path(args[i + 1]);\r\n                outputTracePath = new Path(args[i + 2]);\r\n                i += 2;\r\n            }\r\n            if (\"-topology\".equals(args[i])) {\r\n                anonymizeTopology = true;\r\n                inputTopologyPath = new Path(args[i + 1]);\r\n                outputTopologyPath = new Path(args[i + 2]);\r\n                i += 2;\r\n            }\r\n        }\r\n    } catch (Exception e) {\r\n        throw new IllegalArgumentException(\"Illegal arguments list!\", e);\r\n    }\r\n    if (!anonymizeTopology && !anonymizeTrace) {\r\n        throw new IllegalArgumentException(\"Invalid arguments list!\");\r\n    }\r\n    statePool = new StatePool();\r\n    statePool.initialize(getConf());\r\n    outMapper = new ObjectMapper();\r\n    SimpleModule module = new SimpleModule(\"Anonymization Serializer\", new Version(0, 1, 1, \"FINAL\", \"\", \"\"));\r\n    module.addSerializer(DataType.class, new DefaultRumenSerializer());\r\n    module.addSerializer(String.class, new BlockingSerializer());\r\n    module.addSerializer(ID.class, new ObjectStringSerializer<ID>());\r\n    module.addSerializer(AnonymizableDataType.class, new DefaultAnonymizingRumenSerializer(statePool, getConf()));\r\n    outMapper.registerModule(module);\r\n    outFactory = outMapper.getFactory();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "anonymizeTrace",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void anonymizeTrace() throws Exception\n{\r\n    if (anonymizeTrace) {\r\n        System.out.println(\"Anonymizing trace file: \" + inputTracePath);\r\n        JobTraceReader reader = null;\r\n        JsonGenerator outGen = null;\r\n        Configuration conf = getConf();\r\n        try {\r\n            outGen = createJsonGenerator(conf, outputTracePath);\r\n            reader = new JobTraceReader(inputTracePath, conf);\r\n            LoggedJob job = reader.getNext();\r\n            while (job != null) {\r\n                outGen.writeObject(job);\r\n                job = reader.getNext();\r\n            }\r\n            System.out.println(\"Anonymized trace file: \" + outputTracePath);\r\n        } finally {\r\n            if (outGen != null) {\r\n                outGen.close();\r\n            }\r\n            if (reader != null) {\r\n                reader.close();\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "anonymizeTopology",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void anonymizeTopology() throws Exception\n{\r\n    if (anonymizeTopology) {\r\n        System.out.println(\"Anonymizing topology file: \" + inputTopologyPath);\r\n        ClusterTopologyReader reader = null;\r\n        JsonGenerator outGen = null;\r\n        Configuration conf = getConf();\r\n        try {\r\n            outGen = createJsonGenerator(conf, outputTopologyPath);\r\n            reader = new ClusterTopologyReader(inputTopologyPath, conf);\r\n            LoggedNetworkTopology job = reader.get();\r\n            outGen.writeObject(job);\r\n            System.out.println(\"Anonymized topology file: \" + outputTopologyPath);\r\n        } finally {\r\n            if (outGen != null) {\r\n                outGen.close();\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "createJsonGenerator",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "JsonGenerator createJsonGenerator(Configuration conf, Path path) throws IOException\n{\r\n    FileSystem outFS = path.getFileSystem(conf);\r\n    CompressionCodec codec = new CompressionCodecFactory(conf).getCodec(path);\r\n    OutputStream output;\r\n    Compressor compressor = null;\r\n    if (codec != null) {\r\n        compressor = CodecPool.getCompressor(codec);\r\n        output = codec.createOutputStream(outFS.create(path), compressor);\r\n    } else {\r\n        output = outFS.create(path);\r\n    }\r\n    JsonGenerator outGen = outFactory.createGenerator(output, JsonEncoding.UTF8);\r\n    outGen.useDefaultPrettyPrinter();\r\n    return outGen;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "run",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    try {\r\n        initialize(args);\r\n    } catch (Exception e) {\r\n        e.printStackTrace();\r\n        printUsage();\r\n        return -1;\r\n    }\r\n    return run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "run",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "int run() throws Exception\n{\r\n    try {\r\n        anonymizeTrace();\r\n    } catch (IOException ioe) {\r\n        System.err.println(\"Error running the trace anonymizer!\");\r\n        ioe.printStackTrace();\r\n        System.out.println(\"\\n\\nAnonymization unsuccessful!\");\r\n        return -1;\r\n    }\r\n    try {\r\n        anonymizeTopology();\r\n    } catch (IOException ioe) {\r\n        System.err.println(\"Error running the cluster topology anonymizer!\");\r\n        ioe.printStackTrace();\r\n        System.out.println(\"\\n\\nAnonymization unsuccessful!\");\r\n        return -1;\r\n    }\r\n    statePool.persist();\r\n    System.out.println(\"Anonymization completed successfully!\");\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void printUsage()\n{\r\n    System.out.println(\"\\nUsage:-\");\r\n    System.out.print(\"  Anonymizer\");\r\n    System.out.print(\" [-trace <input-trace-path> <output-trace-path>]\");\r\n    System.out.println(\" [-topology <input-topology-path> \" + \"<output-topology-path>] \");\r\n    System.out.print(\"\\n\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "main",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    Anonymizer instance = new Anonymizer();\r\n    int result = 0;\r\n    try {\r\n        result = ToolRunner.run(instance, args);\r\n    } catch (Exception e) {\r\n        e.printStackTrace(System.err);\r\n        System.exit(-1);\r\n    }\r\n    if (result != 0) {\r\n        System.exit(result);\r\n    }\r\n    return;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "dump",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void dump(PrintStream stream)\n{\r\n    stream.print(\"dumping Histogram \" + name + \":\\n\");\r\n    Iterator<Map.Entry<Long, Long>> iter = iterator();\r\n    while (iter.hasNext()) {\r\n        Map.Entry<Long, Long> ent = iter.next();\r\n        stream.print(\"val/count pair: \" + (long) ent.getKey() + \", \" + (long) ent.getValue() + \"\\n\");\r\n    }\r\n    stream.print(\"*** end *** \\n\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "iterator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Iterator<Map.Entry<Long, Long>> iterator()\n{\r\n    return content.entrySet().iterator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long get(long key)\n{\r\n    Long result = content.get(key);\r\n    return result == null ? 0 : result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getTotalCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTotalCount()\n{\r\n    return totalCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "enter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void enter(long value)\n{\r\n    Long existingValue = content.get(value);\r\n    if (existingValue == null) {\r\n        content.put(value, 1L);\r\n    } else {\r\n        content.put(value, existingValue + 1L);\r\n    }\r\n    ++totalCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getCDF",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "long[] getCDF(int scale, int[] buckets)\n{\r\n    if (totalCount == 0) {\r\n        return null;\r\n    }\r\n    long[] result = new long[buckets.length + 2];\r\n    result[0] = content.firstEntry().getKey();\r\n    result[buckets.length + 1] = content.lastEntry().getKey();\r\n    Iterator<Map.Entry<Long, Long>> iter = content.entrySet().iterator();\r\n    long cumulativeCount = 0;\r\n    int bucketCursor = 0;\r\n    while (iter.hasNext()) {\r\n        long targetCumulativeCount = buckets[bucketCursor] * totalCount / scale;\r\n        Map.Entry<Long, Long> elt = iter.next();\r\n        cumulativeCount += elt.getValue();\r\n        while (cumulativeCount >= targetCumulativeCount) {\r\n            result[bucketCursor + 1] = elt.getKey();\r\n            ++bucketCursor;\r\n            if (bucketCursor < buckets.length) {\r\n                targetCumulativeCount = buckets[bucketCursor] * totalCount / scale;\r\n            } else {\r\n                break;\r\n            }\r\n        }\r\n        if (bucketCursor == buckets.length) {\r\n            break;\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getValue()\n{\r\n    return jobName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes",
  "methodName" : "getPrefix",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPrefix()\n{\r\n    return \"job\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int read() throws IOException\n{\r\n    return coreInputStream.read();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int read(byte[] buffer, int offset, int length) throws IOException\n{\r\n    return coreInputStream.read(buffer, offset, length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    coreInputStream.close();\r\n    if (decompressor != null) {\r\n        CodecPool.returnDecompressor(decompressor);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMachines",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<MachineNode> getMachines()\n{\r\n    parseTopologyTree();\r\n    return machineNodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getRacks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<RackNode> getRacks()\n{\r\n    parseTopologyTree();\r\n    return rackNodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getRandomMachines",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "MachineNode[] getRandomMachines(int expected, Random random)\n{\r\n    if (expected == 0) {\r\n        return new MachineNode[0];\r\n    }\r\n    parseTopologyTree();\r\n    int total = machineNodes.size();\r\n    int select = Math.min(expected, total);\r\n    if (mNodesFlattened == null) {\r\n        mNodesFlattened = machineNodes.toArray(new MachineNode[total]);\r\n    }\r\n    MachineNode[] retval = new MachineNode[select];\r\n    int i = 0;\r\n    while ((i != select) && (total != i + select)) {\r\n        int index = random.nextInt(total - i);\r\n        MachineNode tmp = mNodesFlattened[index];\r\n        mNodesFlattened[index] = mNodesFlattened[total - i - 1];\r\n        mNodesFlattened[total - i - 1] = tmp;\r\n        ++i;\r\n    }\r\n    if (i == select) {\r\n        System.arraycopy(mNodesFlattened, total - i, retval, 0, select);\r\n    } else {\r\n        System.arraycopy(mNodesFlattened, 0, retval, 0, select);\r\n    }\r\n    return retval;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "buildMachineNodeMap",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void buildMachineNodeMap()\n{\r\n    if (mNodeMap == null) {\r\n        mNodeMap = new HashMap<String, MachineNode>(machineNodes.size());\r\n        for (MachineNode mn : machineNodes) {\r\n            mNodeMap.put(mn.getName(), mn);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMachineByName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MachineNode getMachineByName(String name)\n{\r\n    buildMachineNodeMap();\r\n    return mNodeMap.get(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "distance",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int distance(Node a, Node b)\n{\r\n    int lvl_a = a.getLevel();\r\n    int lvl_b = b.getLevel();\r\n    int retval = 0;\r\n    if (lvl_a > lvl_b) {\r\n        retval = lvl_a - lvl_b;\r\n        for (int i = 0; i < retval; ++i) {\r\n            a = a.getParent();\r\n        }\r\n    } else if (lvl_a < lvl_b) {\r\n        retval = lvl_b - lvl_a;\r\n        for (int i = 0; i < retval; ++i) {\r\n            b = b.getParent();\r\n        }\r\n    }\r\n    while (a != b) {\r\n        a = a.getParent();\r\n        b = b.getParent();\r\n        ++retval;\r\n    }\r\n    return retval;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "buildRackNodeMap",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void buildRackNodeMap()\n{\r\n    if (rNodeMap == null) {\r\n        rNodeMap = new HashMap<String, RackNode>(rackNodes.size());\r\n        for (RackNode rn : rackNodes) {\r\n            rNodeMap.put(rn.getName(), rn);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getRackByName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RackNode getRackByName(String name)\n{\r\n    buildRackNodeMap();\r\n    return rNodeMap.get(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMaximumDistance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMaximumDistance()\n{\r\n    parseTopologyTree();\r\n    return maximumDistance;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "parseTopologyTree",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void parseTopologyTree()\n{\r\n    if (machineNodes == null) {\r\n        Node root = getClusterTopology();\r\n        SortedSet<MachineNode> mNodes = new TreeSet<MachineNode>();\r\n        SortedSet<RackNode> rNodes = new TreeSet<RackNode>();\r\n        Deque<Node> unvisited = new ArrayDeque<Node>();\r\n        Deque<Integer> distUnvisited = new ArrayDeque<Integer>();\r\n        unvisited.add(root);\r\n        distUnvisited.add(0);\r\n        for (Node n = unvisited.poll(); n != null; n = unvisited.poll()) {\r\n            int distance = distUnvisited.poll();\r\n            if (n instanceof RackNode) {\r\n                rNodes.add((RackNode) n);\r\n                mNodes.addAll(((RackNode) n).getMachinesInRack());\r\n                if (distance + 1 > maximumDistance) {\r\n                    maximumDistance = distance + 1;\r\n                }\r\n            } else if (n instanceof MachineNode) {\r\n                mNodes.add((MachineNode) n);\r\n                if (distance > maximumDistance) {\r\n                    maximumDistance = distance;\r\n                }\r\n            } else {\r\n                for (Node child : n.getChildren()) {\r\n                    unvisited.addFirst(child);\r\n                    distUnvisited.addFirst(distance + 1);\r\n                }\r\n            }\r\n        }\r\n        machineNodes = Collections.unmodifiableSortedSet(mNodes);\r\n        rackNodes = Collections.unmodifiableSortedSet(rNodes);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\state",
  "methodName" : "addState",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addState(Class id, State state)\n{\r\n    if (pool.containsKey(id.getName())) {\r\n        throw new RuntimeException(\"State '\" + state.getName() + \"' added for the\" + \" class \" + id.getName() + \" already exists!\");\r\n    }\r\n    isUpdated = true;\r\n    pool.put(id.getName(), new StatePair(state));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\state",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "State getState(Class clazz)\n{\r\n    return pool.containsKey(clazz.getName()) ? pool.get(clazz.getName()).getState() : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\state",
  "methodName" : "isUpdated",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isUpdated()\n{\r\n    if (!isUpdated) {\r\n        for (StatePair statePair : pool.values()) {\r\n            if (statePair.getState().isUpdated()) {\r\n                isUpdated = true;\r\n                return true;\r\n            }\r\n        }\r\n    }\r\n    return isUpdated;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\state",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void initialize(Configuration conf) throws Exception\n{\r\n    if (isInitialized) {\r\n        throw new RuntimeException(\"StatePool is already initialized!\");\r\n    }\r\n    this.conf = conf;\r\n    String persistDir = conf.get(DIR_CONFIG);\r\n    reload = conf.getBoolean(RELOAD_CONFIG, false);\r\n    persist = conf.getBoolean(PERSIST_CONFIG, false);\r\n    if (reload || persist) {\r\n        System.out.println(\"State Manager initializing. State directory : \" + persistDir);\r\n        System.out.println(\"Reload:\" + reload + \" Persist:\" + persist);\r\n        if (persistDir == null) {\r\n            throw new RuntimeException(\"No state persist directory configured!\" + \" Disable persistence.\");\r\n        } else {\r\n            this.persistDirPath = new Path(persistDir);\r\n        }\r\n    } else {\r\n        System.out.println(\"State Manager disabled.\");\r\n    }\r\n    reload();\r\n    DateFormat formatter = new SimpleDateFormat(\"dd-MMM-yyyy-hh'H'-mm'M'-ss'S'\");\r\n    Calendar calendar = Calendar.getInstance();\r\n    calendar.setTimeInMillis(System.currentTimeMillis());\r\n    timeStamp = formatter.format(calendar.getTime());\r\n    isInitialized = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\state",
  "methodName" : "reload",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void reload() throws Exception\n{\r\n    if (reload) {\r\n        Path stateFilename = new Path(persistDirPath, COMMIT_STATE_FILENAME);\r\n        if (!reloadState(stateFilename, conf)) {\r\n            throw new RuntimeException(\"No latest state persist directory found!\" + \" Disable persistence and run.\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\state",
  "methodName" : "reloadState",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean reloadState(Path stateFile, Configuration configuration) throws Exception\n{\r\n    FileSystem fs = stateFile.getFileSystem(configuration);\r\n    try (FSDataInputStream in = fs.open(stateFile)) {\r\n        System.out.println(\"Reading state from \" + stateFile.toString());\r\n        read(in);\r\n        return true;\r\n    } catch (FileNotFoundException e) {\r\n        System.out.println(\"No state information found for \" + stateFile);\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\state",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void read(DataInput in) throws IOException\n{\r\n    ObjectMapper mapper = new ObjectMapper();\r\n    SimpleModule module = new SimpleModule(\"State Serializer\", new Version(0, 1, 1, \"FINAL\", \"\", \"\"));\r\n    module.addDeserializer(StatePair.class, new StateDeserializer());\r\n    mapper.registerModule(module);\r\n    JsonParser parser = mapper.getFactory().createParser((InputStream) in);\r\n    StatePool statePool = mapper.readValue(parser, StatePool.class);\r\n    this.setStates(statePool.getStates());\r\n    parser.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\state",
  "methodName" : "persist",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void persist() throws IOException\n{\r\n    if (!persist) {\r\n        return;\r\n    }\r\n    if (isUpdated()) {\r\n        System.out.println(\"State is updated! Committing.\");\r\n        Path currStateFile = new Path(persistDirPath, CURRENT_STATE_FILENAME);\r\n        Path commitStateFile = new Path(persistDirPath, COMMIT_STATE_FILENAME);\r\n        FileSystem fs = currStateFile.getFileSystem(conf);\r\n        System.out.println(\"Starting the persist phase. Persisting to \" + currStateFile.toString());\r\n        FSDataOutputStream out = fs.create(currStateFile, true);\r\n        write(out);\r\n        out.close();\r\n        System.out.println(\"Persist phase over. The best known un-committed state\" + \" is located at \" + currStateFile.toString());\r\n        if (fs.exists(commitStateFile)) {\r\n            Path commitRelocationFile = new Path(persistDirPath, timeStamp);\r\n            System.out.println(\"Starting the pre-commit phase. Moving the previous \" + \"best known state to \" + commitRelocationFile.toString());\r\n            FileUtil.copy(fs, commitStateFile, fs, commitRelocationFile, false, conf);\r\n        }\r\n        System.out.println(\"Starting the commit phase. Committing the states in \" + currStateFile.toString());\r\n        FileUtil.copy(fs, currStateFile, fs, commitStateFile, true, true, conf);\r\n        System.out.println(\"Commit phase successful! The best known committed \" + \"state is located at \" + commitStateFile.toString());\r\n    } else {\r\n        System.out.println(\"State not updated! No commit required.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\state",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    System.out.println(\"Dumping the StatePool's in JSON format.\");\r\n    ObjectMapper outMapper = new ObjectMapper();\r\n    SimpleModule module = new SimpleModule(\"State Serializer\", new Version(0, 1, 1, \"FINAL\", \"\", \"\"));\r\n    outMapper.registerModule(module);\r\n    JsonFactory outFactory = outMapper.getFactory();\r\n    JsonGenerator jGen = outFactory.createGenerator((OutputStream) out, JsonEncoding.UTF8);\r\n    jGen.useDefaultPrettyPrinter();\r\n    jGen.writeObject(this);\r\n    jGen.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\state",
  "methodName" : "getVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getVersion()\n{\r\n    return VERSION;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\state",
  "methodName" : "setVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setVersion(long version)\n{\r\n    if (version != VERSION) {\r\n        throw new RuntimeException(\"Version mismatch! Expected \" + VERSION + \" got \" + version);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\state",
  "methodName" : "getStates",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HashMap<String, StatePair> getStates()\n{\r\n    return pool;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\state",
  "methodName" : "setStates",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStates(HashMap<String, StatePair> states)\n{\r\n    if (pool.size() > 0) {\r\n        throw new RuntimeException(\"Pool not empty!\");\r\n    }\r\n    this.pool = states;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen\\datatypes\\util",
  "methodName" : "parseJobProperty",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DataType<?> parseJobProperty(String key, String value)\n{\r\n    return new DefaultDataType(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobID()\n{\r\n    return jobID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "process",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void process(HistoryEvent event)\n{\r\n    if (finalized) {\r\n        throw new IllegalStateException(\"JobBuilder.process(HistoryEvent event) called after ParsedJob built\");\r\n    }\r\n    if (event instanceof AMStartedEvent) {\r\n        return;\r\n    } else if (event instanceof NormalizedResourceEvent) {\r\n        LOG.warn(\"NormalizedResourceEvent should be ignored in history server.\");\r\n    } else if (event instanceof JobFinishedEvent) {\r\n        processJobFinishedEvent((JobFinishedEvent) event);\r\n    } else if (event instanceof JobInfoChangeEvent) {\r\n        processJobInfoChangeEvent((JobInfoChangeEvent) event);\r\n    } else if (event instanceof JobInitedEvent) {\r\n        processJobInitedEvent((JobInitedEvent) event);\r\n    } else if (event instanceof JobPriorityChangeEvent) {\r\n        processJobPriorityChangeEvent((JobPriorityChangeEvent) event);\r\n    } else if (event instanceof JobQueueChangeEvent) {\r\n        processJobQueueChangeEvent((JobQueueChangeEvent) event);\r\n    } else if (event instanceof JobStatusChangedEvent) {\r\n        processJobStatusChangedEvent((JobStatusChangedEvent) event);\r\n    } else if (event instanceof JobSubmittedEvent) {\r\n        processJobSubmittedEvent((JobSubmittedEvent) event);\r\n    } else if (event instanceof JobUnsuccessfulCompletionEvent) {\r\n        processJobUnsuccessfulCompletionEvent((JobUnsuccessfulCompletionEvent) event);\r\n    } else if (event instanceof MapAttemptFinishedEvent) {\r\n        processMapAttemptFinishedEvent((MapAttemptFinishedEvent) event);\r\n    } else if (event instanceof ReduceAttemptFinishedEvent) {\r\n        processReduceAttemptFinishedEvent((ReduceAttemptFinishedEvent) event);\r\n    } else if (event instanceof TaskAttemptFinishedEvent) {\r\n        processTaskAttemptFinishedEvent((TaskAttemptFinishedEvent) event);\r\n    } else if (event instanceof TaskAttemptStartedEvent) {\r\n        processTaskAttemptStartedEvent((TaskAttemptStartedEvent) event);\r\n    } else if (event instanceof TaskAttemptUnsuccessfulCompletionEvent) {\r\n        processTaskAttemptUnsuccessfulCompletionEvent((TaskAttemptUnsuccessfulCompletionEvent) event);\r\n    } else if (event instanceof TaskFailedEvent) {\r\n        processTaskFailedEvent((TaskFailedEvent) event);\r\n    } else if (event instanceof TaskFinishedEvent) {\r\n        processTaskFinishedEvent((TaskFinishedEvent) event);\r\n    } else if (event instanceof TaskStartedEvent) {\r\n        processTaskStartedEvent((TaskStartedEvent) event);\r\n    } else if (event instanceof TaskUpdatedEvent) {\r\n        processTaskUpdatedEvent((TaskUpdatedEvent) event);\r\n    } else\r\n        throw new IllegalArgumentException(\"JobBuilder.process(HistoryEvent): unknown event type:\" + event.getEventType() + \" for event:\" + event);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "extract",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String extract(Properties conf, String[] names, String defaultValue)\n{\r\n    for (String name : names) {\r\n        String result = conf.getProperty(name);\r\n        if (result != null) {\r\n            return result;\r\n        }\r\n    }\r\n    return defaultValue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "extractMegabytes",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Integer extractMegabytes(Properties conf, String[] names)\n{\r\n    String javaOptions = extract(conf, names, null);\r\n    if (javaOptions == null) {\r\n        return null;\r\n    }\r\n    Matcher matcher = heapPattern.matcher(javaOptions);\r\n    Integer heapMegabytes = null;\r\n    while (matcher.find()) {\r\n        String heapSize = matcher.group(1);\r\n        heapMegabytes = ((int) (StringUtils.TraditionalBinaryPrefix.string2long(heapSize) / BYTES_IN_MEG));\r\n    }\r\n    return heapMegabytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "maybeSetHeapMegabytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void maybeSetHeapMegabytes(Integer megabytes)\n{\r\n    if (megabytes != null) {\r\n        result.setHeapMegabytes(megabytes);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "maybeSetJobMapMB",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void maybeSetJobMapMB(Integer megabytes)\n{\r\n    if (megabytes != null) {\r\n        result.setJobMapMB(megabytes);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "maybeSetJobReduceMB",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void maybeSetJobReduceMB(Integer megabytes)\n{\r\n    if (megabytes != null) {\r\n        result.setJobReduceMB(megabytes);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "process",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void process(Properties conf)\n{\r\n    if (finalized) {\r\n        throw new IllegalStateException(\"JobBuilder.process(Properties conf) called after ParsedJob built\");\r\n    }\r\n    String queue = extract(conf, JobConfPropertyNames.QUEUE_NAMES.getCandidates(), null);\r\n    if (queue != null) {\r\n        result.setQueue(queue);\r\n    }\r\n    result.setJobName(extract(conf, JobConfPropertyNames.JOB_NAMES.getCandidates(), null));\r\n    maybeSetHeapMegabytes(extractMegabytes(conf, JobConfPropertyNames.TASK_JAVA_OPTS_S.getCandidates()));\r\n    maybeSetJobMapMB(extractMegabytes(conf, JobConfPropertyNames.MAP_JAVA_OPTS_S.getCandidates()));\r\n    maybeSetJobReduceMB(extractMegabytes(conf, JobConfPropertyNames.REDUCE_JAVA_OPTS_S.getCandidates()));\r\n    this.jobConfigurationParameters = conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "build",
  "errType" : null,
  "containingMethodsNum" : 44,
  "sourceCodeText" : "ParsedJob build()\n{\r\n    finalized = true;\r\n    if (jobConfigurationParameters != null) {\r\n        result.setJobProperties(jobConfigurationParameters);\r\n    }\r\n    Histogram[] successfulMapAttemptTimes = new Histogram[ParsedHost.numberOfDistances() + 1];\r\n    for (int i = 0; i < successfulMapAttemptTimes.length; ++i) {\r\n        successfulMapAttemptTimes[i] = new Histogram();\r\n    }\r\n    Histogram successfulReduceAttemptTimes = new Histogram();\r\n    Histogram[] failedMapAttemptTimes = new Histogram[ParsedHost.numberOfDistances() + 1];\r\n    for (int i = 0; i < failedMapAttemptTimes.length; ++i) {\r\n        failedMapAttemptTimes[i] = new Histogram();\r\n    }\r\n    Histogram failedReduceAttemptTimes = new Histogram();\r\n    Histogram successfulNthMapperAttempts = new Histogram();\r\n    for (LoggedTask task : result.getMapTasks()) {\r\n        for (LoggedTaskAttempt attempt : task.getAttempts()) {\r\n            int distance = successfulMapAttemptTimes.length - 1;\r\n            Long runtime = null;\r\n            if (attempt.getFinishTime() > 0 && attempt.getStartTime() > 0) {\r\n                runtime = attempt.getFinishTime() - attempt.getStartTime();\r\n                if (attempt.getResult() == Values.SUCCESS) {\r\n                    LoggedLocation host = attempt.getLocation();\r\n                    List<LoggedLocation> locs = task.getPreferredLocations();\r\n                    if (host != null && locs != null) {\r\n                        for (LoggedLocation loc : locs) {\r\n                            ParsedHost preferedLoc = new ParsedHost(loc);\r\n                            distance = Math.min(distance, preferedLoc.distance(new ParsedHost(host)));\r\n                        }\r\n                    }\r\n                    if (attempt.getStartTime() > 0 && attempt.getFinishTime() > 0) {\r\n                        if (runtime != null) {\r\n                            successfulMapAttemptTimes[distance].enter(runtime);\r\n                        }\r\n                    }\r\n                    TaskAttemptID attemptID = attempt.getAttemptID();\r\n                    if (attemptID != null) {\r\n                        successfulNthMapperAttempts.enter(attemptID.getId());\r\n                    }\r\n                } else {\r\n                    if (attempt.getResult() == Pre21JobHistoryConstants.Values.FAILED) {\r\n                        if (runtime != null) {\r\n                            failedMapAttemptTimes[distance].enter(runtime);\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n    for (LoggedTask task : result.getReduceTasks()) {\r\n        for (LoggedTaskAttempt attempt : task.getAttempts()) {\r\n            Long runtime = attempt.getFinishTime() - attempt.getStartTime();\r\n            if (attempt.getFinishTime() > 0 && attempt.getStartTime() > 0) {\r\n                runtime = attempt.getFinishTime() - attempt.getStartTime();\r\n            }\r\n            if (attempt.getResult() == Values.SUCCESS) {\r\n                if (runtime != null) {\r\n                    successfulReduceAttemptTimes.enter(runtime);\r\n                }\r\n            } else if (attempt.getResult() == Pre21JobHistoryConstants.Values.FAILED) {\r\n                failedReduceAttemptTimes.enter(runtime);\r\n            }\r\n        }\r\n    }\r\n    result.setFailedMapAttemptCDFs(mapCDFArrayList(failedMapAttemptTimes));\r\n    LoggedDiscreteCDF failedReduce = new LoggedDiscreteCDF();\r\n    failedReduce.setCDF(failedReduceAttemptTimes, attemptTimesPercentiles, 100);\r\n    result.setFailedReduceAttemptCDF(failedReduce);\r\n    result.setSuccessfulMapAttemptCDFs(mapCDFArrayList(successfulMapAttemptTimes));\r\n    LoggedDiscreteCDF succReduce = new LoggedDiscreteCDF();\r\n    succReduce.setCDF(successfulReduceAttemptTimes, attemptTimesPercentiles, 100);\r\n    result.setSuccessfulReduceAttemptCDF(succReduce);\r\n    long totalSuccessfulAttempts = 0L;\r\n    long maxTriesToSucceed = 0L;\r\n    for (Map.Entry<Long, Long> ent : successfulNthMapperAttempts) {\r\n        totalSuccessfulAttempts += ent.getValue();\r\n        maxTriesToSucceed = Math.max(maxTriesToSucceed, ent.getKey());\r\n    }\r\n    if (totalSuccessfulAttempts > 0L) {\r\n        double[] successAfterI = new double[(int) maxTriesToSucceed + 1];\r\n        for (int i = 0; i < successAfterI.length; ++i) {\r\n            successAfterI[i] = 0.0D;\r\n        }\r\n        for (Map.Entry<Long, Long> ent : successfulNthMapperAttempts) {\r\n            successAfterI[ent.getKey().intValue()] = ((double) ent.getValue()) / totalSuccessfulAttempts;\r\n        }\r\n        result.setMapperTriesToSucceed(successAfterI);\r\n    } else {\r\n        result.setMapperTriesToSucceed(null);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "mapCDFArrayList",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ArrayList<LoggedDiscreteCDF> mapCDFArrayList(Histogram[] data)\n{\r\n    ArrayList<LoggedDiscreteCDF> result = new ArrayList<LoggedDiscreteCDF>();\r\n    for (Histogram hist : data) {\r\n        LoggedDiscreteCDF discCDF = new LoggedDiscreteCDF();\r\n        discCDF.setCDF(hist, attemptTimesPercentiles, 100);\r\n        result.add(discCDF);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getPre21Value",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Values getPre21Value(String name)\n{\r\n    if (name.equalsIgnoreCase(\"JOB_CLEANUP\")) {\r\n        return Values.CLEANUP;\r\n    }\r\n    if (name.equalsIgnoreCase(\"JOB_SETUP\")) {\r\n        return Values.SETUP;\r\n    }\r\n    if (name.equalsIgnoreCase(TaskStatus.State.SUCCEEDED.toString())) {\r\n        return Values.SUCCESS;\r\n    }\r\n    return Values.valueOf(StringUtils.toUpperCase(name));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processTaskUpdatedEvent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void processTaskUpdatedEvent(TaskUpdatedEvent event)\n{\r\n    ParsedTask task = getTask(event.getTaskId().toString());\r\n    if (task == null) {\r\n        return;\r\n    }\r\n    task.setFinishTime(event.getFinishTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processTaskStartedEvent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void processTaskStartedEvent(TaskStartedEvent event)\n{\r\n    ParsedTask task = getOrMakeTask(event.getTaskType(), event.getTaskId().toString(), true);\r\n    task.setStartTime(event.getStartTime());\r\n    task.setPreferredLocations(preferredLocationForSplits(event.getSplitLocations()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processTaskFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void processTaskFinishedEvent(TaskFinishedEvent event)\n{\r\n    ParsedTask task = getOrMakeTask(event.getTaskType(), event.getTaskId().toString(), false);\r\n    if (task == null) {\r\n        return;\r\n    }\r\n    task.setFinishTime(event.getFinishTime());\r\n    task.setTaskStatus(getPre21Value(event.getTaskStatus()));\r\n    task.incorporateCounters(((TaskFinished) event.getDatum()).counters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processTaskFailedEvent",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void processTaskFailedEvent(TaskFailedEvent event)\n{\r\n    ParsedTask task = getOrMakeTask(event.getTaskType(), event.getTaskId().toString(), false);\r\n    if (task == null) {\r\n        return;\r\n    }\r\n    task.setFinishTime(event.getFinishTime());\r\n    task.setTaskStatus(getPre21Value(event.getTaskStatus()));\r\n    TaskFailed t = (TaskFailed) (event.getDatum());\r\n    task.putDiagnosticInfo(t.error.toString());\r\n    if (t.getFailedDueToAttempt() != null) {\r\n        task.putFailedDueToAttemptId(t.getFailedDueToAttempt().toString());\r\n    }\r\n    org.apache.hadoop.mapreduce.jobhistory.JhCounters counters = ((TaskFailed) event.getDatum()).getCounters();\r\n    task.incorporateCounters(counters == null ? EMPTY_COUNTERS : counters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processTaskAttemptUnsuccessfulCompletionEvent",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void processTaskAttemptUnsuccessfulCompletionEvent(TaskAttemptUnsuccessfulCompletionEvent event)\n{\r\n    ParsedTaskAttempt attempt = getOrMakeTaskAttempt(event.getTaskType(), event.getTaskId().toString(), event.getTaskAttemptId().toString());\r\n    if (attempt == null) {\r\n        return;\r\n    }\r\n    attempt.setResult(getPre21Value(event.getTaskStatus()));\r\n    attempt.setHostName(event.getHostname(), event.getRackName());\r\n    ParsedHost pHost = getAndRecordParsedHost(event.getRackName(), event.getHostname());\r\n    if (pHost != null) {\r\n        attempt.setLocation(pHost.makeLoggedLocation());\r\n    }\r\n    attempt.setFinishTime(event.getFinishTime());\r\n    org.apache.hadoop.mapreduce.jobhistory.JhCounters counters = ((TaskAttemptUnsuccessfulCompletion) event.getDatum()).getCounters();\r\n    attempt.incorporateCounters(counters == null ? EMPTY_COUNTERS : counters);\r\n    attempt.arraySetClockSplits(event.getClockSplits());\r\n    attempt.arraySetCpuUsages(event.getCpuUsages());\r\n    attempt.arraySetVMemKbytes(event.getVMemKbytes());\r\n    attempt.arraySetPhysMemKbytes(event.getPhysMemKbytes());\r\n    TaskAttemptUnsuccessfulCompletion t = (TaskAttemptUnsuccessfulCompletion) (event.getDatum());\r\n    attempt.putDiagnosticInfo(t.getError().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processTaskAttemptStartedEvent",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void processTaskAttemptStartedEvent(TaskAttemptStartedEvent event)\n{\r\n    ParsedTaskAttempt attempt = getOrMakeTaskAttempt(event.getTaskType(), event.getTaskId().toString(), event.getTaskAttemptId().toString());\r\n    if (attempt == null) {\r\n        return;\r\n    }\r\n    attempt.setStartTime(event.getStartTime());\r\n    attempt.putTrackerName(event.getTrackerName());\r\n    attempt.putHttpPort(event.getHttpPort());\r\n    attempt.putShufflePort(event.getShufflePort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processTaskAttemptFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void processTaskAttemptFinishedEvent(TaskAttemptFinishedEvent event)\n{\r\n    ParsedTaskAttempt attempt = getOrMakeTaskAttempt(event.getTaskType(), event.getTaskId().toString(), event.getAttemptId().toString());\r\n    if (attempt == null) {\r\n        return;\r\n    }\r\n    attempt.setResult(getPre21Value(event.getTaskStatus()));\r\n    ParsedHost pHost = getAndRecordParsedHost(event.getRackName(), event.getHostname());\r\n    if (pHost != null) {\r\n        attempt.setLocation(pHost.makeLoggedLocation());\r\n    }\r\n    attempt.setFinishTime(event.getFinishTime());\r\n    attempt.incorporateCounters(((TaskAttemptFinished) event.getDatum()).counters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processReduceAttemptFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void processReduceAttemptFinishedEvent(ReduceAttemptFinishedEvent event)\n{\r\n    ParsedTaskAttempt attempt = getOrMakeTaskAttempt(event.getTaskType(), event.getTaskId().toString(), event.getAttemptId().toString());\r\n    if (attempt == null) {\r\n        return;\r\n    }\r\n    attempt.setResult(getPre21Value(event.getTaskStatus()));\r\n    attempt.setHostName(event.getHostname(), event.getRackName());\r\n    ParsedHost pHost = getAndRecordParsedHost(event.getRackName(), event.getHostname());\r\n    if (pHost != null) {\r\n        attempt.setLocation(pHost.makeLoggedLocation());\r\n    }\r\n    attempt.setFinishTime(event.getFinishTime());\r\n    attempt.setShuffleFinished(event.getShuffleFinishTime());\r\n    attempt.setSortFinished(event.getSortFinishTime());\r\n    attempt.incorporateCounters(((ReduceAttemptFinished) event.getDatum()).counters);\r\n    attempt.arraySetClockSplits(event.getClockSplits());\r\n    attempt.arraySetCpuUsages(event.getCpuUsages());\r\n    attempt.arraySetVMemKbytes(event.getVMemKbytes());\r\n    attempt.arraySetPhysMemKbytes(event.getPhysMemKbytes());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processMapAttemptFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void processMapAttemptFinishedEvent(MapAttemptFinishedEvent event)\n{\r\n    ParsedTaskAttempt attempt = getOrMakeTaskAttempt(event.getTaskType(), event.getTaskId().toString(), event.getAttemptId().toString());\r\n    if (attempt == null) {\r\n        return;\r\n    }\r\n    attempt.setResult(getPre21Value(event.getTaskStatus()));\r\n    attempt.setHostName(event.getHostname(), event.getRackName());\r\n    ParsedHost pHost = getAndRecordParsedHost(event.getRackName(), event.getHostname());\r\n    if (pHost != null) {\r\n        attempt.setLocation(pHost.makeLoggedLocation());\r\n    }\r\n    attempt.setFinishTime(event.getFinishTime());\r\n    attempt.incorporateCounters(((MapAttemptFinished) event.getDatum()).counters);\r\n    attempt.arraySetClockSplits(event.getClockSplits());\r\n    attempt.arraySetCpuUsages(event.getCpuUsages());\r\n    attempt.arraySetVMemKbytes(event.getVMemKbytes());\r\n    attempt.arraySetPhysMemKbytes(event.getPhysMemKbytes());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processJobUnsuccessfulCompletionEvent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void processJobUnsuccessfulCompletionEvent(JobUnsuccessfulCompletionEvent event)\n{\r\n    result.setOutcome(Pre21JobHistoryConstants.Values.valueOf(event.getStatus()));\r\n    result.setFinishTime(event.getFinishTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processJobSubmittedEvent",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void processJobSubmittedEvent(JobSubmittedEvent event)\n{\r\n    result.setJobID(event.getJobId().toString());\r\n    result.setJobName(event.getJobName());\r\n    result.setUser(event.getUserName());\r\n    result.setSubmitTime(event.getSubmitTime());\r\n    result.putJobConfPath(event.getJobConfPath());\r\n    result.putJobAcls(event.getJobAcls());\r\n    String queue = event.getJobQueueName();\r\n    if (queue != null) {\r\n        result.setQueue(queue);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processJobQueueChangeEvent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void processJobQueueChangeEvent(JobQueueChangeEvent event)\n{\r\n    String queue = event.getJobQueueName();\r\n    if (queue != null) {\r\n        result.setQueue(queue);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processJobStatusChangedEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void processJobStatusChangedEvent(JobStatusChangedEvent event)\n{\r\n    result.setOutcome(Pre21JobHistoryConstants.Values.valueOf(event.getStatus()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processJobPriorityChangeEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void processJobPriorityChangeEvent(JobPriorityChangeEvent event)\n{\r\n    result.setPriority(LoggedJob.JobPriority.valueOf(event.getPriority().toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processJobInitedEvent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void processJobInitedEvent(JobInitedEvent event)\n{\r\n    result.setLaunchTime(event.getLaunchTime());\r\n    result.setTotalMaps(event.getTotalMaps());\r\n    result.setTotalReduces(event.getTotalReduces());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processJobInfoChangeEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void processJobInfoChangeEvent(JobInfoChangeEvent event)\n{\r\n    result.setLaunchTime(event.getLaunchTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "processJobFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void processJobFinishedEvent(JobFinishedEvent event)\n{\r\n    result.setFinishTime(event.getFinishTime());\r\n    result.setJobID(jobID);\r\n    result.setOutcome(Values.SUCCESS);\r\n    JobFinished job = (JobFinished) event.getDatum();\r\n    Map<String, Long> countersMap = JobHistoryUtils.extractCounters(job.totalCounters);\r\n    result.putTotalCounters(countersMap);\r\n    countersMap = JobHistoryUtils.extractCounters(job.mapCounters);\r\n    result.putMapCounters(countersMap);\r\n    countersMap = JobHistoryUtils.extractCounters(job.reduceCounters);\r\n    result.putReduceCounters(countersMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getTask",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ParsedTask getTask(String taskIDname)\n{\r\n    ParsedTask result = mapTasks.get(taskIDname);\r\n    if (result != null) {\r\n        return result;\r\n    }\r\n    result = reduceTasks.get(taskIDname);\r\n    if (result != null) {\r\n        return result;\r\n    }\r\n    return otherTasks.get(taskIDname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getOrMakeTask",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ParsedTask getOrMakeTask(TaskType type, String taskIDname, boolean allowCreate)\n{\r\n    Map<String, ParsedTask> taskMap = otherTasks;\r\n    List<LoggedTask> tasks = this.result.getOtherTasks();\r\n    switch(type) {\r\n        case MAP:\r\n            taskMap = mapTasks;\r\n            tasks = this.result.getMapTasks();\r\n            break;\r\n        case REDUCE:\r\n            taskMap = reduceTasks;\r\n            tasks = this.result.getReduceTasks();\r\n            break;\r\n        default:\r\n    }\r\n    ParsedTask result = taskMap.get(taskIDname);\r\n    if (result == null && allowCreate) {\r\n        result = new ParsedTask();\r\n        result.setTaskType(getPre21Value(type.toString()));\r\n        result.setTaskID(taskIDname);\r\n        taskMap.put(taskIDname, result);\r\n        tasks.add(result);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getOrMakeTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ParsedTaskAttempt getOrMakeTaskAttempt(TaskType type, String taskIDName, String taskAttemptName)\n{\r\n    ParsedTask task = getOrMakeTask(type, taskIDName, false);\r\n    ParsedTaskAttempt result = attempts.get(taskAttemptName);\r\n    if (result == null && task != null) {\r\n        result = new ParsedTaskAttempt();\r\n        result.setAttemptID(taskAttemptName);\r\n        attempts.put(taskAttemptName, result);\r\n        task.getAttempts().add(result);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getAndRecordParsedHost",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ParsedHost getAndRecordParsedHost(String hostName)\n{\r\n    return getAndRecordParsedHost(null, hostName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getAndRecordParsedHost",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ParsedHost getAndRecordParsedHost(String rackName, String hostName)\n{\r\n    ParsedHost result = null;\r\n    if (rackName == null) {\r\n        result = ParsedHost.parse(hostName);\r\n    } else {\r\n        result = new ParsedHost(rackName, hostName);\r\n    }\r\n    if (result != null) {\r\n        ParsedHost canonicalResult = allHosts.get(result);\r\n        if (canonicalResult != null) {\r\n            return canonicalResult;\r\n        }\r\n        allHosts.put(result, result);\r\n        return result;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "preferredLocationForSplits",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ArrayList<LoggedLocation> preferredLocationForSplits(String splits)\n{\r\n    if (splits != null) {\r\n        ArrayList<LoggedLocation> locations = null;\r\n        StringTokenizer tok = new StringTokenizer(splits, \",\", false);\r\n        if (tok.countTokens() <= MAXIMUM_PREFERRED_LOCATIONS) {\r\n            locations = new ArrayList<LoggedLocation>();\r\n            while (tok.hasMoreTokens()) {\r\n                String nextSplit = tok.nextToken();\r\n                ParsedHost node = getAndRecordParsedHost(nextSplit);\r\n                if (locations != null && node != null) {\r\n                    locations.add(node.makeLoggedLocation());\r\n                }\r\n            }\r\n            return locations;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setUnknownAttribute",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUnknownAttribute(String attributeName, Object ignored)\n{\r\n    if (!alreadySeenAnySetterAttributes.contains(attributeName)) {\r\n        alreadySeenAnySetterAttributes.add(attributeName);\r\n        System.err.println(\"In LoggedJob, we saw the unknown attribute \" + attributeName + \".\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "adjustTimes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void adjustTimes(long adjustment)\n{\r\n    startTime += adjustment;\r\n    finishTime += adjustment;\r\n    for (LoggedTaskAttempt attempt : attempts) {\r\n        attempt.adjustTimes(adjustment);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getInputBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getInputBytes()\n{\r\n    return inputBytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setInputBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setInputBytes(long inputBytes)\n{\r\n    this.inputBytes = inputBytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getInputRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getInputRecords()\n{\r\n    return inputRecords;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setInputRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setInputRecords(long inputRecords)\n{\r\n    this.inputRecords = inputRecords;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getOutputBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getOutputBytes()\n{\r\n    return outputBytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setOutputBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setOutputBytes(long outputBytes)\n{\r\n    this.outputBytes = outputBytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getOutputRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getOutputRecords()\n{\r\n    return outputRecords;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setOutputRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setOutputRecords(long outputRecords)\n{\r\n    this.outputRecords = outputRecords;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getTaskID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskID getTaskID()\n{\r\n    return taskID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setTaskID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTaskID(String taskID)\n{\r\n    this.taskID = TaskID.forName(taskID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStartTime(long startTime)\n{\r\n    this.startTime = startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFinishTime(long finishTime)\n{\r\n    this.finishTime = finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<LoggedTaskAttempt> getAttempts()\n{\r\n    return attempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAttempts(List<LoggedTaskAttempt> attempts)\n{\r\n    if (attempts == null) {\r\n        this.attempts = new ArrayList<LoggedTaskAttempt>();\r\n    } else {\r\n        this.attempts = attempts;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getPreferredLocations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<LoggedLocation> getPreferredLocations()\n{\r\n    return preferredLocations;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setPreferredLocations",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setPreferredLocations(List<LoggedLocation> preferredLocations)\n{\r\n    if (preferredLocations == null || preferredLocations.isEmpty()) {\r\n        this.preferredLocations = Collections.emptyList();\r\n    } else {\r\n        this.preferredLocations = preferredLocations;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getTaskStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Pre21JobHistoryConstants.Values getTaskStatus()\n{\r\n    return taskStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setTaskStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTaskStatus(Pre21JobHistoryConstants.Values taskStatus)\n{\r\n    this.taskStatus = taskStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getTaskType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Pre21JobHistoryConstants.Values getTaskType()\n{\r\n    return taskType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setTaskType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTaskType(Pre21JobHistoryConstants.Values taskType)\n{\r\n    this.taskType = taskType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "incorporateMapCounters",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void incorporateMapCounters(JhCounters counters)\n{\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            task.inputBytes = val;\r\n        }\r\n    }, counters, \"HDFS_BYTES_READ\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            task.outputBytes = val;\r\n        }\r\n    }, counters, \"FILE_BYTES_WRITTEN\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            task.inputRecords = val;\r\n        }\r\n    }, counters, \"MAP_INPUT_RECORDS\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            task.outputRecords = val;\r\n        }\r\n    }, counters, \"MAP_OUTPUT_RECORDS\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "incorporateReduceCounters",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void incorporateReduceCounters(JhCounters counters)\n{\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            task.inputBytes = val;\r\n        }\r\n    }, counters, \"REDUCE_SHUFFLE_BYTES\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            task.outputBytes = val;\r\n        }\r\n    }, counters, \"HDFS_BYTES_WRITTEN\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            task.inputRecords = val;\r\n        }\r\n    }, counters, \"REDUCE_INPUT_RECORDS\");\r\n    incorporateCounter(new SetField(this) {\r\n\r\n        @Override\r\n        void set(long val) {\r\n            task.outputRecords = val;\r\n        }\r\n    }, counters, \"REDUCE_OUTPUT_RECORDS\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "incorporateCounters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void incorporateCounters(JhCounters counters)\n{\r\n    switch(taskType) {\r\n        case MAP:\r\n            incorporateMapCounters(counters);\r\n            return;\r\n        case REDUCE:\r\n            incorporateReduceCounters(counters);\r\n            return;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "canonicalizeCounterName",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String canonicalizeCounterName(String nonCanonicalName)\n{\r\n    String result = StringUtils.toLowerCase(nonCanonicalName);\r\n    result = result.replace(' ', '|');\r\n    result = result.replace('-', '|');\r\n    result = result.replace('_', '|');\r\n    result = result.replace('.', '|');\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "incorporateCounter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void incorporateCounter(SetField thunk, JhCounters counters, String counterName)\n{\r\n    counterName = canonicalizeCounterName(counterName);\r\n    for (JhCounterGroup group : counters.groups) {\r\n        for (JhCounter counter : group.counts) {\r\n            if (counterName.equals(canonicalizeCounterName(counter.name.toString()))) {\r\n                thunk.set(counter.value);\r\n                return;\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void compare1(long c1, long c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 != c2) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void compare1(String c1, String c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    if (c1 == null || c2 == null || !c1.equals(c2)) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void compare1(Pre21JobHistoryConstants.Values c1, Pre21JobHistoryConstants.Values c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    if (c1 == null || c2 == null || !c1.equals(c2)) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compareLoggedLocations",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void compareLoggedLocations(List<LoggedLocation> c1, List<LoggedLocation> c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    if (c1 == null || c2 == null || c1.size() != c2.size()) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n    for (int i = 0; i < c1.size(); ++i) {\r\n        c1.get(i).deepCompare(c2.get(i), new TreePath(loc, eltname, i));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compareLoggedTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void compareLoggedTaskAttempts(List<LoggedTaskAttempt> c1, List<LoggedTaskAttempt> c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    if (c1 == null || c2 == null || c1.size() != c2.size()) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n    for (int i = 0; i < c1.size(); ++i) {\r\n        c1.get(i).deepCompare(c2.get(i), new TreePath(loc, eltname, i));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "deepCompare",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void deepCompare(DeepCompare comparand, TreePath loc) throws DeepInequalityException\n{\r\n    if (!(comparand instanceof LoggedTask)) {\r\n        throw new DeepInequalityException(\"comparand has wrong type\", loc);\r\n    }\r\n    LoggedTask other = (LoggedTask) comparand;\r\n    compare1(inputBytes, other.inputBytes, loc, \"inputBytes\");\r\n    compare1(inputRecords, other.inputRecords, loc, \"inputRecords\");\r\n    compare1(outputBytes, other.outputBytes, loc, \"outputBytes\");\r\n    compare1(outputRecords, other.outputRecords, loc, \"outputRecords\");\r\n    compare1(taskID.toString(), other.taskID.toString(), loc, \"taskID\");\r\n    compare1(startTime, other.startTime, loc, \"startTime\");\r\n    compare1(finishTime, other.finishTime, loc, \"finishTime\");\r\n    compare1(taskType, other.taskType, loc, \"taskType\");\r\n    compare1(taskStatus, other.taskStatus, loc, \"taskStatus\");\r\n    compareLoggedTaskAttempts(attempts, other.attempts, loc, \"attempts\");\r\n    compareLoggedLocations(preferredLocations, other.preferredLocations, loc, \"preferredLocations\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setJobProperties",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobProperties(Properties conf)\n{\r\n    this.jobProperties = new JobProperties(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getJobProperties",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobProperties getJobProperties()\n{\r\n    return jobProperties;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "adjustTimes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void adjustTimes(long adjustment)\n{\r\n    submitTime += adjustment;\r\n    launchTime += adjustment;\r\n    finishTime += adjustment;\r\n    for (LoggedTask task : mapTasks) {\r\n        task.adjustTimes(adjustment);\r\n    }\r\n    for (LoggedTask task : reduceTasks) {\r\n        task.adjustTimes(adjustment);\r\n    }\r\n    for (LoggedTask task : otherTasks) {\r\n        task.adjustTimes(adjustment);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setUnknownAttribute",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUnknownAttribute(String attributeName, Object ignored)\n{\r\n    if (!alreadySeenAnySetterAttributes.contains(attributeName)) {\r\n        alreadySeenAnySetterAttributes.add(attributeName);\r\n        System.err.println(\"In LoggedJob, we saw the unknown attribute \" + attributeName + \".\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "UserName getUser()\n{\r\n    return user;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setUser",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setUser(String user)\n{\r\n    this.user = new UserName(user);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobID getJobID()\n{\r\n    return jobID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setJobID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setJobID(String jobID)\n{\r\n    this.jobID = JobID.forName(jobID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getPriority",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobPriority getPriority()\n{\r\n    return priority;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setPriority",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setPriority(JobPriority priority)\n{\r\n    this.priority = priority;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getComputonsPerMapInputByte",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getComputonsPerMapInputByte()\n{\r\n    return computonsPerMapInputByte;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setComputonsPerMapInputByte",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setComputonsPerMapInputByte(long computonsPerMapInputByte)\n{\r\n    this.computonsPerMapInputByte = computonsPerMapInputByte;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getComputonsPerMapOutputByte",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getComputonsPerMapOutputByte()\n{\r\n    return computonsPerMapOutputByte;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setComputonsPerMapOutputByte",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setComputonsPerMapOutputByte(long computonsPerMapOutputByte)\n{\r\n    this.computonsPerMapOutputByte = computonsPerMapOutputByte;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getComputonsPerReduceInputByte",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getComputonsPerReduceInputByte()\n{\r\n    return computonsPerReduceInputByte;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setComputonsPerReduceInputByte",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setComputonsPerReduceInputByte(long computonsPerReduceInputByte)\n{\r\n    this.computonsPerReduceInputByte = computonsPerReduceInputByte;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getComputonsPerReduceOutputByte",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getComputonsPerReduceOutputByte()\n{\r\n    return computonsPerReduceOutputByte;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setComputonsPerReduceOutputByte",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setComputonsPerReduceOutputByte(long computonsPerReduceOutputByte)\n{\r\n    this.computonsPerReduceOutputByte = computonsPerReduceOutputByte;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getSubmitTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSubmitTime()\n{\r\n    return submitTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setSubmitTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSubmitTime(long submitTime)\n{\r\n    this.submitTime = submitTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getLaunchTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLaunchTime()\n{\r\n    return launchTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setLaunchTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLaunchTime(long startTime)\n{\r\n    this.launchTime = startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFinishTime(long finishTime)\n{\r\n    this.finishTime = finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getHeapMegabytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getHeapMegabytes()\n{\r\n    return heapMegabytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setHeapMegabytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHeapMegabytes(int heapMegabytes)\n{\r\n    this.heapMegabytes = heapMegabytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getTotalMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTotalMaps()\n{\r\n    return totalMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setTotalMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTotalMaps(int totalMaps)\n{\r\n    this.totalMaps = totalMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getTotalReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTotalReduces()\n{\r\n    return totalReduces;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setTotalReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTotalReduces(int totalReduces)\n{\r\n    this.totalReduces = totalReduces;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getOutcome",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Pre21JobHistoryConstants.Values getOutcome()\n{\r\n    return outcome;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setOutcome",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setOutcome(Pre21JobHistoryConstants.Values outcome)\n{\r\n    this.outcome = outcome;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getJobtype",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobType getJobtype()\n{\r\n    return jobtype;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setJobtype",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobtype(JobType jobtype)\n{\r\n    this.jobtype = jobtype;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getDirectDependantJobs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<String> getDirectDependantJobs()\n{\r\n    return directDependantJobs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setDirectDependantJobs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDirectDependantJobs(List<String> directDependantJobs)\n{\r\n    this.directDependantJobs = directDependantJobs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMapTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<LoggedTask> getMapTasks()\n{\r\n    return mapTasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setMapTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMapTasks(List<LoggedTask> mapTasks)\n{\r\n    this.mapTasks = mapTasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getReduceTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<LoggedTask> getReduceTasks()\n{\r\n    return reduceTasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setReduceTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setReduceTasks(List<LoggedTask> reduceTasks)\n{\r\n    this.reduceTasks = reduceTasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getOtherTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<LoggedTask> getOtherTasks()\n{\r\n    return otherTasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setOtherTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setOtherTasks(List<LoggedTask> otherTasks)\n{\r\n    this.otherTasks = otherTasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getSuccessfulMapAttemptCDFs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ArrayList<LoggedDiscreteCDF> getSuccessfulMapAttemptCDFs()\n{\r\n    return successfulMapAttemptCDFs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setSuccessfulMapAttemptCDFs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSuccessfulMapAttemptCDFs(ArrayList<LoggedDiscreteCDF> successfulMapAttemptCDFs)\n{\r\n    this.successfulMapAttemptCDFs = successfulMapAttemptCDFs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getFailedMapAttemptCDFs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ArrayList<LoggedDiscreteCDF> getFailedMapAttemptCDFs()\n{\r\n    return failedMapAttemptCDFs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setFailedMapAttemptCDFs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFailedMapAttemptCDFs(ArrayList<LoggedDiscreteCDF> failedMapAttemptCDFs)\n{\r\n    this.failedMapAttemptCDFs = failedMapAttemptCDFs;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getSuccessfulReduceAttemptCDF",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LoggedDiscreteCDF getSuccessfulReduceAttemptCDF()\n{\r\n    return successfulReduceAttemptCDF;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setSuccessfulReduceAttemptCDF",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSuccessfulReduceAttemptCDF(LoggedDiscreteCDF successfulReduceAttemptCDF)\n{\r\n    this.successfulReduceAttemptCDF = successfulReduceAttemptCDF;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getFailedReduceAttemptCDF",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LoggedDiscreteCDF getFailedReduceAttemptCDF()\n{\r\n    return failedReduceAttemptCDF;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setFailedReduceAttemptCDF",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFailedReduceAttemptCDF(LoggedDiscreteCDF failedReduceAttemptCDF)\n{\r\n    this.failedReduceAttemptCDF = failedReduceAttemptCDF;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getMapperTriesToSucceed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double[] getMapperTriesToSucceed()\n{\r\n    return mapperTriesToSucceed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setMapperTriesToSucceed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMapperTriesToSucceed(double[] mapperTriesToSucceed)\n{\r\n    this.mapperTriesToSucceed = mapperTriesToSucceed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getFailedMapperFraction",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getFailedMapperFraction()\n{\r\n    return failedMapperFraction;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setFailedMapperFraction",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFailedMapperFraction(double failedMapperFraction)\n{\r\n    this.failedMapperFraction = failedMapperFraction;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getRelativeTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getRelativeTime()\n{\r\n    return relativeTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setRelativeTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRelativeTime(long relativeTime)\n{\r\n    this.relativeTime = relativeTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "QueueName getQueue()\n{\r\n    return queue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setQueue(String queue)\n{\r\n    this.queue = new QueueName(queue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getJobName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobName getJobName()\n{\r\n    return jobName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setJobName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobName(String jobName)\n{\r\n    this.jobName = new JobName(jobName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getClusterMapMB",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getClusterMapMB()\n{\r\n    return clusterMapMB;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setClusterMapMB",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setClusterMapMB(int clusterMapMB)\n{\r\n    this.clusterMapMB = clusterMapMB;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getClusterReduceMB",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getClusterReduceMB()\n{\r\n    return clusterReduceMB;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setClusterReduceMB",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setClusterReduceMB(int clusterReduceMB)\n{\r\n    this.clusterReduceMB = clusterReduceMB;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getJobMapMB",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getJobMapMB()\n{\r\n    return jobMapMB;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setJobMapMB",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobMapMB(int jobMapMB)\n{\r\n    this.jobMapMB = jobMapMB;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getJobReduceMB",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getJobReduceMB()\n{\r\n    return jobReduceMB;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setJobReduceMB",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobReduceMB(int jobReduceMB)\n{\r\n    this.jobReduceMB = jobReduceMB;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void compare1(String c1, String c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    if (c1 == null || c2 == null || !c1.equals(c2)) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void compare1(long c1, long c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 != c2) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void compare1(Pre21JobHistoryConstants.Values c1, Pre21JobHistoryConstants.Values c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 != c2) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void compare1(JobType c1, JobType c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 != c2) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void compare1(JobPriority c1, JobPriority c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 != c2) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void compare1(int c1, int c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 != c2) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void compare1(double c1, double c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 != c2) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void compare1(double[] c1, double[] c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    TreePath recursePath = new TreePath(loc, eltname);\r\n    if (c1 == null || c2 == null || c1.length != c2.length) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", recursePath);\r\n    }\r\n    for (int i = 0; i < c1.length; ++i) {\r\n        if (c1[i] != c2[i]) {\r\n            throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname, i));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void compare1(DeepCompare c1, DeepCompare c2, TreePath loc, String eltname, int index) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    TreePath recursePath = new TreePath(loc, eltname, index);\r\n    if (c1 == null || c2 == null) {\r\n        if (index == -1) {\r\n            throw new DeepInequalityException(eltname + \" miscompared\", recursePath);\r\n        } else {\r\n            throw new DeepInequalityException(eltname + \"[\" + index + \"] miscompared\", recursePath);\r\n        }\r\n    }\r\n    c1.deepCompare(c2, recursePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compareStrings",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void compareStrings(List<String> c1, List<String> c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    TreePath recursePath = new TreePath(loc, eltname);\r\n    if (c1 == null || c2 == null || !c1.equals(c2)) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", recursePath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compareLoggedTasks",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void compareLoggedTasks(List<LoggedTask> c1, List<LoggedTask> c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    if (c1 == null || c2 == null || c1.size() != c2.size()) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n    for (int i = 0; i < c1.size(); ++i) {\r\n        c1.get(i).deepCompare(c2.get(i), new TreePath(loc, eltname, i));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compareCDFs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void compareCDFs(List<LoggedDiscreteCDF> c1, List<LoggedDiscreteCDF> c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    if (c1 == null || c2 == null || c1.size() != c2.size()) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n    for (int i = 0; i < c1.size(); ++i) {\r\n        c1.get(i).deepCompare(c2.get(i), new TreePath(loc, eltname, i));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compareJobProperties",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void compareJobProperties(JobProperties jprop1, JobProperties jprop2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (jprop1 == null && jprop2 == null) {\r\n        return;\r\n    }\r\n    if (jprop1 == null || jprop2 == null) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n    Properties prop1 = jprop1.getValue();\r\n    Properties prop2 = jprop2.getValue();\r\n    if (prop1.size() != prop2.size()) {\r\n        throw new DeepInequalityException(eltname + \" miscompared [size]\", new TreePath(loc, eltname));\r\n    }\r\n    for (Map.Entry<Object, Object> entry : prop1.entrySet()) {\r\n        String v1 = entry.getValue().toString();\r\n        String v2 = prop2.get(entry.getKey()).toString();\r\n        compare1(v1, v2, new TreePath(loc, eltname), \"key:\" + entry.getKey());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compare1",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void compare1(DataType<String> c1, DataType<String> c2, TreePath loc, String eltname) throws DeepInequalityException\n{\r\n    if (c1 == null && c2 == null) {\r\n        return;\r\n    }\r\n    if (c1 == null || c2 == null) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(loc, eltname));\r\n    }\r\n    TreePath dtPath = new TreePath(loc, eltname);\r\n    if (!c1.getClass().getName().equals(c2.getClass().getName())) {\r\n        throw new DeepInequalityException(eltname + \" miscompared\", new TreePath(dtPath, \"class\"));\r\n    }\r\n    compare1(c1.getValue(), c2.getValue(), dtPath, \"value\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "deepCompare",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void deepCompare(DeepCompare comparand, TreePath loc) throws DeepInequalityException\n{\r\n    if (!(comparand instanceof LoggedJob)) {\r\n        throw new DeepInequalityException(\"comparand has wrong type\", loc);\r\n    }\r\n    LoggedJob other = (LoggedJob) comparand;\r\n    compare1(jobID.toString(), other.jobID.toString(), loc, \"jobID\");\r\n    compare1(user, other.user, loc, \"user\");\r\n    compare1(computonsPerMapInputByte, other.computonsPerMapInputByte, loc, \"computonsPerMapInputByte\");\r\n    compare1(computonsPerMapOutputByte, other.computonsPerMapOutputByte, loc, \"computonsPerMapOutputByte\");\r\n    compare1(computonsPerReduceInputByte, other.computonsPerReduceInputByte, loc, \"computonsPerReduceInputByte\");\r\n    compare1(computonsPerReduceOutputByte, other.computonsPerReduceOutputByte, loc, \"computonsPerReduceOutputByte\");\r\n    compare1(submitTime, other.submitTime, loc, \"submitTime\");\r\n    compare1(launchTime, other.launchTime, loc, \"launchTime\");\r\n    compare1(finishTime, other.finishTime, loc, \"finishTime\");\r\n    compare1(heapMegabytes, other.heapMegabytes, loc, \"heapMegabytes\");\r\n    compare1(totalMaps, other.totalMaps, loc, \"totalMaps\");\r\n    compare1(totalReduces, other.totalReduces, loc, \"totalReduces\");\r\n    compare1(outcome, other.outcome, loc, \"outcome\");\r\n    compare1(jobtype, other.jobtype, loc, \"jobtype\");\r\n    compare1(priority, other.priority, loc, \"priority\");\r\n    compareStrings(directDependantJobs, other.directDependantJobs, loc, \"directDependantJobs\");\r\n    compareLoggedTasks(mapTasks, other.mapTasks, loc, \"mapTasks\");\r\n    compareLoggedTasks(reduceTasks, other.reduceTasks, loc, \"reduceTasks\");\r\n    compareLoggedTasks(otherTasks, other.otherTasks, loc, \"otherTasks\");\r\n    compare1(relativeTime, other.relativeTime, loc, \"relativeTime\");\r\n    compareCDFs(successfulMapAttemptCDFs, other.successfulMapAttemptCDFs, loc, \"successfulMapAttemptCDFs\");\r\n    compareCDFs(failedMapAttemptCDFs, other.failedMapAttemptCDFs, loc, \"failedMapAttemptCDFs\");\r\n    compare1(successfulReduceAttemptCDF, other.successfulReduceAttemptCDF, loc, \"successfulReduceAttemptCDF\", -1);\r\n    compare1(failedReduceAttemptCDF, other.failedReduceAttemptCDF, loc, \"failedReduceAttemptCDF\", -1);\r\n    compare1(mapperTriesToSucceed, other.mapperTriesToSucceed, loc, \"mapperTriesToSucceed\");\r\n    compare1(failedMapperFraction, other.failedMapperFraction, loc, \"failedMapperFraction\");\r\n    compare1(queue, other.queue, loc, \"queue\");\r\n    compare1(jobName, other.jobName, loc, \"jobName\");\r\n    compare1(clusterMapMB, other.clusterMapMB, loc, \"clusterMapMB\");\r\n    compare1(clusterReduceMB, other.clusterReduceMB, loc, \"clusterReduceMB\");\r\n    compare1(jobMapMB, other.jobMapMB, loc, \"jobMapMB\");\r\n    compare1(jobReduceMB, other.jobReduceMB, loc, \"jobReduceMB\");\r\n    compareJobProperties(jobProperties, other.getJobProperties(), loc, \"JobProperties\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "parse",
  "errType" : [ "ParserConfigurationException", "SAXException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "Properties parse(InputStream input) throws IOException\n{\r\n    Properties result = new Properties();\r\n    try {\r\n        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n        DocumentBuilder db = dbf.newDocumentBuilder();\r\n        Document doc = db.parse(input);\r\n        Element root = doc.getDocumentElement();\r\n        if (!\"configuration\".equals(root.getTagName())) {\r\n            System.out.print(\"root is not a configuration node\");\r\n            return null;\r\n        }\r\n        NodeList props = root.getChildNodes();\r\n        for (int i = 0; i < props.getLength(); ++i) {\r\n            Node propNode = props.item(i);\r\n            if (!(propNode instanceof Element))\r\n                continue;\r\n            Element prop = (Element) propNode;\r\n            if (!\"property\".equals(prop.getTagName())) {\r\n                System.out.print(\"bad conf file: element not <property>\");\r\n            }\r\n            NodeList fields = prop.getChildNodes();\r\n            String attr = null;\r\n            String value = null;\r\n            @SuppressWarnings(\"unused\")\r\n            boolean finalParameter = false;\r\n            for (int j = 0; j < fields.getLength(); j++) {\r\n                Node fieldNode = fields.item(j);\r\n                if (!(fieldNode instanceof Element)) {\r\n                    continue;\r\n                }\r\n                Element field = (Element) fieldNode;\r\n                if (\"name\".equals(field.getTagName()) && field.hasChildNodes()) {\r\n                    attr = ((Text) field.getFirstChild()).getData().trim();\r\n                }\r\n                if (\"value\".equals(field.getTagName()) && field.hasChildNodes()) {\r\n                    value = ((Text) field.getFirstChild()).getData();\r\n                }\r\n                if (\"final\".equals(field.getTagName()) && field.hasChildNodes()) {\r\n                    finalParameter = \"true\".equals(((Text) field.getFirstChild()).getData());\r\n                }\r\n            }\r\n            if (attr != null && value != null) {\r\n                result.put(attr, value);\r\n            }\r\n        }\r\n    } catch (ParserConfigurationException e) {\r\n        return null;\r\n    } catch (SAXException e) {\r\n        return null;\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getLevel",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getLevel()\n{\r\n    return level;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "checkChildren",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkChildren()\n{\r\n    if (children == null) {\r\n        children = new TreeSet<Node>();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "addChild",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean addChild(Node child)\n{\r\n    if (child.parent != null) {\r\n        throw new IllegalArgumentException(\"The child is already under another node:\" + child.parent);\r\n    }\r\n    checkChildren();\r\n    boolean retval = children.add(child);\r\n    if (retval)\r\n        child.parent = this;\r\n    return retval;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "hasChildren",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasChildren()\n{\r\n    return children != null && !children.isEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getChildren",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<Node> getChildren()\n{\r\n    return (children == null) ? EMPTY_SET : Collections.unmodifiableSortedSet(children);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getParent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Node getParent()\n{\r\n    return parent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return name.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (this == obj)\r\n        return true;\r\n    if (obj == null)\r\n        return false;\r\n    if (obj.getClass() != this.getClass())\r\n        return false;\r\n    Node other = (Node) obj;\r\n    return name.equals(other.name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"(\" + name + \", \" + level + \")\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\main\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int compareTo(Node o)\n{\r\n    return name.compareTo(o.name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
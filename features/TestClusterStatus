[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGraylistedTrackers",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGraylistedTrackers()\n{\r\n    Assert.assertEquals(0, clusterStatus.getGraylistedTrackers());\r\n    Assert.assertTrue(clusterStatus.getGraylistedTrackerNames().isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJobTrackerState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJobTrackerState()\n{\r\n    Assert.assertEquals(JobTracker.State.RUNNING, clusterStatus.getJobTrackerState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testSetOutputPathException",
  "errType" : [ "RuntimeException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSetOutputPathException() throws Exception\n{\r\n    Job job = Job.getInstance();\r\n    try {\r\n        FileOutputFormat.setOutputPath(job, new Path(\"foo:///bar\"));\r\n        fail(\"Should have thrown a RuntimeException with an IOException inside\");\r\n    } catch (RuntimeException re) {\r\n        assertTrue(re.getCause() instanceof IOException);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCheckOutputSpecsException",
  "errType" : [ "FileAlreadyExistsException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testCheckOutputSpecsException() throws Exception\n{\r\n    Job job = Job.getInstance();\r\n    Path outDir = new Path(System.getProperty(\"test.build.data\", \"/tmp\"), \"output\");\r\n    FileSystem fs = outDir.getFileSystem(new Configuration());\r\n    fs.mkdirs(outDir);\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    FileOutputFormat fof = new FileOutputFormat() {\r\n\r\n        @Override\r\n        public RecordWriter getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException {\r\n            return null;\r\n        }\r\n    };\r\n    try {\r\n        try {\r\n            fof.checkOutputSpecs(job);\r\n            fail(\"Should have thrown a FileAlreadyExistsException\");\r\n        } catch (FileAlreadyExistsException re) {\r\n        }\r\n    } finally {\r\n        if (fs.exists(outDir)) {\r\n            fs.delete(outDir, true);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setup()\n{\r\n    testDir.mkdirs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup()\n{\r\n    FileUtil.fullyDelete(testDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testQueue",
  "errType" : null,
  "containingMethodsNum" : 62,
  "sourceCodeText" : "void testQueue() throws IOException\n{\r\n    File f = null;\r\n    try {\r\n        f = writeFile();\r\n        QueueManager manager = new QueueManager(f.getCanonicalPath(), true);\r\n        manager.setSchedulerInfo(\"first\", \"queueInfo\");\r\n        manager.setSchedulerInfo(\"second\", \"queueInfoqueueInfo\");\r\n        Queue root = manager.getRoot();\r\n        assertThat(root.getChildren().size()).isEqualTo(2);\r\n        Iterator<Queue> iterator = root.getChildren().iterator();\r\n        Queue firstSubQueue = iterator.next();\r\n        assertEquals(\"first\", firstSubQueue.getName());\r\n        assertEquals(firstSubQueue.getAcls().get(\"mapred.queue.first.acl-submit-job\").toString(), \"Users [user1, user2] and members of the groups [group1, group2] are allowed\");\r\n        Queue secondSubQueue = iterator.next();\r\n        assertEquals(\"second\", secondSubQueue.getName());\r\n        assertThat(secondSubQueue.getProperties().getProperty(\"key\")).isEqualTo(\"value\");\r\n        assertThat(secondSubQueue.getProperties().getProperty(\"key1\")).isEqualTo(\"value1\");\r\n        assertThat(firstSubQueue.getState().getStateName()).isEqualTo(\"running\");\r\n        assertThat(secondSubQueue.getState().getStateName()).isEqualTo(\"stopped\");\r\n        Set<String> template = new HashSet<String>();\r\n        template.add(\"first\");\r\n        template.add(\"second\");\r\n        assertEquals(manager.getLeafQueueNames(), template);\r\n        UserGroupInformation mockUGI = mock(UserGroupInformation.class);\r\n        when(mockUGI.getShortUserName()).thenReturn(\"user1\");\r\n        String[] groups = { \"group1\" };\r\n        when(mockUGI.getGroupNames()).thenReturn(groups);\r\n        assertTrue(manager.hasAccess(\"first\", QueueACL.SUBMIT_JOB, mockUGI));\r\n        assertFalse(manager.hasAccess(\"second\", QueueACL.SUBMIT_JOB, mockUGI));\r\n        assertFalse(manager.hasAccess(\"first\", QueueACL.ADMINISTER_JOBS, mockUGI));\r\n        when(mockUGI.getShortUserName()).thenReturn(\"user3\");\r\n        assertTrue(manager.hasAccess(\"first\", QueueACL.ADMINISTER_JOBS, mockUGI));\r\n        QueueAclsInfo[] qai = manager.getQueueAcls(mockUGI);\r\n        assertThat(qai.length).isEqualTo(1);\r\n        manager.refreshQueues(getConfiguration(), null);\r\n        iterator = root.getChildren().iterator();\r\n        Queue firstSubQueue1 = iterator.next();\r\n        Queue secondSubQueue1 = iterator.next();\r\n        assertThat(firstSubQueue).isEqualTo(firstSubQueue1);\r\n        assertThat(firstSubQueue1.getState().getStateName()).isEqualTo(\"running\");\r\n        assertThat(secondSubQueue1.getState().getStateName()).isEqualTo(\"stopped\");\r\n        assertThat(firstSubQueue1.getSchedulingInfo()).isEqualTo(\"queueInfo\");\r\n        assertThat(secondSubQueue1.getSchedulingInfo()).isEqualTo(\"queueInfoqueueInfo\");\r\n        assertThat(firstSubQueue.getJobQueueInfo().getQueueName()).isEqualTo(\"first\");\r\n        assertThat(firstSubQueue.getJobQueueInfo().getState().toString()).isEqualTo(\"running\");\r\n        assertThat(firstSubQueue.getJobQueueInfo().getSchedulingInfo()).isEqualTo(\"queueInfo\");\r\n        assertThat(secondSubQueue.getJobQueueInfo().getChildren().size()).isEqualTo(0);\r\n        assertThat(manager.getSchedulerInfo(\"first\")).isEqualTo(\"queueInfo\");\r\n        Set<String> queueJobQueueInfos = new HashSet<String>();\r\n        for (JobQueueInfo jobInfo : manager.getJobQueueInfos()) {\r\n            queueJobQueueInfos.add(jobInfo.getQueueName());\r\n        }\r\n        Set<String> rootJobQueueInfos = new HashSet<String>();\r\n        for (Queue queue : root.getChildren()) {\r\n            rootJobQueueInfos.add(queue.getJobQueueInfo().getQueueName());\r\n        }\r\n        assertEquals(queueJobQueueInfos, rootJobQueueInfos);\r\n        assertThat(manager.getJobQueueInfoMapping().get(\"first\").getQueueName()).isEqualTo(\"first\");\r\n        Writer writer = new StringWriter();\r\n        Configuration conf = getConfiguration();\r\n        conf.unset(DeprecatedQueueConfigurationParser.MAPRED_QUEUE_NAMES_KEY);\r\n        QueueManager.dumpConfiguration(writer, f.getAbsolutePath(), conf);\r\n        String result = writer.toString();\r\n        assertTrue(result.indexOf(\"\\\"name\\\":\\\"first\\\",\\\"state\\\":\\\"running\\\",\\\"acl_submit_job\\\":\\\"user1,user2 group1,group2\\\",\\\"acl_administer_jobs\\\":\\\"user3,user4 group3,group4\\\",\\\"properties\\\":[],\\\"children\\\":[]\") > 0);\r\n        writer = new StringWriter();\r\n        QueueManager.dumpConfiguration(writer, conf);\r\n        result = writer.toString();\r\n        assertTrue(result.contains(\"{\\\"queues\\\":[{\\\"name\\\":\\\"default\\\",\\\"state\\\":\\\"running\\\",\\\"acl_submit_job\\\":\\\"*\\\",\\\"acl_administer_jobs\\\":\\\"*\\\",\\\"properties\\\":[],\\\"children\\\":[]},{\\\"name\\\":\\\"q1\\\",\\\"state\\\":\\\"running\\\",\\\"acl_submit_job\\\":\\\" \\\",\\\"acl_administer_jobs\\\":\\\" \\\",\\\"properties\\\":[],\\\"children\\\":[{\\\"name\\\":\\\"q1:q2\\\",\\\"state\\\":\\\"running\\\",\\\"acl_submit_job\\\":\\\" \\\",\\\"acl_administer_jobs\\\":\\\" \\\",\\\"properties\\\":[\"));\r\n        assertTrue(result.contains(\"{\\\"key\\\":\\\"capacity\\\",\\\"value\\\":\\\"20\\\"}\"));\r\n        assertTrue(result.contains(\"{\\\"key\\\":\\\"user-limit\\\",\\\"value\\\":\\\"30\\\"}\"));\r\n        assertTrue(result.contains(\"],\\\"children\\\":[]}]}]}\"));\r\n        QueueAclsInfo qi = new QueueAclsInfo();\r\n        assertNull(qi.getQueueName());\r\n    } finally {\r\n        if (f != null) {\r\n            f.delete();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(DeprecatedQueueConfigurationParser.MAPRED_QUEUE_NAMES_KEY, \"first,second\");\r\n    conf.set(QueueManager.QUEUE_CONF_PROPERTY_NAME_PREFIX + \"first.acl-submit-job\", \"user1,user2 group1,group2\");\r\n    conf.set(MRConfig.MR_ACLS_ENABLED, \"true\");\r\n    conf.set(QueueManager.QUEUE_CONF_PROPERTY_NAME_PREFIX + \"first.state\", \"running\");\r\n    conf.set(QueueManager.QUEUE_CONF_PROPERTY_NAME_PREFIX + \"second.state\", \"stopped\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testDefaultConfig",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDefaultConfig()\n{\r\n    QueueManager manager = new QueueManager(true);\r\n    assertThat(manager.getRoot().getChildren().size()).isEqualTo(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "test2Queue",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void test2Queue() throws IOException\n{\r\n    Configuration conf = getConfiguration();\r\n    QueueManager manager = new QueueManager(conf);\r\n    manager.setSchedulerInfo(\"first\", \"queueInfo\");\r\n    manager.setSchedulerInfo(\"second\", \"queueInfoqueueInfo\");\r\n    Queue root = manager.getRoot();\r\n    assertTrue(root.getChildren().size() == 2);\r\n    Iterator<Queue> iterator = root.getChildren().iterator();\r\n    Queue firstSubQueue = iterator.next();\r\n    assertEquals(\"first\", firstSubQueue.getName());\r\n    assertThat(firstSubQueue.getAcls().get(\"mapred.queue.first.acl-submit-job\").toString()).isEqualTo(\"Users [user1, user2] and members of \" + \"the groups [group1, group2] are allowed\");\r\n    Queue secondSubQueue = iterator.next();\r\n    assertEquals(\"second\", secondSubQueue.getName());\r\n    assertThat(firstSubQueue.getState().getStateName()).isEqualTo(\"running\");\r\n    assertThat(secondSubQueue.getState().getStateName()).isEqualTo(\"stopped\");\r\n    assertTrue(manager.isRunning(\"first\"));\r\n    assertFalse(manager.isRunning(\"second\"));\r\n    assertThat(firstSubQueue.getSchedulingInfo()).isEqualTo(\"queueInfo\");\r\n    assertThat(secondSubQueue.getSchedulingInfo()).isEqualTo(\"queueInfoqueueInfo\");\r\n    Set<String> template = new HashSet<String>();\r\n    template.add(\"first\");\r\n    template.add(\"second\");\r\n    assertEquals(manager.getLeafQueueNames(), template);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "File writeFile() throws IOException\n{\r\n    File f = new File(testDir, \"tst.xml\");\r\n    BufferedWriter out = new BufferedWriter(new FileWriter(f));\r\n    String properties = \"<properties><property key=\\\"key\\\" value=\\\"value\\\"/><property key=\\\"key1\\\" value=\\\"value1\\\"/></properties>\";\r\n    out.write(\"<queues>\");\r\n    out.newLine();\r\n    out.write(\"<queue><name>first</name><acl-submit-job>user1,user2 group1,group2</acl-submit-job><acl-administer-jobs>user3,user4 group3,group4</acl-administer-jobs><state>running</state></queue>\");\r\n    out.newLine();\r\n    out.write(\"<queue><name>second</name><acl-submit-job>u1,u2 g1,g2</acl-submit-job>\" + properties + \"<state>stopped</state></queue>\");\r\n    out.newLine();\r\n    out.write(\"</queues>\");\r\n    out.flush();\r\n    out.close();\r\n    return f;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testProgressIsReportedIfInputASeriesOfEmptyFiles",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testProgressIsReportedIfInputASeriesOfEmptyFiles() throws IOException, InterruptedException\n{\r\n    JobConf conf = new JobConf();\r\n    Path[] paths = new Path[3];\r\n    File[] files = new File[3];\r\n    long[] fileLength = new long[3];\r\n    try {\r\n        for (int i = 0; i < 3; i++) {\r\n            File dir = new File(outDir.toString());\r\n            dir.mkdir();\r\n            files[i] = new File(dir, \"testfile\" + i);\r\n            FileWriter fileWriter = new FileWriter(files[i]);\r\n            fileWriter.flush();\r\n            fileWriter.close();\r\n            fileLength[i] = i;\r\n            paths[i] = new Path(outDir + \"/testfile\" + i);\r\n        }\r\n        CombineFileSplit combineFileSplit = new CombineFileSplit(paths, fileLength);\r\n        TaskAttemptID taskAttemptID = Mockito.mock(TaskAttemptID.class);\r\n        TaskReporter reporter = Mockito.mock(TaskReporter.class);\r\n        TaskAttemptContextImpl taskAttemptContext = new TaskAttemptContextImpl(conf, taskAttemptID, reporter);\r\n        CombineFileRecordReader cfrr = new CombineFileRecordReader(combineFileSplit, taskAttemptContext, TextRecordReaderWrapper.class);\r\n        cfrr.initialize(combineFileSplit, taskAttemptContext);\r\n        verify(reporter).progress();\r\n        Assert.assertFalse(cfrr.nextKeyValue());\r\n        verify(reporter, times(3)).progress();\r\n    } finally {\r\n        FileUtil.fullyDelete(new File(outDir.toString()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testProtocolProviderCreation",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testProtocolProviderCreation() throws Exception\n{\r\n    Iterator iterator = mock(Iterator.class);\r\n    when(iterator.hasNext()).thenReturn(true, true, true, true);\r\n    when(iterator.next()).thenReturn(getClientProtocolProvider()).thenThrow(new ServiceConfigurationError(\"Test error\")).thenReturn(getClientProtocolProvider());\r\n    Iterable frameworkLoader = mock(Iterable.class);\r\n    when(frameworkLoader.iterator()).thenReturn(iterator);\r\n    Cluster.frameworkLoader = frameworkLoader;\r\n    Cluster testCluster = new Cluster(new Configuration());\r\n    assertNotNull(\"ClientProtocol is expected\", testCluster.getClient());\r\n    verify(iterator, times(2)).next();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getClientProtocolProvider",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ClientProtocolProvider getClientProtocolProvider()\n{\r\n    return new ClientProtocolProvider() {\r\n\r\n        @Override\r\n        public ClientProtocol create(Configuration conf) throws IOException {\r\n            return mock(ClientProtocol.class);\r\n        }\r\n\r\n        @Override\r\n        public ClientProtocol create(InetSocketAddress addr, Configuration conf) throws IOException {\r\n            return mock(ClientProtocol.class);\r\n        }\r\n\r\n        @Override\r\n        public void close(ClientProtocol clientProtocol) throws IOException {\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    taskAttempt00 = TASK_IDS.getTaskAttempt(TASK0, TA0);\r\n    taskAttempt01 = TASK_IDS.getTaskAttempt(TASK0, TA1);\r\n    taskAttempt10 = TASK_IDS.getTaskAttempt(TASK1, TA0);\r\n    taskAttempt11 = TASK_IDS.getTaskAttempt(TASK1, TA1);\r\n    setSharedPath(path(\"TestJobThroughManifestCommitter\"));\r\n    destDir = new Path(sharedTestRoot, \"out put\");\r\n    jobId = TASK_IDS.getJobId();\r\n    dirs = new ManifestCommitterSupport.AttemptDirectories(destDir, jobId, 1);\r\n    setJobStageConfig(createStageConfigForJob(JOB1, destDir).build());\r\n    ta00Config = createStageConfig(JOB1, TASK0, TA0, destDir).build();\r\n    ta01Config = createStageConfig(JOB1, TASK0, TA1, destDir).build();\r\n    ta10Config = createStageConfig(JOB1, TASK1, TA0, destDir).build();\r\n    ta11Config = createStageConfig(JOB1, TASK1, TA1, destDir).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "deleteTestDirInTeardown",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void deleteTestDirInTeardown() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "shouldDeleteTestRootAtEndOfTestRun",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldDeleteTestRootAtEndOfTestRun()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "deleteSharedTestRoot",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void deleteSharedTestRoot() throws IOException\n{\r\n    describe(\"Deleting shared test root %s\", sharedTestRoot);\r\n    rm(getFileSystem(), sharedTestRoot, true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setSharedPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean setSharedPath(final Path path)\n{\r\n    if (sharedTestRoot == null) {\r\n        LOG.info(\"Set shared path to {}\", path);\r\n        sharedTestRoot = path;\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0000_setupTestDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void test_0000_setupTestDir() throws Throwable\n{\r\n    describe(\"always ensure directory setup is empty\");\r\n    deleteSharedTestRoot();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0100_setupJobStage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void test_0100_setupJobStage() throws Throwable\n{\r\n    describe(\"Set up a job\");\r\n    verifyPath(\"Job attempt dir\", dirs.getJobAttemptDir(), new SetupJobStage(getJobStageConfig()).apply(true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "verifyJobSetupCompleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyJobSetupCompleted() throws IOException\n{\r\n    assertPathExists(\"Job attempt dir from test_0100\", dirs.getJobAttemptDir());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0110_setupJobOnlyAllowedOnce",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void test_0110_setupJobOnlyAllowedOnce() throws Throwable\n{\r\n    describe(\"a second creation of a job attempt must fail\");\r\n    verifyJobSetupCompleted();\r\n    intercept(FileAlreadyExistsException.class, \"\", () -> new SetupJobStage(getJobStageConfig()).apply(true));\r\n    assertPathExists(\"Job attempt dir\", dirs.getJobAttemptDir());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0120_setupJobNewAttemptNumber",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void test_0120_setupJobNewAttemptNumber() throws Throwable\n{\r\n    describe(\"Creating a new job attempt is supported\");\r\n    verifyJobSetupCompleted();\r\n    Path path = pathMustExist(\"Job attempt 2 dir\", new SetupJobStage(createStageConfig(2, -1, 0, destDir)).apply(false));\r\n    Assertions.assertThat(path).describedAs(\"Stage created path\").isNotEqualTo(dirs.getJobAttemptDir());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0200_setupTask00",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void test_0200_setupTask00() throws Throwable\n{\r\n    describe(\"Set up a task; job must have been set up first\");\r\n    verifyJobSetupCompleted();\r\n    verifyPath(\"Task attempt 00\", dirs.getTaskAttemptPath(taskAttempt00), new SetupTaskStage(ta00Config).apply(\"first\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "verifyTaskAttempt00SetUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyTaskAttempt00SetUp() throws IOException\n{\r\n    pathMustExist(\"Dir from taskAttempt00 setup\", dirs.getTaskAttemptPath(taskAttempt00));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0210_setupTask00OnlyAllowedOnce",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void test_0210_setupTask00OnlyAllowedOnce() throws Throwable\n{\r\n    describe(\"Second attempt to set up task00 must fail.\");\r\n    verifyTaskAttempt00SetUp();\r\n    intercept(FileAlreadyExistsException.class, \"second\", () -> new SetupTaskStage(ta00Config).apply(\"second\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0220_setupTask01",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void test_0220_setupTask01() throws Throwable\n{\r\n    describe(\"Setup task attempt 01\");\r\n    verifyTaskAttempt00SetUp();\r\n    verifyPath(\"Task attempt 01\", dirs.getTaskAttemptPath(taskAttempt01), new SetupTaskStage(ta01Config).apply(\"01\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0230_setupTask10",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void test_0230_setupTask10() throws Throwable\n{\r\n    describe(\"Setup task attempt 10\");\r\n    verifyJobSetupCompleted();\r\n    verifyPath(\"Task attempt 10\", dirs.getTaskAttemptPath(taskAttempt10), new SetupTaskStage(ta10Config).apply(\"10\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0240_setupThenAbortTask11",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void test_0240_setupThenAbortTask11() throws Throwable\n{\r\n    describe(\"Setup then abort task attempt 11\");\r\n    verifyJobSetupCompleted();\r\n    Path ta11Path = new SetupTaskStage(ta11Config).apply(\"11\");\r\n    Path deletedDir = new AbortTaskStage(ta11Config).apply(false);\r\n    Assertions.assertThat(ta11Path).isEqualTo(deletedDir);\r\n    assertPathDoesNotExist(\"aborted directory\", ta11Path);\r\n    intercept(FileNotFoundException.class, () -> new CommitTaskStage(ta11Config).apply(null));\r\n    assertPathDoesNotExist(\"task manifest\", manifestPathForTask(dirs.getTaskManifestDir(), TASK_IDS.getTaskId(TASK1)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0300_executeTask00",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void test_0300_executeTask00() throws Throwable\n{\r\n    describe(\"Create the files for Task 00, then commit the task\");\r\n    List<Path> files = createFilesOrDirs(dirs.getTaskAttemptPath(taskAttempt00), \"part-00\", getExecutorService(), DEPTH, WIDTH, FILES_PER_DIRECTORY, false);\r\n    CommitTaskStage.Result result = new CommitTaskStage(ta00Config).apply(null);\r\n    verifyPathExists(getFileSystem(), \"manifest\", result.getPath());\r\n    TaskManifest manifest = result.getTaskManifest();\r\n    manifest.validate();\r\n    manifest.setIOStatistics(null);\r\n    LOG.info(\"Task Manifest {}\", manifest.toJson());\r\n    validateTaskAttemptManifest(this.taskAttempt00, files, manifest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "validateTaskAttemptManifest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void validateTaskAttemptManifest(String attemptId, List<Path> files, TaskManifest manifest) throws IOException\n{\r\n    verifyManifestTaskAttemptID(manifest, attemptId);\r\n    verifyManifestFilesMatch(manifest, files);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0310_executeTask01",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void test_0310_executeTask01() throws Throwable\n{\r\n    describe(\"Create the files for Task 01, then commit the task\");\r\n    List<Path> files = createFilesOrDirs(dirs.getTaskAttemptPath(taskAttempt01), \"part-00\", getExecutorService(), DEPTH, WIDTH, FILES_PER_DIRECTORY, false);\r\n    CommitTaskStage.Result result = new CommitTaskStage(ta01Config).apply(null);\r\n    Path manifestPath = verifyPathExists(getFileSystem(), \"manifest\", result.getPath()).getPath();\r\n    TaskManifest manifest = TaskManifest.load(getFileSystem(), manifestPath);\r\n    manifest.validate();\r\n    manifest.setIOStatistics(null);\r\n    LOG.info(\"Task Manifest {}\", manifest.toJson());\r\n    validateTaskAttemptManifest(taskAttempt01, files, manifest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0320_executeTask10",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void test_0320_executeTask10() throws Throwable\n{\r\n    describe(\"Create the files for Task 10, then commit the task\");\r\n    List<Path> files = createFilesOrDirs(dirs.getTaskAttemptPath(ta10Config.getTaskAttemptId()), \"part-01\", getExecutorService(), DEPTH, WIDTH + 1, FILES_PER_DIRECTORY - 1, false);\r\n    CommitTaskStage.Result result = new CommitTaskStage(ta10Config).apply(null);\r\n    TaskManifest manifest = result.getTaskManifest();\r\n    validateTaskAttemptManifest(taskAttempt10, files, manifest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0340_setupThenAbortTask11",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void test_0340_setupThenAbortTask11() throws Throwable\n{\r\n    describe(\"Setup then abort task attempt 11\");\r\n    Path ta11Path = new SetupTaskStage(ta11Config).apply(\"11\");\r\n    createFilesOrDirs(ta11Path, \"part-01\", getExecutorService(), 2, 1, 1, false);\r\n    new AbortTaskStage(ta11Config).apply(false);\r\n    assertPathDoesNotExist(\"aborted directory\", ta11Path);\r\n    intercept(FileNotFoundException.class, () -> new CommitTaskStage(ta11Config).apply(null));\r\n    Path manifestPathForTask1 = manifestPathForTask(dirs.getTaskManifestDir(), TASK_IDS.getTaskId(TASK1));\r\n    verifyManifestTaskAttemptID(TaskManifest.load(getFileSystem(), manifestPathForTask1), taskAttempt10);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0400_loadManifests",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void test_0400_loadManifests() throws Throwable\n{\r\n    describe(\"Load all manifests; committed must be TA01 and TA10\");\r\n    LoadManifestsStage.Result result = new LoadManifestsStage(getJobStageConfig()).apply(true);\r\n    String summary = result.getSummary().toString();\r\n    LOG.info(\"Manifest summary {}\", summary);\r\n    List<TaskManifest> manifests = result.getManifests();\r\n    Assertions.assertThat(manifests).describedAs(\"Loaded manifests in %s\", summary).hasSize(2);\r\n    Map<String, TaskManifest> manifestMap = toMap(manifests);\r\n    verifyManifestTaskAttemptID(manifestMap.get(taskAttempt01), taskAttempt01);\r\n    verifyManifestTaskAttemptID(manifestMap.get(taskAttempt10), taskAttempt10);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0410_commitJob",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void test_0410_commitJob() throws Throwable\n{\r\n    describe(\"Commit the job\");\r\n    CommitJobStage stage = new CommitJobStage(getJobStageConfig());\r\n    stage.apply(new CommitJobStage.Arguments(true, false, null, DISABLED));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0420_validateJob",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void test_0420_validateJob() throws Throwable\n{\r\n    describe(\"Validate the output of the job through the validation\" + \" stage\");\r\n    ManifestSuccessData successData = loadAndPrintSuccessData(getFileSystem(), getJobStageConfig().getJobSuccessMarkerPath());\r\n    List<TaskManifest> manifests = new LoadManifestsStage(getJobStageConfig()).apply(true).getManifests();\r\n    List<String> committedFiles = new ValidateRenamedFilesStage(getJobStageConfig()).apply(manifests).stream().map(FileEntry::getDest).collect(Collectors.toList());\r\n    Assertions.assertThat(committedFiles).containsAll(successData.getFilenames());\r\n    FileEntry entry = manifests.get(0).getFilesToCommit().get(0);\r\n    String oldName = entry.getDest();\r\n    String newName = oldName + \".missing\";\r\n    entry.setDest(newName);\r\n    intercept(OutputValidationException.class, \".missing\", () -> new ValidateRenamedFilesStage(getJobStageConfig()).apply(manifests));\r\n    entry.setDest(oldName);\r\n    entry.setSize(128_000_000);\r\n    intercept(OutputValidationException.class, () -> new ValidateRenamedFilesStage(getJobStageConfig()).apply(manifests));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0430_validateStatistics",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void test_0430_validateStatistics() throws Throwable\n{\r\n    ManifestSuccessData successData = ManifestSuccessData.load(getFileSystem(), getJobStageConfig().getJobSuccessMarkerPath());\r\n    String json = successData.toJson();\r\n    LOG.info(\"Success data is {}\", json);\r\n    Assertions.assertThat(successData).describedAs(\"Manifest \" + json).returns(NetUtils.getLocalHostname(), ManifestSuccessData::getHostname).returns(MANIFEST_COMMITTER_CLASSNAME, ManifestSuccessData::getCommitter).returns(jobId, ManifestSuccessData::getJobId).returns(true, ManifestSuccessData::getSuccess).returns(JOB_ID_SOURCE_MAPREDUCE, ManifestSuccessData::getJobIdSource);\r\n    Assertions.assertThat(successData.getDiagnostics()).containsEntry(PRINCIPAL, getCurrentUser().getShortUserName()).containsEntry(STAGE, OP_STAGE_JOB_COMMIT);\r\n    IOStatisticsSnapshot iostats = successData.getIOStatistics();\r\n    int files = successData.getFilenames().size();\r\n    verifyStatisticCounterValue(iostats, OP_STAGE_JOB_COMMIT, 1);\r\n    assertThatStatisticCounter(iostats, COMMITTER_FILES_COMMITTED_COUNT).isGreaterThanOrEqualTo(files);\r\n    Long totalFiles = iostats.counters().get(COMMITTER_FILES_COMMITTED_COUNT);\r\n    verifyStatisticCounterValue(iostats, COMMITTER_BYTES_COMMITTED_COUNT, totalFiles * 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_440_validateSuccessFiles",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void test_440_validateSuccessFiles() throws Throwable\n{\r\n    final FileSystem fs = getFileSystem();\r\n    ManifestSuccessData successData = loadAndPrintSuccessData(fs, getJobStageConfig().getJobSuccessMarkerPath());\r\n    validateGeneratedFiles(fs, getJobStageConfig().getDestinationDir(), successData, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_0900_cleanupJob",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void test_0900_cleanupJob() throws Throwable\n{\r\n    describe(\"Cleanup job\");\r\n    CleanupJobStage.Arguments arguments = new CleanupJobStage.Arguments(OP_STAGE_JOB_CLEANUP, true, true, false);\r\n    CleanupJobStage.Result result = new CleanupJobStage(getJobStageConfig()).apply(arguments);\r\n    assertCleanupResult(result, CleanupJobStage.Outcome.PARALLEL_DELETE, 1 + 3);\r\n    assertPathDoesNotExist(\"Job attempt dir\", result.getDirectory());\r\n    result = new CleanupJobStage(getJobStageConfig()).apply(arguments);\r\n    assertCleanupResult(result, CleanupJobStage.Outcome.NOTHING_TO_CLEAN_UP, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "test_9999_cleanupTestDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void test_9999_cleanupTestDir() throws Throwable\n{\r\n    if (shouldDeleteTestRootAtEndOfTestRun()) {\r\n        deleteSharedTestRoot();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cleanup() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = outDir.getFileSystem(conf);\r\n    fs.delete(outDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    cleanup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws IOException\n{\r\n    cleanup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "writeOutput",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void writeOutput(RecordWriter theRecordWriter, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    NullWritable nullWritable = NullWritable.get();\r\n    try {\r\n        theRecordWriter.write(key1, val1);\r\n        theRecordWriter.write(null, nullWritable);\r\n        theRecordWriter.write(null, val1);\r\n        theRecordWriter.write(nullWritable, val2);\r\n        theRecordWriter.write(key2, nullWritable);\r\n        theRecordWriter.write(key1, null);\r\n        theRecordWriter.write(null, null);\r\n        theRecordWriter.write(key2, val2);\r\n    } finally {\r\n        theRecordWriter.close(context);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "writeMapFileOutput",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writeMapFileOutput(RecordWriter theRecordWriter, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    try {\r\n        int key = 0;\r\n        for (int i = 0; i < 10; ++i) {\r\n            key = i;\r\n            Text val = (i % 2 == 1) ? val1 : val2;\r\n            theRecordWriter.write(new LongWritable(key), val);\r\n        }\r\n    } finally {\r\n        theRecordWriter.close(context);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testRecoveryInternal",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void testRecoveryInternal(int commitVersion, int recoveryVersion) throws Exception\n{\r\n    Job job = Job.getInstance();\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1);\r\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, commitVersion);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\r\n    writeOutput(theRecordWriter, tContext);\r\n    committer.commitTask(tContext);\r\n    Path jobTempDir1 = committer.getCommittedTaskPath(tContext);\r\n    File jtd = new File(jobTempDir1.toUri().getPath());\r\n    if (commitVersion == 1) {\r\n        assertTrue(\"Version 1 commits to temporary dir \" + jtd, jtd.exists());\r\n        validateContent(jtd);\r\n    } else {\r\n        assertFalse(\"Version 2 commits to output dir \" + jtd, jtd.exists());\r\n    }\r\n    Configuration conf2 = job.getConfiguration();\r\n    conf2.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\r\n    conf2.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 2);\r\n    conf2.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, recoveryVersion);\r\n    JobContext jContext2 = new JobContextImpl(conf2, taskID.getJobID());\r\n    TaskAttemptContext tContext2 = new TaskAttemptContextImpl(conf2, taskID);\r\n    FileOutputCommitter committer2 = new FileOutputCommitter(outDir, tContext2);\r\n    committer2.setupJob(tContext2);\r\n    Path jobTempDir2 = committer2.getCommittedTaskPath(tContext2);\r\n    File jtd2 = new File(jobTempDir2.toUri().getPath());\r\n    committer2.recoverTask(tContext2);\r\n    if (recoveryVersion == 1) {\r\n        assertTrue(\"Version 1 recovers to \" + jtd2, jtd2.exists());\r\n        validateContent(jtd2);\r\n    } else {\r\n        assertFalse(\"Version 2 commits to output dir \" + jtd2, jtd2.exists());\r\n        if (commitVersion == 1) {\r\n            assertTrue(\"Version 2  recovery moves to output dir from \" + jtd, jtd.list().length == 0);\r\n        }\r\n    }\r\n    committer2.commitJob(jContext2);\r\n    validateContent(outDir);\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testRecoveryV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRecoveryV1() throws Exception\n{\r\n    testRecoveryInternal(1, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testRecoveryV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRecoveryV2() throws Exception\n{\r\n    testRecoveryInternal(2, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testRecoveryUpgradeV1V2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRecoveryUpgradeV1V2() throws Exception\n{\r\n    testRecoveryInternal(1, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "validateContent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validateContent(Path dir) throws IOException\n{\r\n    validateContent(new File(dir.toUri().getPath()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "validateContent",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void validateContent(File dir) throws IOException\n{\r\n    File expectedFile = new File(dir, partFile);\r\n    assertTrue(\"Could not find \" + expectedFile, expectedFile.exists());\r\n    StringBuffer expectedOutput = new StringBuffer();\r\n    expectedOutput.append(key1).append('\\t').append(val1).append(\"\\n\");\r\n    expectedOutput.append(val1).append(\"\\n\");\r\n    expectedOutput.append(val2).append(\"\\n\");\r\n    expectedOutput.append(key2).append(\"\\n\");\r\n    expectedOutput.append(key1).append(\"\\n\");\r\n    expectedOutput.append(key2).append('\\t').append(val2).append(\"\\n\");\r\n    String output = slurp(expectedFile);\r\n    assertThat(output).isEqualTo(expectedOutput.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "validateMapFileOutputContent",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void validateMapFileOutputContent(FileSystem fs, Path dir) throws IOException\n{\r\n    Path expectedMapDir = new Path(dir, partFile);\r\n    assert (fs.getFileStatus(expectedMapDir).isDirectory());\r\n    FileStatus[] files = fs.listStatus(expectedMapDir);\r\n    int fileCount = 0;\r\n    boolean dataFileFound = false;\r\n    boolean indexFileFound = false;\r\n    for (FileStatus f : files) {\r\n        if (f.isFile()) {\r\n            ++fileCount;\r\n            if (f.getPath().getName().equals(MapFile.INDEX_FILE_NAME)) {\r\n                indexFileFound = true;\r\n            } else if (f.getPath().getName().equals(MapFile.DATA_FILE_NAME)) {\r\n                dataFileFound = true;\r\n            }\r\n        }\r\n    }\r\n    assert (fileCount > 0);\r\n    assert (dataFileFound && indexFileFound);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterInternal",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testCommitterInternal(int version, boolean taskCleanup) throws Exception\n{\r\n    Job job = Job.getInstance();\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, version);\r\n    conf.setBoolean(FileOutputCommitter.FILEOUTPUTCOMMITTER_TASK_CLEANUP_ENABLED, taskCleanup);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\r\n    writeOutput(theRecordWriter, tContext);\r\n    File jobOutputDir = new File(new Path(outDir, FileOutputCommitter.PENDING_DIR_NAME).toString());\r\n    File taskOutputDir = new File(Path.getPathWithoutSchemeAndAuthority(committer.getWorkPath()).toString());\r\n    assertTrue(\"job temp dir does not exist\", jobOutputDir.exists());\r\n    assertTrue(\"task temp dir does not exist\", taskOutputDir.exists());\r\n    committer.commitTask(tContext);\r\n    assertTrue(\"job temp dir does not exist\", jobOutputDir.exists());\r\n    if (version == 1 || taskCleanup) {\r\n        assertFalse(\"task temp dir still exists\", taskOutputDir.exists());\r\n    } else {\r\n        assertTrue(\"task temp dir does not exist\", taskOutputDir.exists());\r\n    }\r\n    committer.commitJob(jContext);\r\n    assertFalse(\"job temp dir still exists\", jobOutputDir.exists());\r\n    assertFalse(\"task temp dir still exists\", taskOutputDir.exists());\r\n    validateContent(outDir);\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitterV1() throws Exception\n{\r\n    testCommitterInternal(1, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitterV2() throws Exception\n{\r\n    testCommitterInternal(2, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterV2TaskCleanupEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitterV2TaskCleanupEnabled() throws Exception\n{\r\n    testCommitterInternal(2, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterWithDuplicatedCommitV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitterWithDuplicatedCommitV1() throws Exception\n{\r\n    testCommitterWithDuplicatedCommitInternal(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterWithDuplicatedCommitV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitterWithDuplicatedCommitV2() throws Exception\n{\r\n    testCommitterWithDuplicatedCommitInternal(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterWithDuplicatedCommitInternal",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testCommitterWithDuplicatedCommitInternal(int version) throws Exception\n{\r\n    Job job = Job.getInstance();\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, version);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\r\n    writeOutput(theRecordWriter, tContext);\r\n    committer.commitTask(tContext);\r\n    committer.commitJob(jContext);\r\n    validateContent(outDir);\r\n    try {\r\n        committer.commitJob(jContext);\r\n        if (version == 1) {\r\n            Assert.fail(\"Duplicate commit success: wrong behavior for version 1.\");\r\n        }\r\n    } catch (IOException e) {\r\n        if (version == 2) {\r\n            Assert.fail(\"Duplicate commit failed: wrong behavior for version 2.\");\r\n        }\r\n    }\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterWithFailureV1",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCommitterWithFailureV1() throws Exception\n{\r\n    testCommitterWithFailureInternal(1, 1);\r\n    testCommitterWithFailureInternal(1, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterWithFailureV2",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCommitterWithFailureV2() throws Exception\n{\r\n    testCommitterWithFailureInternal(2, 1);\r\n    testCommitterWithFailureInternal(2, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterWithFailureInternal",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testCommitterWithFailureInternal(int version, int maxAttempts) throws Exception\n{\r\n    Job job = Job.getInstance();\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, version);\r\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_FAILURE_ATTEMPTS, maxAttempts);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new CommitterWithFailedThenSucceed(outDir, tContext);\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\r\n    writeOutput(theRecordWriter, tContext);\r\n    committer.commitTask(tContext);\r\n    try {\r\n        committer.commitJob(jContext);\r\n        if (version == 1 || maxAttempts <= 1) {\r\n            Assert.fail(\"Commit successful: wrong behavior for version 1.\");\r\n        }\r\n    } catch (IOException e) {\r\n        if (version == 2 && maxAttempts > 2) {\r\n            Assert.fail(\"Commit failed: wrong behavior for version 2.\");\r\n        }\r\n    }\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testProgressDuringMerge",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testProgressDuringMerge() throws Exception\n{\r\n    Job job = Job.getInstance();\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, 2);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = spy(new TaskAttemptContextImpl(conf, taskID));\r\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    MapFileOutputFormat theOutputFormat = new MapFileOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\r\n    writeMapFileOutput(theRecordWriter, tContext);\r\n    committer.commitTask(tContext);\r\n    verify(tContext, atLeast(2)).progress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterRepeatableV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitterRepeatableV1() throws Exception\n{\r\n    testCommitterRetryInternal(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterRepeatableV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitterRepeatableV2() throws Exception\n{\r\n    testCommitterRetryInternal(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterRetryInternal",
  "errType" : [ "IOException", "FileNotFoundException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testCommitterRetryInternal(int version) throws Exception\n{\r\n    Job job = Job.getInstance();\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, version);\r\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_FAILURE_ATTEMPTS, 1);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new CommitterWithFailedThenSucceed(outDir, tContext);\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\r\n    writeOutput(theRecordWriter, tContext);\r\n    committer.commitTask(tContext);\r\n    try {\r\n        committer.commitJob(jContext);\r\n        Assert.fail(\"Commit successful: wrong behavior for the first time \" + \"commit.\");\r\n    } catch (IOException e) {\r\n        try {\r\n            committer.commitJob(jContext);\r\n            if (version == 1) {\r\n                Assert.fail(\"Commit successful after retry: wrong behavior for \" + \"version 1.\");\r\n            }\r\n        } catch (FileNotFoundException ex) {\r\n            if (version == 2) {\r\n                Assert.fail(\"Commit failed after retry: wrong behavior for\" + \" version 2.\");\r\n            }\r\n            assertTrue(ex.getMessage().contains(committer.getJobAttemptPath(jContext).toString() + \" does not exist\"));\r\n        }\r\n    }\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testMapFileOutputCommitterInternal",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testMapFileOutputCommitterInternal(int version) throws Exception\n{\r\n    Job job = Job.getInstance();\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, version);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    MapFileOutputFormat theOutputFormat = new MapFileOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\r\n    writeMapFileOutput(theRecordWriter, tContext);\r\n    committer.commitTask(tContext);\r\n    committer.commitJob(jContext);\r\n    MapFile.Reader[] readers = {};\r\n    try {\r\n        readers = MapFileOutputFormat.getReaders(outDir, conf);\r\n        validateMapFileOutputContent(FileSystem.get(job.getConfiguration()), outDir);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(null, readers);\r\n        FileUtil.fullyDelete(new File(outDir.toString()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testMapFileOutputCommitterV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMapFileOutputCommitterV1() throws Exception\n{\r\n    testMapFileOutputCommitterInternal(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testMapFileOutputCommitterV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMapFileOutputCommitterV2() throws Exception\n{\r\n    testMapFileOutputCommitterInternal(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testInvalidVersionNumber",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testInvalidVersionNumber() throws IOException\n{\r\n    Job job = Job.getInstance();\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, 3);\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    try {\r\n        new FileOutputCommitter(outDir, tContext);\r\n        fail(\"should've thrown an exception!\");\r\n    } catch (IOException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testAbortInternal",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testAbortInternal(int version) throws IOException, InterruptedException\n{\r\n    Job job = Job.getInstance();\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, version);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\r\n    writeOutput(theRecordWriter, tContext);\r\n    committer.abortTask(tContext);\r\n    File expectedFile = new File(new Path(committer.getWorkPath(), partFile).toString());\r\n    assertFalse(\"task temp dir still exists\", expectedFile.exists());\r\n    committer.abortJob(jContext, JobStatus.State.FAILED);\r\n    expectedFile = new File(new Path(outDir, FileOutputCommitter.PENDING_DIR_NAME).toString());\r\n    assertFalse(\"job temp dir still exists\", expectedFile.exists());\r\n    assertEquals(\"Output directory not empty\", 0, new File(outDir.toString()).listFiles().length);\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testAbortV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAbortV1() throws IOException, InterruptedException\n{\r\n    testAbortInternal(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testAbortV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAbortV2() throws IOException, InterruptedException\n{\r\n    testAbortInternal(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testFailAbortInternal",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testFailAbortInternal(int version) throws IOException, InterruptedException\n{\r\n    Job job = Job.getInstance();\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(FileSystem.FS_DEFAULT_NAME_KEY, \"faildel:///\");\r\n    conf.setClass(\"fs.faildel.impl\", FakeFileSystem.class, FileSystem.class);\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1);\r\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, version);\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    TextOutputFormat<?, ?> theOutputFormat = new TextOutputFormat();\r\n    RecordWriter<?, ?> theRecordWriter = theOutputFormat.getRecordWriter(tContext);\r\n    writeOutput(theRecordWriter, tContext);\r\n    Throwable th = null;\r\n    try {\r\n        committer.abortTask(tContext);\r\n    } catch (IOException ie) {\r\n        th = ie;\r\n    }\r\n    assertNotNull(th);\r\n    assertTrue(th instanceof IOException);\r\n    assertTrue(th.getMessage().contains(\"fake delete failed\"));\r\n    Path jtd = committer.getJobAttemptPath(jContext);\r\n    File jobTmpDir = new File(jtd.toUri().getPath());\r\n    Path ttd = committer.getTaskAttemptPath(tContext);\r\n    File taskTmpDir = new File(ttd.toUri().getPath());\r\n    File expectedFile = new File(taskTmpDir, partFile);\r\n    assertTrue(expectedFile + \" does not exists\", expectedFile.exists());\r\n    th = null;\r\n    try {\r\n        committer.abortJob(jContext, JobStatus.State.FAILED);\r\n    } catch (IOException ie) {\r\n        th = ie;\r\n    }\r\n    assertNotNull(th);\r\n    assertTrue(th instanceof IOException);\r\n    assertTrue(th.getMessage().contains(\"fake delete failed\"));\r\n    assertTrue(\"job temp dir does not exists\", jobTmpDir.exists());\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testFailAbortV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFailAbortV1() throws Exception\n{\r\n    testFailAbortInternal(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testFailAbortV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFailAbortV2() throws Exception\n{\r\n    testFailAbortInternal(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testConcurrentCommitTaskWithSubDir",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testConcurrentCommitTaskWithSubDir(int version) throws Exception\n{\r\n    final Job job = Job.getInstance();\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    final Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, version);\r\n    conf.setClass(\"fs.file.impl\", RLFS.class, FileSystem.class);\r\n    FileSystem.closeAll();\r\n    final JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    final FileOutputCommitter amCommitter = new FileOutputCommitter(outDir, jContext);\r\n    amCommitter.setupJob(jContext);\r\n    final TaskAttemptContext[] taCtx = new TaskAttemptContextImpl[2];\r\n    taCtx[0] = new TaskAttemptContextImpl(conf, taskID);\r\n    taCtx[1] = new TaskAttemptContextImpl(conf, taskID1);\r\n    final TextOutputFormat[] tof = new TextOutputFormat[2];\r\n    for (int i = 0; i < tof.length; i++) {\r\n        tof[i] = new TextOutputFormat() {\r\n\r\n            @Override\r\n            public Path getDefaultWorkFile(TaskAttemptContext context, String extension) throws IOException {\r\n                final FileOutputCommitter foc = (FileOutputCommitter) getOutputCommitter(context);\r\n                return new Path(new Path(foc.getWorkPath(), SUB_DIR), getUniqueFile(context, getOutputName(context), extension));\r\n            }\r\n        };\r\n    }\r\n    final ExecutorService executor = HadoopExecutors.newFixedThreadPool(2);\r\n    try {\r\n        for (int i = 0; i < taCtx.length; i++) {\r\n            final int taskIdx = i;\r\n            executor.submit(new Callable<Void>() {\r\n\r\n                @Override\r\n                public Void call() throws IOException, InterruptedException {\r\n                    final OutputCommitter outputCommitter = tof[taskIdx].getOutputCommitter(taCtx[taskIdx]);\r\n                    outputCommitter.setupTask(taCtx[taskIdx]);\r\n                    final RecordWriter rw = tof[taskIdx].getRecordWriter(taCtx[taskIdx]);\r\n                    writeOutput(rw, taCtx[taskIdx]);\r\n                    outputCommitter.commitTask(taCtx[taskIdx]);\r\n                    return null;\r\n                }\r\n            });\r\n        }\r\n    } finally {\r\n        executor.shutdown();\r\n        while (!executor.awaitTermination(1, TimeUnit.SECONDS)) {\r\n            LOG.info(\"Awaiting thread termination!\");\r\n        }\r\n    }\r\n    amCommitter.commitJob(jContext);\r\n    final RawLocalFileSystem lfs = new RawLocalFileSystem();\r\n    lfs.setConf(conf);\r\n    assertFalse(\"Must not end up with sub_dir/sub_dir\", lfs.exists(new Path(OUT_SUB_DIR, SUB_DIR)));\r\n    validateContent(OUT_SUB_DIR);\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testConcurrentCommitTaskWithSubDirV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testConcurrentCommitTaskWithSubDirV1() throws Exception\n{\r\n    testConcurrentCommitTaskWithSubDir(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testConcurrentCommitTaskWithSubDirV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testConcurrentCommitTaskWithSubDirV2() throws Exception\n{\r\n    testConcurrentCommitTaskWithSubDir(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "slurp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String slurp(File f) throws IOException\n{\r\n    int len = (int) f.length();\r\n    byte[] buf = new byte[len];\r\n    FileInputStream in = new FileInputStream(f);\r\n    String contents = null;\r\n    try {\r\n        in.read(buf, 0, len);\r\n        contents = new String(buf, \"UTF-8\");\r\n    } finally {\r\n        in.close();\r\n    }\r\n    return contents;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "testInitNextRecordReader",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testInitNextRecordReader() throws IOException\n{\r\n    JobConf conf = new JobConf();\r\n    Path[] paths = new Path[3];\r\n    long[] fileLength = new long[3];\r\n    File[] files = new File[3];\r\n    LongWritable key = new LongWritable(1);\r\n    Text value = new Text();\r\n    try {\r\n        for (int i = 0; i < 3; i++) {\r\n            fileLength[i] = i;\r\n            File dir = new File(outDir.toString());\r\n            dir.mkdir();\r\n            files[i] = new File(dir, \"testfile\" + i);\r\n            FileWriter fileWriter = new FileWriter(files[i]);\r\n            fileWriter.close();\r\n            paths[i] = new Path(outDir + \"/testfile\" + i);\r\n        }\r\n        CombineFileSplit combineFileSplit = new CombineFileSplit(conf, paths, fileLength);\r\n        Reporter reporter = Mockito.mock(Reporter.class);\r\n        CombineFileRecordReader cfrr = new CombineFileRecordReader(conf, combineFileSplit, reporter, TextRecordReaderWrapper.class);\r\n        verify(reporter).progress();\r\n        Assert.assertFalse(cfrr.next(key, value));\r\n        verify(reporter, times(3)).progress();\r\n    } finally {\r\n        FileUtil.fullyDelete(new File(outDir.toString()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "testCheckpointCreate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCheckpointCreate() throws Exception\n{\r\n    checkpointCreate(ByteBuffer.allocate(BUFSIZE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "testCheckpointCreateDirect",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCheckpointCreateDirect() throws Exception\n{\r\n    checkpointCreate(ByteBuffer.allocateDirect(BUFSIZE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "checkpointCreate",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void checkpointCreate(ByteBuffer b) throws Exception\n{\r\n    int WRITES = 128;\r\n    FileSystem fs = mock(FileSystem.class);\r\n    DataOutputBuffer dob = new DataOutputBuffer();\r\n    FSDataOutputStream hdfs = spy(new FSDataOutputStream(dob, null));\r\n    @SuppressWarnings(\"resource\")\r\n    DataOutputBuffer verif = new DataOutputBuffer();\r\n    when(fs.create(isA(Path.class), eq((short) 1))).thenReturn(hdfs);\r\n    when(fs.rename(isA(Path.class), isA(Path.class))).thenReturn(true);\r\n    Path base = new Path(\"/chk\");\r\n    Path finalLoc = new Path(\"/chk/checkpoint_chk0\");\r\n    Path tmp = FSCheckpointService.tmpfile(finalLoc);\r\n    FSCheckpointService chk = new FSCheckpointService(fs, base, new SimpleNamingService(\"chk0\"), (short) 1);\r\n    CheckpointWriteChannel out = chk.create();\r\n    Random r = new Random();\r\n    final byte[] randBytes = new byte[BUFSIZE];\r\n    for (int i = 0; i < WRITES; ++i) {\r\n        r.nextBytes(randBytes);\r\n        int s = r.nextInt(BUFSIZE - 1);\r\n        int e = r.nextInt(BUFSIZE - s) + 1;\r\n        verif.write(randBytes, s, e);\r\n        b.clear();\r\n        b.put(randBytes).flip();\r\n        b.position(s).limit(b.position() + e);\r\n        out.write(b);\r\n    }\r\n    verify(fs, never()).rename(any(Path.class), eq(finalLoc));\r\n    CheckpointID cid = chk.commit(out);\r\n    verify(hdfs).close();\r\n    verify(fs).rename(eq(tmp), eq(finalLoc));\r\n    assertArrayEquals(Arrays.copyOfRange(verif.getData(), 0, verif.getLength()), Arrays.copyOfRange(dob.getData(), 0, dob.getLength()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "testDelete",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testDelete() throws Exception\n{\r\n    FileSystem fs = mock(FileSystem.class);\r\n    Path chkloc = new Path(\"/chk/chk0\");\r\n    when(fs.delete(eq(chkloc), eq(false))).thenReturn(true);\r\n    Path base = new Path(\"/otherchk\");\r\n    FSCheckpointID id = new FSCheckpointID(chkloc);\r\n    FSCheckpointService chk = new FSCheckpointService(fs, base, new SimpleNamingService(\"chk0\"), (short) 1);\r\n    assertTrue(chk.delete(id));\r\n    verify(fs).delete(eq(chkloc), eq(false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testPartitionerShouldNotBeCalledWhenOneReducerIsPresent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testPartitionerShouldNotBeCalledWhenOneReducerIsPresent() throws Exception\n{\r\n    MapFileOutputFormat outputFormat = new MapFileOutputFormat();\r\n    Reader reader = Mockito.mock(Reader.class);\r\n    Reader[] readers = new Reader[] { reader };\r\n    outputFormat.getEntry(readers, new MyPartitioner(), new Text(), new Text());\r\n    assertTrue(!MyPartitioner.isGetPartitionCalled());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    MyPartitioner.setGetPartitionCalled(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "LoggingLineRecordWriter<K, V> getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    boolean isCompressed = getCompressOutput(job);\r\n    String keyValueSeparator = conf.get(SEPARATOR, \"\\t\");\r\n    CompressionCodec codec = null;\r\n    String extension = \"\";\r\n    if (isCompressed) {\r\n        Class<? extends CompressionCodec> codecClass = getOutputCompressorClass(job, GzipCodec.class);\r\n        codec = ReflectionUtils.newInstance(codecClass, conf);\r\n        extension = codec.getDefaultExtension();\r\n    }\r\n    Path file = getDefaultWorkFile(job, extension);\r\n    FileSystem fs = file.getFileSystem(conf);\r\n    FSDataOutputStream fileOut = fs.create(file, true);\r\n    LOG.debug(\"Creating LineRecordWriter with destination {}\", file);\r\n    if (isCompressed) {\r\n        return new LoggingLineRecordWriter<>(file, new DataOutputStream(codec.createOutputStream(fileOut)), keyValueSeparator);\r\n    } else {\r\n        return new LoggingLineRecordWriter<>(file, fileOut, keyValueSeparator);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "bind",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void bind(Configuration conf)\n{\r\n    conf.setClass(MRJobConfig.OUTPUT_FORMAT_CLASS_ATTR, TextOutputForTests.class, OutputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\split",
  "methodName" : "testMaxBlockLocationsNewSplits",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testMaxBlockLocationsNewSplits() throws Exception\n{\r\n    TEST_DIR.mkdirs();\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        conf.setInt(MRConfig.MAX_BLOCK_LOCATIONS_KEY, 4);\r\n        Path submitDir = new Path(TEST_DIR.getAbsolutePath());\r\n        FileSystem fs = FileSystem.getLocal(conf);\r\n        FileSplit split = new FileSplit(new Path(\"/some/path\"), 0, 1, new String[] { \"loc1\", \"loc2\", \"loc3\", \"loc4\", \"loc5\" });\r\n        JobSplitWriter.createSplitFiles(submitDir, conf, fs, new FileSplit[] { split });\r\n        JobSplit.TaskSplitMetaInfo[] infos = SplitMetaInfoReader.readSplitMetaInfo(new JobID(), fs, conf, submitDir);\r\n        assertEquals(\"unexpected number of splits\", 1, infos.length);\r\n        assertEquals(\"unexpected number of split locations\", 4, infos[0].getLocations().length);\r\n    } finally {\r\n        FileUtil.fullyDelete(TEST_DIR);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\split",
  "methodName" : "testMaxBlockLocationsOldSplits",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testMaxBlockLocationsOldSplits() throws Exception\n{\r\n    TEST_DIR.mkdirs();\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        conf.setInt(MRConfig.MAX_BLOCK_LOCATIONS_KEY, 4);\r\n        Path submitDir = new Path(TEST_DIR.getAbsolutePath());\r\n        FileSystem fs = FileSystem.getLocal(conf);\r\n        org.apache.hadoop.mapred.FileSplit split = new org.apache.hadoop.mapred.FileSplit(new Path(\"/some/path\"), 0, 1, new String[] { \"loc1\", \"loc2\", \"loc3\", \"loc4\", \"loc5\" });\r\n        JobSplitWriter.createSplitFiles(submitDir, conf, fs, new org.apache.hadoop.mapred.InputSplit[] { split });\r\n        JobSplit.TaskSplitMetaInfo[] infos = SplitMetaInfoReader.readSplitMetaInfo(new JobID(), fs, conf, submitDir);\r\n        assertEquals(\"unexpected number of splits\", 1, infos.length);\r\n        assertEquals(\"unexpected number of split locations\", 4, infos[0].getLocations().length);\r\n    } finally {\r\n        FileUtil.fullyDelete(TEST_DIR);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "testDBInputFormat",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testDBInputFormat() throws Exception\n{\r\n    JobConf configuration = new JobConf();\r\n    setupDriver(configuration);\r\n    DBInputFormat<NullDBWritable> format = new DBInputFormat<NullDBWritable>();\r\n    format.setConf(configuration);\r\n    format.setConf(configuration);\r\n    DBInputFormat.DBInputSplit splitter = new DBInputFormat.DBInputSplit(1, 10);\r\n    Reporter reporter = mock(Reporter.class);\r\n    RecordReader<LongWritable, NullDBWritable> reader = format.getRecordReader(splitter, configuration, reporter);\r\n    configuration.setInt(MRJobConfig.NUM_MAPS, 3);\r\n    InputSplit[] lSplits = format.getSplits(configuration, 3);\r\n    assertEquals(5, lSplits[0].getLength());\r\n    assertEquals(3, lSplits.length);\r\n    assertEquals(LongWritable.class, reader.createKey().getClass());\r\n    assertEquals(0, reader.getPos());\r\n    assertEquals(0, reader.getProgress(), 0.001);\r\n    reader.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "testSetInput",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testSetInput()\n{\r\n    JobConf configuration = new JobConf();\r\n    String[] fieldNames = { \"field1\", \"field2\" };\r\n    DBInputFormat.setInput(configuration, NullDBWritable.class, \"table\", \"conditions\", \"orderBy\", fieldNames);\r\n    assertEquals(\"org.apache.hadoop.mapred.lib.db.DBInputFormat$NullDBWritable\", configuration.getClass(DBConfiguration.INPUT_CLASS_PROPERTY, null).getName());\r\n    assertEquals(\"table\", configuration.get(DBConfiguration.INPUT_TABLE_NAME_PROPERTY, null));\r\n    String[] fields = configuration.getStrings(DBConfiguration.INPUT_FIELD_NAMES_PROPERTY);\r\n    assertEquals(\"field1\", fields[0]);\r\n    assertEquals(\"field2\", fields[1]);\r\n    assertEquals(\"conditions\", configuration.get(DBConfiguration.INPUT_CONDITIONS_PROPERTY, null));\r\n    assertEquals(\"orderBy\", configuration.get(DBConfiguration.INPUT_ORDER_BY_PROPERTY, null));\r\n    configuration = new JobConf();\r\n    DBInputFormat.setInput(configuration, NullDBWritable.class, \"query\", \"countQuery\");\r\n    assertEquals(\"query\", configuration.get(DBConfiguration.INPUT_QUERY, null));\r\n    assertEquals(\"countQuery\", configuration.get(DBConfiguration.INPUT_COUNT_QUERY, null));\r\n    JobConf jConfiguration = new JobConf();\r\n    DBConfiguration.configureDB(jConfiguration, \"driverClass\", \"dbUrl\", \"user\", \"password\");\r\n    assertEquals(\"driverClass\", jConfiguration.get(DBConfiguration.DRIVER_CLASS_PROPERTY));\r\n    assertEquals(\"dbUrl\", jConfiguration.get(DBConfiguration.URL_PROPERTY));\r\n    assertEquals(\"user\", jConfiguration.get(DBConfiguration.USERNAME_PROPERTY));\r\n    assertEquals(\"password\", jConfiguration.get(DBConfiguration.PASSWORD_PROPERTY));\r\n    jConfiguration = new JobConf();\r\n    DBConfiguration.configureDB(jConfiguration, \"driverClass\", \"dbUrl\");\r\n    assertEquals(\"driverClass\", jConfiguration.get(DBConfiguration.DRIVER_CLASS_PROPERTY));\r\n    assertEquals(\"dbUrl\", jConfiguration.get(DBConfiguration.URL_PROPERTY));\r\n    assertNull(jConfiguration.get(DBConfiguration.USERNAME_PROPERTY));\r\n    assertNull(jConfiguration.get(DBConfiguration.PASSWORD_PROPERTY));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "testDBRecordReader",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testDBRecordReader() throws Exception\n{\r\n    JobConf job = mock(JobConf.class);\r\n    DBConfiguration dbConfig = mock(DBConfiguration.class);\r\n    String[] fields = { \"field1\", \"filed2\" };\r\n    @SuppressWarnings(\"rawtypes\")\r\n    DBRecordReader reader = new DBInputFormat<NullDBWritable>().new DBRecordReader(new DBInputSplit(), NullDBWritable.class, job, DriverForTest.getConnection(), dbConfig, \"condition\", fields, \"table\");\r\n    LongWritable key = reader.createKey();\r\n    assertEquals(0, key.get());\r\n    DBWritable value = reader.createValue();\r\n    assertEquals(\"org.apache.hadoop.mapred.lib.db.DBInputFormat$NullDBWritable\", value.getClass().getName());\r\n    assertEquals(0, reader.getPos());\r\n    assertFalse(reader.next(key, value));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "setupDriver",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setupDriver(JobConf configuration) throws Exception\n{\r\n    configuration.set(DBConfiguration.URL_PROPERTY, \"testUrl\");\r\n    DriverManager.registerDriver(new DriverForTest());\r\n    configuration.set(DBConfiguration.DRIVER_CLASS_PROPERTY, DriverForTest.class.getCanonicalName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJobInfo",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testJobInfo() throws IOException\n{\r\n    JobID jid = new JobID(\"001\", 1);\r\n    Text user = new Text(\"User\");\r\n    Path path = new Path(\"/tmp/test\");\r\n    JobInfo info = new JobInfo(jid, user, path);\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    info.write(new DataOutputStream(out));\r\n    JobInfo copyinfo = new JobInfo();\r\n    copyinfo.readFields(new DataInputStream(new ByteArrayInputStream(out.toByteArray())));\r\n    assertEquals(info.getJobID().toString(), copyinfo.getJobID().toString());\r\n    assertEquals(info.getJobSubmitDir().getName(), copyinfo.getJobSubmitDir().getName());\r\n    assertEquals(info.getUser().toString(), copyinfo.getUser().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testTaskID",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testTaskID() throws IOException, InterruptedException\n{\r\n    JobID jobid = new JobID(\"1014873536921\", 6);\r\n    TaskID tid = new TaskID(jobid, TaskType.MAP, 0);\r\n    org.apache.hadoop.mapred.TaskID tid1 = org.apache.hadoop.mapred.TaskID.downgrade(tid);\r\n    org.apache.hadoop.mapred.TaskReport treport = new org.apache.hadoop.mapred.TaskReport(tid1, 0.0f, State.FAILED.toString(), null, TIPStatus.FAILED, 100, 100, new org.apache.hadoop.mapred.Counters());\r\n    assertThat(treport.getTaskId()).isEqualTo(\"task_1014873536921_0006_m_000000\");\r\n    assertThat(treport.getTaskID().toString()).isEqualTo(\"task_1014873536921_0006_m_000000\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testSplitRecords",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSplitRecords(String testFileName, long firstSplitLength) throws IOException\n{\r\n    URL testFileUrl = getClass().getClassLoader().getResource(testFileName);\r\n    assertNotNull(\"Cannot find \" + testFileName, testFileUrl);\r\n    File testFile = new File(testFileUrl.getFile());\r\n    long testFileSize = testFile.length();\r\n    Path testFilePath = new Path(testFile.getAbsolutePath());\r\n    Configuration conf = new Configuration();\r\n    testSplitRecordsForFile(conf, firstSplitLength, testFileSize, testFilePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testSplitRecordsForFile",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSplitRecordsForFile(Configuration conf, long firstSplitLength, long testFileSize, Path testFilePath) throws IOException\n{\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH, Integer.MAX_VALUE);\r\n    assertTrue(\"unexpected test data at \" + testFilePath, testFileSize > firstSplitLength);\r\n    String delimiter = conf.get(\"textinputformat.record.delimiter\");\r\n    byte[] recordDelimiterBytes = null;\r\n    if (null != delimiter) {\r\n        recordDelimiterBytes = delimiter.getBytes(StandardCharsets.UTF_8);\r\n    }\r\n    FileSplit split = new FileSplit(testFilePath, 0, testFileSize, (String[]) null);\r\n    LineRecordReader reader = new LineRecordReader(conf, split, recordDelimiterBytes);\r\n    LongWritable key = new LongWritable();\r\n    Text value = new Text();\r\n    int numRecordsNoSplits = 0;\r\n    while (reader.next(key, value)) {\r\n        ++numRecordsNoSplits;\r\n    }\r\n    reader.close();\r\n    split = new FileSplit(testFilePath, 0, firstSplitLength, (String[]) null);\r\n    reader = new LineRecordReader(conf, split, recordDelimiterBytes);\r\n    int numRecordsFirstSplit = 0;\r\n    while (reader.next(key, value)) {\r\n        ++numRecordsFirstSplit;\r\n    }\r\n    reader.close();\r\n    split = new FileSplit(testFilePath, firstSplitLength, testFileSize - firstSplitLength, (String[]) null);\r\n    reader = new LineRecordReader(conf, split, recordDelimiterBytes);\r\n    int numRecordsRemainingSplits = 0;\r\n    while (reader.next(key, value)) {\r\n        ++numRecordsRemainingSplits;\r\n    }\r\n    reader.close();\r\n    assertEquals(\"Unexpected number of records in split\", numRecordsNoSplits, numRecordsFirstSplit + numRecordsRemainingSplits);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLargeSplitRecordForFile",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testLargeSplitRecordForFile(Configuration conf, long firstSplitLength, long testFileSize, Path testFilePath) throws IOException\n{\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH, Integer.MAX_VALUE);\r\n    assertTrue(\"unexpected firstSplitLength:\" + firstSplitLength, testFileSize < firstSplitLength);\r\n    String delimiter = conf.get(\"textinputformat.record.delimiter\");\r\n    byte[] recordDelimiterBytes = null;\r\n    if (null != delimiter) {\r\n        recordDelimiterBytes = delimiter.getBytes(StandardCharsets.UTF_8);\r\n    }\r\n    FileSplit split = new FileSplit(testFilePath, 0, testFileSize, (String[]) null);\r\n    LineRecordReader reader = new LineRecordReader(conf, split, recordDelimiterBytes);\r\n    LongWritable key = new LongWritable();\r\n    Text value = new Text();\r\n    int numRecordsNoSplits = 0;\r\n    while (reader.next(key, value)) {\r\n        ++numRecordsNoSplits;\r\n    }\r\n    reader.close();\r\n    split = new FileSplit(testFilePath, 0, firstSplitLength, (String[]) null);\r\n    reader = new LineRecordReader(conf, split, recordDelimiterBytes);\r\n    int numRecordsFirstSplit = 0;\r\n    while (reader.next(key, value)) {\r\n        ++numRecordsFirstSplit;\r\n    }\r\n    reader.close();\r\n    assertEquals(\"Unexpected number of records in split\", numRecordsNoSplits, numRecordsFirstSplit);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBzip2SplitEndsAtCR",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testBzip2SplitEndsAtCR() throws IOException\n{\r\n    testSplitRecords(\"blockEndingInCR.txt.bz2\", 136498);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBzip2SplitEndsAtCRThenLF",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testBzip2SplitEndsAtCRThenLF() throws IOException\n{\r\n    testSplitRecords(\"blockEndingInCRThenLF.txt.bz2\", 136498);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBzip2SplitStartAtBlockMarker",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testBzip2SplitStartAtBlockMarker() throws IOException\n{\r\n    testSplitRecords(\"blockEndingInCR.txt.bz2\", 136504);\r\n    testSplitRecords(\"blockEndingInCR.txt.bz2\", 136505);\r\n    testSplitRecords(\"blockEndingInCR.txt.bz2\", 136508);\r\n    testSplitRecords(\"blockEndingInCR.txt.bz2\", 136494);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testSafeguardSplittingUnsplittableFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSafeguardSplittingUnsplittableFiles() throws IOException\n{\r\n    testSplitRecords(\"TestSafeguardSplittingUnsplittableFiles.txt.gz\", 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readRecords",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ArrayList<String> readRecords(URL testFileUrl, int splitSize) throws IOException\n{\r\n    File testFile = new File(testFileUrl.getFile());\r\n    long testFileSize = testFile.length();\r\n    Path testFilePath = new Path(testFile.getAbsolutePath());\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(\"io.file.buffer.size\", 1);\r\n    ArrayList<String> records = new ArrayList<String>();\r\n    long offset = 0;\r\n    LongWritable key = new LongWritable();\r\n    Text value = new Text();\r\n    while (offset < testFileSize) {\r\n        FileSplit split = new FileSplit(testFilePath, offset, splitSize, (String[]) null);\r\n        LineRecordReader reader = new LineRecordReader(conf, split);\r\n        while (reader.next(key, value)) {\r\n            records.add(value.toString());\r\n        }\r\n        offset += splitSize;\r\n    }\r\n    return records;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readRecordsDirectly",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String[] readRecordsDirectly(URL testFileUrl, boolean bzip) throws IOException\n{\r\n    int MAX_DATA_SIZE = 1024 * 1024;\r\n    byte[] data = new byte[MAX_DATA_SIZE];\r\n    FileInputStream fis = new FileInputStream(testFileUrl.getFile());\r\n    int count;\r\n    if (bzip) {\r\n        BZip2CompressorInputStream bzIn = new BZip2CompressorInputStream(fis);\r\n        count = bzIn.read(data);\r\n        bzIn.close();\r\n    } else {\r\n        count = fis.read(data);\r\n    }\r\n    fis.close();\r\n    assertTrue(\"Test file data too big for buffer\", count < data.length);\r\n    return new String(data, 0, count, \"UTF-8\").split(\"\\n\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "checkRecordSpanningMultipleSplits",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void checkRecordSpanningMultipleSplits(String testFile, int splitSize, boolean bzip) throws IOException\n{\r\n    URL testFileUrl = getClass().getClassLoader().getResource(testFile);\r\n    ArrayList<String> records = readRecords(testFileUrl, splitSize);\r\n    String[] actuals = readRecordsDirectly(testFileUrl, bzip);\r\n    assertEquals(\"Wrong number of records\", actuals.length, records.size());\r\n    boolean hasLargeRecord = false;\r\n    for (int i = 0; i < actuals.length; ++i) {\r\n        assertEquals(actuals[i], records.get(i));\r\n        if (actuals[i].length() > 2 * splitSize) {\r\n            hasLargeRecord = true;\r\n        }\r\n    }\r\n    assertTrue(\"Invalid test data. Doesn't have a large enough record\", hasLargeRecord);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRecordSpanningMultipleSplits",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRecordSpanningMultipleSplits() throws IOException\n{\r\n    checkRecordSpanningMultipleSplits(\"recordSpanningMultipleSplits.txt\", 10, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRecordSpanningMultipleSplitsCompressed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRecordSpanningMultipleSplitsCompressed() throws IOException\n{\r\n    checkRecordSpanningMultipleSplits(\"recordSpanningMultipleSplits.txt.bz2\", 200 * 1000, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testStripBOM",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testStripBOM() throws IOException\n{\r\n    String UTF8_BOM = \"\\uFEFF\";\r\n    URL testFileUrl = getClass().getClassLoader().getResource(\"testBOM.txt\");\r\n    assertNotNull(\"Cannot find testBOM.txt\", testFileUrl);\r\n    File testFile = new File(testFileUrl.getFile());\r\n    Path testFilePath = new Path(testFile.getAbsolutePath());\r\n    long testFileSize = testFile.length();\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH, Integer.MAX_VALUE);\r\n    FileSplit split = new FileSplit(testFilePath, 0, testFileSize, (String[]) null);\r\n    LineRecordReader reader = new LineRecordReader(conf, split);\r\n    LongWritable key = new LongWritable();\r\n    Text value = new Text();\r\n    int numRecords = 0;\r\n    boolean firstLine = true;\r\n    boolean skipBOM = true;\r\n    while (reader.next(key, value)) {\r\n        if (firstLine) {\r\n            firstLine = false;\r\n            if (value.toString().startsWith(UTF8_BOM)) {\r\n                skipBOM = false;\r\n            }\r\n        }\r\n        ++numRecords;\r\n    }\r\n    reader.close();\r\n    assertTrue(\"BOM is not skipped\", skipBOM);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMultipleClose",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testMultipleClose() throws IOException\n{\r\n    URL testFileUrl = getClass().getClassLoader().getResource(\"recordSpanningMultipleSplits.txt.bz2\");\r\n    assertNotNull(\"Cannot find recordSpanningMultipleSplits.txt.bz2\", testFileUrl);\r\n    File testFile = new File(testFileUrl.getFile());\r\n    Path testFilePath = new Path(testFile.getAbsolutePath());\r\n    long testFileSize = testFile.length();\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH, Integer.MAX_VALUE);\r\n    FileSplit split = new FileSplit(testFilePath, 0, testFileSize, (String[]) null);\r\n    LineRecordReader reader = new LineRecordReader(conf, split);\r\n    LongWritable key = new LongWritable();\r\n    Text value = new Text();\r\n    while (reader.next(key, value)) ;\r\n    reader.close();\r\n    reader.close();\r\n    BZip2Codec codec = new BZip2Codec();\r\n    codec.setConf(conf);\r\n    Set<Decompressor> decompressors = new HashSet<Decompressor>();\r\n    for (int i = 0; i < 10; ++i) {\r\n        decompressors.add(CodecPool.getDecompressor(codec));\r\n    }\r\n    assertEquals(10, decompressors.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createInputFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path createInputFile(Configuration conf, String data) throws IOException\n{\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    Path file = new Path(inputDir, \"test.txt\");\r\n    Writer writer = new OutputStreamWriter(localFs.create(file));\r\n    try {\r\n        writer.write(data);\r\n    } finally {\r\n        writer.close();\r\n    }\r\n    return file;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testUncompressedInputWithLargeSplitSize",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testUncompressedInputWithLargeSplitSize() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    String inputData = \"abcde +fghij+ klmno+pqrst+uvwxyz\";\r\n    Path inputFile = createInputFile(conf, inputData);\r\n    conf.set(\"textinputformat.record.delimiter\", \"+\");\r\n    long longSplitSize = (long) Integer.MAX_VALUE + 1;\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n        testLargeSplitRecordForFile(conf, longSplitSize, inputData.length(), inputFile);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testUncompressedInput",
  "errType" : null,
  "containingMethodsNum" : 42,
  "sourceCodeText" : "void testUncompressedInput() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    String inputData = \"abc+def+ghi+jkl+mno+pqr+stu+vw +xyz\";\r\n    Path inputFile = createInputFile(conf, inputData);\r\n    conf.set(\"textinputformat.record.delimiter\", \"+\");\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            testSplitRecordsForFile(conf, splitSize, inputData.length(), inputFile);\r\n        }\r\n    }\r\n    inputData = \"abc|+|def|+|ghi|+|jkl|+|mno|+|pqr|+|stu|+|vw |+|xyz\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    conf.set(\"textinputformat.record.delimiter\", \"|+|\");\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            testSplitRecordsForFile(conf, splitSize, inputData.length(), inputFile);\r\n        }\r\n    }\r\n    inputData = \"abc+def++ghi+jkl++mno+pqr++stu+vw ++xyz\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    conf.set(\"textinputformat.record.delimiter\", \"+\");\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            testSplitRecordsForFile(conf, splitSize, inputData.length(), inputFile);\r\n        }\r\n    }\r\n    inputData = \"abc|+||+|defghi|+|jkl|+||+|mno|+|pqr|+||+|stu|+|vw |+||+|xyz\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    conf.set(\"textinputformat.record.delimiter\", \"|+|\");\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            testSplitRecordsForFile(conf, splitSize, inputData.length(), inputFile);\r\n        }\r\n    }\r\n    inputData = \"abc+def+-ghi+jkl+-mno+pqr+-stu+vw +-xyz\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    conf.set(\"textinputformat.record.delimiter\", \"+-\");\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            testSplitRecordsForFile(conf, splitSize, inputData.length(), inputFile);\r\n        }\r\n    }\r\n    inputData = \"abc\\n+def\\n+ghi\\n+jkl\\n+mno\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    conf.set(\"textinputformat.record.delimiter\", \"\\n+\");\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            testSplitRecordsForFile(conf, splitSize, inputData.length(), inputFile);\r\n        }\r\n    }\r\n    inputData = \"abc\\ndef+\\nghi+\\njkl\\nmno\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    conf.set(\"textinputformat.record.delimiter\", \"+\\n\");\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            testSplitRecordsForFile(conf, splitSize, inputData.length(), inputFile);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testUncompressedInputContainingCRLF",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testUncompressedInputContainingCRLF() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    String inputData = \"a\\r\\nb\\rc\\nd\\r\\n\";\r\n    Path inputFile = createInputFile(conf, inputData);\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            testSplitRecordsForFile(conf, splitSize, inputData.length(), inputFile);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testUncompressedInputCustomDelimiterPosValue",
  "errType" : null,
  "containingMethodsNum" : 57,
  "sourceCodeText" : "void testUncompressedInputCustomDelimiterPosValue() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(\"io.file.buffer.size\", 10);\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH, Integer.MAX_VALUE);\r\n    String inputData = \"abcdefghij++kl++mno\";\r\n    Path inputFile = createInputFile(conf, inputData);\r\n    String delimiter = \"++\";\r\n    byte[] recordDelimiterBytes = delimiter.getBytes(StandardCharsets.UTF_8);\r\n    int splitLength = 15;\r\n    FileSplit split = new FileSplit(inputFile, 0, splitLength, (String[]) null);\r\n    LineRecordReader reader = new LineRecordReader(conf, split, recordDelimiterBytes);\r\n    LongWritable key = new LongWritable();\r\n    Text value = new Text();\r\n    assertTrue(\"Expected record got nothing\", reader.next(key, value));\r\n    assertEquals(\"Wrong length for record value\", 10, value.getLength());\r\n    assertEquals(\"Wrong position after record read\", 12, reader.getPos());\r\n    assertTrue(\"Expected record got nothing\", reader.next(key, value));\r\n    assertEquals(\"Wrong length for record value\", 2, value.getLength());\r\n    assertEquals(\"Wrong position after record read\", 16, reader.getPos());\r\n    assertTrue(\"Expected record got nothing\", reader.next(key, value));\r\n    assertEquals(\"Wrong length for record value\", 3, value.getLength());\r\n    assertEquals(\"Wrong position after record read\", 19, reader.getPos());\r\n    assertFalse(reader.next(key, value));\r\n    assertEquals(\"Wrong position after record read\", 19, reader.getPos());\r\n    reader.close();\r\n    split = new FileSplit(inputFile, splitLength, inputData.length() - splitLength, (String[]) null);\r\n    reader = new LineRecordReader(conf, split, recordDelimiterBytes);\r\n    assertEquals(\"Wrong position after record read\", 19, reader.getPos());\r\n    assertFalse(\"Unexpected record returned\", reader.next(key, value));\r\n    assertEquals(\"Wrong position after record read\", 19, reader.getPos());\r\n    reader.close();\r\n    inputData = \"abcd+efgh++ijk++mno\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    splitLength = 5;\r\n    split = new FileSplit(inputFile, 0, splitLength, (String[]) null);\r\n    reader = new LineRecordReader(conf, split, recordDelimiterBytes);\r\n    assertTrue(\"Expected record got nothing\", reader.next(key, value));\r\n    assertEquals(\"Wrong position after record read\", 11, reader.getPos());\r\n    assertEquals(\"Wrong length for record value\", 9, value.getLength());\r\n    assertFalse(\"Unexpected record returned\", reader.next(key, value));\r\n    assertEquals(\"Wrong position after record read\", 11, reader.getPos());\r\n    reader.close();\r\n    split = new FileSplit(inputFile, splitLength, inputData.length() - splitLength, (String[]) null);\r\n    reader = new LineRecordReader(conf, split, recordDelimiterBytes);\r\n    assertTrue(\"Expected record got nothing\", reader.next(key, value));\r\n    assertEquals(\"Wrong position after record read\", 16, reader.getPos());\r\n    assertEquals(\"Wrong length for record value\", 3, value.getLength());\r\n    assertTrue(\"Expected record got nothing\", reader.next(key, value));\r\n    assertEquals(\"Wrong position after record read\", 19, reader.getPos());\r\n    assertEquals(\"Wrong length for record value\", 3, value.getLength());\r\n    assertFalse(reader.next(key, value));\r\n    assertEquals(\"Wrong position after record read\", 19, reader.getPos());\r\n    reader.close();\r\n    inputData = \"abcd|efgh|+|ij|kl|+|mno|pqr\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    delimiter = \"|+|\";\r\n    recordDelimiterBytes = delimiter.getBytes(StandardCharsets.UTF_8);\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            split = new FileSplit(inputFile, 0, bufferSize, (String[]) null);\r\n            reader = new LineRecordReader(conf, split, recordDelimiterBytes);\r\n            assertTrue(\"Expected record got nothing\", reader.next(key, value));\r\n            assertThat(value.toString()).isEqualTo(\"abcd|efgh\");\r\n            assertEquals(\"Wrong position after record read\", 9, value.getLength());\r\n            int recordPos = 12;\r\n            assertEquals(\"Wrong position after record read\", recordPos, reader.getPos());\r\n            if (reader.next(key, value)) {\r\n                assertThat(value.toString()).isEqualTo(\"ij|kl\");\r\n                recordPos = 20;\r\n                assertEquals(\"Wrong position after record read\", recordPos, reader.getPos());\r\n            }\r\n            if (reader.next(key, value)) {\r\n                assertThat(value.toString()).isEqualTo(\"mno|pqr\");\r\n                recordPos = inputData.length();\r\n                assertEquals(\"Wrong position after record read\", recordPos, reader.getPos());\r\n            }\r\n            assertFalse(\"Unexpected record returned\", reader.next(key, value));\r\n            assertEquals(\"Wrong position after record read\", recordPos, reader.getPos());\r\n            reader.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testUncompressedInputDefaultDelimiterPosValue",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testUncompressedInputDefaultDelimiterPosValue() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    String inputData = \"1234567890\\r\\n12\\r\\n345\";\r\n    Path inputFile = createInputFile(conf, inputData);\r\n    conf.setInt(\"io.file.buffer.size\", 10);\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH, Integer.MAX_VALUE);\r\n    FileSplit split = new FileSplit(inputFile, 0, 15, (String[]) null);\r\n    LineRecordReader reader = new LineRecordReader(conf, split, null);\r\n    LongWritable key = new LongWritable();\r\n    Text value = new Text();\r\n    reader.next(key, value);\r\n    assertEquals(10, value.getLength());\r\n    assertEquals(12, reader.getPos());\r\n    reader.next(key, value);\r\n    assertEquals(2, value.getLength());\r\n    assertEquals(16, reader.getPos());\r\n    assertFalse(reader.next(key, value));\r\n    split = new FileSplit(inputFile, 15, 4, (String[]) null);\r\n    reader = new LineRecordReader(conf, split, null);\r\n    assertEquals(16, reader.getPos());\r\n    reader.next(key, value);\r\n    assertEquals(3, value.getLength());\r\n    assertEquals(19, reader.getPos());\r\n    assertFalse(reader.next(key, value));\r\n    assertEquals(19, reader.getPos());\r\n    inputData = \"123456789\\r\\r\\n\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    split = new FileSplit(inputFile, 0, 12, (String[]) null);\r\n    reader = new LineRecordReader(conf, split, null);\r\n    reader.next(key, value);\r\n    assertEquals(9, value.getLength());\r\n    assertEquals(10, reader.getPos());\r\n    reader.next(key, value);\r\n    assertEquals(0, value.getLength());\r\n    assertEquals(12, reader.getPos());\r\n    assertFalse(reader.next(key, value));\r\n    assertEquals(12, reader.getPos());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBzipWithMultibyteDelimiter",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testBzipWithMultibyteDelimiter() throws IOException\n{\r\n    String testFileName = \"compressedMultibyteDelimiter.txt.bz2\";\r\n    int firstSplitLength = 100;\r\n    URL testFileUrl = getClass().getClassLoader().getResource(testFileName);\r\n    assertNotNull(\"Cannot find \" + testFileName, testFileUrl);\r\n    File testFile = new File(testFileUrl.getFile());\r\n    long testFileSize = testFile.length();\r\n    Path testFilePath = new Path(testFile.getAbsolutePath());\r\n    assertTrue(\"Split size is smaller than header length\", firstSplitLength > 9);\r\n    assertTrue(\"Split size is larger than compressed file size \" + testFilePath, testFileSize > firstSplitLength);\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH, Integer.MAX_VALUE);\r\n    String delimiter = \"<E-LINE>\\r\\r\\n\";\r\n    conf.set(\"textinputformat.record.delimiter\", delimiter);\r\n    testSplitRecordsForFile(conf, firstSplitLength, testFileSize, testFilePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testShuffleMetricsTags",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testShuffleMetricsTags()\n{\r\n    JobID jobID = mock(JobID.class);\r\n    when(jobID.toString()).thenReturn(TEST_JOB_ID);\r\n    TaskAttemptID reduceId = mock(TaskAttemptID.class);\r\n    when(reduceId.getJobID()).thenReturn(jobID);\r\n    when(reduceId.toString()).thenReturn(TEST_TASK_ID);\r\n    JobConf jobConf = mock(JobConf.class);\r\n    when(jobConf.getUser()).thenReturn(TEST_USER_NAME);\r\n    when(jobConf.getJobName()).thenReturn(TEST_JOB_NAME);\r\n    ShuffleClientMetrics shuffleClientMetrics = ShuffleClientMetrics.create(reduceId, jobConf);\r\n    MetricsTag userMetrics = shuffleClientMetrics.getMetricsRegistry().getTag(\"user\");\r\n    assertEquals(TEST_USER_NAME, userMetrics.value());\r\n    MetricsTag jobNameMetrics = shuffleClientMetrics.getMetricsRegistry().getTag(\"jobName\");\r\n    assertEquals(TEST_JOB_NAME, jobNameMetrics.value());\r\n    MetricsTag jobIdMetrics = shuffleClientMetrics.getMetricsRegistry().getTag(\"jobId\");\r\n    assertEquals(TEST_JOB_ID, jobIdMetrics.value());\r\n    MetricsTag taskIdMetrics = shuffleClientMetrics.getMetricsRegistry().getTag(\"taskId\");\r\n    assertEquals(TEST_TASK_ID, taskIdMetrics.value());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "isResilientCommit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isResilientCommit()\n{\r\n    return resilientCommit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "isEtagsPreserved",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isEtagsPreserved()\n{\r\n    return etagsPreserved;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "isEtagsSupported",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isEtagsSupported()\n{\r\n    return etagsSupported;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    final FileSystem fs = getFileSystem();\r\n    final Path methodPath = methodPath();\r\n    etagsSupported = fs.hasPathCapability(methodPath, CommonPathCapabilities.ETAGS_AVAILABLE);\r\n    etagsPreserved = fs.hasPathCapability(methodPath, CommonPathCapabilities.ETAGS_PRESERVED_IN_RENAME);\r\n    final ManifestStoreOperations wrappedOperations = getStoreOperations();\r\n    failures = new UnreliableManifestStoreOperations(wrappedOperations);\r\n    setStoreOperations(failures);\r\n    resilientCommit = wrappedOperations.storeSupportsResilientCommit();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "requireRenameResilience",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean requireRenameResilience()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testResilienceAsExpected",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testResilienceAsExpected() throws Throwable\n{\r\n    Assertions.assertThat(isResilientCommit()).describedAs(\"resilient commit support\").isEqualTo(requireRenameResilience());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testRenameSourceException",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRenameSourceException() throws Throwable\n{\r\n    describe(\"rename fails raising an IOE -expect stage to fail\" + \" and exception message preserved\");\r\n    Path destDir = methodPath();\r\n    StageConfig stageConfig = createStageConfigForJob(JOB1, destDir);\r\n    Path jobAttemptTaskSubDir = stageConfig.getJobAttemptTaskSubDir();\r\n    TaskManifest manifest = new TaskManifest();\r\n    createFileset(destDir, jobAttemptTaskSubDir, manifest, filesToCreate());\r\n    final List<FileEntry> filesToCommit = manifest.getFilesToCommit();\r\n    final FileEntry entry = filesToCommit.get(FAILING_FILE_INDEX);\r\n    failures.addRenameSourceFilesToFail(entry.getSourcePath());\r\n    expectRenameFailure(new RenameFilesStage(stageConfig), manifest, filesToCommit.size(), SIMULATED_FAILURE, PathIOException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "filesToCreate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int filesToCreate()\n{\r\n    return 100;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testCommitMissingFile",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCommitMissingFile() throws Throwable\n{\r\n    describe(\"commit a file which doesn't exist. Expect FNFE always\");\r\n    Path destDir = methodPath();\r\n    StageConfig stageConfig = createStageConfigForJob(JOB1, destDir);\r\n    Path jobAttemptTaskSubDir = stageConfig.getJobAttemptTaskSubDir();\r\n    TaskManifest manifest = new TaskManifest();\r\n    final List<FileEntry> filesToCommit = manifest.getFilesToCommit();\r\n    Path source = new Path(jobAttemptTaskSubDir, \"source.parquet\");\r\n    Path dest = new Path(destDir, \"destdir.parquet\");\r\n    filesToCommit.add(new FileEntry(source, dest, 0, null));\r\n    final FileNotFoundException ex = expectRenameFailure(new RenameFilesStage(stageConfig), manifest, 0, \"\", FileNotFoundException.class);\r\n    LOG.info(\"Exception raised: {}\", ex.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testDeleteTargetPaths",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testDeleteTargetPaths() throws Throwable\n{\r\n    describe(\"Verify that target path deletion works\");\r\n    Path destDir = methodPath();\r\n    StageConfig stageConfig = createStageConfigForJob(JOB1, destDir).withDeleteTargetPaths(true);\r\n    Path jobAttemptTaskSubDir = stageConfig.getJobAttemptTaskSubDir();\r\n    final Path source = new Path(jobAttemptTaskSubDir, \"source.txt\");\r\n    final Path dest = new Path(destDir, \"source.txt\");\r\n    final byte[] sourceData = \"data\".getBytes(StandardCharsets.UTF_8);\r\n    final FileSystem fs = getFileSystem();\r\n    ContractTestUtils.createFile(fs, source, false, sourceData);\r\n    touch(fs, dest);\r\n    TaskManifest manifest = new TaskManifest();\r\n    final FileEntry entry = createEntryWithEtag(source, dest);\r\n    manifest.addFileToCommit(entry);\r\n    List<TaskManifest> manifests = new ArrayList<>();\r\n    manifests.add(manifest);\r\n    boolean renameOverwritesDest = isSupported(RENAME_OVERWRITES_DEST);\r\n    if (!renameOverwritesDest) {\r\n        final IOException ex = expectRenameFailure(new RenameFilesStage(stageConfig.withDeleteTargetPaths(false)), manifest, 0, \"\", IOException.class);\r\n        LOG.info(\"Exception raised: {}\", ex.toString());\r\n    }\r\n    new RenameFilesStage(stageConfig.withDeleteTargetPaths(true)).apply(Pair.of(manifests, Collections.emptySet()));\r\n    verifyFileContents(fs, dest, sourceData);\r\n    if (isEtagsPreserved()) {\r\n        Assertions.assertThat(getEtag(fs.getFileStatus(dest))).describedAs(\"Etag of destination file %s\", dest).isEqualTo(entry.getEtag());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testRenameReturnsFalse",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRenameReturnsFalse() throws Throwable\n{\r\n    describe(\"commit where rename() returns false for one file.\" + \" Expect failure to be escalated to an IOE\");\r\n    Assume.assumeTrue(\"not used when resilient commits are available\", !resilientCommit);\r\n    Path destDir = methodPath();\r\n    StageConfig stageConfig = createStageConfigForJob(JOB1, destDir);\r\n    Path jobAttemptTaskSubDir = stageConfig.getJobAttemptTaskSubDir();\r\n    TaskManifest manifest = new TaskManifest();\r\n    createFileset(destDir, jobAttemptTaskSubDir, manifest, filesToCreate());\r\n    final List<FileEntry> filesToCommit = manifest.getFilesToCommit();\r\n    final FileEntry entry = filesToCommit.get(FAILING_FILE_INDEX);\r\n    failures.addRenameSourceFilesToFail(entry.getSourcePath());\r\n    failures.setRenameToFailWithException(false);\r\n    expectRenameFailure(new RenameFilesStage(stageConfig), manifest, filesToCommit.size(), FAILED_TO_RENAME_PREFIX, PathIOException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createFileset",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createFileset(final Path destDir, final Path taskAttemptDir, final TaskManifest manifest, final int fileCount) throws IOException\n{\r\n    final FileSystem fs = getFileSystem();\r\n    for (int i = 0; i < fileCount; i++) {\r\n        String name = String.format(\"file%04d\", i);\r\n        Path src = new Path(taskAttemptDir, name);\r\n        Path dest = new Path(destDir, name);\r\n        touch(fs, src);\r\n        final FileEntry entry = createEntryWithEtag(src, dest);\r\n        manifest.addFileToCommit(entry);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createEntryWithEtag",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FileEntry createEntryWithEtag(final Path source, final Path dest) throws IOException\n{\r\n    final FileStatus st = getFileSystem().getFileStatus(source);\r\n    final String etag = isEtagsSupported() ? getEtag(st) : null;\r\n    return new FileEntry(source, dest, st.getLen(), etag);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "expectRenameFailure",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "E expectRenameFailure(RenameFilesStage stage, TaskManifest manifest, int files, String errorText, Class<E> exceptionClass) throws Exception\n{\r\n    List<TaskManifest> manifests = new ArrayList<>();\r\n    manifests.add(manifest);\r\n    ProgressCounter progressCounter = getProgressCounter();\r\n    progressCounter.reset();\r\n    IOStatisticsStore iostatistics = stage.getIOStatistics();\r\n    long failures0 = iostatistics.counters().get(RENAME_FAILURES);\r\n    E ex = intercept(exceptionClass, errorText, () -> stage.apply(Pair.of(manifests, Collections.emptySet())));\r\n    LOG.info(\"Statistics {}\", ioStatisticsToPrettyString(iostatistics));\r\n    assertThatStatisticCounter(iostatistics, RENAME_FAILURES).isEqualTo(failures0 + 1);\r\n    if (files > 0) {\r\n        Assertions.assertThat(stage.getFilesCommitted()).describedAs(\"Files Committed by stage\").isNotEmpty().hasSizeLessThan(files);\r\n    }\r\n    Assertions.assertThat(progressCounter.value()).describedAs(\"Progress counter %s\", progressCounter).isGreaterThan(0);\r\n    return ex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    conf = new Configuration();\r\n    jobId = new JobID(\"test\", 1);\r\n    jobContext = new JobContextImpl(conf, jobId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testCloneContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCloneContext() throws Exception\n{\r\n    ContextFactory.cloneContext(jobContext, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testCloneMapContext",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCloneMapContext() throws Exception\n{\r\n    TaskID taskId = new TaskID(jobId, TaskType.MAP, 0);\r\n    TaskAttemptID taskAttemptid = new TaskAttemptID(taskId, 0);\r\n    MapContext<IntWritable, IntWritable, IntWritable, IntWritable> mapContext = new MapContextImpl<IntWritable, IntWritable, IntWritable, IntWritable>(conf, taskAttemptid, null, null, null, null, null);\r\n    Mapper<IntWritable, IntWritable, IntWritable, IntWritable>.Context mapperContext = new WrappedMapper<IntWritable, IntWritable, IntWritable, IntWritable>().getMapContext(mapContext);\r\n    ContextFactory.cloneMapContext(mapperContext, conf, null, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    destDir = methodPath();\r\n    destDir.getFileSystem(getConfiguration()).delete(destDir, true);\r\n    setStoreOperations(createManifestStoreOperations());\r\n    stageConfig = createStageConfigForJob(JOB1, destDir).withDeleteTargetPaths(true);\r\n    setJobStageConfig(stageConfig);\r\n    new SetupJobStage(stageConfig).apply(true);\r\n    mkdirStage = new CreateOutputDirectoriesStage(stageConfig);\r\n    iostats = stageConfig.getIOStatistics();\r\n    verifyStatisticCounterValue(iostats, OP_MKDIRS, DIRECTORIES_CREATED_IN_SETUP);\r\n    iostats.getCounterReference(OP_MKDIRS).set(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testPrepareSomeDirs",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testPrepareSomeDirs() throws Throwable\n{\r\n    final long initialFileStatusCount = lookupCounterStatistic(iostats, OP_GET_FILE_STATUS);\r\n    final int dirCount = 8;\r\n    final List<Path> dirs = subpaths(destDir, dirCount);\r\n    final List<DirEntry> dirEntries = dirEntries(dirs, 1, EntryStatus.not_found);\r\n    final List<TaskManifest> manifests = Lists.newArrayList(manifestWithDirsToCreate(dirEntries), manifestWithDirsToCreate(dirEntries));\r\n    final CreateOutputDirectoriesStage.Result result = mkdirStage.apply(manifests);\r\n    Assertions.assertThat(result.getCreatedDirectories()).describedAs(\"output of %s\", mkdirStage).containsExactlyInAnyOrderElementsOf(dirs);\r\n    LOG.info(\"Job Statistics\\n{}\", ioStatisticsToPrettyString(iostats));\r\n    verifyStatisticCounterValue(iostats, OP_MKDIRS, dirCount);\r\n    final CreateOutputDirectoriesStage s2 = new CreateOutputDirectoriesStage(stageConfig);\r\n    final CreateOutputDirectoriesStage.Result r2 = s2.apply(Lists.newArrayList(manifestWithDirsToCreate(dirEntries(dirs, 1, EntryStatus.dir))));\r\n    Assertions.assertThat(r2.getCreatedDirectories()).describedAs(\"output of %s\", s2).isEmpty();\r\n    LOG.info(\"Job Statistics after second pass\\n{}\", ioStatisticsToPrettyString(iostats));\r\n    verifyStatisticCounterValue(iostats, OP_GET_FILE_STATUS, initialFileStatusCount);\r\n    verifyStatisticCounterValue(iostats, OP_MKDIRS, dirCount);\r\n    verifyStatisticCounterValue(iostats, OP_DELETE_FILE_UNDER_DESTINATION, 0);\r\n    verifyStatisticCounterValue(iostats, OP_IS_FILE, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "dirEntries",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<DirEntry> dirEntries(Collection<Path> paths, int level, EntryStatus entryStatus)\n{\r\n    return paths.stream().map(p -> DirEntry.dirEntry(p, entryStatus, level)).collect(Collectors.toList());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "manifestWithDirsToCreate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskManifest manifestWithDirsToCreate(List<DirEntry> dirEntries)\n{\r\n    final TaskManifest taskManifest = new TaskManifest();\r\n    taskManifest.getDestDirectories().addAll(dirEntries);\r\n    return taskManifest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "assertDirMapStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertDirMapStatus(CreateOutputDirectoriesStage.Result result, Path path, CreateOutputDirectoriesStage.DirMapState expected)\n{\r\n    Assertions.assertThat(result.getDirMap()).describedAs(\"Directory Map entry for %s\", path).isNotNull().containsKey(path).containsEntry(path, expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testPrepareDirtyTree",
  "errType" : null,
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void testPrepareDirtyTree() throws Throwable\n{\r\n    final int c = getDeepTreeWidth();\r\n    final List<Path> level1 = subpaths(destDir, c);\r\n    final List<Path> level2 = level1.stream().flatMap(p -> subpaths(p, c).stream()).collect(Collectors.toList());\r\n    final List<Path> level3 = level2.stream().flatMap(p -> subpaths(p, c).stream()).collect(Collectors.toList());\r\n    final List<DirEntry> directories = new ArrayList<>();\r\n    final List<DirEntry> l1 = dirEntries(level1, 1, EntryStatus.not_found);\r\n    directories.addAll(l1);\r\n    final List<DirEntry> l3 = dirEntries(level3, 3, EntryStatus.not_found);\r\n    directories.addAll(l3);\r\n    final List<DirEntry> l2 = dirEntries(level2, 2, EntryStatus.not_found);\r\n    directories.addAll(l2);\r\n    final DirEntry parentIsFile = l1.get(1);\r\n    final DirEntry parentIsDir = l2.get(0);\r\n    final DirEntry leafIsFile = l3.get(0);\r\n    CompletableFuture.allOf(asyncPut(parentIsFile.getDestPath(), NO_DATA), asyncPut(leafIsFile.getDestPath(), NO_DATA), asyncMkdir(parentIsDir.getDestPath())).join();\r\n    parentIsFile.setStatus(EntryStatus.file);\r\n    parentIsDir.setStatus(EntryStatus.dir);\r\n    leafIsFile.setStatus(EntryStatus.file);\r\n    final List<TaskManifest> manifests = Lists.newArrayList(manifestWithDirsToCreate(directories));\r\n    final CreateOutputDirectoriesStage.Result result = mkdirStage.apply(manifests);\r\n    LOG.info(\"Job Statistics\\n{}\", ioStatisticsToPrettyString(iostats));\r\n    assertDirMapStatus(result, leafIsFile.getDestPath(), CreateOutputDirectoriesStage.DirMapState.fileNowDeleted);\r\n    assertDirMapStatus(result, parentIsFile.getDestPath(), CreateOutputDirectoriesStage.DirMapState.fileNowDeleted);\r\n    Assertions.assertThat(result.getCreatedDirectories()).describedAs(\"output of %s\", mkdirStage).containsExactlyInAnyOrderElementsOf(level3);\r\n    verifyStatisticCounterValue(iostats, OP_MKDIRS, level3.size());\r\n    CreateOutputDirectoriesStage attempt2 = new CreateOutputDirectoriesStage(createStageConfigForJob(JOB1, destDir).withDeleteTargetPaths(true));\r\n    LOG.info(\"Executing failing attempt to create the directories\");\r\n    intercept(IOException.class, () -> attempt2.apply(manifests));\r\n    verifyStatisticCounterValue(iostats, OP_PREPARE_DIR_ANCESTORS + SUFFIX_FAILURES, 1);\r\n    verifyStatisticCounterValue(iostats, OP_DELETE + SUFFIX_FAILURES, 1);\r\n    final List<DirEntry> directories3 = new ArrayList<>();\r\n    directories3.addAll(dirEntries(level1, 1, EntryStatus.dir));\r\n    directories3.addAll(dirEntries(level2, 2, EntryStatus.dir));\r\n    directories3.addAll(dirEntries(level3, 3, EntryStatus.dir));\r\n    final List<TaskManifest> manifests3 = Lists.newArrayList(manifestWithDirsToCreate(directories3));\r\n    CreateOutputDirectoriesStage attempt3 = new CreateOutputDirectoriesStage(createStageConfigForJob(JOB1, destDir).withDeleteTargetPaths(true));\r\n    final CreateOutputDirectoriesStage.Result r3 = attempt3.apply(manifests3);\r\n    assertDirMapStatus(r3, leafIsFile.getDestPath(), CreateOutputDirectoriesStage.DirMapState.dirFoundInStore);\r\n    Assertions.assertThat(r3.getCreatedDirectories()).describedAs(\"created directories\").isEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getDeepTreeWidth",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getDeepTreeWidth()\n{\r\n    return DEEP_TREE_WIDTH;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "testAddFileToClassPath",
  "errType" : [ "NullPointerException", "NullPointerException", "NullPointerException", "NullPointerException" ],
  "containingMethodsNum" : 36,
  "sourceCodeText" : "void testAddFileToClassPath() throws Exception\n{\r\n    Configuration conf = new Configuration(false);\r\n    try {\r\n        DistributedCache.addFileToClassPath(null, conf);\r\n        fail(\"Accepted null archives argument\");\r\n    } catch (NullPointerException ex) {\r\n    }\r\n    DistributedCache.addFileToClassPath(new Path(\"file:///a\"), conf);\r\n    assertEquals(\"The mapreduce.job.classpath.files property was not \" + \"set correctly\", \"file:/a\", conf.get(MRJobConfig.CLASSPATH_FILES));\r\n    assertEquals(\"The mapreduce.job.cache.files property was not set \" + \"correctly\", \"file:///a\", conf.get(MRJobConfig.CACHE_FILES));\r\n    DistributedCache.addFileToClassPath(new Path(\"file:///b\"), conf);\r\n    assertEquals(\"The mapreduce.job.classpath.files property was not \" + \"set correctly\", \"file:/a,file:/b\", conf.get(MRJobConfig.CLASSPATH_FILES));\r\n    assertEquals(\"The mapreduce.job.cache.files property was not set \" + \"correctly\", \"file:///a,file:///b\", conf.get(MRJobConfig.CACHE_FILES));\r\n    FileSystem fs = FileSystem.newInstance(conf);\r\n    conf.clear();\r\n    try {\r\n        DistributedCache.addFileToClassPath(null, conf, fs);\r\n        fail(\"Accepted null archives argument\");\r\n    } catch (NullPointerException ex) {\r\n    }\r\n    DistributedCache.addFileToClassPath(new Path(\"file:///a\"), conf, fs);\r\n    assertEquals(\"The mapreduce.job.classpath.files property was not \" + \"set correctly\", \"file:/a\", conf.get(MRJobConfig.CLASSPATH_FILES));\r\n    assertEquals(\"The mapreduce.job.cache.files property was not set \" + \"correctly\", \"file:///a\", conf.get(MRJobConfig.CACHE_FILES));\r\n    DistributedCache.addFileToClassPath(new Path(\"file:///b\"), conf, fs);\r\n    assertEquals(\"The mapreduce.job.classpath.files property was not \" + \"set correctly\", \"file:/a,file:/b\", conf.get(MRJobConfig.CLASSPATH_FILES));\r\n    assertEquals(\"The mapreduce.job.cache.files property was not set \" + \"correctly\", \"file:///a,file:///b\", conf.get(MRJobConfig.CACHE_FILES));\r\n    conf.clear();\r\n    try {\r\n        DistributedCache.addFileToClassPath(null, conf, fs, true);\r\n        fail(\"Accepted null archives argument\");\r\n    } catch (NullPointerException ex) {\r\n    }\r\n    DistributedCache.addFileToClassPath(new Path(\"file:///a\"), conf, fs, true);\r\n    assertEquals(\"The mapreduce.job.classpath.files property was not \" + \"set correctly\", \"file:/a\", conf.get(MRJobConfig.CLASSPATH_FILES));\r\n    assertEquals(\"The mapreduce.job.cache.files property was not set \" + \"correctly\", \"file:///a\", conf.get(MRJobConfig.CACHE_FILES));\r\n    DistributedCache.addFileToClassPath(new Path(\"file:///b\"), conf, fs, true);\r\n    assertEquals(\"The mapreduce.job.classpath.files property was not \" + \"set correctly\", \"file:/a,file:/b\", conf.get(MRJobConfig.CLASSPATH_FILES));\r\n    assertEquals(\"The mapreduce.job.cache.files property was not set \" + \"correctly\", \"file:///a,file:///b\", conf.get(MRJobConfig.CACHE_FILES));\r\n    conf.clear();\r\n    try {\r\n        DistributedCache.addFileToClassPath(null, conf, fs, false);\r\n        fail(\"Accepted null archives argument\");\r\n    } catch (NullPointerException ex) {\r\n    }\r\n    DistributedCache.addFileToClassPath(new Path(\"file:///a\"), conf, fs, false);\r\n    assertEquals(\"The mapreduce.job.classpath.files property was not \" + \"set correctly\", \"file:/a\", conf.get(MRJobConfig.CLASSPATH_FILES));\r\n    assertEquals(\"The mapreduce.job.cache.files property was not set \" + \"correctly\", \"\", conf.get(MRJobConfig.CACHE_FILES, \"\"));\r\n    DistributedCache.addFileToClassPath(new Path(\"file:///b\"), conf, fs, false);\r\n    assertEquals(\"The mapreduce.job.classpath.files property was not \" + \"set correctly\", \"file:/a,file:/b\", conf.get(MRJobConfig.CLASSPATH_FILES));\r\n    assertEquals(\"The mapreduce.job.cache.files property was not set \" + \"correctly\", \"\", conf.get(MRJobConfig.CACHE_FILES, \"\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testTaskLogAppender",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testTaskLogAppender()\n{\r\n    TaskLogAppender appender = new TaskLogAppender();\r\n    System.setProperty(TaskLogAppender.TASKID_PROPERTY, \"attempt_01_02_m03_04_001\");\r\n    System.setProperty(TaskLogAppender.LOGSIZE_PROPERTY, \"1003\");\r\n    appender.activateOptions();\r\n    assertThat(appender.getTaskId()).isEqualTo(\"attempt_01_02_m03_04_001\");\r\n    assertThat(appender.getTotalLogFileSize()).isEqualTo(1000);\r\n    assertFalse(appender.getIsCleanup());\r\n    Writer writer = new StringWriter();\r\n    appender.setWriter(writer);\r\n    Layout layout = new PatternLayout(\"%-5p [%t]: %m%n\");\r\n    appender.setLayout(layout);\r\n    Category logger = Logger.getLogger(getClass().getName());\r\n    LoggingEvent event = new LoggingEvent(\"fqnOfCategoryClass\", logger, Priority.INFO, \"message\", new Throwable());\r\n    appender.append(event);\r\n    appender.flush();\r\n    appender.close();\r\n    assertTrue(writer.toString().length() > 0);\r\n    appender = new TaskLogAppender();\r\n    appender.setIsCleanup(true);\r\n    appender.activateOptions();\r\n    assertTrue(appender.getIsCleanup());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setup()\n{\r\n    task = new StubTask();\r\n    ExitUtil.disableSystemExit();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testStatusUpdateDoesNotExitInUberMode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testStatusUpdateDoesNotExitInUberMode() throws Exception\n{\r\n    setupTest(true);\r\n    task.statusUpdate(umbilical);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testStatusUpdateExitsInNonUberMode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testStatusUpdateExitsInNonUberMode() throws Exception\n{\r\n    setupTest(false);\r\n    task.statusUpdate(umbilical);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupTest",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setupTest(boolean uberized) throws IOException, InterruptedException\n{\r\n    Configuration conf = new Configuration(false);\r\n    conf.setBoolean(\"mapreduce.task.uberized\", uberized);\r\n    task.setConf(conf);\r\n    when(umbilical.statusUpdate(any(TaskAttemptID.class), any(TaskStatus.class))).thenReturn(feedback);\r\n    when(feedback.getTaskFound()).thenReturn(false, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testPrintJobQueueInfo",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testPrintJobQueueInfo() throws IOException\n{\r\n    JobQueueClient queueClient = new JobQueueClient();\r\n    JobQueueInfo parent = new JobQueueInfo();\r\n    JobQueueInfo child = new JobQueueInfo();\r\n    JobQueueInfo grandChild = new JobQueueInfo();\r\n    child.addChild(grandChild);\r\n    parent.addChild(child);\r\n    grandChild.setQueueName(\"GrandChildQueue\");\r\n    ByteArrayOutputStream bbos = new ByteArrayOutputStream();\r\n    PrintWriter writer = new PrintWriter(bbos);\r\n    queueClient.printJobQueueInfo(parent, writer);\r\n    Assert.assertTrue(\"printJobQueueInfo did not print grandchild's name\", bbos.toString().contains(\"GrandChildQueue\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup() throws Exception\n{\r\n    FileUtil.fullyDelete(TEST_ROOT_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testShufflePermissions",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testShufflePermissions() throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    conf.set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY, \"077\");\r\n    conf.set(MRConfig.LOCAL_DIR, TEST_ROOT_DIR.getAbsolutePath());\r\n    MapOutputFile mof = new MROutputFiles();\r\n    mof.setConf(conf);\r\n    TaskAttemptID attemptId = new TaskAttemptID(\"12345\", 1, TaskType.MAP, 1, 1);\r\n    MapTask mockTask = mock(MapTask.class);\r\n    doReturn(mof).when(mockTask).getMapOutputFile();\r\n    doReturn(attemptId).when(mockTask).getTaskID();\r\n    doReturn(new Progress()).when(mockTask).getSortPhase();\r\n    TaskReporter mockReporter = mock(TaskReporter.class);\r\n    doReturn(new Counter()).when(mockReporter).getCounter(any(TaskCounter.class));\r\n    MapOutputCollector.Context ctx = new MapOutputCollector.Context(mockTask, conf, mockReporter);\r\n    MapOutputBuffer<Object, Object> mob = new MapOutputBuffer<>();\r\n    mob.init(ctx);\r\n    mob.flush();\r\n    mob.close();\r\n    Path outputFile = mof.getOutputFile();\r\n    FileSystem lfs = FileSystem.getLocal(conf);\r\n    FsPermission perms = lfs.getFileStatus(outputFile).getPermission();\r\n    Assert.assertEquals(\"Incorrect output file perms\", (short) 0640, perms.toShort());\r\n    Path indexFile = mof.getOutputIndexFile();\r\n    perms = lfs.getFileStatus(indexFile).getPermission();\r\n    Assert.assertEquals(\"Incorrect index file perms\", (short) 0640, perms.toShort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testMemoryMerge",
  "errType" : null,
  "containingMethodsNum" : 32,
  "sourceCodeText" : "void testMemoryMerge() throws Exception\n{\r\n    final int TOTAL_MEM_BYTES = 10000;\r\n    final int OUTPUT_SIZE = 7950;\r\n    JobConf conf = new JobConf();\r\n    conf.setFloat(MRJobConfig.SHUFFLE_INPUT_BUFFER_PERCENT, 1.0f);\r\n    conf.setLong(MRJobConfig.REDUCE_MEMORY_TOTAL_BYTES, TOTAL_MEM_BYTES);\r\n    conf.setFloat(MRJobConfig.SHUFFLE_MEMORY_LIMIT_PERCENT, 0.8f);\r\n    conf.setFloat(MRJobConfig.SHUFFLE_MERGE_PERCENT, 0.9f);\r\n    TestExceptionReporter reporter = new TestExceptionReporter();\r\n    CyclicBarrier mergeStart = new CyclicBarrier(2);\r\n    CyclicBarrier mergeComplete = new CyclicBarrier(2);\r\n    StubbedMergeManager mgr = new StubbedMergeManager(conf, reporter, mergeStart, mergeComplete);\r\n    MapOutput<Text, Text> out1 = mgr.reserve(null, OUTPUT_SIZE, 0);\r\n    Assert.assertTrue(\"Should be a memory merge\", (out1 instanceof InMemoryMapOutput));\r\n    InMemoryMapOutput<Text, Text> mout1 = (InMemoryMapOutput<Text, Text>) out1;\r\n    fillOutput(mout1);\r\n    MapOutput<Text, Text> out2 = mgr.reserve(null, OUTPUT_SIZE, 0);\r\n    Assert.assertTrue(\"Should be a memory merge\", (out2 instanceof InMemoryMapOutput));\r\n    InMemoryMapOutput<Text, Text> mout2 = (InMemoryMapOutput<Text, Text>) out2;\r\n    fillOutput(mout2);\r\n    MapOutput<Text, Text> out3 = mgr.reserve(null, OUTPUT_SIZE, 0);\r\n    assertThat(out3).withFailMessage(\"Should be told to wait\").isNull();\r\n    mout1.commit();\r\n    mout2.commit();\r\n    mergeStart.await();\r\n    Assert.assertEquals(1, mgr.getNumMerges());\r\n    out1 = mgr.reserve(null, OUTPUT_SIZE, 0);\r\n    Assert.assertTrue(\"Should be a memory merge\", (out1 instanceof InMemoryMapOutput));\r\n    mout1 = (InMemoryMapOutput<Text, Text>) out1;\r\n    fillOutput(mout1);\r\n    out2 = mgr.reserve(null, OUTPUT_SIZE, 0);\r\n    Assert.assertTrue(\"Should be a memory merge\", (out2 instanceof InMemoryMapOutput));\r\n    mout2 = (InMemoryMapOutput<Text, Text>) out2;\r\n    fillOutput(mout2);\r\n    out3 = mgr.reserve(null, OUTPUT_SIZE, 0);\r\n    assertThat(out3).withFailMessage(\"Should be told to wait\").isNull();\r\n    mout1.commit();\r\n    mout2.commit();\r\n    mergeComplete.await();\r\n    mergeStart.await();\r\n    Assert.assertEquals(2, mgr.getNumMerges());\r\n    mergeComplete.await();\r\n    Assert.assertEquals(2, mgr.getNumMerges());\r\n    Assert.assertEquals(\"exception reporter invoked\", 0, reporter.getNumExceptions());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "fillOutput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void fillOutput(InMemoryMapOutput<Text, Text> output) throws IOException\n{\r\n    BoundedByteArrayOutputStream stream = output.getArrayStream();\r\n    int count = stream.getLimit();\r\n    for (int i = 0; i < count; ++i) {\r\n        stream.write(i);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testIoSortDefaults",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testIoSortDefaults()\n{\r\n    final JobConf jobConf = new JobConf();\r\n    assertEquals(10, jobConf.getInt(MRJobConfig.IO_SORT_FACTOR, 100));\r\n    assertEquals(100, jobConf.getInt(MRJobConfig.IO_SORT_MB, 10));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testOnDiskMerger",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testOnDiskMerger() throws IOException, URISyntaxException, InterruptedException\n{\r\n    JobConf jobConf = new JobConf();\r\n    final int SORT_FACTOR = 5;\r\n    jobConf.setInt(MRJobConfig.IO_SORT_FACTOR, SORT_FACTOR);\r\n    MapOutputFile mapOutputFile = new MROutputFiles();\r\n    FileSystem fs = FileSystem.getLocal(jobConf);\r\n    MergeManagerImpl<IntWritable, IntWritable> manager = new MergeManagerImpl<IntWritable, IntWritable>(null, jobConf, fs, null, null, null, null, null, null, null, null, null, null, mapOutputFile);\r\n    MergeThread<MapOutput<IntWritable, IntWritable>, IntWritable, IntWritable> onDiskMerger = (MergeThread<MapOutput<IntWritable, IntWritable>, IntWritable, IntWritable>) Whitebox.getInternalState(manager, \"onDiskMerger\");\r\n    int mergeFactor = (Integer) Whitebox.getInternalState(onDiskMerger, \"mergeFactor\");\r\n    assertEquals(mergeFactor, SORT_FACTOR);\r\n    onDiskMerger.suspend();\r\n    Random rand = new Random();\r\n    for (int i = 0; i < 2 * SORT_FACTOR; ++i) {\r\n        Path path = new Path(\"somePath\");\r\n        CompressAwarePath cap = new CompressAwarePath(path, 1l, rand.nextInt());\r\n        manager.closeOnDiskFile(cap);\r\n    }\r\n    LinkedList<List<CompressAwarePath>> pendingToBeMerged = (LinkedList<List<CompressAwarePath>>) Whitebox.getInternalState(onDiskMerger, \"pendingToBeMerged\");\r\n    assertTrue(\"No inputs were added to list pending to merge\", pendingToBeMerged.size() > 0);\r\n    for (int i = 0; i < pendingToBeMerged.size(); ++i) {\r\n        List<CompressAwarePath> inputs = pendingToBeMerged.get(i);\r\n        for (int j = 1; j < inputs.size(); ++j) {\r\n            assertTrue(\"Not enough / too many inputs were going to be merged\", inputs.size() > 0 && inputs.size() <= SORT_FACTOR);\r\n            assertTrue(\"Inputs to be merged were not sorted according to size: \", inputs.get(j).getCompressedSize() >= inputs.get(j - 1).getCompressedSize());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testLargeMemoryLimits",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testLargeMemoryLimits() throws Exception\n{\r\n    final JobConf conf = new JobConf();\r\n    conf.setLong(MRJobConfig.REDUCE_MEMORY_TOTAL_BYTES, 8L * 1024 * 1024 * 1024);\r\n    conf.setFloat(MRJobConfig.SHUFFLE_INPUT_BUFFER_PERCENT, 1.0f);\r\n    conf.setFloat(MRJobConfig.SHUFFLE_MEMORY_LIMIT_PERCENT, 0.95f);\r\n    conf.setFloat(MRJobConfig.SHUFFLE_MERGE_PERCENT, 1.0f);\r\n    conf.setFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 1.0f);\r\n    final MergeManagerImpl<Text, Text> mgr = new MergeManagerImpl<Text, Text>(null, conf, mock(LocalFileSystem.class), null, null, null, null, null, null, null, null, null, null, new MROutputFiles());\r\n    assertTrue(\"Large shuffle area unusable: \" + mgr.memoryLimit, mgr.memoryLimit > Integer.MAX_VALUE);\r\n    final long maxInMemReduce = mgr.getMaxInMemReduceLimit();\r\n    assertTrue(\"Large in-memory reduce area unusable: \" + maxInMemReduce, maxInMemReduce > Integer.MAX_VALUE);\r\n    assertEquals(\"maxSingleShuffleLimit to be capped at Integer.MAX_VALUE\", Integer.MAX_VALUE, mgr.maxSingleShuffleLimit);\r\n    verifyReservedMapOutputType(mgr, 10L, \"MEMORY\");\r\n    verifyReservedMapOutputType(mgr, 1L + Integer.MAX_VALUE, \"DISK\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "verifyReservedMapOutputType",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyReservedMapOutputType(MergeManagerImpl<Text, Text> mgr, long size, String expectedShuffleMode) throws IOException\n{\r\n    final TaskAttemptID mapId = TaskAttemptID.forName(\"attempt_0_1_m_1_1\");\r\n    final MapOutput<Text, Text> mapOutput = mgr.reserve(mapId, size, 1);\r\n    assertEquals(\"Shuffled bytes: \" + size, expectedShuffleMode, mapOutput.getDescription());\r\n    mgr.unreserve(size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testZeroShuffleMemoryLimitPercent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testZeroShuffleMemoryLimitPercent() throws Exception\n{\r\n    final JobConf jobConf = new JobConf();\r\n    jobConf.setFloat(MRJobConfig.SHUFFLE_MEMORY_LIMIT_PERCENT, 0.0f);\r\n    final MergeManagerImpl<Text, Text> mgr = new MergeManagerImpl<>(null, jobConf, mock(LocalFileSystem.class), null, null, null, null, null, null, null, null, null, null, new MROutputFiles());\r\n    verifyReservedMapOutputType(mgr, 10L, \"DISK\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getConnection",
  "errType" : [ "SQLException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "Connection getConnection()\n{\r\n    Connection connection = mock(FakeConnection.class);\r\n    try {\r\n        Statement statement = mock(Statement.class);\r\n        ResultSet results = mock(ResultSet.class);\r\n        when(results.getLong(1)).thenReturn(15L);\r\n        when(statement.executeQuery(any(String.class))).thenReturn(results);\r\n        when(connection.createStatement()).thenReturn(statement);\r\n        DatabaseMetaData metadata = mock(DatabaseMetaData.class);\r\n        when(metadata.getDatabaseProductName()).thenReturn(\"Test\");\r\n        when(connection.getMetaData()).thenReturn(metadata);\r\n        PreparedStatement reparedStatement0 = mock(PreparedStatement.class);\r\n        when(connection.prepareStatement(anyString())).thenReturn(reparedStatement0);\r\n        PreparedStatement preparedStatement = mock(PreparedStatement.class);\r\n        ResultSet resultSet = mock(ResultSet.class);\r\n        when(resultSet.next()).thenReturn(false);\r\n        when(preparedStatement.executeQuery()).thenReturn(resultSet);\r\n        when(connection.prepareStatement(anyString(), anyInt(), anyInt())).thenReturn(preparedStatement);\r\n    } catch (SQLException e) {\r\n        ;\r\n    }\r\n    return connection;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "acceptsURL",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean acceptsURL(String arg0) throws SQLException\n{\r\n    return \"testUrl\".equals(arg0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "connect",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Connection connect(String arg0, Properties arg1) throws SQLException\n{\r\n    return getConnection();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getMajorVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMajorVersion()\n{\r\n    return 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getMinorVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMinorVersion()\n{\r\n    return 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getPropertyInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DriverPropertyInfo[] getPropertyInfo(String arg0, Properties arg1) throws SQLException\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "jdbcCompliant",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean jdbcCompliant()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getParentLogger",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Logger getParentLogger() throws SQLFeatureNotSupportedException\n{\r\n    throw new SQLFeatureNotSupportedException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "randomJobId",
  "errType" : [ "NumberFormatException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String randomJobId()\n{\r\n    String testUniqueForkId = System.getProperty(\"test.unique.fork.id\", \"0001\");\r\n    int l = testUniqueForkId.length();\r\n    String trailingDigits = testUniqueForkId.substring(l - 4, l);\r\n    int digitValue;\r\n    try {\r\n        digitValue = Integer.valueOf(trailingDigits);\r\n    } catch (NumberFormatException e) {\r\n        digitValue = 0;\r\n    }\r\n    return String.format(\"%s%04d_%04d\", FORMATTER.format(LocalDateTime.now()), (long) (Math.random() * 1000), digitValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getProjectBuildDir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "File getProjectBuildDir()\n{\r\n    String propval = System.getProperty(PROJECT_BUILD_DIRECTORY_PROPERTY);\r\n    if (StringUtils.isEmpty(propval)) {\r\n        propval = \"target\";\r\n    }\r\n    return new File(propval).getAbsoluteFile();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "loadSuccessFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ManifestSuccessData loadSuccessFile(final FileSystem fs, final Path outputPath) throws IOException\n{\r\n    Path success = new Path(outputPath, SUCCESS_MARKER);\r\n    return ManifestSuccessData.load(fs, success);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "validateSuccessFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ManifestSuccessData validateSuccessFile(final FileSystem fs, final Path outputDir, final int minimumFileCount, final String jobId) throws IOException\n{\r\n    Path successPath = new Path(outputDir, SUCCESS_MARKER);\r\n    ManifestSuccessData successData = loadAndPrintSuccessData(fs, successPath);\r\n    assertThat(successData.getCommitter()).describedAs(\"Committer field\").isEqualTo(MANIFEST_COMMITTER_CLASSNAME);\r\n    assertThat(successData.getFilenames()).describedAs(\"Files committed\").hasSizeGreaterThanOrEqualTo(minimumFileCount);\r\n    if (isNotEmpty(jobId)) {\r\n        assertThat(successData.getJobId()).describedAs(\"JobID\").isEqualTo(jobId);\r\n    }\r\n    return successData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "loadAndPrintSuccessData",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ManifestSuccessData loadAndPrintSuccessData(FileSystem fs, Path successPath) throws IOException\n{\r\n    LOG.info(\"Manifest {}\", successPath);\r\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\r\n    PrintStream ps = new PrintStream(baos);\r\n    final ManifestPrinter showManifest = new ManifestPrinter(fs.getConf(), ps);\r\n    ManifestSuccessData successData = showManifest.loadAndPrintManifest(fs, successPath);\r\n    LOG.info(\"{}\", baos);\r\n    return successData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "validateGeneratedFiles",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<Path, LocatedFileStatus> validateGeneratedFiles(FileSystem fs, Path destDir, ManifestSuccessData successData, boolean exclusive) throws IOException\n{\r\n    Map<Path, LocatedFileStatus> map = new HashMap<>();\r\n    RemoteIterators.foreach(fs.listFiles(destDir, true), e -> {\r\n        if (!e.getPath().getName().startsWith(\"_\")) {\r\n            map.put(e.getPath(), e);\r\n        }\r\n    });\r\n    final List<Path> expected = filesInManifest(successData);\r\n    Assertions.assertThat(map.keySet()).describedAs(\"Files in FS compared to manifest\").containsAll(expected);\r\n    if (exclusive) {\r\n        Assertions.assertThat(map.keySet()).describedAs(\"Files in FS compared to manifest\").containsExactlyInAnyOrderElementsOf(expected);\r\n    }\r\n    return map;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "filesInManifest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<Path> filesInManifest(ManifestSuccessData successData)\n{\r\n    return successData.getFilenames().stream().map(AbstractManifestData::unmarshallPath).collect(Collectors.toList());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "lsR",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long lsR(FileSystem fileSystem, Path path, boolean recursive) throws Exception\n{\r\n    if (path == null) {\r\n        LOG.info(\"Empty path\");\r\n        return 0;\r\n    }\r\n    return RemoteIterators.foreach(fileSystem.listFiles(path, recursive), (status) -> LOG.info(\"{}\", status));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "assertFileEntryMatch",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void assertFileEntryMatch(final FileEntry fileOrDir, final Path src, final Path dest, final long l)\n{\r\n    String entry = fileOrDir.toString();\r\n    assertThat(fileOrDir.getSourcePath()).describedAs(\"Source path of \" + entry).isEqualTo(src);\r\n    assertThat(fileOrDir.getDestPath()).describedAs(\"Dest path of \" + entry).isEqualTo(dest);\r\n    assertThat(fileOrDir.getSize()).describedAs(\"Size of \" + entry).isEqualTo(l);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "assertDirEntryMatch",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertDirEntryMatch(final DirEntry fileOrDir, final Path dest, final long type)\n{\r\n    String entry = fileOrDir.toString();\r\n    assertThat(fileOrDir.getDestPath()).describedAs(\"Dest path of \" + entry).isEqualTo(dest);\r\n    assertThat(fileOrDir.getType()).describedAs(\"type of \" + entry).isEqualTo(type);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void setup()\n{\r\n    LOG.info(\">>>> \" + name.getMethodName());\r\n    ReadaheadPool.resetInstance();\r\n    job = new JobConf();\r\n    job.setBoolean(MRJobConfig.SHUFFLE_FETCH_RETRY_ENABLED, false);\r\n    jobWithRetry = new JobConf();\r\n    jobWithRetry.setBoolean(MRJobConfig.SHUFFLE_FETCH_RETRY_ENABLED, true);\r\n    id = TaskAttemptID.forName(\"attempt_0_1_r_1_1\");\r\n    ss = mock(ShuffleSchedulerImpl.class);\r\n    mm = mock(MergeManagerImpl.class);\r\n    r = mock(Reporter.class);\r\n    metrics = mock(ShuffleClientMetrics.class);\r\n    except = mock(ExceptionReporter.class);\r\n    key = JobTokenSecretManager.createSecretKey(new byte[] { 0, 0, 0, 0 });\r\n    connection = mock(HttpURLConnection.class);\r\n    allErrs = mock(Counters.Counter.class);\r\n    when(r.getCounter(anyString(), anyString())).thenReturn(allErrs);\r\n    ArrayList<TaskAttemptID> maps = new ArrayList<TaskAttemptID>(1);\r\n    maps.add(map1ID);\r\n    maps.add(map2ID);\r\n    when(ss.getMapsForHost(host)).thenReturn(maps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws IllegalArgumentException, IOException\n{\r\n    LOG.info(\"<<<< \" + name.getMethodName());\r\n    if (fs != null) {\r\n        fs.delete(new Path(name.getMethodName()), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testReduceOutOfDiskSpace",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testReduceOutOfDiskSpace() throws Throwable\n{\r\n    LOG.info(\"testReduceOutOfDiskSpace\");\r\n    Fetcher<Text, Text> underTest = new FakeFetcher<Text, Text>(job, id, ss, mm, r, metrics, except, key, connection);\r\n    String replyHash = SecureShuffleUtils.generateHash(encHash.getBytes(), key);\r\n    ShuffleHeader header = new ShuffleHeader(map1ID.toString(), 10, 10, 1);\r\n    ByteArrayOutputStream bout = new ByteArrayOutputStream();\r\n    header.write(new DataOutputStream(bout));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(bout.toByteArray());\r\n    when(connection.getResponseCode()).thenReturn(200);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\r\n    when(connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH)).thenReturn(replyHash);\r\n    when(connection.getInputStream()).thenReturn(in);\r\n    when(mm.reserve(any(TaskAttemptID.class), anyLong(), anyInt())).thenThrow(new DiskErrorException(\"No disk space available\"));\r\n    underTest.copyFromHost(host);\r\n    verify(ss).reportLocalError(any(IOException.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testCopyFromHostConnectionTimeout",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCopyFromHostConnectionTimeout() throws Exception\n{\r\n    when(connection.getInputStream()).thenThrow(new SocketTimeoutException(\"This is a fake timeout :)\"));\r\n    Fetcher<Text, Text> underTest = new FakeFetcher<Text, Text>(job, id, ss, mm, r, metrics, except, key, connection);\r\n    underTest.copyFromHost(host);\r\n    verify(connection).addRequestProperty(SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\r\n    verify(allErrs).increment(1);\r\n    verify(ss).copyFailed(map1ID, host, false, false);\r\n    verify(ss).copyFailed(map2ID, host, false, false);\r\n    verify(ss).putBackKnownMapOutput(any(MapHost.class), eq(map1ID));\r\n    verify(ss).putBackKnownMapOutput(any(MapHost.class), eq(map2ID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testCopyFromHostConnectionRejected",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCopyFromHostConnectionRejected() throws Exception\n{\r\n    when(connection.getResponseCode()).thenReturn(Fetcher.TOO_MANY_REQ_STATUS_CODE);\r\n    Fetcher<Text, Text> fetcher = new FakeFetcher<>(job, id, ss, mm, r, metrics, except, key, connection);\r\n    fetcher.copyFromHost(host);\r\n    assertThat(ss.hostFailureCount(host.getHostName())).withFailMessage(\"No host failure is expected.\").isEqualTo(0);\r\n    assertThat(ss.fetchFailureCount(map1ID)).withFailMessage(\"No fetch failure is expected.\").isEqualTo(0);\r\n    assertThat(ss.fetchFailureCount(map2ID)).withFailMessage(\"No fetch failure is expected.\").isEqualTo(0);\r\n    verify(ss).penalize(eq(host), anyLong());\r\n    verify(ss).putBackKnownMapOutput(any(MapHost.class), eq(map1ID));\r\n    verify(ss).putBackKnownMapOutput(any(MapHost.class), eq(map2ID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testCopyFromHostBogusHeader",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testCopyFromHostBogusHeader() throws Exception\n{\r\n    Fetcher<Text, Text> underTest = new FakeFetcher<Text, Text>(job, id, ss, mm, r, metrics, except, key, connection);\r\n    String replyHash = SecureShuffleUtils.generateHash(encHash.getBytes(), key);\r\n    when(connection.getResponseCode()).thenReturn(200);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\r\n    when(connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH)).thenReturn(replyHash);\r\n    ByteArrayInputStream in = new ByteArrayInputStream(\"\\u00010 BOGUS DATA\\nBOGUS DATA\\nBOGUS DATA\\n\".getBytes());\r\n    when(connection.getInputStream()).thenReturn(in);\r\n    underTest.copyFromHost(host);\r\n    verify(connection).addRequestProperty(SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\r\n    verify(allErrs).increment(1);\r\n    verify(ss).copyFailed(map1ID, host, true, false);\r\n    verify(ss).copyFailed(map2ID, host, true, false);\r\n    verify(ss).putBackKnownMapOutput(any(MapHost.class), eq(map1ID));\r\n    verify(ss).putBackKnownMapOutput(any(MapHost.class), eq(map2ID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testCopyFromHostIncompatibleShuffleVersion",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testCopyFromHostIncompatibleShuffleVersion() throws Exception\n{\r\n    String replyHash = SecureShuffleUtils.generateHash(encHash.getBytes(), key);\r\n    when(connection.getResponseCode()).thenReturn(200);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME)).thenReturn(\"mapreduce\").thenReturn(\"other\").thenReturn(\"other\");\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION)).thenReturn(\"1.0.1\").thenReturn(\"1.0.0\").thenReturn(\"1.0.1\");\r\n    when(connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH)).thenReturn(replyHash);\r\n    ByteArrayInputStream in = new ByteArrayInputStream(new byte[0]);\r\n    when(connection.getInputStream()).thenReturn(in);\r\n    for (int i = 0; i < 3; ++i) {\r\n        Fetcher<Text, Text> underTest = new FakeFetcher<Text, Text>(job, id, ss, mm, r, metrics, except, key, connection);\r\n        underTest.copyFromHost(host);\r\n    }\r\n    verify(connection, times(3)).addRequestProperty(SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\r\n    verify(allErrs, times(3)).increment(1);\r\n    verify(ss, times(3)).copyFailed(map1ID, host, false, false);\r\n    verify(ss, times(3)).copyFailed(map2ID, host, false, false);\r\n    verify(ss, times(3)).putBackKnownMapOutput(any(MapHost.class), eq(map1ID));\r\n    verify(ss, times(3)).putBackKnownMapOutput(any(MapHost.class), eq(map2ID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testCopyFromHostIncompatibleShuffleVersionWithRetry",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testCopyFromHostIncompatibleShuffleVersionWithRetry() throws Exception\n{\r\n    String replyHash = SecureShuffleUtils.generateHash(encHash.getBytes(), key);\r\n    when(connection.getResponseCode()).thenReturn(200);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME)).thenReturn(\"mapreduce\").thenReturn(\"other\").thenReturn(\"other\");\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION)).thenReturn(\"1.0.1\").thenReturn(\"1.0.0\").thenReturn(\"1.0.1\");\r\n    when(connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH)).thenReturn(replyHash);\r\n    ByteArrayInputStream in = new ByteArrayInputStream(new byte[0]);\r\n    when(connection.getInputStream()).thenReturn(in);\r\n    for (int i = 0; i < 3; ++i) {\r\n        Fetcher<Text, Text> underTest = new FakeFetcher<Text, Text>(jobWithRetry, id, ss, mm, r, metrics, except, key, connection);\r\n        underTest.copyFromHost(host);\r\n    }\r\n    verify(connection, times(3)).addRequestProperty(SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\r\n    verify(allErrs, times(3)).increment(1);\r\n    verify(ss, times(3)).copyFailed(map1ID, host, false, false);\r\n    verify(ss, times(3)).copyFailed(map2ID, host, false, false);\r\n    verify(ss, times(3)).putBackKnownMapOutput(any(MapHost.class), eq(map1ID));\r\n    verify(ss, times(3)).putBackKnownMapOutput(any(MapHost.class), eq(map2ID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testCopyFromHostWait",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testCopyFromHostWait() throws Exception\n{\r\n    Fetcher<Text, Text> underTest = new FakeFetcher<Text, Text>(job, id, ss, mm, r, metrics, except, key, connection);\r\n    String replyHash = SecureShuffleUtils.generateHash(encHash.getBytes(), key);\r\n    when(connection.getResponseCode()).thenReturn(200);\r\n    when(connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH)).thenReturn(replyHash);\r\n    ShuffleHeader header = new ShuffleHeader(map1ID.toString(), 10, 10, 1);\r\n    ByteArrayOutputStream bout = new ByteArrayOutputStream();\r\n    header.write(new DataOutputStream(bout));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(bout.toByteArray());\r\n    when(connection.getInputStream()).thenReturn(in);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\r\n    when(mm.reserve(any(TaskAttemptID.class), anyLong(), anyInt())).thenReturn(null);\r\n    underTest.copyFromHost(host);\r\n    verify(connection).addRequestProperty(SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\r\n    verify(allErrs, never()).increment(1);\r\n    verify(ss, never()).copyFailed(map1ID, host, true, false);\r\n    verify(ss, never()).copyFailed(map2ID, host, true, false);\r\n    verify(ss).putBackKnownMapOutput(any(MapHost.class), eq(map1ID));\r\n    verify(ss).putBackKnownMapOutput(any(MapHost.class), eq(map2ID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testCopyFromHostCompressFailure",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCopyFromHostCompressFailure() throws Exception\n{\r\n    InMemoryMapOutput<Text, Text> immo = mock(InMemoryMapOutput.class);\r\n    Fetcher<Text, Text> underTest = new FakeFetcher<Text, Text>(job, id, ss, mm, r, metrics, except, key, connection);\r\n    String replyHash = SecureShuffleUtils.generateHash(encHash.getBytes(), key);\r\n    when(connection.getResponseCode()).thenReturn(200);\r\n    when(connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH)).thenReturn(replyHash);\r\n    ShuffleHeader header = new ShuffleHeader(map1ID.toString(), 10, 10, 1);\r\n    ByteArrayOutputStream bout = new ByteArrayOutputStream();\r\n    header.write(new DataOutputStream(bout));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(bout.toByteArray());\r\n    when(connection.getInputStream()).thenReturn(in);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\r\n    when(mm.reserve(any(TaskAttemptID.class), anyLong(), anyInt())).thenReturn(immo);\r\n    doThrow(new java.lang.InternalError()).when(immo).shuffle(any(MapHost.class), any(InputStream.class), anyLong(), anyLong(), any(ShuffleClientMetrics.class), any(Reporter.class));\r\n    underTest.copyFromHost(host);\r\n    verify(connection).addRequestProperty(SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\r\n    verify(ss, times(1)).copyFailed(map1ID, host, true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testCopyFromHostOnAnyException",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCopyFromHostOnAnyException() throws Exception\n{\r\n    InMemoryMapOutput<Text, Text> immo = mock(InMemoryMapOutput.class);\r\n    Fetcher<Text, Text> underTest = new FakeFetcher<Text, Text>(job, id, ss, mm, r, metrics, except, key, connection);\r\n    String replyHash = SecureShuffleUtils.generateHash(encHash.getBytes(), key);\r\n    when(connection.getResponseCode()).thenReturn(200);\r\n    when(connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH)).thenReturn(replyHash);\r\n    ShuffleHeader header = new ShuffleHeader(map1ID.toString(), 10, 10, 1);\r\n    ByteArrayOutputStream bout = new ByteArrayOutputStream();\r\n    header.write(new DataOutputStream(bout));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(bout.toByteArray());\r\n    when(connection.getInputStream()).thenReturn(in);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\r\n    when(mm.reserve(any(TaskAttemptID.class), anyLong(), anyInt())).thenReturn(immo);\r\n    doThrow(new ArrayIndexOutOfBoundsException()).when(immo).shuffle(any(MapHost.class), any(InputStream.class), anyLong(), anyLong(), any(ShuffleClientMetrics.class), any(Reporter.class));\r\n    underTest.copyFromHost(host);\r\n    verify(connection).addRequestProperty(SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\r\n    verify(ss, times(1)).copyFailed(map1ID, host, true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testCopyFromHostWithRetry",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testCopyFromHostWithRetry() throws Exception\n{\r\n    InMemoryMapOutput<Text, Text> immo = mock(InMemoryMapOutput.class);\r\n    ss = mock(ShuffleSchedulerImpl.class);\r\n    Fetcher<Text, Text> underTest = new FakeFetcher<Text, Text>(jobWithRetry, id, ss, mm, r, metrics, except, key, connection, true);\r\n    String replyHash = SecureShuffleUtils.generateHash(encHash.getBytes(), key);\r\n    when(connection.getResponseCode()).thenReturn(200);\r\n    when(connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH)).thenReturn(replyHash);\r\n    ShuffleHeader header = new ShuffleHeader(map1ID.toString(), 10, 10, 1);\r\n    ByteArrayOutputStream bout = new ByteArrayOutputStream();\r\n    header.write(new DataOutputStream(bout));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(bout.toByteArray());\r\n    when(connection.getInputStream()).thenReturn(in);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\r\n    when(mm.reserve(any(TaskAttemptID.class), anyLong(), anyInt())).thenReturn(immo);\r\n    final long retryTime = Time.monotonicNow();\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock ignore) throws IOException {\r\n            if ((Time.monotonicNow() - retryTime) <= 3000) {\r\n                throw new java.lang.InternalError();\r\n            }\r\n            return null;\r\n        }\r\n    }).when(immo).shuffle(any(MapHost.class), any(InputStream.class), anyLong(), anyLong(), any(ShuffleClientMetrics.class), any(Reporter.class));\r\n    underTest.copyFromHost(host);\r\n    verify(ss, never()).copyFailed(any(TaskAttemptID.class), any(MapHost.class), anyBoolean(), anyBoolean());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testCopyFromHostWithRetryThenTimeout",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testCopyFromHostWithRetryThenTimeout() throws Exception\n{\r\n    InMemoryMapOutput<Text, Text> immo = mock(InMemoryMapOutput.class);\r\n    Fetcher<Text, Text> underTest = new FakeFetcher<Text, Text>(jobWithRetry, id, ss, mm, r, metrics, except, key, connection);\r\n    String replyHash = SecureShuffleUtils.generateHash(encHash.getBytes(), key);\r\n    when(connection.getResponseCode()).thenReturn(200).thenThrow(new SocketTimeoutException(\"forced timeout\"));\r\n    when(connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH)).thenReturn(replyHash);\r\n    ShuffleHeader header = new ShuffleHeader(map1ID.toString(), 10, 10, 1);\r\n    ByteArrayOutputStream bout = new ByteArrayOutputStream();\r\n    header.write(new DataOutputStream(bout));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(bout.toByteArray());\r\n    when(connection.getInputStream()).thenReturn(in);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\r\n    when(mm.reserve(any(TaskAttemptID.class), anyLong(), anyInt())).thenReturn(immo);\r\n    doThrow(new IOException(\"forced error\")).when(immo).shuffle(any(MapHost.class), any(InputStream.class), anyLong(), anyLong(), any(ShuffleClientMetrics.class), any(Reporter.class));\r\n    underTest.copyFromHost(host);\r\n    verify(allErrs).increment(1);\r\n    verify(ss, times(1)).copyFailed(map1ID, host, false, false);\r\n    verify(ss, times(1)).copyFailed(map2ID, host, false, false);\r\n    verify(ss, times(1)).putBackKnownMapOutput(any(MapHost.class), eq(map1ID));\r\n    verify(ss, times(1)).putBackKnownMapOutput(any(MapHost.class), eq(map2ID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testCopyFromHostExtraBytes",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testCopyFromHostExtraBytes() throws Exception\n{\r\n    Fetcher<Text, Text> underTest = new FakeFetcher<Text, Text>(job, id, ss, mm, r, metrics, except, key, connection);\r\n    String replyHash = SecureShuffleUtils.generateHash(encHash.getBytes(), key);\r\n    when(connection.getResponseCode()).thenReturn(200);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\r\n    when(connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH)).thenReturn(replyHash);\r\n    ShuffleHeader header = new ShuffleHeader(map1ID.toString(), 14, 10, 1);\r\n    ByteArrayOutputStream bout = new ByteArrayOutputStream();\r\n    DataOutputStream dos = new DataOutputStream(bout);\r\n    IFileOutputStream ios = new IFileOutputStream(dos);\r\n    header.write(dos);\r\n    ios.write(\"MAPDATA123\".getBytes());\r\n    ios.finish();\r\n    ShuffleHeader header2 = new ShuffleHeader(map2ID.toString(), 14, 10, 1);\r\n    IFileOutputStream ios2 = new IFileOutputStream(dos);\r\n    header2.write(dos);\r\n    ios2.write(\"MAPDATA456\".getBytes());\r\n    ios2.finish();\r\n    ByteArrayInputStream in = new ByteArrayInputStream(bout.toByteArray());\r\n    when(connection.getInputStream()).thenReturn(in);\r\n    IFileWrappedMapOutput<Text, Text> mapOut = new InMemoryMapOutput<Text, Text>(job, map1ID, mm, 8, null, true);\r\n    IFileWrappedMapOutput<Text, Text> mapOut2 = new InMemoryMapOutput<Text, Text>(job, map2ID, mm, 10, null, true);\r\n    when(mm.reserve(eq(map1ID), anyLong(), anyInt())).thenReturn(mapOut);\r\n    when(mm.reserve(eq(map2ID), anyLong(), anyInt())).thenReturn(mapOut2);\r\n    underTest.copyFromHost(host);\r\n    verify(allErrs).increment(1);\r\n    verify(ss).copyFailed(map1ID, host, true, false);\r\n    verify(ss, never()).copyFailed(map2ID, host, true, false);\r\n    verify(ss).putBackKnownMapOutput(any(MapHost.class), eq(map1ID));\r\n    verify(ss).putBackKnownMapOutput(any(MapHost.class), eq(map2ID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testCorruptedIFile",
  "errType" : [ "ChecksumException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testCorruptedIFile() throws Exception\n{\r\n    final int fetcher = 7;\r\n    Path onDiskMapOutputPath = new Path(name.getMethodName() + \"/foo\");\r\n    Path shuffledToDisk = OnDiskMapOutput.getTempPath(onDiskMapOutputPath, fetcher);\r\n    fs = FileSystem.getLocal(job).getRaw();\r\n    IFileWrappedMapOutput<Text, Text> odmo = new OnDiskMapOutput<Text, Text>(map1ID, mm, 100L, job, fetcher, true, fs, onDiskMapOutputPath);\r\n    String mapData = \"MAPDATA12345678901234567890\";\r\n    ShuffleHeader header = new ShuffleHeader(map1ID.toString(), 14, 10, 1);\r\n    ByteArrayOutputStream bout = new ByteArrayOutputStream();\r\n    DataOutputStream dos = new DataOutputStream(bout);\r\n    IFileOutputStream ios = new IFileOutputStream(dos);\r\n    header.write(dos);\r\n    int headerSize = dos.size();\r\n    try {\r\n        ios.write(mapData.getBytes());\r\n    } finally {\r\n        ios.close();\r\n    }\r\n    int dataSize = bout.size() - headerSize;\r\n    MapHost host = new MapHost(\"TestHost\", \"http://test/url\");\r\n    ByteArrayInputStream bin = new ByteArrayInputStream(bout.toByteArray());\r\n    try {\r\n        bin.read(new byte[headerSize], 0, headerSize);\r\n        odmo.shuffle(host, bin, dataSize, dataSize, metrics, Reporter.NULL);\r\n    } finally {\r\n        bin.close();\r\n    }\r\n    byte[] corrupted = bout.toByteArray();\r\n    corrupted[headerSize + (dataSize / 2)] = 0x0;\r\n    try {\r\n        bin = new ByteArrayInputStream(corrupted);\r\n        bin.read(new byte[headerSize], 0, headerSize);\r\n        odmo.shuffle(host, bin, dataSize, dataSize, metrics, Reporter.NULL);\r\n        fail(\"OnDiskMapOutput.shuffle didn't detect the corrupted map partition file\");\r\n    } catch (ChecksumException e) {\r\n        LOG.info(\"The expected checksum exception was thrown.\", e);\r\n    } finally {\r\n        bin.close();\r\n    }\r\n    IFileInputStream iFin = new IFileInputStream(fs.open(shuffledToDisk), dataSize, job);\r\n    try {\r\n        iFin.read(new byte[dataSize], 0, dataSize);\r\n    } finally {\r\n        iFin.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testInterruptInMemory",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testInterruptInMemory() throws Exception\n{\r\n    final int FETCHER = 2;\r\n    IFileWrappedMapOutput<Text, Text> immo = spy(new InMemoryMapOutput<Text, Text>(job, id, mm, 100, null, true));\r\n    when(mm.reserve(any(TaskAttemptID.class), anyLong(), anyInt())).thenReturn(immo);\r\n    doNothing().when(mm).waitForResource();\r\n    when(ss.getHost()).thenReturn(host);\r\n    String replyHash = SecureShuffleUtils.generateHash(encHash.getBytes(), key);\r\n    when(connection.getResponseCode()).thenReturn(200);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\r\n    when(connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH)).thenReturn(replyHash);\r\n    ShuffleHeader header = new ShuffleHeader(map1ID.toString(), 10, 10, 1);\r\n    ByteArrayOutputStream bout = new ByteArrayOutputStream();\r\n    header.write(new DataOutputStream(bout));\r\n    final StuckInputStream in = new StuckInputStream(new ByteArrayInputStream(bout.toByteArray()));\r\n    when(connection.getInputStream()).thenReturn(in);\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock ignore) throws IOException {\r\n            in.close();\r\n            return null;\r\n        }\r\n    }).when(connection).disconnect();\r\n    Fetcher<Text, Text> underTest = new FakeFetcher<Text, Text>(job, id, ss, mm, r, metrics, except, key, connection, FETCHER);\r\n    underTest.start();\r\n    in.waitForFetcher();\r\n    underTest.shutDown();\r\n    underTest.join();\r\n    assertTrue(in.wasClosedProperly());\r\n    verify(immo).abort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testInterruptOnDisk",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testInterruptOnDisk() throws Exception\n{\r\n    final int FETCHER = 7;\r\n    Path p = new Path(\"file:///tmp/foo\");\r\n    Path pTmp = OnDiskMapOutput.getTempPath(p, FETCHER);\r\n    FileSystem mFs = mock(FileSystem.class, RETURNS_DEEP_STUBS);\r\n    IFileWrappedMapOutput<Text, Text> odmo = spy(new OnDiskMapOutput<Text, Text>(map1ID, mm, 100L, job, FETCHER, true, mFs, p));\r\n    when(mm.reserve(any(TaskAttemptID.class), anyLong(), anyInt())).thenReturn(odmo);\r\n    doNothing().when(mm).waitForResource();\r\n    when(ss.getHost()).thenReturn(host);\r\n    String replyHash = SecureShuffleUtils.generateHash(encHash.getBytes(), key);\r\n    when(connection.getResponseCode()).thenReturn(200);\r\n    when(connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH)).thenReturn(replyHash);\r\n    ShuffleHeader header = new ShuffleHeader(map1ID.toString(), 10, 10, 1);\r\n    ByteArrayOutputStream bout = new ByteArrayOutputStream();\r\n    header.write(new DataOutputStream(bout));\r\n    final StuckInputStream in = new StuckInputStream(new ByteArrayInputStream(bout.toByteArray()));\r\n    when(connection.getInputStream()).thenReturn(in);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock ignore) throws IOException {\r\n            in.close();\r\n            return null;\r\n        }\r\n    }).when(connection).disconnect();\r\n    Fetcher<Text, Text> underTest = new FakeFetcher<Text, Text>(job, id, ss, mm, r, metrics, except, key, connection, FETCHER);\r\n    underTest.start();\r\n    in.waitForFetcher();\r\n    underTest.shutDown();\r\n    underTest.join();\r\n    assertTrue(in.wasClosedProperly());\r\n    verify(mFs).create(eq(pTmp));\r\n    verify(mFs).delete(eq(pTmp), eq(false));\r\n    verify(odmo).abort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testCopyFromHostWithRetryUnreserve",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testCopyFromHostWithRetryUnreserve() throws Exception\n{\r\n    InMemoryMapOutput<Text, Text> immo = mock(InMemoryMapOutput.class);\r\n    Fetcher<Text, Text> underTest = new FakeFetcher<Text, Text>(jobWithRetry, id, ss, mm, r, metrics, except, key, connection);\r\n    String replyHash = SecureShuffleUtils.generateHash(encHash.getBytes(), key);\r\n    when(connection.getResponseCode()).thenReturn(200);\r\n    when(connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH)).thenReturn(replyHash);\r\n    ShuffleHeader header = new ShuffleHeader(map1ID.toString(), 10, 10, 1);\r\n    ByteArrayOutputStream bout = new ByteArrayOutputStream();\r\n    header.write(new DataOutputStream(bout));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(bout.toByteArray());\r\n    when(connection.getInputStream()).thenReturn(in);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\r\n    when(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION)).thenReturn(ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\r\n    when(mm.reserve(any(TaskAttemptID.class), anyLong(), anyInt())).thenReturn(immo);\r\n    doThrow(new IOException(\"forced error\")).when(immo).shuffle(any(MapHost.class), any(InputStream.class), anyLong(), anyLong(), any(ShuffleClientMetrics.class), any(Reporter.class));\r\n    underTest.copyFromHost(host);\r\n    verify(immo).abort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    taskIDs = new ManifestCommitterTestSupport.JobAndTaskIDsForTests(2, 2);\r\n    source = new TaskManifest();\r\n    taskAttempt00 = taskIDs.getTaskAttempt(0, 0);\r\n    source.setTaskAttemptID(taskAttempt00);\r\n    testPath = methodPath();\r\n    taPath = new Path(testPath, \"  \" + taskAttempt00);\r\n    source.setTaskAttemptDir(marshallPath(taPath));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testJsonRoundTrip",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testJsonRoundTrip() throws Throwable\n{\r\n    describe(\"Save manifest file to string and back\");\r\n    Path subdirS = new Path(taPath, \"subdir\");\r\n    Path subdirD = new Path(testPath, \"subdir\");\r\n    source.addDirectory(DirEntry.dirEntry(subdirD, 0, 0));\r\n    Path subfileS = new Path(subdirS, \"file\");\r\n    Path subfileD = new Path(subdirD, \"file\");\r\n    long len = 256L;\r\n    FileEntry subFileEntry = new FileEntry(subfileS, subfileD, len, \"etag\");\r\n    source.addFileToCommit(subFileEntry);\r\n    JsonSerialization<TaskManifest> serializer = TaskManifest.serializer();\r\n    String json = serializer.toJson(source);\r\n    LOG.info(\"serialized form\\n{}\", json);\r\n    TaskManifest deser = serializer.fromJson(json);\r\n    deser.validate();\r\n    Assertions.assertThat(deser.getTaskAttemptID()).describedAs(\"Task attempt ID\").isEqualTo(taskAttempt00);\r\n    Assertions.assertThat(unmarshallPath(deser.getTaskAttemptDir())).describedAs(\"Task attempt Dir %s\", deser.getTaskAttemptDir()).isEqualTo(taPath);\r\n    Assertions.assertThat(deser.getDestDirectories()).hasSize(1).allSatisfy(d -> assertDirEntryMatch(d, subdirD, 0));\r\n    Assertions.assertThat(deser.getFilesToCommit()).hasSize(1).allSatisfy(d -> assertFileEntryMatch(d, subfileS, subfileD, len));\r\n    final FileEntry entry = deser.getFilesToCommit().get(0);\r\n    assertFileEntryMatch(entry, subfileS, subfileD, len);\r\n    Assertions.assertThat(entry.getEtag()).describedAs(\"etag of %s\", entry).isEqualTo(\"etag\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testValidateRejectsTwoCommitsToSameDest",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testValidateRejectsTwoCommitsToSameDest() throws Throwable\n{\r\n    Path subdirS = new Path(taPath, \"subdir\");\r\n    Path subdirD = new Path(testPath, \"subdir\");\r\n    source.addDirectory(DirEntry.dirEntry(subdirD, 0, 0));\r\n    Path subfileS = new Path(subdirS, \"file\");\r\n    Path subfileS2 = new Path(subdirS, \"file2\");\r\n    Path subfileD = new Path(subdirD, \"file\");\r\n    long len = 256L;\r\n    source.addFileToCommit(new FileEntry(subfileS, subfileD, len, \"tag1\"));\r\n    source.addFileToCommit(new FileEntry(subfileS2, subfileD, len, \"tag2\"));\r\n    assertValidationFailureOnRoundTrip(source);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testValidateRejectsIncompleteFileEntry",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testValidateRejectsIncompleteFileEntry() throws Throwable\n{\r\n    source.addFileToCommit(new FileEntry(taPath, null, 0, null));\r\n    assertValidationFailureOnRoundTrip(source);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testValidateRejectsInvalidFileLength",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testValidateRejectsInvalidFileLength() throws Throwable\n{\r\n    source.addFileToCommit(new FileEntry(taPath, testPath, -1, null));\r\n    assertValidationFailureOnRoundTrip(source);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testRejectIncompatibleVersion",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRejectIncompatibleVersion() throws Throwable\n{\r\n    source.setVersion(5);\r\n    assertValidationFailureOnRoundTrip(source);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testRejectIncompatibleType",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRejectIncompatibleType() throws Throwable\n{\r\n    source.setType(\"Incompatible type\");\r\n    assertValidationFailureOnRoundTrip(source);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "assertValidationFailureOnRoundTrip",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void assertValidationFailureOnRoundTrip(final TaskManifest manifest) throws Exception\n{\r\n    JsonSerialization<TaskManifest> serializer = TaskManifest.serializer();\r\n    String json = serializer.toJson(manifest);\r\n    LOG.info(\"serialized form\\n{}\", json);\r\n    TaskManifest deser = serializer.fromJson(json);\r\n    intercept(IOException.class, deser::validate);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "data",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> data()\n{\r\n    Object[][] data = new Object[][] { { 1 }, { 5 } };\r\n    return Arrays.asList(data);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    LOG.info(\"Using Test Dir: \" + TEST_ROOT_DIR);\r\n    localFs = FileSystem.getLocal(new Configuration());\r\n    localFs.delete(TEST_ROOT_DIR, true);\r\n    localFs.mkdirs(TEST_ROOT_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup() throws IOException\n{\r\n    localFs.delete(TEST_ROOT_DIR, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testNumInputFilesRecursively",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testNumInputFilesRecursively() throws Exception\n{\r\n    Configuration conf = getConfiguration();\r\n    conf.set(FileInputFormat.INPUT_DIR_RECURSIVE, \"true\");\r\n    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);\r\n    Job job = Job.getInstance(conf);\r\n    FileInputFormat<?, ?> fileInputFormat = new TextInputFormat();\r\n    List<InputSplit> splits = fileInputFormat.getSplits(job);\r\n    Assert.assertEquals(\"Input splits are not correct\", 3, splits.size());\r\n    verifySplits(Lists.newArrayList(\"test:/a1/a2/file2\", \"test:/a1/a2/file3\", \"test:/a1/file1\"), splits);\r\n    conf = getConfiguration();\r\n    conf.set(\"mapred.input.dir.recursive\", \"true\");\r\n    job = Job.getInstance(conf);\r\n    splits = fileInputFormat.getSplits(job);\r\n    verifySplits(Lists.newArrayList(\"test:/a1/a2/file2\", \"test:/a1/a2/file3\", \"test:/a1/file1\"), splits);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testNumInputFilesWithoutRecursively",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testNumInputFilesWithoutRecursively() throws Exception\n{\r\n    Configuration conf = getConfiguration();\r\n    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);\r\n    Job job = Job.getInstance(conf);\r\n    FileInputFormat<?, ?> fileInputFormat = new TextInputFormat();\r\n    List<InputSplit> splits = fileInputFormat.getSplits(job);\r\n    Assert.assertEquals(\"Input splits are not correct\", 2, splits.size());\r\n    verifySplits(Lists.newArrayList(\"test:/a1/a2\", \"test:/a1/file1\"), splits);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testNumInputFilesIgnoreDirs",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testNumInputFilesIgnoreDirs() throws Exception\n{\r\n    Configuration conf = getConfiguration();\r\n    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);\r\n    conf.setBoolean(FileInputFormat.INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, true);\r\n    Job job = Job.getInstance(conf);\r\n    FileInputFormat<?, ?> fileInputFormat = new TextInputFormat();\r\n    List<InputSplit> splits = fileInputFormat.getSplits(job);\r\n    Assert.assertEquals(\"Input splits are not correct\", 1, splits.size());\r\n    verifySplits(Lists.newArrayList(\"test:/a1/file1\"), splits);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testListLocatedStatus",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testListLocatedStatus() throws Exception\n{\r\n    Configuration conf = getConfiguration();\r\n    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);\r\n    conf.setBoolean(\"fs.test.impl.disable.cache\", false);\r\n    conf.set(FileInputFormat.INPUT_DIR, \"test:///a1/a2\");\r\n    MockFileSystem mockFs = (MockFileSystem) new Path(\"test:///\").getFileSystem(conf);\r\n    Assert.assertEquals(\"listLocatedStatus already called\", 0, mockFs.numListLocatedStatusCalls);\r\n    Job job = Job.getInstance(conf);\r\n    FileInputFormat<?, ?> fileInputFormat = new TextInputFormat();\r\n    List<InputSplit> splits = fileInputFormat.getSplits(job);\r\n    Assert.assertEquals(\"Input splits are not correct\", 2, splits.size());\r\n    Assert.assertEquals(\"listLocatedStatuss calls\", 1, mockFs.numListLocatedStatusCalls);\r\n    FileSystem.closeAll();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testSplitLocationInfo",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testSplitLocationInfo() throws Exception\n{\r\n    Configuration conf = getConfiguration();\r\n    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, \"test:///a1/a2\");\r\n    Job job = Job.getInstance(conf);\r\n    TextInputFormat fileInputFormat = new TextInputFormat();\r\n    List<InputSplit> splits = fileInputFormat.getSplits(job);\r\n    String[] locations = splits.get(0).getLocations();\r\n    Assert.assertEquals(2, locations.length);\r\n    SplitLocationInfo[] locationInfo = splits.get(0).getLocationInfo();\r\n    Assert.assertEquals(2, locationInfo.length);\r\n    SplitLocationInfo localhostInfo = locations[0].equals(\"localhost\") ? locationInfo[0] : locationInfo[1];\r\n    SplitLocationInfo otherhostInfo = locations[0].equals(\"otherhost\") ? locationInfo[0] : locationInfo[1];\r\n    Assert.assertTrue(localhostInfo.isOnDisk());\r\n    Assert.assertTrue(localhostInfo.isInMemory());\r\n    Assert.assertTrue(otherhostInfo.isOnDisk());\r\n    Assert.assertFalse(otherhostInfo.isInMemory());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testListStatusSimple",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testListStatusSimple() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);\r\n    List<Path> expectedPaths = configureTestSimple(conf, localFs);\r\n    Job job = Job.getInstance(conf);\r\n    FileInputFormat<?, ?> fif = new TextInputFormat();\r\n    List<FileStatus> statuses = fif.listStatus(job);\r\n    verifyFileStatuses(expectedPaths, statuses, localFs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testListStatusNestedRecursive",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testListStatusNestedRecursive() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);\r\n    List<Path> expectedPaths = configureTestNestedRecursive(conf, localFs);\r\n    Job job = Job.getInstance(conf);\r\n    FileInputFormat<?, ?> fif = new TextInputFormat();\r\n    List<FileStatus> statuses = fif.listStatus(job);\r\n    verifyFileStatuses(expectedPaths, statuses, localFs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testListStatusNestedNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testListStatusNestedNonRecursive() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);\r\n    List<Path> expectedPaths = configureTestNestedNonRecursive(conf, localFs);\r\n    Job job = Job.getInstance(conf);\r\n    FileInputFormat<?, ?> fif = new TextInputFormat();\r\n    List<FileStatus> statuses = fif.listStatus(job);\r\n    verifyFileStatuses(expectedPaths, statuses, localFs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testListStatusErrorOnNonExistantDir",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testListStatusErrorOnNonExistantDir() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);\r\n    configureTestErrorOnNonExistantDir(conf, localFs);\r\n    Job job = Job.getInstance(conf);\r\n    FileInputFormat<?, ?> fif = new TextInputFormat();\r\n    try {\r\n        fif.listStatus(job);\r\n        Assert.fail(\"Expecting an IOException for a missing Input path\");\r\n    } catch (IOException e) {\r\n        Path expectedExceptionPath = new Path(TEST_ROOT_DIR, \"input2\");\r\n        expectedExceptionPath = localFs.makeQualified(expectedExceptionPath);\r\n        Assert.assertTrue(e instanceof InvalidInputException);\r\n        Assert.assertEquals(\"Input path does not exist: \" + expectedExceptionPath.toString(), e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testShrinkStatus",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testShrinkStatus() throws IOException\n{\r\n    Configuration conf = getConfiguration();\r\n    MockFileSystem mockFs = (MockFileSystem) new Path(\"test:///\").getFileSystem(conf);\r\n    Path dir1 = new Path(\"test:/a1\");\r\n    RemoteIterator<LocatedFileStatus> statuses = mockFs.listLocatedStatus(dir1);\r\n    boolean verified = false;\r\n    while (statuses.hasNext()) {\r\n        LocatedFileStatus orig = statuses.next();\r\n        LocatedFileStatus shrink = (LocatedFileStatus) FileInputFormat.shrinkStatus(orig);\r\n        Assert.assertTrue(orig.equals(shrink));\r\n        if (shrink.getBlockLocations() != null) {\r\n            Assert.assertEquals(orig.getBlockLocations().length, shrink.getBlockLocations().length);\r\n            for (int i = 0; i < shrink.getBlockLocations().length; i++) {\r\n                verified = true;\r\n                BlockLocation location = shrink.getBlockLocations()[i];\r\n                BlockLocation actual = orig.getBlockLocations()[i];\r\n                Assert.assertNotNull(((HdfsBlockLocation) actual).getLocatedBlock());\r\n                Assert.assertEquals(BlockLocation.class.getName(), location.getClass().getName());\r\n                Assert.assertArrayEquals(actual.getHosts(), location.getHosts());\r\n                Assert.assertArrayEquals(actual.getCachedHosts(), location.getCachedHosts());\r\n                Assert.assertArrayEquals(actual.getStorageIds(), location.getStorageIds());\r\n                Assert.assertArrayEquals(actual.getStorageTypes(), location.getStorageTypes());\r\n                Assert.assertArrayEquals(actual.getTopologyPaths(), location.getTopologyPaths());\r\n                Assert.assertArrayEquals(actual.getNames(), location.getNames());\r\n                Assert.assertEquals(actual.getLength(), location.getLength());\r\n                Assert.assertEquals(actual.getOffset(), location.getOffset());\r\n                Assert.assertEquals(actual.isCorrupt(), location.isCorrupt());\r\n            }\r\n        } else {\r\n            Assert.assertTrue(orig.getBlockLocations() == null);\r\n        }\r\n    }\r\n    Assert.assertTrue(verified);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "configureTestSimple",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "List<Path> configureTestSimple(Configuration conf, FileSystem localFs) throws IOException\n{\r\n    Path base1 = new Path(TEST_ROOT_DIR, \"input1\");\r\n    Path base2 = new Path(TEST_ROOT_DIR, \"input2\");\r\n    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, localFs.makeQualified(base1) + \",\" + localFs.makeQualified(base2));\r\n    localFs.mkdirs(base1);\r\n    localFs.mkdirs(base2);\r\n    Path in1File1 = new Path(base1, \"file1\");\r\n    Path in1File2 = new Path(base1, \"file2\");\r\n    localFs.createNewFile(in1File1);\r\n    localFs.createNewFile(in1File2);\r\n    Path in2File1 = new Path(base2, \"file1\");\r\n    Path in2File2 = new Path(base2, \"file2\");\r\n    localFs.createNewFile(in2File1);\r\n    localFs.createNewFile(in2File2);\r\n    List<Path> expectedPaths = Lists.newArrayList(in1File1, in1File2, in2File1, in2File2);\r\n    return expectedPaths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "configureTestNestedRecursive",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "List<Path> configureTestNestedRecursive(Configuration conf, FileSystem localFs) throws IOException\n{\r\n    Path base1 = new Path(TEST_ROOT_DIR, \"input1\");\r\n    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, localFs.makeQualified(base1).toString());\r\n    conf.setBoolean(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR_RECURSIVE, true);\r\n    localFs.mkdirs(base1);\r\n    Path inDir1 = new Path(base1, \"dir1\");\r\n    Path inDir2 = new Path(base1, \"dir2\");\r\n    Path inFile1 = new Path(base1, \"file1\");\r\n    Path dir1File1 = new Path(inDir1, \"file1\");\r\n    Path dir1File2 = new Path(inDir1, \"file2\");\r\n    Path dir2File1 = new Path(inDir2, \"file1\");\r\n    Path dir2File2 = new Path(inDir2, \"file2\");\r\n    localFs.mkdirs(inDir1);\r\n    localFs.mkdirs(inDir2);\r\n    localFs.createNewFile(inFile1);\r\n    localFs.createNewFile(dir1File1);\r\n    localFs.createNewFile(dir1File2);\r\n    localFs.createNewFile(dir2File1);\r\n    localFs.createNewFile(dir2File2);\r\n    List<Path> expectedPaths = Lists.newArrayList(inFile1, dir1File1, dir1File2, dir2File1, dir2File2);\r\n    return expectedPaths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "configureTestNestedNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "List<Path> configureTestNestedNonRecursive(Configuration conf, FileSystem localFs) throws IOException\n{\r\n    Path base1 = new Path(TEST_ROOT_DIR, \"input1\");\r\n    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, localFs.makeQualified(base1).toString());\r\n    conf.setBoolean(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR_RECURSIVE, false);\r\n    localFs.mkdirs(base1);\r\n    Path inDir1 = new Path(base1, \"dir1\");\r\n    Path inDir2 = new Path(base1, \"dir2\");\r\n    Path inFile1 = new Path(base1, \"file1\");\r\n    Path dir1File1 = new Path(inDir1, \"file1\");\r\n    Path dir1File2 = new Path(inDir1, \"file2\");\r\n    Path dir2File1 = new Path(inDir2, \"file1\");\r\n    Path dir2File2 = new Path(inDir2, \"file2\");\r\n    localFs.mkdirs(inDir1);\r\n    localFs.mkdirs(inDir2);\r\n    localFs.createNewFile(inFile1);\r\n    localFs.createNewFile(dir1File1);\r\n    localFs.createNewFile(dir1File2);\r\n    localFs.createNewFile(dir2File1);\r\n    localFs.createNewFile(dir2File2);\r\n    List<Path> expectedPaths = Lists.newArrayList(inFile1, inDir1, inDir2);\r\n    return expectedPaths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "configureTestErrorOnNonExistantDir",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "List<Path> configureTestErrorOnNonExistantDir(Configuration conf, FileSystem localFs) throws IOException\n{\r\n    Path base1 = new Path(TEST_ROOT_DIR, \"input1\");\r\n    Path base2 = new Path(TEST_ROOT_DIR, \"input2\");\r\n    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, localFs.makeQualified(base1) + \",\" + localFs.makeQualified(base2));\r\n    conf.setBoolean(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR_RECURSIVE, true);\r\n    localFs.mkdirs(base1);\r\n    Path inFile1 = new Path(base1, \"file1\");\r\n    Path inFile2 = new Path(base1, \"file2\");\r\n    localFs.createNewFile(inFile1);\r\n    localFs.createNewFile(inFile2);\r\n    List<Path> expectedPaths = Lists.newArrayList();\r\n    return expectedPaths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "verifyFileStatuses",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void verifyFileStatuses(List<Path> expectedPaths, List<FileStatus> fetchedStatuses, final FileSystem localFs)\n{\r\n    Assert.assertEquals(expectedPaths.size(), fetchedStatuses.size());\r\n    Iterable<Path> fqExpectedPaths = expectedPaths.stream().map(input -> localFs.makeQualified(input)).collect(Collectors.toList());\r\n    Set<Path> expectedPathSet = Sets.newHashSet(fqExpectedPaths);\r\n    for (FileStatus fileStatus : fetchedStatuses) {\r\n        if (!expectedPathSet.remove(localFs.makeQualified(fileStatus.getPath()))) {\r\n            Assert.fail(\"Found extra fetched status: \" + fileStatus.getPath());\r\n        }\r\n    }\r\n    Assert.assertEquals(\"Not all expectedPaths matched: \" + expectedPathSet.toString(), 0, expectedPathSet.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "verifySplits",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifySplits(List<String> expected, List<InputSplit> splits)\n{\r\n    Iterable<String> pathsFromSplits = splits.stream().map(input -> ((FileSplit) input).getPath().toString()).collect(Collectors.toList());\r\n    Set<String> expectedSet = Sets.newHashSet(expected);\r\n    for (String splitPathString : pathsFromSplits) {\r\n        if (!expectedSet.remove(splitPathString)) {\r\n            Assert.fail(\"Found extra split: \" + splitPathString);\r\n        }\r\n    }\r\n    Assert.assertEquals(\"Not all expectedPaths matched: \" + expectedSet.toString(), 0, expectedSet.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(\"fs.test.impl.disable.cache\", \"true\");\r\n    conf.setClass(\"fs.test.impl\", MockFileSystem.class, FileSystem.class);\r\n    conf.set(FileInputFormat.INPUT_DIR, \"test:///a1\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    conf = new Configuration();\r\n    fs = FileSystem.get(conf);\r\n    firstCacheFile = new Path(TEST_VISIBILITY_PARENT_DIR, FIRST_CACHE_FILE);\r\n    secondCacheFile = new Path(TEST_VISIBILITY_CHILD_DIR, SECOND_CACHE_FILE);\r\n    createTempFile(firstCacheFile, conf);\r\n    createTempFile(secondCacheFile, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws IOException\n{\r\n    if (fs.delete(TEST_ROOT_DIR, true)) {\r\n        LOG.warn(\"Failed to delete test root dir and its content under \" + TEST_ROOT_DIR);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "testDetermineTimestamps",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testDetermineTimestamps() throws IOException\n{\r\n    Job job = Job.getInstance(conf);\r\n    job.addCacheFile(firstCacheFile.toUri());\r\n    job.addCacheFile(secondCacheFile.toUri());\r\n    Configuration jobConf = job.getConfiguration();\r\n    Map<URI, FileStatus> statCache = new HashMap<>();\r\n    ClientDistributedCacheManager.determineTimestamps(jobConf, statCache);\r\n    FileStatus firstStatus = statCache.get(firstCacheFile.toUri());\r\n    FileStatus secondStatus = statCache.get(secondCacheFile.toUri());\r\n    assertNotNull(firstCacheFile + \" was not found in the stats cache\", firstStatus);\r\n    assertNotNull(secondCacheFile + \" was not found in the stats cache\", secondStatus);\r\n    assertEquals(\"Missing/extra entries found in the stats cache\", 2, statCache.size());\r\n    String expected = firstStatus.getModificationTime() + \",\" + secondStatus.getModificationTime();\r\n    assertEquals(expected, jobConf.get(MRJobConfig.CACHE_FILE_TIMESTAMPS));\r\n    job = Job.getInstance(conf);\r\n    job.addCacheFile(new Path(TEST_VISIBILITY_CHILD_DIR, \"*\").toUri());\r\n    jobConf = job.getConfiguration();\r\n    statCache.clear();\r\n    ClientDistributedCacheManager.determineTimestamps(jobConf, statCache);\r\n    FileStatus thirdStatus = statCache.get(TEST_VISIBILITY_CHILD_DIR.toUri());\r\n    assertEquals(\"Missing/extra entries found in the stats cache\", 1, statCache.size());\r\n    assertNotNull(TEST_VISIBILITY_CHILD_DIR + \" was not found in the stats cache\", thirdStatus);\r\n    expected = Long.toString(thirdStatus.getModificationTime());\r\n    assertEquals(\"Incorrect timestamp for \" + TEST_VISIBILITY_CHILD_DIR, expected, jobConf.get(MRJobConfig.CACHE_FILE_TIMESTAMPS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "testDetermineCacheVisibilities",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "void testDetermineCacheVisibilities() throws IOException\n{\r\n    fs.setPermission(TEST_VISIBILITY_PARENT_DIR, new FsPermission((short) 00777));\r\n    fs.setPermission(TEST_VISIBILITY_CHILD_DIR, new FsPermission((short) 00777));\r\n    fs.setWorkingDirectory(TEST_VISIBILITY_CHILD_DIR);\r\n    Job job = Job.getInstance(conf);\r\n    Path relativePath = new Path(SECOND_CACHE_FILE);\r\n    Path wildcardPath = new Path(\"*\");\r\n    Map<URI, FileStatus> statCache = new HashMap<>();\r\n    Configuration jobConf;\r\n    job.addCacheFile(firstCacheFile.toUri());\r\n    job.addCacheFile(relativePath.toUri());\r\n    jobConf = job.getConfiguration();\r\n    assumeTrue(TEST_VISIBILITY_PARENT_DIR + \" is not public\", ClientDistributedCacheManager.isPublic(jobConf, TEST_VISIBILITY_PARENT_DIR.toUri(), statCache));\r\n    ClientDistributedCacheManager.determineCacheVisibilities(jobConf, statCache);\r\n    assertEquals(\"The file paths were not found to be publicly visible \" + \"even though the full path is publicly accessible\", \"true,true\", jobConf.get(MRJobConfig.CACHE_FILE_VISIBILITIES));\r\n    checkCacheEntries(statCache, null, firstCacheFile, relativePath);\r\n    job = Job.getInstance(conf);\r\n    job.addCacheFile(wildcardPath.toUri());\r\n    jobConf = job.getConfiguration();\r\n    statCache.clear();\r\n    ClientDistributedCacheManager.determineCacheVisibilities(jobConf, statCache);\r\n    assertEquals(\"The file path was not found to be publicly visible \" + \"even though the full path is publicly accessible\", \"true\", jobConf.get(MRJobConfig.CACHE_FILE_VISIBILITIES));\r\n    checkCacheEntries(statCache, null, wildcardPath.getParent());\r\n    Path qualifiedParent = fs.makeQualified(TEST_VISIBILITY_PARENT_DIR);\r\n    fs.setPermission(TEST_VISIBILITY_PARENT_DIR, new FsPermission((short) 00700));\r\n    job = Job.getInstance(conf);\r\n    job.addCacheFile(firstCacheFile.toUri());\r\n    job.addCacheFile(relativePath.toUri());\r\n    jobConf = job.getConfiguration();\r\n    statCache.clear();\r\n    ClientDistributedCacheManager.determineCacheVisibilities(jobConf, statCache);\r\n    assertEquals(\"The file paths were found to be publicly visible \" + \"even though the parent directory is not publicly accessible\", \"false,false\", jobConf.get(MRJobConfig.CACHE_FILE_VISIBILITIES));\r\n    checkCacheEntries(statCache, qualifiedParent, firstCacheFile, relativePath);\r\n    job = Job.getInstance(conf);\r\n    job.addCacheFile(wildcardPath.toUri());\r\n    jobConf = job.getConfiguration();\r\n    statCache.clear();\r\n    ClientDistributedCacheManager.determineCacheVisibilities(jobConf, statCache);\r\n    assertEquals(\"The file path was found to be publicly visible \" + \"even though the parent directory is not publicly accessible\", \"false\", jobConf.get(MRJobConfig.CACHE_FILE_VISIBILITIES));\r\n    checkCacheEntries(statCache, qualifiedParent, wildcardPath.getParent());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "checkCacheEntries",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void checkCacheEntries(Map<URI, FileStatus> statCache, Path top, Path... paths)\n{\r\n    Set<URI> expected = new HashSet<>();\r\n    for (Path path : paths) {\r\n        Path p = fs.makeQualified(path);\r\n        while (!p.isRoot() && !p.equals(top)) {\r\n            expected.add(p.toUri());\r\n            p = p.getParent();\r\n        }\r\n        expected.add(p.toUri());\r\n    }\r\n    Set<URI> uris = statCache.keySet();\r\n    Set<URI> missing = new HashSet<>(uris);\r\n    Set<URI> extra = new HashSet<>(expected);\r\n    missing.removeAll(expected);\r\n    extra.removeAll(uris);\r\n    assertTrue(\"File status cache does not contain an entries for \" + missing, missing.isEmpty());\r\n    assertTrue(\"File status cache contains extra extries: \" + extra, extra.isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "createTempFile",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createTempFile(Path p, Configuration conf) throws IOException\n{\r\n    SequenceFile.Writer writer = null;\r\n    try {\r\n        writer = SequenceFile.createWriter(fs, conf, p, Text.class, Text.class, CompressionType.NONE);\r\n        writer.append(new Text(\"text\"), new Text(\"moretext\"));\r\n    } catch (Exception e) {\r\n        throw new IOException(e.getLocalizedMessage());\r\n    } finally {\r\n        if (writer != null) {\r\n            writer.close();\r\n        }\r\n        writer = null;\r\n    }\r\n    LOG.info(\"created: \" + p);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "testFSCheckpointIDSerialization",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testFSCheckpointIDSerialization() throws IOException\n{\r\n    Path inpath = new Path(\"/tmp/blah\");\r\n    FSCheckpointID cidin = new FSCheckpointID(inpath);\r\n    DataOutputBuffer out = new DataOutputBuffer();\r\n    cidin.write(out);\r\n    out.close();\r\n    FSCheckpointID cidout = new FSCheckpointID(null);\r\n    DataInputBuffer in = new DataInputBuffer();\r\n    in.reset(out.getData(), 0, out.getLength());\r\n    cidout.readFields(in);\r\n    in.close();\r\n    assert cidin.equals(cidout);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "cleanupOutputDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanupOutputDir() throws IOException\n{\r\n    if (outputDir != null) {\r\n        getFileSystem().delete(outputDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "suitename",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String suitename()\n{\r\n    return \"TestManifestCommitProtocolLocalFS\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "log",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Logger log()\n{\r\n    return LOG;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getMethodName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getMethodName()\n{\r\n    return suitename() + \"-\" + super.getMethodName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    outputDir = path(getMethodName());\r\n    cleanupOutputDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "teardown",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    describe(\"teardown\");\r\n    Thread.currentThread().setName(\"teardown\");\r\n    for (JobData jobData : abortInTeardown) {\r\n        abortJobQuietly(jobData);\r\n        IOSTATISTICS.aggregate(jobData.committer.getIOStatistics());\r\n    }\r\n    try {\r\n        cleanupOutputDir();\r\n    } catch (IOException e) {\r\n        log().info(\"Exception during cleanup\", e);\r\n    }\r\n    super.teardown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "logAggregateIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void logAggregateIOStatistics()\n{\r\n    LOG.info(\"Final IOStatistics {}\", ioStatisticsToPrettyString(IOSTATISTICS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "abortInTeardown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortInTeardown(JobData jobData)\n{\r\n    abortInTeardown.add(jobData);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    bindCommitter(conf);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "bindCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void bindCommitter(Configuration conf)\n{\r\n    conf.set(COMMITTER_FACTORY_CLASS, MANIFEST_COMMITTER_FACTORY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ManifestCommitter createCommitter(TaskAttemptContext context) throws IOException\n{\r\n    return createCommitter(getOutputDir(), context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ManifestCommitter createCommitter(Path outputPath, TaskAttemptContext context) throws IOException\n{\r\n    return new ManifestCommitter(outputPath, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getOutputDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getOutputDir()\n{\r\n    return outputDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobId()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getAttempt0",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAttempt0()\n{\r\n    return attempt0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getTaskAttempt0",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getTaskAttempt0()\n{\r\n    return taskAttempt0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getAttempt1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAttempt1()\n{\r\n    return attempt1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getTaskAttempt1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getTaskAttempt1()\n{\r\n    return taskAttempt1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "assertCommitterFactoryIsManifestCommitter",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void assertCommitterFactoryIsManifestCommitter(JobContext context, Path output)\n{\r\n    final Configuration conf = context.getConfiguration();\r\n    assertConfigurationUsesManifestCommitter(conf);\r\n    final String factoryName = conf.get(COMMITTER_FACTORY_CLASS, \"\");\r\n    final PathOutputCommitterFactory factory = PathOutputCommitterFactory.getCommitterFactory(output, conf);\r\n    Assertions.assertThat(factory).describedAs(\"Committer for output path %s\" + \" and factory name \\\"%s\\\"\", output, factoryName).isInstanceOf(ManifestCommitterFactory.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "assertConfigurationUsesManifestCommitter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertConfigurationUsesManifestCommitter(Configuration conf)\n{\r\n    final String factoryName = conf.get(COMMITTER_FACTORY_CLASS, null);\r\n    Assertions.assertThat(factoryName).describedAs(\"Value of %s\", COMMITTER_FACTORY_CLASS).isEqualTo(MANIFEST_COMMITTER_FACTORY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "writeTextOutput",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Path writeTextOutput(TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    describe(\"write output\");\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Writing Text output for task %s\", context.getTaskAttemptID())) {\r\n        TextOutputForTests.LoggingLineRecordWriter<Writable, Object> writer = new TextOutputForTests<Writable, Object>().getRecordWriter(context);\r\n        writeOutput(writer, context);\r\n        return writer.getDest();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "writeOutput",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void writeOutput(RecordWriter<Writable, Object> writer, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    NullWritable nullWritable = NullWritable.get();\r\n    try (ManifestCommitterTestSupport.CloseWriter<Writable, Object> cw = new ManifestCommitterTestSupport.CloseWriter<>(writer, context)) {\r\n        writer.write(KEY_1, VAL_1);\r\n        writer.write(null, nullWritable);\r\n        writer.write(null, VAL_1);\r\n        writer.write(nullWritable, VAL_2);\r\n        writer.write(KEY_2, nullWritable);\r\n        writer.write(KEY_1, null);\r\n        writer.write(null, null);\r\n        writer.write(KEY_2, VAL_2);\r\n        writer.close(context);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "writeMapFileOutput",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void writeMapFileOutput(RecordWriter<WritableComparable<?>, Writable> writer, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    describe(\"\\nWrite map output\");\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Writing Text output for task %s\", context.getTaskAttemptID());\r\n        ManifestCommitterTestSupport.CloseWriter<WritableComparable<?>, Writable> cw = new ManifestCommitterTestSupport.CloseWriter<>(writer, context)) {\r\n        for (int i = 0; i < 10; ++i) {\r\n            Text val = ((i & 1) == 1) ? VAL_1 : VAL_2;\r\n            writer.write(new LongWritable(i), val);\r\n        }\r\n        LOG.debug(\"Closing writer {}\", writer);\r\n        writer.close(context);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "newJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job newJob() throws IOException\n{\r\n    return newJob(outputDir, getConfiguration(), attempt0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "newJob",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Job newJob(Path dir, Configuration configuration, String taskAttemptId) throws IOException\n{\r\n    Job job = Job.getInstance(configuration);\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttemptId);\r\n    enableManifestCommitter(conf);\r\n    FileOutputFormat.setOutputPath(job, dir);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "startJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobData startJob(boolean writeText) throws IOException, InterruptedException\n{\r\n    return startJob(localCommitterFactory, writeText);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "startJob",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "JobData startJob(CommitterFactory factory, boolean writeText) throws IOException, InterruptedException\n{\r\n    Job job = newJob();\r\n    Configuration conf = job.getConfiguration();\r\n    assertConfigurationUsesManifestCommitter(conf);\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt0);\r\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1);\r\n    JobContext jContext = new JobContextImpl(conf, taskAttempt0.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskAttempt0);\r\n    ManifestCommitter committer = factory.createCommitter(tContext);\r\n    JobData jobData = new JobData(job, jContext, tContext, committer);\r\n    setupJob(jobData);\r\n    abortInTeardown(jobData);\r\n    if (writeText) {\r\n        jobData.writtenTextPath = writeTextOutput(tContext);\r\n    }\r\n    return jobData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setupJob",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setupJob(JobData jobData) throws IOException\n{\r\n    ManifestCommitter committer = jobData.committer;\r\n    JobContext jContext = jobData.jContext;\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    describe(\"\\nsetup job\");\r\n    try (DurationInfo d = new DurationInfo(LOG, \"setup job %s\", jContext.getJobID())) {\r\n        committer.setupJob(jContext);\r\n    }\r\n    setupCommitter(committer, tContext);\r\n    describe(\"setup complete\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setupCommitter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setupCommitter(final ManifestCommitter committer, final TaskAttemptContext tContext) throws IOException\n{\r\n    try (DurationInfo d = new DurationInfo(LOG, \"setup task %s\", tContext.getTaskAttemptID())) {\r\n        committer.setupTask(tContext);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "abortJobQuietly",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortJobQuietly(JobData jobData)\n{\r\n    abortJobQuietly(jobData.committer, jobData.jContext, jobData.tContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "abortJobQuietly",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void abortJobQuietly(ManifestCommitter committer, JobContext jContext, TaskAttemptContext tContext)\n{\r\n    describe(\"\\naborting task\");\r\n    try {\r\n        committer.abortTask(tContext);\r\n    } catch (Exception e) {\r\n        log().warn(\"Exception aborting task:\", e);\r\n    }\r\n    describe(\"\\naborting job\");\r\n    try {\r\n        committer.abortJob(jContext, JobStatus.State.KILLED);\r\n    } catch (Exception e) {\r\n        log().warn(\"Exception aborting job\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "commitTaskAndJob",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void commitTaskAndJob(ManifestCommitter committer, JobContext jContext, TaskAttemptContext tContext) throws IOException\n{\r\n    try (DurationInfo d = new DurationInfo(LOG, \"committing Job %s\", jContext.getJobID())) {\r\n        describe(\"\\ncommitting task\");\r\n        committer.commitTask(tContext);\r\n        describe(\"\\ncommitting job\");\r\n        committer.commitJob(jContext);\r\n        describe(\"commit complete\\n\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "executeWork",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void executeWork(String name, ActionToTest action) throws Exception\n{\r\n    executeWork(name, startJob(false), action);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "executeWork",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void executeWork(String name, JobData jobData, ActionToTest action) throws Exception\n{\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Executing %s\", name)) {\r\n        action.exec(jobData.job, jobData.jContext, jobData.tContext, jobData.committer);\r\n    } finally {\r\n        abortJobQuietly(jobData);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "loadManifest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskManifest loadManifest(Path path) throws IOException\n{\r\n    return TaskManifest.load(getFileSystem(), path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testRecoveryAndCleanup",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testRecoveryAndCleanup() throws Exception\n{\r\n    describe(\"Test (unsupported) task recovery.\");\r\n    JobData jobData = startJob(true);\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    ManifestCommitter committer = jobData.committer;\r\n    Assertions.assertThat(committer.getWorkPath()).as(\"null workPath in committer \" + committer).isNotNull();\r\n    Assertions.assertThat(committer.getOutputPath()).as(\"null outputPath in committer \" + committer).isNotNull();\r\n    commitTask(committer, tContext);\r\n    final TaskManifest manifest = loadManifest(committer.getTaskManifestPath(tContext));\r\n    LOG.info(\"Manifest {}\", manifest);\r\n    Configuration conf2 = jobData.job.getConfiguration();\r\n    conf2.set(MRJobConfig.TASK_ATTEMPT_ID, attempt0);\r\n    conf2.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 2);\r\n    JobContext jContext2 = new JobContextImpl(conf2, taskAttempt0.getJobID());\r\n    TaskAttemptContext tContext2 = new TaskAttemptContextImpl(conf2, taskAttempt0);\r\n    ManifestCommitter committer2 = createCommitter(tContext2);\r\n    committer2.setupJob(tContext2);\r\n    Assertions.assertThat(committer2.isRecoverySupported()).as(\"recoverySupported in \" + committer2).isFalse();\r\n    intercept(IOException.class, \"recover\", () -> committer2.recoverTask(tContext2));\r\n    describe(\"aborting task attempt 2; expect nothing to clean up\");\r\n    committer2.abortTask(tContext2);\r\n    describe(\"Aborting job 2; expect pending commits to be aborted\");\r\n    committer2.abortJob(jContext2, JobStatus.State.KILLED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "assertTaskAttemptPathDoesNotExist",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertTaskAttemptPathDoesNotExist(ManifestCommitter committer, TaskAttemptContext context) throws IOException\n{\r\n    Path attemptPath = committer.getTaskAttemptPath(context);\r\n    ContractTestUtils.assertPathDoesNotExist(attemptPath.getFileSystem(context.getConfiguration()), \"task attempt dir\", attemptPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "assertJobAttemptPathDoesNotExist",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertJobAttemptPathDoesNotExist(ManifestCommitter committer, JobContext context) throws IOException\n{\r\n    Path attemptPath = committer.getJobAttemptPath(context);\r\n    ContractTestUtils.assertPathDoesNotExist(attemptPath.getFileSystem(context.getConfiguration()), \"job attempt dir\", attemptPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "validateContent",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "ManifestSuccessData validateContent(Path dir, boolean expectSuccessMarker, String expectedJobId) throws Exception\n{\r\n    lsR(getFileSystem(), dir, true);\r\n    ManifestSuccessData successData;\r\n    if (expectSuccessMarker) {\r\n        successData = verifySuccessMarker(dir, expectedJobId);\r\n    } else {\r\n        successData = null;\r\n    }\r\n    Path expectedFile = getPart0000(dir);\r\n    log().debug(\"Validating content in {}\", expectedFile);\r\n    StringBuilder expectedOutput = new StringBuilder();\r\n    expectedOutput.append(KEY_1).append('\\t').append(VAL_1).append(\"\\n\");\r\n    expectedOutput.append(VAL_1).append(\"\\n\");\r\n    expectedOutput.append(VAL_2).append(\"\\n\");\r\n    expectedOutput.append(KEY_2).append(\"\\n\");\r\n    expectedOutput.append(KEY_1).append(\"\\n\");\r\n    expectedOutput.append(KEY_2).append('\\t').append(VAL_2).append(\"\\n\");\r\n    String output = readFile(expectedFile);\r\n    Assertions.assertThat(output).describedAs(\"Content of %s\", expectedFile).isEqualTo(expectedOutput.toString());\r\n    return successData;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getPart0000",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path getPart0000(final Path dir) throws Exception\n{\r\n    final FileSystem fs = dir.getFileSystem(getConfiguration());\r\n    FileStatus[] statuses = fs.listStatus(dir, path -> path.getName().startsWith(PART_00000));\r\n    if (statuses.length != 1) {\r\n        ContractTestUtils.assertPathExists(fs, \"Output file\", new Path(dir, PART_00000));\r\n    }\r\n    return statuses[0].getPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "validateMapFileOutputContent",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void validateMapFileOutputContent(FileSystem fs, Path dir) throws Exception\n{\r\n    assertPathExists(\"Map output\", dir);\r\n    Path expectedMapDir = getPart0000(dir);\r\n    assertPathExists(\"Map output\", expectedMapDir);\r\n    assertIsDirectory(expectedMapDir);\r\n    FileStatus[] files = fs.listStatus(expectedMapDir);\r\n    Assertions.assertThat(files).as(\"No files found in \" + expectedMapDir).isNotEmpty();\r\n    assertPathExists(\"index file in \" + expectedMapDir, new Path(expectedMapDir, MapFile.INDEX_FILE_NAME));\r\n    assertPathExists(\"data file in \" + expectedMapDir, new Path(expectedMapDir, MapFile.DATA_FILE_NAME));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testCommitLifecycle",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 36,
  "sourceCodeText" : "void testCommitLifecycle() throws Exception\n{\r\n    describe(\"Full test of the expected lifecycle:\\n\" + \" start job, task, write, commit task, commit job.\\n\" + \"Verify:\\n\" + \"* no files are visible after task commit\\n\" + \"* the expected file is visible after job commit\\n\");\r\n    JobData jobData = startJob(false);\r\n    JobContext jContext = jobData.jContext;\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    ManifestCommitter committer = jobData.committer;\r\n    assertCommitterFactoryIsManifestCommitter(tContext, tContext.getWorkingDirectory());\r\n    validateTaskAttemptWorkingDirectory(committer, tContext);\r\n    describe(\"1. Writing output\");\r\n    final Path textOutputPath = writeTextOutput(tContext);\r\n    describe(\"Output written to %s\", textOutputPath);\r\n    describe(\"2. Committing task\");\r\n    Assertions.assertThat(committer.needsTaskCommit(tContext)).as(\"No files to commit were found by \" + committer).isTrue();\r\n    commitTask(committer, tContext);\r\n    final TaskManifest taskManifest = requireNonNull(committer.getTaskAttemptCommittedManifest(), \"committerTaskManifest\");\r\n    final String manifestJSON = taskManifest.toJson();\r\n    LOG.info(\"Task manifest {}\", manifestJSON);\r\n    int filesCreated = 1;\r\n    Assertions.assertThat(taskManifest.getFilesToCommit()).describedAs(\"Files to commit in task manifest %s\", manifestJSON).hasSize(filesCreated);\r\n    Assertions.assertThat(taskManifest.getDestDirectories()).describedAs(\"Directories to create in task manifest %s\", manifestJSON).isEmpty();\r\n    try {\r\n        RemoteIterators.foreach(getFileSystem().listFiles(outputDir, false), (status) -> Assertions.assertThat(status.getPath().toString()).as(\"task committed file to dest :\" + status).contains(\"part\"));\r\n    } catch (FileNotFoundException ignored) {\r\n        log().info(\"Outdir {} is not created by task commit phase \", outputDir);\r\n    }\r\n    describe(\"3. Committing job\");\r\n    commitJob(committer, jContext);\r\n    describe(\"4. Validating content\");\r\n    String jobUniqueId = jobData.jobId();\r\n    ManifestSuccessData successData = validateContent(outputDir, true, jobUniqueId);\r\n    Assertions.assertThat(successData.getDiagnostics()).describedAs(\"Stage entry in SUCCESS\").containsEntry(STAGE, OP_STAGE_JOB_COMMIT);\r\n    IOStatisticsSnapshot jobStats = successData.getIOStatistics();\r\n    verifyStatisticCounterValue(jobStats, OP_LOAD_MANIFEST, 1);\r\n    FileStatus st = getFileSystem().getFileStatus(getPart0000(outputDir));\r\n    verifyStatisticCounterValue(jobStats, COMMITTER_FILES_COMMITTED_COUNT, filesCreated);\r\n    verifyStatisticCounterValue(jobStats, COMMITTER_BYTES_COMMITTED_COUNT, st.getLen());\r\n    ManifestSuccessData report = loadReport(jobUniqueId, true);\r\n    Map<String, String> diag = report.getDiagnostics();\r\n    Assertions.assertThat(diag).describedAs(\"Stage entry in report\").containsEntry(STAGE, OP_STAGE_JOB_COMMIT);\r\n    IOStatisticsSnapshot reportStats = report.getIOStatistics();\r\n    verifyStatisticCounterValue(reportStats, OP_LOAD_MANIFEST, 1);\r\n    verifyStatisticCounterValue(reportStats, OP_STAGE_JOB_COMMIT, 1);\r\n    verifyStatisticCounterValue(reportStats, COMMITTER_FILES_COMMITTED_COUNT, filesCreated);\r\n    verifyStatisticCounterValue(reportStats, COMMITTER_BYTES_COMMITTED_COUNT, st.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "loadReport",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ManifestSuccessData loadReport(String jobUniqueId, boolean expectSuccess) throws IOException\n{\r\n    File file = new File(getReportDir(), createJobSummaryFilename(jobUniqueId));\r\n    ContractTestUtils.assertIsFile(FileSystem.getLocal(getConfiguration()), new Path(file.toURI()));\r\n    ManifestSuccessData report = ManifestSuccessData.serializer().load(file);\r\n    LOG.info(\"Report for job {}:\\n{}\", jobUniqueId, report.toJson());\r\n    Assertions.assertThat(report.getSuccess()).describedAs(\"success flag in report\").isEqualTo(expectSuccess);\r\n    return report;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testCommitterWithDuplicatedCommit",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testCommitterWithDuplicatedCommit() throws Exception\n{\r\n    describe(\"Call a task then job commit twice;\" + \"expect the second task commit to fail.\");\r\n    JobData jobData = startJob(true);\r\n    JobContext jContext = jobData.jContext;\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    ManifestCommitter committer = jobData.committer;\r\n    describe(\"committing task\");\r\n    committer.commitTask(tContext);\r\n    committer.commitTask(tContext);\r\n    describe(\"committing job\");\r\n    committer.commitJob(jContext);\r\n    describe(\"commit complete\\n\");\r\n    describe(\"cleanup\");\r\n    committer.cleanupJob(jContext);\r\n    validateContent(outputDir, shouldExpectSuccessMarker(), committer.getJobUniqueId());\r\n    describe(\"Attempting commit of the same task after job commit -expecting failure\");\r\n    expectFNFEonTaskCommit(committer, tContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testTwoTaskAttemptsCommit",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testTwoTaskAttemptsCommit() throws Exception\n{\r\n    describe(\"Commit two task attempts;\" + \" expect the second attempt to succeed.\");\r\n    JobData jobData = startJob(false);\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    ManifestCommitter committer = jobData.committer;\r\n    describe(\"\\ncommitting task\");\r\n    Path outputTA1 = writeTextOutput(tContext);\r\n    Configuration conf2 = jobData.conf;\r\n    conf2.set(\"mapreduce.output.basename\", \"attempt2\");\r\n    String attempt2 = \"attempt_\" + jobId + \"_m_000000_1\";\r\n    TaskAttemptID ta2 = TaskAttemptID.forName(attempt2);\r\n    TaskAttemptContext tContext2 = new TaskAttemptContextImpl(conf2, ta2);\r\n    ManifestCommitter committer2 = localCommitterFactory.createCommitter(tContext2);\r\n    setupCommitter(committer2, tContext2);\r\n    Assertions.assertThat(committer.getWorkPath()).describedAs(\"Working dir of %s\", committer).isNotEqualTo(committer2.getWorkPath());\r\n    Path outputTA2 = writeTextOutput(tContext2);\r\n    String name1 = outputTA1.getName();\r\n    String name2 = outputTA2.getName();\r\n    Assertions.assertThat(name1).describedAs(\"name of task attempt output %s\", outputTA1).isNotEqualTo(name2);\r\n    committer.commitTask(tContext);\r\n    committer2.commitTask(tContext2);\r\n    committer2.commitJob(tContext);\r\n    FileSystem fs = getFileSystem();\r\n    ManifestSuccessData successData = validateSuccessFile(fs, outputDir, 1, \"\");\r\n    Assertions.assertThat(successData.getFilenames()).describedAs(\"Files committed\").hasSize(1);\r\n    assertPathExists(\"attempt2 output\", new Path(outputDir, name2));\r\n    assertPathDoesNotExist(\"attempt1 output\", new Path(outputDir, name1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "shouldExpectSuccessMarker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldExpectSuccessMarker()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "expectJobCommitToFail",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectJobCommitToFail(JobContext jContext, ManifestCommitter committer) throws Exception\n{\r\n    expectJobCommitFailure(jContext, committer, FileNotFoundException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "expectJobCommitFailure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "E expectJobCommitFailure(JobContext jContext, ManifestCommitter committer, Class<E> clazz) throws Exception\n{\r\n    return intercept(clazz, () -> {\r\n        committer.commitJob(jContext);\r\n        return committer.toString();\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "expectFNFEonTaskCommit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectFNFEonTaskCommit(ManifestCommitter committer, TaskAttemptContext tContext) throws Exception\n{\r\n    intercept(FileNotFoundException.class, () -> {\r\n        committer.commitTask(tContext);\r\n        return committer.toString();\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testCommitterWithNoOutputs",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCommitterWithNoOutputs() throws Exception\n{\r\n    describe(\"Have a task and job with no outputs: expect success\");\r\n    JobData jobData = startJob(localCommitterFactory, false);\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    ManifestCommitter committer = jobData.committer;\r\n    committer.commitTask(tContext);\r\n    Path attemptPath = committer.getTaskAttemptPath(tContext);\r\n    ContractTestUtils.assertPathExists(attemptPath.getFileSystem(tContext.getConfiguration()), \"task attempt dir\", attemptPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testMapFileOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testMapFileOutputCommitter() throws Exception\n{\r\n    describe(\"Test that the committer generates map output into a directory\\n\" + \"starting with the prefix part-\");\r\n    JobData jobData = startJob(false);\r\n    JobContext jContext = jobData.jContext;\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    ManifestCommitter committer = jobData.committer;\r\n    Configuration conf = jobData.conf;\r\n    writeMapFileOutput(new MapFileOutputFormat().getRecordWriter(tContext), tContext);\r\n    commitTaskAndJob(committer, jContext, tContext);\r\n    FileSystem fs = getFileSystem();\r\n    lsR(fs, outputDir, true);\r\n    String ls = ls(outputDir);\r\n    describe(\"\\nvalidating\");\r\n    verifySuccessMarker(outputDir, committer.getJobUniqueId());\r\n    describe(\"validate output of %s\", outputDir);\r\n    validateMapFileOutputContent(fs, outputDir);\r\n    describe(\"listing\");\r\n    FileStatus[] filtered = fs.listStatus(outputDir, HIDDEN_FILE_FILTER);\r\n    Assertions.assertThat(filtered).describedAs(\"listed children under %s\", ls).hasSize(1);\r\n    FileStatus fileStatus = filtered[0];\r\n    Assertions.assertThat(fileStatus.getPath().getName()).as(\"Not the part file: \" + fileStatus).startsWith(PART_00000);\r\n    describe(\"getReaders()\");\r\n    Assertions.assertThat(getReaders(fs, outputDir, conf)).describedAs(\"getReaders() MapFile.Reader entries with shared FS %s %s\", outputDir, ls).hasSize(1);\r\n    describe(\"getReaders(new FS)\");\r\n    FileSystem fs2 = FileSystem.get(outputDir.toUri(), conf);\r\n    Assertions.assertThat(getReaders(fs2, outputDir, conf)).describedAs(\"getReaders(new FS) %s %s\", outputDir, ls).hasSize(1);\r\n    describe(\"MapFileOutputFormat.getReaders\");\r\n    Assertions.assertThat(MapFileOutputFormat.getReaders(outputDir, conf)).describedAs(\"MapFileOutputFormat.getReaders(%s) %s\", outputDir, ls).hasSize(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getReaders",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MapFile.Reader[] getReaders(FileSystem fs, Path dir, Configuration conf) throws IOException\n{\r\n    Path[] names = FileUtil.stat2Paths(fs.listStatus(dir, HIDDEN_FILE_FILTER));\r\n    Arrays.sort(names);\r\n    MapFile.Reader[] parts = new MapFile.Reader[names.length];\r\n    for (int i = 0; i < names.length; i++) {\r\n        parts[i] = new MapFile.Reader(names[i], conf);\r\n    }\r\n    return parts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testAbortTaskNoWorkDone",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAbortTaskNoWorkDone() throws Exception\n{\r\n    executeWork(\"abort task no work\", (job, jContext, tContext, committer) -> committer.abortTask(tContext));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testAbortJobNoWorkDone",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAbortJobNoWorkDone() throws Exception\n{\r\n    executeWork(\"abort task no work\", (job, jContext, tContext, committer) -> committer.abortJob(jContext, JobStatus.State.RUNNING));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testCommitJobButNotTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitJobButNotTask() throws Exception\n{\r\n    executeWork(\"commit a job while a task's work is pending, \" + \"expect task writes to be cancelled.\", (job, jContext, tContext, committer) -> {\r\n        writeTextOutput(tContext);\r\n        createCommitter(tContext).commitJob(tContext);\r\n        assertPart0000DoesNotExist(outputDir);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testAbortTaskThenJob",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testAbortTaskThenJob() throws Exception\n{\r\n    JobData jobData = startJob(true);\r\n    ManifestCommitter committer = jobData.committer;\r\n    committer.abortTask(jobData.tContext);\r\n    intercept(FileNotFoundException.class, \"\", () -> getPart0000(committer.getWorkPath()));\r\n    committer.abortJob(jobData.jContext, JobStatus.State.FAILED);\r\n    assertJobAbortCleanedUp(jobData);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "assertJobAbortCleanedUp",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void assertJobAbortCleanedUp(JobData jobData) throws Exception\n{\r\n    FileSystem fs = getFileSystem();\r\n    try {\r\n        FileStatus[] children = listChildren(fs, outputDir);\r\n        if (children.length != 0) {\r\n            lsR(fs, outputDir, true);\r\n        }\r\n        Assertions.assertThat(children).as(\"Output directory not empty \" + ls(outputDir)).containsExactly(new FileStatus[0]);\r\n    } catch (FileNotFoundException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testFailAbort",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testFailAbort() throws Exception\n{\r\n    describe(\"Abort the task, then job (failed), abort the job again\");\r\n    JobData jobData = startJob(true);\r\n    JobContext jContext = jobData.jContext;\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    ManifestCommitter committer = jobData.committer;\r\n    committer.abortTask(tContext);\r\n    committer.getJobAttemptPath(jContext);\r\n    committer.getTaskAttemptPath(tContext);\r\n    assertPart0000DoesNotExist(outputDir);\r\n    assertSuccessMarkerDoesNotExist(outputDir);\r\n    describe(\"Aborting job into %s\", outputDir);\r\n    committer.abortJob(jContext, JobStatus.State.FAILED);\r\n    assertTaskAttemptPathDoesNotExist(committer, tContext);\r\n    assertJobAttemptPathDoesNotExist(committer, jContext);\r\n    ManifestSuccessData report = loadReport(jobData.jobId(), false);\r\n    Map<String, String> diag = report.getDiagnostics();\r\n    Assertions.assertThat(diag).describedAs(\"Stage entry in report\").containsEntry(STAGE, OP_STAGE_JOB_ABORT);\r\n    IOStatisticsSnapshot reportStats = report.getIOStatistics();\r\n    verifyStatisticCounterValue(reportStats, OP_STAGE_JOB_ABORT, 1);\r\n    committer.abortJob(jContext, JobStatus.State.FAILED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "assertSuccessMarkerDoesNotExist",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertSuccessMarkerDoesNotExist(Path dir) throws IOException\n{\r\n    assertPathDoesNotExist(\"Success marker\", new Path(dir, SUCCESS_MARKER));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "assertPart0000DoesNotExist",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertPart0000DoesNotExist(Path dir) throws Exception\n{\r\n    intercept(FileNotFoundException.class, () -> getPart0000(dir));\r\n    assertPathDoesNotExist(\"expected output file\", new Path(dir, PART_00000));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testAbortJobNotTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAbortJobNotTask() throws Exception\n{\r\n    executeWork(\"abort task no work\", (job, jContext, tContext, committer) -> {\r\n        writeTextOutput(tContext);\r\n        committer.abortJob(jContext, JobStatus.State.RUNNING);\r\n        assertTaskAttemptPathDoesNotExist(committer, tContext);\r\n        assertJobAttemptPathDoesNotExist(committer, jContext);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testConcurrentCommitTaskWithSubDir",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testConcurrentCommitTaskWithSubDir() throws Exception\n{\r\n    Job job = newJob();\r\n    FileOutputFormat.setOutputPath(job, outputDir);\r\n    final Configuration conf = job.getConfiguration();\r\n    final JobContext jContext = new JobContextImpl(conf, taskAttempt0.getJobID());\r\n    ManifestCommitter amCommitter = createCommitter(new TaskAttemptContextImpl(conf, taskAttempt0));\r\n    amCommitter.setupJob(jContext);\r\n    final TaskAttemptContext[] taCtx = new TaskAttemptContextImpl[2];\r\n    taCtx[0] = new TaskAttemptContextImpl(conf, taskAttempt0);\r\n    taCtx[1] = new TaskAttemptContextImpl(conf, taskAttempt1);\r\n    final TextOutputFormat<Writable, Object>[] tof = new TextOutputForTests[2];\r\n    for (int i = 0; i < tof.length; i++) {\r\n        tof[i] = new TextOutputForTests<Writable, Object>() {\r\n\r\n            @Override\r\n            public Path getDefaultWorkFile(TaskAttemptContext context, String extension) throws IOException {\r\n                final ManifestCommitter foc = (ManifestCommitter) getOutputCommitter(context);\r\n                return new Path(new Path(foc.getWorkPath(), SUB_DIR), getUniqueFile(context, getOutputName(context), extension));\r\n            }\r\n        };\r\n    }\r\n    final ExecutorService executor = HadoopExecutors.newFixedThreadPool(2);\r\n    try {\r\n        for (int i = 0; i < taCtx.length; i++) {\r\n            final int taskIdx = i;\r\n            executor.submit(() -> {\r\n                final OutputCommitter outputCommitter = tof[taskIdx].getOutputCommitter(taCtx[taskIdx]);\r\n                outputCommitter.setupTask(taCtx[taskIdx]);\r\n                writeOutput(tof[taskIdx].getRecordWriter(taCtx[taskIdx]), taCtx[taskIdx]);\r\n                describe(\"Committing Task %d\", taskIdx);\r\n                outputCommitter.commitTask(taCtx[taskIdx]);\r\n                return null;\r\n            });\r\n        }\r\n    } finally {\r\n        executor.shutdown();\r\n        while (!executor.awaitTermination(1, TimeUnit.SECONDS)) {\r\n            log().info(\"Awaiting thread termination!\");\r\n        }\r\n    }\r\n    describe(\"\\nCommitting Job\");\r\n    amCommitter.commitJob(jContext);\r\n    assertPathExists(\"base output directory\", outputDir);\r\n    assertPart0000DoesNotExist(outputDir);\r\n    Path outSubDir = new Path(outputDir, SUB_DIR);\r\n    assertPathDoesNotExist(\"Must not end up with sub_dir/sub_dir\", new Path(outSubDir, SUB_DIR));\r\n    validateContent(outSubDir, false, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testUnsupportedSchema",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUnsupportedSchema() throws Throwable\n{\r\n    intercept(PathIOException.class, () -> new ManifestCommitterFactory().createOutputCommitter(new Path(\"s3a://unsupported/\"), null));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testOutputFormatIntegration",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testOutputFormatIntegration() throws Throwable\n{\r\n    Configuration conf = getConfiguration();\r\n    Job job = newJob();\r\n    assertCommitterFactoryIsManifestCommitter(job, outputDir);\r\n    job.setOutputFormatClass(TextOutputForTests.class);\r\n    conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt0);\r\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1);\r\n    JobContext jContext = new JobContextImpl(conf, taskAttempt0.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskAttempt0);\r\n    TextOutputForTests<IntWritable, IntWritable> outputFormat = (TextOutputForTests<IntWritable, IntWritable>) ReflectionUtils.newInstance(tContext.getOutputFormatClass(), conf);\r\n    ManifestCommitter committer = (ManifestCommitter) outputFormat.getOutputCommitter(tContext);\r\n    JobData jobData = new JobData(job, jContext, tContext, committer);\r\n    setupJob(jobData);\r\n    abortInTeardown(jobData);\r\n    TextOutputForTests.LoggingLineRecordWriter<IntWritable, IntWritable> recordWriter = outputFormat.getRecordWriter(tContext);\r\n    IntWritable iw = new IntWritable(1);\r\n    recordWriter.write(iw, iw);\r\n    long expectedLength = 4;\r\n    Path dest = recordWriter.getDest();\r\n    validateTaskAttemptPathDuringWrite(dest, expectedLength);\r\n    recordWriter.close(tContext);\r\n    validateTaskAttemptPathAfterWrite(dest, expectedLength);\r\n    Assertions.assertThat(committer.needsTaskCommit(tContext)).as(\"Committer does not have data to commit \" + committer).isTrue();\r\n    commitTask(committer, tContext);\r\n    IOStatisticsSnapshot snapshot = new IOStatisticsSnapshot(committer.getIOStatistics());\r\n    String commitsCompleted = COMMITTER_TASKS_COMPLETED_COUNT;\r\n    LOG.info(\"after task commit {}\", ioStatisticsToPrettyString(snapshot));\r\n    verifyStatisticCounterValue(snapshot, commitsCompleted, 1);\r\n    final TaskManifest manifest = loadManifest(committer.getTaskManifestPath(tContext));\r\n    LOG.info(\"Manifest {}\", manifest.toJson());\r\n    commitJob(committer, jContext);\r\n    LOG.info(\"committer iostatistics {}\", ioStatisticsSourceToString(committer));\r\n    ManifestSuccessData successData = verifySuccessMarker(outputDir, committer.getJobUniqueId());\r\n    IOStatisticsSnapshot successStats = successData.getIOStatistics();\r\n    LOG.info(\"loaded statistics {}\", successStats);\r\n    verifyStatisticCounterValue(successStats, commitsCompleted, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testAMWorkflow",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testAMWorkflow() throws Throwable\n{\r\n    describe(\"Create a committer with a null output path & use as an AM\");\r\n    JobData jobData = startJob(true);\r\n    JobContext jContext = jobData.jContext;\r\n    TaskAttemptContext tContext = jobData.tContext;\r\n    TaskAttemptContext newAttempt = new TaskAttemptContextImpl(jContext.getConfiguration(), taskAttempt0);\r\n    Configuration conf = jContext.getConfiguration();\r\n    TextOutputForTests.bind(conf);\r\n    OutputFormat<?, ?> outputFormat = ReflectionUtils.newInstance(newAttempt.getOutputFormatClass(), conf);\r\n    Path outputPath = FileOutputFormat.getOutputPath(newAttempt);\r\n    Assertions.assertThat(outputPath).as(\"null output path in new task attempt\").isNotNull();\r\n    ManifestCommitter committer2 = (ManifestCommitter) outputFormat.getOutputCommitter(newAttempt);\r\n    committer2.abortTask(tContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testParallelJobsToAdjacentPaths",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testParallelJobsToAdjacentPaths() throws Throwable\n{\r\n    describe(\"Run two jobs in parallel, assert they both complete\");\r\n    JobData jobData = startJob(true);\r\n    Job job1 = jobData.job;\r\n    ManifestCommitter committer1 = jobData.committer;\r\n    JobContext jContext1 = jobData.jContext;\r\n    TaskAttemptContext tContext1 = jobData.tContext;\r\n    String jobId2 = randomJobId();\r\n    String attempt20 = \"attempt_\" + jobId2 + \"_m_000000_0\";\r\n    TaskAttemptID taskAttempt20 = TaskAttemptID.forName(attempt20);\r\n    String attempt21 = \"attempt_\" + jobId2 + \"_m_000001_0\";\r\n    TaskAttemptID taskAttempt21 = TaskAttemptID.forName(attempt21);\r\n    Path job1Dest = outputDir;\r\n    Path job2Dest = new Path(getOutputDir().getParent(), getMethodName() + \"job2Dest\");\r\n    Assertions.assertThat(job2Dest).describedAs(\"Job destinations\").isNotEqualTo(job1Dest);\r\n    Job job2 = newJob(job2Dest, unsetUUIDOptions(new JobConf(getConfiguration())), attempt20);\r\n    Configuration conf2 = job2.getConfiguration();\r\n    conf2.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1);\r\n    ManifestCommitter committer2 = null;\r\n    try {\r\n        JobContext jContext2 = new JobContextImpl(conf2, taskAttempt20.getJobID());\r\n        TaskAttemptContext tContext2 = new TaskAttemptContextImpl(conf2, taskAttempt20);\r\n        committer2 = createCommitter(job2Dest, tContext2);\r\n        JobData jobData2 = new JobData(job2, jContext2, tContext2, committer2);\r\n        setupJob(jobData2);\r\n        abortInTeardown(jobData2);\r\n        Assertions.assertThat(committer1.getOutputPath()).describedAs(\"Committer output path of %s and %s\", committer1, committer2).isNotEqualTo(committer2.getOutputPath());\r\n        Assertions.assertThat(committer1.getJobUniqueId()).describedAs(\"JobUnique IDs of %s and %s\", committer1, committer2).isNotEqualTo(committer2.getJobUniqueId());\r\n        writeTextOutput(tContext2);\r\n        commitTask(committer2, tContext2);\r\n        commitTask(committer1, tContext1);\r\n        commitJob(committer1, jContext1);\r\n        getPart0000(job1Dest);\r\n        commitJob(committer2, jContext2);\r\n        getPart0000(job2Dest);\r\n    } finally {\r\n        FileSystem fs = getFileSystem();\r\n        if (committer1 != null) {\r\n            fs.delete(committer1.getOutputPath(), true);\r\n        }\r\n        if (committer2 != null) {\r\n            fs.delete(committer2.getOutputPath(), true);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "unsetUUIDOptions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration unsetUUIDOptions(final Configuration conf)\n{\r\n    conf.unset(SPARK_WRITE_UUID);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "assertJobAttemptPathExists",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertJobAttemptPathExists(final ManifestCommitter committer, final JobContext jobContext) throws IOException\n{\r\n    Path attemptPath = committer.getJobAttemptPath(jobContext);\r\n    ContractTestUtils.assertIsDirectory(attemptPath.getFileSystem(committer.getConf()), attemptPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "validateTaskAttemptPathDuringWrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void validateTaskAttemptPathDuringWrite(Path p, final long expectedLength) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "validateTaskAttemptPathAfterWrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void validateTaskAttemptPathAfterWrite(Path p, final long expectedLength) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "validateTaskAttemptWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void validateTaskAttemptWorkingDirectory(ManifestCommitter committer, TaskAttemptContext context) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "commitTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commitTask(final ManifestCommitter committer, final TaskAttemptContext tContext) throws IOException\n{\r\n    committer.commitTask(tContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "commitJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commitJob(final ManifestCommitter committer, final JobContext jContext) throws IOException\n{\r\n    committer.commitJob(jContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\split",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    Configuration hdfsConf = new HdfsConfiguration();\r\n    hdfsConf.setLong(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, 0);\r\n    String namenodeDir = new File(MiniDFSCluster.getBaseDirectory(), \"name\").getAbsolutePath();\r\n    hdfsConf.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY, namenodeDir);\r\n    hdfsConf.set(DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY, namenodeDir);\r\n    hdfsConf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCKSIZE);\r\n    cluster = new MiniDFSCluster.Builder(hdfsConf).numDataNodes(15).build();\r\n    fs = cluster.getFileSystem();\r\n    fs.enableErasureCodingPolicy(ecPolicy.getName());\r\n    fs.setErasureCodingPolicy(new Path(\"/\"), ecPolicy.getName());\r\n    cluster.waitActive();\r\n    conf = new Configuration();\r\n    submitDir = new Path(\"/\");\r\n    testFile = new Path(\"/testfile\");\r\n    DFSTestUtil.writeFile(fs, testFile, StripedFileTestUtil.generateBytes(BLOCKSIZE));\r\n    conf.set(FileInputFormat.INPUT_DIR, fs.getUri().toString() + testFile.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\split",
  "methodName" : "after",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void after()\n{\r\n    cluster.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\split",
  "methodName" : "testMaxBlockLocationsNewSplitsWithErasureCoding",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testMaxBlockLocationsNewSplitsWithErasureCoding() throws Exception\n{\r\n    Job job = Job.getInstance(conf);\r\n    final FileInputFormat<?, ?> fileInputFormat = new TextInputFormat();\r\n    final List<InputSplit> splits = fileInputFormat.getSplits(job);\r\n    JobSplitWriter.createSplitFiles(submitDir, conf, fs, splits);\r\n    validateSplitMetaInfo();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\split",
  "methodName" : "testMaxBlockLocationsOldSplitsWithErasureCoding",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testMaxBlockLocationsOldSplitsWithErasureCoding() throws Exception\n{\r\n    JobConf jobConf = new JobConf(conf);\r\n    org.apache.hadoop.mapred.TextInputFormat fileInputFormat = new org.apache.hadoop.mapred.TextInputFormat();\r\n    fileInputFormat.configure(jobConf);\r\n    final org.apache.hadoop.mapred.InputSplit[] splits = fileInputFormat.getSplits(jobConf, 1);\r\n    JobSplitWriter.createSplitFiles(submitDir, conf, fs, splits);\r\n    validateSplitMetaInfo();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\split",
  "methodName" : "validateSplitMetaInfo",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void validateSplitMetaInfo() throws IOException\n{\r\n    JobSplit.TaskSplitMetaInfo[] splitInfo = SplitMetaInfoReader.readSplitMetaInfo(new JobID(), fs, conf, submitDir);\r\n    assertEquals(\"Number of splits\", 1, splitInfo.length);\r\n    assertEquals(\"Number of block locations\", 14, splitInfo[0].getLocations().length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testDataDrivenDBInputFormatSplitter",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testDataDrivenDBInputFormatSplitter()\n{\r\n    DataDrivenDBInputFormat<NullDBWritable> format = new DataDrivenDBInputFormat<NullDBWritable>();\r\n    testCommonSplitterTypes(format);\r\n    assertEquals(DateSplitter.class, format.getSplitter(Types.TIMESTAMP).getClass());\r\n    assertEquals(DateSplitter.class, format.getSplitter(Types.DATE).getClass());\r\n    assertEquals(DateSplitter.class, format.getSplitter(Types.TIME).getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testDataDrivenDBInputFormat",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testDataDrivenDBInputFormat() throws Exception\n{\r\n    JobContext jobContext = mock(JobContext.class);\r\n    Configuration configuration = new Configuration();\r\n    configuration.setInt(MRJobConfig.NUM_MAPS, 1);\r\n    when(jobContext.getConfiguration()).thenReturn(configuration);\r\n    DataDrivenDBInputFormat<NullDBWritable> format = new DataDrivenDBInputFormat<NullDBWritable>();\r\n    List<InputSplit> splits = format.getSplits(jobContext);\r\n    assertEquals(1, splits.size());\r\n    DataDrivenDBInputSplit split = (DataDrivenDBInputSplit) splits.get(0);\r\n    assertEquals(\"1=1\", split.getLowerClause());\r\n    assertEquals(\"1=1\", split.getUpperClause());\r\n    configuration.setInt(MRJobConfig.NUM_MAPS, 2);\r\n    DataDrivenDBInputFormat.setBoundingQuery(configuration, \"query\");\r\n    assertEquals(\"query\", configuration.get(DBConfiguration.INPUT_BOUNDING_QUERY));\r\n    Job job = mock(Job.class);\r\n    when(job.getConfiguration()).thenReturn(configuration);\r\n    DataDrivenDBInputFormat.setInput(job, NullDBWritable.class, \"query\", \"Bounding Query\");\r\n    assertEquals(\"Bounding Query\", configuration.get(DBConfiguration.INPUT_BOUNDING_QUERY));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testOracleDataDrivenDBInputFormat",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testOracleDataDrivenDBInputFormat() throws Exception\n{\r\n    OracleDataDrivenDBInputFormat<NullDBWritable> format = new OracleDataDrivenDBInputFormatForTest();\r\n    testCommonSplitterTypes(format);\r\n    assertEquals(OracleDateSplitter.class, format.getSplitter(Types.TIMESTAMP).getClass());\r\n    assertEquals(OracleDateSplitter.class, format.getSplitter(Types.DATE).getClass());\r\n    assertEquals(OracleDateSplitter.class, format.getSplitter(Types.TIME).getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testOracleDBRecordReader",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testOracleDBRecordReader() throws Exception\n{\r\n    DBInputSplit splitter = new DBInputSplit(1, 10);\r\n    Configuration configuration = new Configuration();\r\n    Connection connect = DriverForTest.getConnection();\r\n    DBConfiguration dbConfiguration = new DBConfiguration(configuration);\r\n    dbConfiguration.setInputOrderBy(\"Order\");\r\n    String[] fields = { \"f1\", \"f2\" };\r\n    OracleDBRecordReader<NullDBWritable> recorder = new OracleDBRecordReader<NullDBWritable>(splitter, NullDBWritable.class, configuration, connect, dbConfiguration, \"condition\", fields, \"table\");\r\n    assertEquals(\"SELECT * FROM (SELECT a.*,ROWNUM dbif_rno FROM ( SELECT f1, f2 FROM table WHERE condition ORDER BY Order ) a WHERE rownum <= 10 ) WHERE dbif_rno > 1\", recorder.getSelectQuery());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testCommonSplitterTypes",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCommonSplitterTypes(DataDrivenDBInputFormat<NullDBWritable> format)\n{\r\n    assertEquals(BigDecimalSplitter.class, format.getSplitter(Types.DECIMAL).getClass());\r\n    assertEquals(BigDecimalSplitter.class, format.getSplitter(Types.NUMERIC).getClass());\r\n    assertEquals(BooleanSplitter.class, format.getSplitter(Types.BOOLEAN).getClass());\r\n    assertEquals(BooleanSplitter.class, format.getSplitter(Types.BIT).getClass());\r\n    assertEquals(IntegerSplitter.class, format.getSplitter(Types.BIGINT).getClass());\r\n    assertEquals(IntegerSplitter.class, format.getSplitter(Types.TINYINT).getClass());\r\n    assertEquals(IntegerSplitter.class, format.getSplitter(Types.SMALLINT).getClass());\r\n    assertEquals(IntegerSplitter.class, format.getSplitter(Types.INTEGER).getClass());\r\n    assertEquals(FloatSplitter.class, format.getSplitter(Types.DOUBLE).getClass());\r\n    assertEquals(FloatSplitter.class, format.getSplitter(Types.REAL).getClass());\r\n    assertEquals(FloatSplitter.class, format.getSplitter(Types.FLOAT).getClass());\r\n    assertEquals(TextSplitter.class, format.getSplitter(Types.LONGVARCHAR).getClass());\r\n    assertEquals(TextSplitter.class, format.getSplitter(Types.CHAR).getClass());\r\n    assertEquals(TextSplitter.class, format.getSplitter(Types.VARCHAR).getClass());\r\n    assertNull(format.getSplitter(Types.BINARY));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "testCircularDependency",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testCircularDependency() throws IOException\n{\r\n    ControlledJob job1 = new ControlledJob(new Configuration());\r\n    job1.setJobName(\"job1\");\r\n    ControlledJob job2 = new ControlledJob(new Configuration());\r\n    job2.setJobName(\"job2\");\r\n    ControlledJob job3 = new ControlledJob(new Configuration());\r\n    job3.setJobName(\"job3\");\r\n    job1.addDependingJob(job2);\r\n    job2.addDependingJob(job3);\r\n    job3.addDependingJob(job1);\r\n    JobControl jobControl = new JobControl(\"test\");\r\n    jobControl.addJob(job1);\r\n    jobControl.addJob(job2);\r\n    jobControl.addJob(job3);\r\n    try {\r\n        jobControl.run();\r\n    } catch (Exception e) {\r\n        assertTrue(e instanceof IllegalArgumentException);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testStringToPath",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testStringToPath() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    JobResourceUploader uploader = new JobResourceUploader(FileSystem.getLocal(conf), false);\r\n    Assert.assertEquals(\"Failed: absolute, no scheme, with fragment\", \"/testWithFragment.txt\", uploader.stringToPath(\"/testWithFragment.txt#fragment.txt\").toString());\r\n    Assert.assertEquals(\"Failed: absolute, with scheme, with fragment\", \"file:/testWithFragment.txt\", uploader.stringToPath(\"file:///testWithFragment.txt#fragment.txt\").toString());\r\n    Assert.assertEquals(\"Failed: relative, no scheme, with fragment\", \"testWithFragment.txt\", uploader.stringToPath(\"testWithFragment.txt#fragment.txt\").toString());\r\n    Assert.assertEquals(\"Failed: relative, no scheme, no fragment\", \"testWithFragment.txt\", uploader.stringToPath(\"testWithFragment.txt\").toString());\r\n    Assert.assertEquals(\"Failed: absolute, with scheme, no fragment\", \"file:/testWithFragment.txt\", uploader.stringToPath(\"file:///testWithFragment.txt\").toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testAllDefaults",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAllDefaults() throws IOException\n{\r\n    ResourceConf.Builder b = new ResourceConf.Builder();\r\n    runLimitsTest(b.build(), true, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testNoLimitsWithResources",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testNoLimitsWithResources() throws IOException\n{\r\n    ResourceConf.Builder b = new ResourceConf.Builder();\r\n    b.setNumOfDCArchives(1);\r\n    b.setNumOfDCFiles(1);\r\n    b.setNumOfTmpArchives(10);\r\n    b.setNumOfTmpFiles(1);\r\n    b.setNumOfTmpLibJars(1);\r\n    b.setJobJar(true);\r\n    b.setSizeOfResource(10);\r\n    runLimitsTest(b.build(), true, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testAtResourceLimit",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testAtResourceLimit() throws IOException\n{\r\n    ResourceConf.Builder b = new ResourceConf.Builder();\r\n    b.setNumOfDCArchives(1);\r\n    b.setNumOfDCFiles(1);\r\n    b.setNumOfTmpArchives(1);\r\n    b.setNumOfTmpFiles(1);\r\n    b.setNumOfTmpLibJars(1);\r\n    b.setJobJar(true);\r\n    b.setMaxResources(6);\r\n    runLimitsTest(b.build(), true, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testOverResourceLimit",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testOverResourceLimit() throws IOException\n{\r\n    ResourceConf.Builder b = new ResourceConf.Builder();\r\n    b.setNumOfDCArchives(1);\r\n    b.setNumOfDCFiles(1);\r\n    b.setNumOfTmpArchives(1);\r\n    b.setNumOfTmpFiles(2);\r\n    b.setNumOfTmpLibJars(1);\r\n    b.setJobJar(true);\r\n    b.setMaxResources(6);\r\n    runLimitsTest(b.build(), false, ResourceViolation.NUMBER_OF_RESOURCES);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testAtResourcesMBLimit",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testAtResourcesMBLimit() throws IOException\n{\r\n    ResourceConf.Builder b = new ResourceConf.Builder();\r\n    b.setNumOfDCArchives(1);\r\n    b.setNumOfDCFiles(1);\r\n    b.setNumOfTmpArchives(1);\r\n    b.setNumOfTmpFiles(2);\r\n    b.setNumOfTmpLibJars(1);\r\n    b.setJobJar(true);\r\n    b.setMaxResourcesMB(7);\r\n    b.setSizeOfResource(1);\r\n    runLimitsTest(b.build(), true, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testOverResourcesMBLimit",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testOverResourcesMBLimit() throws IOException\n{\r\n    ResourceConf.Builder b = new ResourceConf.Builder();\r\n    b.setNumOfDCArchives(1);\r\n    b.setNumOfDCFiles(2);\r\n    b.setNumOfTmpArchives(1);\r\n    b.setNumOfTmpFiles(2);\r\n    b.setNumOfTmpLibJars(1);\r\n    b.setJobJar(true);\r\n    b.setMaxResourcesMB(7);\r\n    b.setSizeOfResource(1);\r\n    runLimitsTest(b.build(), false, ResourceViolation.TOTAL_RESOURCE_SIZE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testAtSingleResourceMBLimit",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testAtSingleResourceMBLimit() throws IOException\n{\r\n    ResourceConf.Builder b = new ResourceConf.Builder();\r\n    b.setNumOfDCArchives(1);\r\n    b.setNumOfDCFiles(2);\r\n    b.setNumOfTmpArchives(1);\r\n    b.setNumOfTmpFiles(2);\r\n    b.setNumOfTmpLibJars(1);\r\n    b.setJobJar(true);\r\n    b.setMaxSingleResourceMB(1);\r\n    b.setSizeOfResource(1);\r\n    runLimitsTest(b.build(), true, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testOverSingleResourceMBLimit",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testOverSingleResourceMBLimit() throws IOException\n{\r\n    ResourceConf.Builder b = new ResourceConf.Builder();\r\n    b.setNumOfDCArchives(1);\r\n    b.setNumOfDCFiles(2);\r\n    b.setNumOfTmpArchives(1);\r\n    b.setNumOfTmpFiles(2);\r\n    b.setNumOfTmpLibJars(1);\r\n    b.setJobJar(true);\r\n    b.setMaxSingleResourceMB(1);\r\n    b.setSizeOfResource(10);\r\n    runLimitsTest(b.build(), false, ResourceViolation.SINGLE_RESOURCE_SIZE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testPathsWithNoFragNoSchemeRelative",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testPathsWithNoFragNoSchemeRelative() throws IOException\n{\r\n    ResourceConf.Builder b = new ResourceConf.Builder();\r\n    b.setNumOfTmpFiles(5);\r\n    b.setNumOfTmpLibJars(2);\r\n    b.setNumOfTmpArchives(2);\r\n    b.setJobJar(true);\r\n    b.setPathsWithScheme(false);\r\n    b.setPathsWithFrags(false);\r\n    ResourceConf rConf = b.build();\r\n    JobConf jConf = new JobConf();\r\n    JobResourceUploader uploader = new StubedUploader(jConf);\r\n    runTmpResourcePathTest(uploader, rConf, jConf, expectedFilesNoFrags, expectedArchivesNoFrags, basicExpectedJobJar);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testPathsWithNoFragNoSchemeAbsolute",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testPathsWithNoFragNoSchemeAbsolute() throws IOException\n{\r\n    ResourceConf.Builder b = new ResourceConf.Builder();\r\n    b.setNumOfTmpFiles(5);\r\n    b.setNumOfTmpLibJars(2);\r\n    b.setNumOfTmpArchives(2);\r\n    b.setJobJar(true);\r\n    b.setPathsWithFrags(false);\r\n    b.setPathsWithScheme(false);\r\n    b.setAbsolutePaths(true);\r\n    ResourceConf rConf = b.build();\r\n    JobConf jConf = new JobConf();\r\n    JobResourceUploader uploader = new StubedUploader(jConf);\r\n    runTmpResourcePathTest(uploader, rConf, jConf, expectedFilesNoFrags, expectedArchivesNoFrags, basicExpectedJobJar);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testPathsWithFragNoSchemeAbsolute",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testPathsWithFragNoSchemeAbsolute() throws IOException\n{\r\n    ResourceConf.Builder b = new ResourceConf.Builder();\r\n    b.setNumOfTmpFiles(5);\r\n    b.setNumOfTmpLibJars(2);\r\n    b.setNumOfTmpArchives(2);\r\n    b.setJobJar(true);\r\n    b.setPathsWithFrags(true);\r\n    b.setPathsWithScheme(false);\r\n    b.setAbsolutePaths(true);\r\n    ResourceConf rConf = b.build();\r\n    JobConf jConf = new JobConf();\r\n    JobResourceUploader uploader = new StubedUploader(jConf);\r\n    runTmpResourcePathTest(uploader, rConf, jConf, expectedFilesWithFrags, expectedArchivesWithFrags, basicExpectedJobJar);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testPathsWithFragNoSchemeRelative",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testPathsWithFragNoSchemeRelative() throws IOException\n{\r\n    ResourceConf.Builder b = new ResourceConf.Builder();\r\n    b.setNumOfTmpFiles(5);\r\n    b.setNumOfTmpLibJars(2);\r\n    b.setNumOfTmpArchives(2);\r\n    b.setJobJar(true);\r\n    b.setPathsWithFrags(true);\r\n    b.setAbsolutePaths(false);\r\n    b.setPathsWithScheme(false);\r\n    ResourceConf rConf = b.build();\r\n    JobConf jConf = new JobConf();\r\n    JobResourceUploader uploader = new StubedUploader(jConf);\r\n    runTmpResourcePathTest(uploader, rConf, jConf, expectedFilesWithFrags, expectedArchivesWithFrags, basicExpectedJobJar);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testPathsWithFragSchemeAbsolute",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testPathsWithFragSchemeAbsolute() throws IOException\n{\r\n    ResourceConf.Builder b = new ResourceConf.Builder();\r\n    b.setNumOfTmpFiles(5);\r\n    b.setNumOfTmpLibJars(2);\r\n    b.setNumOfTmpArchives(2);\r\n    b.setJobJar(true);\r\n    b.setPathsWithFrags(true);\r\n    b.setAbsolutePaths(true);\r\n    b.setPathsWithScheme(true);\r\n    ResourceConf rConf = b.build();\r\n    JobConf jConf = new JobConf();\r\n    JobResourceUploader uploader = new StubedUploader(jConf);\r\n    runTmpResourcePathTest(uploader, rConf, jConf, expectedFilesWithFrags, expectedArchivesWithFrags, basicExpectedJobJar);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testPathsWithNoFragWithSchemeAbsolute",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testPathsWithNoFragWithSchemeAbsolute() throws IOException\n{\r\n    ResourceConf.Builder b = new ResourceConf.Builder();\r\n    b.setNumOfTmpFiles(5);\r\n    b.setNumOfTmpLibJars(2);\r\n    b.setNumOfTmpArchives(2);\r\n    b.setJobJar(true);\r\n    b.setPathsWithFrags(false);\r\n    b.setPathsWithScheme(true);\r\n    b.setAbsolutePaths(true);\r\n    ResourceConf rConf = b.build();\r\n    JobConf jConf = new JobConf();\r\n    JobResourceUploader uploader = new StubedUploader(jConf);\r\n    runTmpResourcePathTest(uploader, rConf, jConf, expectedFilesNoFrags, expectedArchivesNoFrags, basicExpectedJobJar);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testPathsWithNoFragAndWildCard",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testPathsWithNoFragAndWildCard() throws IOException\n{\r\n    ResourceConf.Builder b = new ResourceConf.Builder();\r\n    b.setNumOfTmpFiles(5);\r\n    b.setNumOfTmpLibJars(4);\r\n    b.setNumOfTmpArchives(2);\r\n    b.setJobJar(true);\r\n    b.setPathsWithFrags(false);\r\n    b.setPathsWithScheme(true);\r\n    b.setAbsolutePaths(true);\r\n    ResourceConf rConf = b.build();\r\n    JobConf jConf = new JobConf();\r\n    JobResourceUploader uploader = new StubedUploader(jConf, true);\r\n    runTmpResourcePathTest(uploader, rConf, jConf, expectedFilesWithWildcard, expectedArchivesNoFrags, basicExpectedJobJar);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testPathsWithFragsAndWildCard",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testPathsWithFragsAndWildCard() throws IOException\n{\r\n    ResourceConf.Builder b = new ResourceConf.Builder();\r\n    b.setNumOfTmpFiles(5);\r\n    b.setNumOfTmpLibJars(2);\r\n    b.setNumOfTmpArchives(2);\r\n    b.setJobJar(true);\r\n    b.setPathsWithFrags(true);\r\n    b.setPathsWithScheme(true);\r\n    b.setAbsolutePaths(true);\r\n    ResourceConf rConf = b.build();\r\n    JobConf jConf = new JobConf();\r\n    JobResourceUploader uploader = new StubedUploader(jConf, true);\r\n    runTmpResourcePathTest(uploader, rConf, jConf, expectedFilesWithFrags, expectedArchivesWithFrags, basicExpectedJobJar);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testErasureCodingDefault",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testErasureCodingDefault() throws IOException\n{\r\n    testErasureCodingSetting(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testErasureCodingDisabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testErasureCodingDisabled() throws IOException\n{\r\n    testErasureCodingSetting(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testOriginalPathEndsInSlash",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testOriginalPathEndsInSlash() throws IOException, URISyntaxException\n{\r\n    testOriginalPathWithTrailingSlash(new Path(new URI(\"file:/local/mapred/test/\")), new Path(\"hdfs://localhost:1234/home/hadoop/test/\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testOriginalPathIsRoot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testOriginalPathIsRoot() throws IOException, URISyntaxException\n{\r\n    testOriginalPathWithTrailingSlash(new Path(new URI(\"file:/\")), new Path(\"hdfs://localhost:1234/home/hadoop/\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testOriginalPathWithTrailingSlash",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testOriginalPathWithTrailingSlash(Path path, Path expectedRemotePath) throws IOException, URISyntaxException\n{\r\n    Path dstPath = new Path(\"hdfs://localhost:1234/home/hadoop/\");\r\n    DistributedFileSystem fs = mock(DistributedFileSystem.class);\r\n    when(fs.mkdirs(any(Path.class))).thenReturn(false);\r\n    when(fs.getUri()).thenReturn(dstPath.toUri());\r\n    JobResourceUploader uploader = new StubedUploader(fs, true, true);\r\n    JobConf jConf = new JobConf();\r\n    Path originalPath = spy(path);\r\n    FileSystem localFs = mock(FileSystem.class);\r\n    FileStatus fileStatus = mock(FileStatus.class);\r\n    when(localFs.getFileStatus(any(Path.class))).thenReturn(fileStatus);\r\n    when(fileStatus.isDirectory()).thenReturn(true);\r\n    when(fileStatus.getPath()).thenReturn(originalPath);\r\n    doReturn(localFs).when(originalPath).getFileSystem(any(Configuration.class));\r\n    when(localFs.getUri()).thenReturn(path.toUri());\r\n    uploader.copyRemoteFiles(dstPath, originalPath, jConf, (short) 1);\r\n    ArgumentCaptor<Path> pathCaptor = ArgumentCaptor.forClass(Path.class);\r\n    verify(fs).makeQualified(pathCaptor.capture());\r\n    Assert.assertEquals(\"Path\", expectedRemotePath, pathCaptor.getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testErasureCodingSetting",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testErasureCodingSetting(boolean defaultBehavior) throws IOException\n{\r\n    JobConf jConf = new JobConf();\r\n    if (!defaultBehavior) {\r\n        jConf.setBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED, true);\r\n    }\r\n    DistributedFileSystem fs = mock(DistributedFileSystem.class);\r\n    Path path = new Path(\"/\");\r\n    when(fs.makeQualified(any(Path.class))).thenReturn(path);\r\n    JobResourceUploader uploader = new StubedUploader(fs, true, false);\r\n    Job job = Job.getInstance(jConf);\r\n    uploader.uploadResources(job, new Path(\"/test\"));\r\n    String replicationPolicyName = SystemErasureCodingPolicies.getReplicationPolicy().getName();\r\n    VerificationMode mode = defaultBehavior ? times(1) : never();\r\n    verify(fs, mode).setErasureCodingPolicy(eq(path), eq(replicationPolicyName));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "runTmpResourcePathTest",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void runTmpResourcePathTest(JobResourceUploader uploader, ResourceConf rConf, JobConf jConf, String[] expectedFiles, String[] expectedArchives, String expectedJobJar) throws IOException\n{\r\n    Job job = rConf.setupJobConf(jConf);\r\n    uploadResources(uploader, job);\r\n    validateResourcePaths(job, expectedFiles, expectedArchives, expectedJobJar);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "uploadResources",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void uploadResources(JobResourceUploader uploader, Job job) throws IOException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    Collection<String> files = conf.getStringCollection(\"tmpfiles\");\r\n    Collection<String> libjars = conf.getStringCollection(\"tmpjars\");\r\n    Collection<String> archives = conf.getStringCollection(\"tmparchives\");\r\n    Map<URI, FileStatus> statCache = new HashMap<>();\r\n    Map<String, Boolean> fileSCUploadPolicies = new HashMap<>();\r\n    String jobJar = job.getJar();\r\n    uploader.uploadFiles(job, files, new Path(\"/files-submit-dir\"), null, (short) 3, fileSCUploadPolicies, statCache);\r\n    uploader.uploadArchives(job, archives, new Path(\"/archives-submit-dir\"), null, (short) 3, fileSCUploadPolicies, statCache);\r\n    uploader.uploadLibJars(job, libjars, new Path(\"/libjars-submit-dir\"), null, (short) 3, fileSCUploadPolicies, statCache);\r\n    uploader.uploadJobJar(job, jobJar, new Path(jobjarSubmitDir), (short) 3, statCache);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "validateResourcePaths",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void validateResourcePaths(Job job, String[] expectedFiles, String[] expectedArchives, String expectedJobJar) throws IOException\n{\r\n    validateResourcePathsSub(job.getCacheFiles(), expectedFiles);\r\n    validateResourcePathsSub(job.getCacheArchives(), expectedArchives);\r\n    Assert.assertEquals(\"Job jar path is different than expected!\", expectedJobJar, job.getJar());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "validateResourcePathsSub",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void validateResourcePathsSub(URI[] actualURIs, String[] expectedURIs)\n{\r\n    List<URI> actualList = Arrays.asList(actualURIs);\r\n    Set<String> expectedSet = new HashSet<>(Arrays.asList(expectedURIs));\r\n    if (actualList.size() != expectedSet.size()) {\r\n        Assert.fail(\"Expected list of resources (\" + expectedSet.size() + \") and actual list of resources (\" + actualList.size() + \") are different lengths!\");\r\n    }\r\n    for (URI u : actualList) {\r\n        if (!expectedSet.contains(u.toString())) {\r\n            Assert.fail(\"Resource list contained unexpected path: \" + u.toString());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "runLimitsTest",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void runLimitsTest(ResourceConf rlConf, boolean checkShouldSucceed, ResourceViolation violation) throws IOException\n{\r\n    if (!checkShouldSucceed && violation == null) {\r\n        Assert.fail(\"Test is misconfigured. checkShouldSucceed is set to false\" + \" and a ResourceViolation is not specified.\");\r\n    }\r\n    JobConf conf = new JobConf();\r\n    rlConf.setupJobConf(conf);\r\n    JobResourceUploader uploader = new StubedUploader(conf);\r\n    long configuredSizeOfResourceBytes = rlConf.sizeOfResource * 1024 * 1024;\r\n    when(mockedStatus.getLen()).thenReturn(configuredSizeOfResourceBytes);\r\n    when(mockedStatus.isDirectory()).thenReturn(false);\r\n    Map<URI, FileStatus> statCache = new HashMap<URI, FileStatus>();\r\n    try {\r\n        uploader.checkLocalizationLimits(conf, conf.getStringCollection(\"tmpfiles\"), conf.getStringCollection(\"tmpjars\"), conf.getStringCollection(\"tmparchives\"), conf.getJar(), statCache);\r\n        Assert.assertTrue(\"Limits check succeeded when it should have failed.\", checkShouldSucceed);\r\n    } catch (IOException e) {\r\n        if (checkShouldSucceed) {\r\n            Assert.fail(\"Limits check failed when it should have succeeded: \" + e);\r\n        }\r\n        switch(violation) {\r\n            case NUMBER_OF_RESOURCES:\r\n                if (!e.getMessage().contains(JobResourceUploader.MAX_RESOURCE_ERR_MSG)) {\r\n                    Assert.fail(\"Test failed unexpectedly: \" + e);\r\n                }\r\n                break;\r\n            case TOTAL_RESOURCE_SIZE:\r\n                if (!e.getMessage().contains(JobResourceUploader.MAX_TOTAL_RESOURCE_MB_ERR_MSG)) {\r\n                    Assert.fail(\"Test failed unexpectedly: \" + e);\r\n                }\r\n                break;\r\n            case SINGLE_RESOURCE_SIZE:\r\n                if (!e.getMessage().contains(JobResourceUploader.MAX_SINGLE_RESOURCE_MB_ERR_MSG)) {\r\n                    Assert.fail(\"Test failed unexpectedly: \" + e);\r\n                }\r\n                break;\r\n            default:\r\n                Assert.fail(\"Test failed unexpectedly: \" + e);\r\n                break;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    conf = new Configuration();\r\n    conf.set(YarnConfiguration.RM_PRINCIPAL, \"mapred/host@REALM\");\r\n    renewer = Master.getMasterPrincipal(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "testObtainTokens",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testObtainTokens() throws Exception\n{\r\n    Credentials credentials = new Credentials();\r\n    FileSystem fs = mock(FileSystem.class);\r\n    TokenCache.obtainTokensForNamenodesInternal(fs, credentials, conf, renewer);\r\n    verify(fs).addDelegationTokens(eq(renewer), eq(credentials));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "testBinaryCredentialsWithoutScheme",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testBinaryCredentialsWithoutScheme() throws Exception\n{\r\n    testBinaryCredentials(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "testBinaryCredentialsWithScheme",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testBinaryCredentialsWithScheme() throws Exception\n{\r\n    testBinaryCredentials(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "testBinaryCredentials",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testBinaryCredentials(boolean hasScheme) throws Exception\n{\r\n    Path TEST_ROOT_DIR = new Path(System.getProperty(\"test.build.data\", \"test/build/data\"));\r\n    String binaryTokenFile = hasScheme ? FileSystem.getLocal(conf).makeQualified(new Path(TEST_ROOT_DIR, \"tokenFile\")).toString() : FileSystem.getLocal(conf).makeQualified(new Path(TEST_ROOT_DIR, \"tokenFile\")).toUri().getPath();\r\n    MockFileSystem fs1 = createFileSystemForServiceName(\"service1\");\r\n    MockFileSystem fs2 = createFileSystemForServiceName(\"service2\");\r\n    MockFileSystem fs3 = createFileSystemForServiceName(\"service3\");\r\n    Credentials creds = new Credentials();\r\n    Token<?> token1 = fs1.getDelegationToken(renewer);\r\n    Token<?> token2 = fs2.getDelegationToken(renewer);\r\n    creds.addToken(token1.getService(), token1);\r\n    creds.addToken(token2.getService(), token2);\r\n    conf.set(MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY, binaryTokenFile);\r\n    creds.writeTokenStorageFile(new Path(binaryTokenFile), conf);\r\n    creds = new Credentials();\r\n    Token<?> newerToken1 = fs1.getDelegationToken(renewer);\r\n    assertNotSame(newerToken1, token1);\r\n    creds.addToken(newerToken1.getService(), newerToken1);\r\n    checkToken(creds, newerToken1);\r\n    TokenCache.obtainTokensForNamenodesInternal(fs1, creds, conf, renewer);\r\n    checkToken(creds, newerToken1, token2);\r\n    TokenCache.obtainTokensForNamenodesInternal(fs2, creds, conf, renewer);\r\n    checkToken(creds, newerToken1, token2);\r\n    TokenCache.obtainTokensForNamenodesInternal(fs3, creds, conf, renewer);\r\n    Token<?> token3 = creds.getToken(new Text(fs3.getCanonicalServiceName()));\r\n    assertThat(token3).isNotNull();\r\n    checkToken(creds, newerToken1, token2, token3);\r\n    TokenCache.obtainTokensForNamenodesInternal(fs1, creds, conf, renewer);\r\n    TokenCache.obtainTokensForNamenodesInternal(fs2, creds, conf, renewer);\r\n    TokenCache.obtainTokensForNamenodesInternal(fs3, creds, conf, renewer);\r\n    checkToken(creds, newerToken1, token2, token3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "checkToken",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void checkToken(Credentials creds, Token<?>... tokens)\n{\r\n    assertEquals(tokens.length, creds.getAllTokens().size());\r\n    for (Token<?> token : tokens) {\r\n        Token<?> credsToken = creds.getToken(token.getService());\r\n        assertThat(credsToken).isNotNull();\r\n        assertEquals(token, credsToken);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "createFileSystemForServiceName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MockFileSystem createFileSystemForServiceName(final String service) throws IOException\n{\r\n    MockFileSystem mockFs = new MockFileSystem();\r\n    when(mockFs.getCanonicalServiceName()).thenReturn(service);\r\n    when(mockFs.getDelegationToken(any(String.class))).thenAnswer(new Answer<Token<?>>() {\r\n\r\n        int unique = 0;\r\n\r\n        @Override\r\n        public Token<?> answer(InvocationOnMock invocation) throws Throwable {\r\n            Token<?> token = new Token<TokenIdentifier>();\r\n            token.setService(new Text(service));\r\n            token.setKind(new Text(\"token\" + unique++));\r\n            return token;\r\n        }\r\n    });\r\n    return mockFs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "testSingleTokenFetch",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSingleTokenFetch() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(YarnConfiguration.RM_PRINCIPAL, \"mapred/host@REALM\");\r\n    String renewer = Master.getMasterPrincipal(conf);\r\n    Credentials credentials = new Credentials();\r\n    final MockFileSystem fs = new MockFileSystem();\r\n    final MockFileSystem mockFs = (MockFileSystem) fs.getRawFileSystem();\r\n    when(mockFs.getCanonicalServiceName()).thenReturn(\"host:0\");\r\n    when(mockFs.getUri()).thenReturn(new URI(\"mockfs://host:0\"));\r\n    Path mockPath = mock(Path.class);\r\n    when(mockPath.getFileSystem(conf)).thenReturn(mockFs);\r\n    Path[] paths = new Path[] { mockPath, mockPath };\r\n    when(mockFs.addDelegationTokens(\"me\", credentials)).thenReturn(null);\r\n    TokenCache.obtainTokensForNamenodesInternal(credentials, paths, conf);\r\n    verify(mockFs, times(1)).addDelegationTokens(renewer, credentials);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "testCleanUpTokenReferral",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCleanUpTokenReferral() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY, \"foo\");\r\n    TokenCache.cleanUpTokenReferral(conf);\r\n    assertNull(conf.get(MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "testGetTokensForNamenodes",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testGetTokensForNamenodes() throws IOException, URISyntaxException\n{\r\n    Path TEST_ROOT_DIR = new Path(System.getProperty(\"test.build.data\", \"test/build/data\"));\r\n    String binaryTokenFile = FileSystem.getLocal(conf).makeQualified(new Path(TEST_ROOT_DIR, \"tokenFile\")).toUri().getPath();\r\n    MockFileSystem fs1 = createFileSystemForServiceName(\"service1\");\r\n    Credentials creds = new Credentials();\r\n    Token<?> token1 = fs1.getDelegationToken(renewer);\r\n    creds.addToken(token1.getService(), token1);\r\n    conf.set(MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY, binaryTokenFile);\r\n    creds.writeTokenStorageFile(new Path(binaryTokenFile), conf);\r\n    TokenCache.obtainTokensForNamenodesInternal(fs1, creds, conf, renewer);\r\n    String fs_addr = fs1.getCanonicalServiceName();\r\n    Token<?> nnt = TokenCache.getDelegationToken(creds, fs_addr);\r\n    assertNotNull(\"Token for nn is null\", nnt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    return getContract().getConf();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getStoreOperations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ManifestStoreOperations getStoreOperations()\n{\r\n    return storeOperations;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setStoreOperations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStoreOperations(final ManifestStoreOperations storeOperations)\n{\r\n    this.storeOperations = storeOperations;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getTaskAttemptIds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<String> getTaskAttemptIds()\n{\r\n    return taskAttemptIds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getTaskIds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<String> getTaskIds()\n{\r\n    return taskIds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getTotalDataSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTotalDataSize()\n{\r\n    return totalDataSize.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getManifestDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getManifestDir()\n{\r\n    return manifestDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "withManifestDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractManifestCommitterTest withManifestDir(Path value)\n{\r\n    manifestDir = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "describe",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void describe(String text, Object... args)\n{\r\n    LOG.info(\"\\n\\n{}: {}\\n\", getMethodName(), String.format(text, args));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new LocalFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return enableManifestCommitter(super.createConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    reportDir = new File(getProjectBuildDir(), \"reports\");\r\n    reportDir.mkdirs();\r\n    super.setup();\r\n    manifestDir = path(\"manifests\");\r\n    setDestDir(methodPath());\r\n    setStageStatistics(createIOStatisticsStore().build());\r\n    setSubmitter(createCloseableTaskSubmitter(POOL_SIZE, TASK_IDS.getJobId()));\r\n    storeOperations = createManifestStoreOperations();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createManifestStoreOperations",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ManifestStoreOperations createManifestStoreOperations() throws IOException\n{\r\n    final FileSystem fs = getFileSystem();\r\n    return ManifestCommitterSupport.createManifestStoreOperations(fs.getConf(), fs, getTestPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    Thread.currentThread().setName(\"teardown\");\r\n    IOUtils.cleanupWithLogger(LOG, storeOperations, getSubmitter());\r\n    storeOperations = null;\r\n    super.teardown();\r\n    FILESYSTEM_IOSTATS.aggregate(retrieveIOStatistics(getFileSystem()));\r\n    FILESYSTEM_IOSTATS.aggregate(getStageStatistics());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return 600_000;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getTestPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTestPath()\n{\r\n    return getContract().getTestPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getSubmitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CloseableTaskPoolSubmitter getSubmitter()\n{\r\n    return submitter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setSubmitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSubmitter(CloseableTaskPoolSubmitter submitter)\n{\r\n    this.submitter = submitter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getExecutorService",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExecutorService getExecutorService()\n{\r\n    return getSubmitter().getPool();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getStageStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsStore getStageStatistics()\n{\r\n    return stageStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setStageStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStageStatistics(IOStatisticsStore stageStatistics)\n{\r\n    this.stageStatistics = stageStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getProgressCounter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ProgressCounter getProgressCounter()\n{\r\n    return progressCounter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getReportDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "File getReportDir()\n{\r\n    return reportDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getReportDirUri",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI getReportDirUri()\n{\r\n    return getReportDir().toURI();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getThreadLeakTracker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ThreadLeakTracker getThreadLeakTracker()\n{\r\n    return THREAD_LEAK_TRACKER;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "threadLeakage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void threadLeakage()\n{\r\n    THREAD_LEAK_TRACKER.assertNoThreadLeakage();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "dumpFileSystemIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void dumpFileSystemIOStatistics()\n{\r\n    LOG.info(\"Aggregate FileSystem Statistics {}\", ioStatisticsToPrettyString(FILESYSTEM_IOSTATS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createFilesOrDirs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<Path> createFilesOrDirs(Path base, String prefix, ExecutorService executor, int depth, int width, int files, boolean createDirs) throws IOException\n{\r\n    try (DurationInfo ignored = new DurationInfo(LOG, true, \"Creating Files %s (%d, %d, %d) under %s\", prefix, depth, width, files, base)) {\r\n        assertPathExists(\"Task attempt dir\", base);\r\n        List<Future<Path>> futures = createFilesOrDirs(new ArrayList<>(), base, prefix, executor, depth, width, files, createDirs);\r\n        List<Path> result = new ArrayList<>();\r\n        for (Future<Path> f : futures) {\r\n            result.add(awaitFuture(f));\r\n        }\r\n        return result;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createFilesOrDirs",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "List<Future<Path>> createFilesOrDirs(List<Future<Path>> futures, Path base, String prefix, ExecutorService executor, int depth, int width, int files, boolean createDirs)\n{\r\n    if (depth > 0) {\r\n        for (int i = 0; i < width; i++) {\r\n            Path child = new Path(base, String.format(\"dir-%02d-%02d\", depth, i));\r\n            createFilesOrDirs(futures, child, prefix, executor, depth - 1, width, files, false);\r\n        }\r\n    } else {\r\n        for (int i = 0; i < files; i++) {\r\n            Path file = new Path(base, String.format(\"%s-%04d\", prefix, CREATE_FILE_COUNTER.incrementAndGet()));\r\n            long entry = fileDataGenerator.incrementAndGet() & 0xffff;\r\n            byte[] data = new byte[2];\r\n            data[0] = (byte) (entry & 0xff);\r\n            data[1] = (byte) ((entry & 0xff00) >> 8);\r\n            Future<Path> f = executor.submit(() -> {\r\n                if (!createDirs) {\r\n                    ContractTestUtils.createFile(getFileSystem(), file, true, data);\r\n                } else {\r\n                    mkdirs(file);\r\n                }\r\n                return file;\r\n            });\r\n            futures.add(f);\r\n        }\r\n    }\r\n    return futures;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "subpaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<Path> subpaths(Path base, int count)\n{\r\n    return IntStream.rangeClosed(1, count).mapToObj(i -> new Path(base, String.format(\"entry-%02d\", i))).collect(Collectors.toList());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "asyncMkdir",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CompletableFuture<Path> asyncMkdir(final Path path)\n{\r\n    CompletableFuture<Path> f = new CompletableFuture<>();\r\n    getExecutorService().submit(() -> {\r\n        try {\r\n            mkdirs(path);\r\n            f.complete(path);\r\n        } catch (IOException e) {\r\n            f.completeExceptionally(e);\r\n        }\r\n    });\r\n    return f;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "asyncMkdirs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void asyncMkdirs(Collection<Path> paths) throws IOException\n{\r\n    List<CompletableFuture<Path>> futures = new ArrayList<>();\r\n    for (Path path : paths) {\r\n        futures.add(asyncMkdir(path));\r\n    }\r\n    for (Future<Path> f : futures) {\r\n        awaitFuture(f);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "asyncPut",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CompletableFuture<Path> asyncPut(final Path path, byte[] data)\n{\r\n    CompletableFuture<Path> f = new CompletableFuture<>();\r\n    getExecutorService().submit(() -> {\r\n        try {\r\n            ContractTestUtils.createFile(getFileSystem(), path, true, data);\r\n            f.complete(path);\r\n        } catch (IOException e) {\r\n            f.completeExceptionally(e);\r\n        }\r\n    });\r\n    return f;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "toMap",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, TaskManifest> toMap(List<TaskManifest> list)\n{\r\n    return list.stream().collect(Collectors.toMap(TaskManifest::getTaskAttemptID, x -> x));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "verifyManifestFilesMatch",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyManifestFilesMatch(final TaskManifest manifest, final List<Path> files)\n{\r\n    Set<Path> filesToRename = manifest.getFilesToCommit().stream().map(FileEntry::getSourcePath).collect(Collectors.toSet());\r\n    Assertions.assertThat(filesToRename).containsExactlyInAnyOrderElementsOf(files);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "verifyManifestTaskAttemptID",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskManifest verifyManifestTaskAttemptID(final TaskManifest manifest, final String attemptId)\n{\r\n    Assertions.assertThat(manifest).describedAs(\"Manifest of task %s\", attemptId).isNotNull();\r\n    Assertions.assertThat(manifest.getTaskAttemptID()).describedAs(\"Task Attempt ID of manifest %s\", manifest).isEqualTo(attemptId);\r\n    return manifest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "pathMustExist",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path pathMustExist(final String message, final Path path) throws IOException\n{\r\n    assertPathExists(message, path);\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "verifyPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path verifyPath(final String message, final Path expectedPath, final Path actualPath) throws IOException\n{\r\n    Assertions.assertThat(actualPath).describedAs(message).isEqualTo(expectedPath);\r\n    return pathMustExist(message, actualPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "verifySuccessMarker",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ManifestSuccessData verifySuccessMarker(Path dir, String jobId) throws IOException\n{\r\n    return validateSuccessFile(getFileSystem(), dir, 0, jobId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "readFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String readFile(Path path) throws IOException\n{\r\n    return ContractTestUtils.readUTF8(getFileSystem(), path, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "enableManifestCommitter",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Configuration enableManifestCommitter(final Configuration conf)\n{\r\n    conf.set(COMMITTER_FACTORY_CLASS, MANIFEST_COMMITTER_FACTORY);\r\n    conf.setBoolean(SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true);\r\n    conf.setBoolean(OPT_VALIDATE_OUTPUT, true);\r\n    if (getManifestDir() != null) {\r\n        conf.set(OPT_DIAGNOSTICS_MANIFEST_DIR, getManifestDir().toUri().toString());\r\n    }\r\n    conf.set(OPT_SUMMARY_REPORT_DIR, getReportDirUri().toString());\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createStageConfigForJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig createStageConfigForJob(final int jobAttemptNumber, final Path outputPath)\n{\r\n    return createStageConfig(jobAttemptNumber, -1, 0, outputPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createStageConfig",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "StageConfig createStageConfig(final int jobAttemptNumber, final int taskIndex, final int taskAttemptNumber, final Path outputPath)\n{\r\n    final String jobId = TASK_IDS.getJobId();\r\n    ManifestCommitterSupport.AttemptDirectories attemptDirs = new ManifestCommitterSupport.AttemptDirectories(outputPath, jobId, jobAttemptNumber);\r\n    StageConfig config = new StageConfig();\r\n    config.withIOProcessors(getSubmitter()).withIOStatistics(getStageStatistics()).withJobId(jobId).withJobIdSource(JOB_ID_SOURCE_MAPREDUCE).withJobAttemptNumber(jobAttemptNumber).withJobDirectories(attemptDirs).withName(String.format(NAME_FORMAT_JOB_ATTEMPT, jobId)).withOperations(getStoreOperations()).withProgressable(getProgressCounter());\r\n    if (taskIndex >= 0) {\r\n        String taskAttempt = TASK_IDS.getTaskAttempt(taskIndex, taskAttemptNumber);\r\n        config.withTaskAttemptId(taskAttempt).withTaskId(TASK_IDS.getTaskIdType(taskIndex).toString()).withTaskAttemptDir(attemptDirs.getTaskAttemptPath(taskAttempt));\r\n    }\r\n    return config;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getJobStageConfig",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StageConfig getJobStageConfig()\n{\r\n    return jobStageConfig;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setJobStageConfig",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobStageConfig(StageConfig jobStageConfig)\n{\r\n    this.jobStageConfig = jobStageConfig;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getDestDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getDestDir()\n{\r\n    return destDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setDestDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDestDir(Path destDir)\n{\r\n    this.destDir = destDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "executeTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<TaskManifest> executeTaskAttempts(int taskAttemptCount, int filesPerTaskAttempt) throws IOException\n{\r\n    try (DurationInfo di = new DurationInfo(LOG, true, \"create manifests\")) {\r\n        List<Integer> taskIdList = new ArrayList<>(taskAttemptCount);\r\n        for (int t = 0; t < taskAttemptCount; t++) {\r\n            taskIdList.add(t);\r\n        }\r\n        List<TaskManifest> manifests = Collections.synchronizedList(new ArrayList<>());\r\n        TaskPool.foreach(taskIdList).executeWith(getSubmitter()).stopOnFailure().run(i -> {\r\n            manifests.add(executeOneTaskAttempt(i, i & 0x03, filesPerTaskAttempt));\r\n        });\r\n        return manifests;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "executeOneTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "TaskManifest executeOneTaskAttempt(final int task, int taskAttempt, final int filesPerTaskAttempt) throws IOException\n{\r\n    String tid = String.format(\"task_%03d\", task);\r\n    String taskAttemptId = String.format(\"%s_%02d\", tid, taskAttempt);\r\n    synchronized (this) {\r\n        taskIds.add(tid);\r\n        taskAttemptIds.add(taskAttemptId);\r\n    }\r\n    StageConfig taskStageConfig = createTaskStageConfig(JOB1, tid, taskAttemptId);\r\n    LOG.info(\"Generating manifest for {}\", taskAttemptId);\r\n    new SetupTaskStage(taskStageConfig).apply(\"task \" + taskAttemptId);\r\n    final TaskManifest manifest = createTaskManifest(taskStageConfig);\r\n    Path taDir = taskStageConfig.getTaskAttemptDir();\r\n    long size = task * 1000_0000L;\r\n    for (int i = 0; i < filesPerTaskAttempt; i++) {\r\n        Path in = new Path(taDir, \"dir-\" + i);\r\n        Path out = new Path(getDestDir(), \"dir-\" + i);\r\n        manifest.addDirectory(DirEntry.dirEntry(out, 0, 1));\r\n        String name = taskStageConfig.getTaskAttemptId() + \".csv\";\r\n        Path src = new Path(in, name);\r\n        Path dest = new Path(out, name);\r\n        long fileSize = size + i * 1000L;\r\n        manifest.addFileToCommit(new FileEntry(src, dest, fileSize, Long.toString(fileSize, 16)));\r\n        totalDataSize.addAndGet(fileSize);\r\n    }\r\n    new SaveTaskManifestStage(taskStageConfig).apply(manifest);\r\n    return manifest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createTaskStageConfig",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "StageConfig createTaskStageConfig(final int jobId, final String tid, final String taskAttemptId)\n{\r\n    Path jobAttemptTaskSubDir = getJobStageConfig().getJobAttemptTaskSubDir();\r\n    StageConfig taskStageConfig = createStageConfigForJob(jobId, getDestDir()).withTaskId(tid).withTaskAttemptId(taskAttemptId).withTaskAttemptDir(new Path(jobAttemptTaskSubDir, taskAttemptId));\r\n    return taskStageConfig;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "verifyJobDirsCleanedUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyJobDirsCleanedUp() throws IOException\n{\r\n    StageConfig stageConfig = getJobStageConfig();\r\n    assertPathDoesNotExist(\"Job attempt dir\", stageConfig.getJobAttemptDir());\r\n    assertPathDoesNotExist(\"dest temp dir\", stageConfig.getOutputTempSubDir());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "lsR",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long lsR(FileSystem fileSystem, Path path, boolean recursive) throws Exception\n{\r\n    if (path == null) {\r\n        LOG.info(\"Empty path\");\r\n        return 0;\r\n    } else {\r\n        LOG.info(\"Listing of {}\", path);\r\n        final long count = RemoteIterators.foreach(fileSystem.listFiles(path, recursive), (status) -> LOG.info(\"{}\", status));\r\n        LOG.info(\"Count of entries: {}\", count);\r\n        return count;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "assertCleanupResult",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertCleanupResult(CleanupJobStage.Result result, CleanupJobStage.Outcome outcome, int expectedDirsDeleted)\n{\r\n    Assertions.assertThat(result.getOutcome()).describedAs(\"Outcome of cleanup() in %s\", result).isEqualTo(outcome);\r\n    if (expectedDirsDeleted >= 0) {\r\n        Assertions.assertThat(result.getDeleteCalls()).describedAs(\"Number of directories deleted in cleanup %s\", result).isEqualTo(expectedDirsDeleted);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "CleanupJobStage.Result cleanup(final boolean enabled, final boolean deleteTaskAttemptDirsInParallel, final boolean suppressExceptions, final CleanupJobStage.Outcome outcome, final int expectedDirsDeleted) throws IOException\n{\r\n    StageConfig stageConfig = getJobStageConfig();\r\n    CleanupJobStage.Result result = new CleanupJobStage(stageConfig).apply(new CleanupJobStage.Arguments(OP_STAGE_JOB_CLEANUP, enabled, deleteTaskAttemptDirsInParallel, suppressExceptions));\r\n    assertCleanupResult(result, outcome, expectedDirsDeleted);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "readText",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String readText(final Path path) throws IOException\n{\r\n    final FileSystem fs = getFileSystem();\r\n    final FileStatus st = fs.getFileStatus(path);\r\n    Assertions.assertThat(st.getLen()).describedAs(\"length of file %s\", st).isLessThanOrEqualTo(MAX_LEN);\r\n    return new String(readDataset(fs, path, (int) st.getLen()), StandardCharsets.UTF_8);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "progressOf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ProgressCounter progressOf(StageConfig stageConfig)\n{\r\n    return (ProgressCounter) stageConfig.getProgressable();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testSkipBadRecords",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testSkipBadRecords()\n{\r\n    Configuration conf = new Configuration();\r\n    assertEquals(2, SkipBadRecords.getAttemptsToStartSkipping(conf));\r\n    assertTrue(SkipBadRecords.getAutoIncrMapperProcCount(conf));\r\n    assertTrue(SkipBadRecords.getAutoIncrReducerProcCount(conf));\r\n    assertEquals(0, SkipBadRecords.getMapperMaxSkipRecords(conf));\r\n    assertEquals(0, SkipBadRecords.getReducerMaxSkipGroups(conf), 0);\r\n    assertNull(SkipBadRecords.getSkipOutputPath(conf));\r\n    SkipBadRecords.setAttemptsToStartSkipping(conf, 5);\r\n    SkipBadRecords.setAutoIncrMapperProcCount(conf, false);\r\n    SkipBadRecords.setAutoIncrReducerProcCount(conf, false);\r\n    SkipBadRecords.setMapperMaxSkipRecords(conf, 6L);\r\n    SkipBadRecords.setReducerMaxSkipGroups(conf, 7L);\r\n    JobConf jc = new JobConf();\r\n    SkipBadRecords.setSkipOutputPath(jc, new Path(\"test\"));\r\n    assertEquals(5, SkipBadRecords.getAttemptsToStartSkipping(conf));\r\n    assertFalse(SkipBadRecords.getAutoIncrMapperProcCount(conf));\r\n    assertFalse(SkipBadRecords.getAutoIncrReducerProcCount(conf));\r\n    assertEquals(6L, SkipBadRecords.getMapperMaxSkipRecords(conf));\r\n    assertEquals(7L, SkipBadRecords.getReducerMaxSkipGroups(conf), 0);\r\n    assertEquals(\"test\", SkipBadRecords.getSkipOutputPath(jc).toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup()\n{\r\n    FileUtil.fullyDelete(new File(testDir));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testTaskLog",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testTaskLog() throws IOException\n{\r\n    System.setProperty(YarnConfiguration.YARN_APP_CONTAINER_LOG_DIR, \"testString\");\r\n    assertThat(TaskLog.getMRv2LogDir()).isEqualTo(\"testString\");\r\n    TaskAttemptID taid = mock(TaskAttemptID.class);\r\n    JobID jid = new JobID(\"job\", 1);\r\n    when(taid.getJobID()).thenReturn(jid);\r\n    when(taid.toString()).thenReturn(\"JobId\");\r\n    File f = TaskLog.getTaskLogFile(taid, true, LogName.STDOUT);\r\n    assertTrue(f.getAbsolutePath().endsWith(\"testString\" + File.separatorChar + \"stdout\"));\r\n    File indexFile = TaskLog.getIndexFile(taid, true);\r\n    if (!indexFile.getParentFile().exists()) {\r\n        indexFile.getParentFile().mkdirs();\r\n    }\r\n    indexFile.delete();\r\n    indexFile.createNewFile();\r\n    TaskLog.syncLogs(testDir, taid, true);\r\n    assertTrue(indexFile.getAbsolutePath().endsWith(\"userlogs\" + File.separatorChar + \"job_job_0001\" + File.separatorChar + \"JobId.cleanup\" + File.separatorChar + \"log.index\"));\r\n    f = TaskLog.getRealTaskLogFileLocation(taid, true, LogName.DEBUGOUT);\r\n    if (f != null) {\r\n        assertTrue(f.getAbsolutePath().endsWith(testDirName + File.separatorChar + \"debugout\"));\r\n        FileUtils.copyFile(indexFile, f);\r\n    }\r\n    assertTrue(TaskLog.obtainLogDirOwner(taid).length() > 0);\r\n    assertTrue(readTaskLog(TaskLog.LogName.DEBUGOUT, taid, true).length() > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readTaskLog",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String readTaskLog(TaskLog.LogName filter, org.apache.hadoop.mapred.TaskAttemptID taskId, boolean isCleanup) throws IOException\n{\r\n    StringBuilder result = new StringBuilder();\r\n    int res;\r\n    InputStream taskLogReader = new TaskLog.Reader(taskId, filter, 0, -1, isCleanup);\r\n    byte[] b = new byte[65536];\r\n    while (true) {\r\n        res = taskLogReader.read(b);\r\n        if (res > 0) {\r\n            result.append(new String(b));\r\n        } else {\r\n            break;\r\n        }\r\n    }\r\n    taskLogReader.close();\r\n    String str = result.toString();\r\n    str = str.trim();\r\n    return str;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testTaskLogWithoutTaskLogDir",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testTaskLogWithoutTaskLogDir() throws IOException\n{\r\n    System.clearProperty(YarnConfiguration.YARN_APP_CONTAINER_LOG_DIR);\r\n    assertThat(TaskLog.getMRv2LogDir()).isNull();\r\n    TaskAttemptID taid = mock(TaskAttemptID.class);\r\n    JobID jid = new JobID(\"job\", 1);\r\n    when(taid.getJobID()).thenReturn(jid);\r\n    when(taid.toString()).thenReturn(\"JobId\");\r\n    File f = TaskLog.getTaskLogFile(taid, true, LogName.STDOUT);\r\n    assertTrue(f.getAbsolutePath().endsWith(\"stdout\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testClock",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testClock()\n{\r\n    Clock clock = new Clock();\r\n    long templateTime = System.currentTimeMillis();\r\n    long time = clock.getTime();\r\n    assertEquals(templateTime, time, 30);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    new File(System.getProperty(\"build.webapps\", \"build/webapps\") + \"/test\").mkdirs();\r\n    server = new HttpServer2.Builder().setName(\"test\").addEndpoint(URI.create(\"http://localhost:0\")).setFindPort(true).build();\r\n    server.addServlet(\"delay\", \"/delay\", DelayServlet.class);\r\n    server.addServlet(\"jobend\", \"/jobend\", JobEndServlet.class);\r\n    server.addServlet(\"fail\", \"/fail\", FailServlet.class);\r\n    server.start();\r\n    int port = server.getConnectorAddress(0).getPort();\r\n    baseUrl = new URL(\"http://localhost:\" + port + \"/\");\r\n    JobEndServlet.calledTimes = 0;\r\n    JobEndServlet.requestUri = null;\r\n    DelayServlet.calledTimes = 0;\r\n    FailServlet.calledTimes = 0;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    server.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLocalJobRunnerUriSubstitution",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testLocalJobRunnerUriSubstitution() throws InterruptedException\n{\r\n    JobStatus jobStatus = createTestJobStatus(\"job_20130313155005308_0001\", JobStatus.SUCCEEDED);\r\n    JobConf jobConf = createTestJobConf(new Configuration(), 0, baseUrl + \"jobend?jobid=$jobId&status=$jobStatus\");\r\n    JobEndNotifier.localRunnerNotification(jobConf, jobStatus);\r\n    assertEquals(1, JobEndServlet.calledTimes);\r\n    assertEquals(\"jobid=job_20130313155005308_0001&status=SUCCEEDED\", JobEndServlet.requestUri.getQuery());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLocalJobRunnerRetryCount",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testLocalJobRunnerRetryCount() throws InterruptedException\n{\r\n    int retryAttempts = 3;\r\n    JobStatus jobStatus = createTestJobStatus(\"job_20130313155005308_0001\", JobStatus.SUCCEEDED);\r\n    JobConf jobConf = createTestJobConf(new Configuration(), retryAttempts, baseUrl + \"fail\");\r\n    JobEndNotifier.localRunnerNotification(jobConf, jobStatus);\r\n    assertEquals(retryAttempts + 1, FailServlet.calledTimes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNotificationTimeout",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testNotificationTimeout() throws InterruptedException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(\"mapreduce.job.end-notification.timeout\", 1000);\r\n    JobStatus jobStatus = createTestJobStatus(\"job_20130313155005308_0001\", JobStatus.SUCCEEDED);\r\n    JobConf jobConf = createTestJobConf(conf, 0, baseUrl + \"delay\");\r\n    long startTime = System.currentTimeMillis();\r\n    JobEndNotifier.localRunnerNotification(jobConf, jobStatus);\r\n    long elapsedTime = System.currentTimeMillis() - startTime;\r\n    assertEquals(1, DelayServlet.calledTimes);\r\n    assertTrue(elapsedTime < 2000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createTestJobStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobStatus createTestJobStatus(String jobId, int state)\n{\r\n    return new JobStatus(JobID.forName(jobId), 0.5f, 0.0f, state, \"root\", \"TestJobEndNotifier\", null, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createTestJobConf",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobConf createTestJobConf(Configuration conf, int retryAttempts, String notificationUri)\n{\r\n    JobConf jobConf = new JobConf(conf);\r\n    jobConf.setInt(\"job.end.retry.attempts\", retryAttempts);\r\n    jobConf.set(\"job.end.retry.interval\", \"0\");\r\n    jobConf.setJobEndNotificationURI(notificationUri);\r\n    return jobConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getEnumCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counters getEnumCounters(Enum[] keys)\n{\r\n    Counters counters = new Counters();\r\n    for (Enum key : keys) {\r\n        for (long i = 0; i < MAX_VALUE; ++i) {\r\n            counters.incrCounter(key, i);\r\n        }\r\n    }\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getEnumCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counters getEnumCounters(String[] gNames, String[] cNames)\n{\r\n    Counters counters = new Counters();\r\n    for (String gName : gNames) {\r\n        for (String cName : cNames) {\r\n            for (long i = 0; i < MAX_VALUE; ++i) {\r\n                counters.incrCounter(gName, cName, i);\r\n            }\r\n        }\r\n    }\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCounter",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCounter(Counters counter) throws ParseException\n{\r\n    String compactEscapedString = counter.makeEscapedCompactString();\r\n    assertFalse(\"compactEscapedString should not contain null\", compactEscapedString.contains(\"null\"));\r\n    Counters recoveredCounter = Counters.fromEscapedCompactString(compactEscapedString);\r\n    assertEquals(\"Recovered counter does not match on content\", counter, recoveredCounter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCounters",
  "errType" : [ "ParseException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCounters() throws IOException\n{\r\n    Enum[] keysWithResource = { TaskCounter.MAP_INPUT_RECORDS, TaskCounter.MAP_OUTPUT_BYTES };\r\n    Enum[] keysWithoutResource = { myCounters.TEST1, myCounters.TEST2 };\r\n    String[] groups = { \"group1\", \"group2\", \"group{}()[]\" };\r\n    String[] counters = { \"counter1\", \"counter2\", \"counter{}()[]\" };\r\n    try {\r\n        testCounter(getEnumCounters(keysWithResource));\r\n        testCounter(getEnumCounters(keysWithoutResource));\r\n        testCounter(getEnumCounters(groups, counters));\r\n    } catch (ParseException pe) {\r\n        throw new IOException(pe);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCounterValue",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testCounterValue()\n{\r\n    Counters counters = new Counters();\r\n    final int NUMBER_TESTS = 100;\r\n    final int NUMBER_INC = 10;\r\n    final Random rand = new Random();\r\n    for (int i = 0; i < NUMBER_TESTS; i++) {\r\n        long initValue = rand.nextInt();\r\n        long expectedValue = initValue;\r\n        Counter counter = counters.findCounter(\"foo\", \"bar\");\r\n        counter.setValue(initValue);\r\n        assertEquals(\"Counter value is not initialized correctly\", expectedValue, counter.getValue());\r\n        for (int j = 0; j < NUMBER_INC; j++) {\r\n            int incValue = rand.nextInt();\r\n            counter.increment(incValue);\r\n            expectedValue += incValue;\r\n            assertEquals(\"Counter value is not incremented correctly\", expectedValue, counter.getValue());\r\n        }\r\n        expectedValue = rand.nextInt();\r\n        counter.setValue(expectedValue);\r\n        assertEquals(\"Counter value is not set correctly\", expectedValue, counter.getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReadWithLegacyNames",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testReadWithLegacyNames()\n{\r\n    Counters counters = new Counters();\r\n    counters.incrCounter(TaskCounter.MAP_INPUT_RECORDS, 1);\r\n    counters.incrCounter(JobCounter.DATA_LOCAL_MAPS, 1);\r\n    counters.findCounter(\"file\", FileSystemCounter.BYTES_READ).increment(1);\r\n    checkLegacyNames(counters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testWriteWithLegacyNames",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testWriteWithLegacyNames()\n{\r\n    Counters counters = new Counters();\r\n    counters.incrCounter(Task.Counter.MAP_INPUT_RECORDS, 1);\r\n    counters.incrCounter(JobInProgress.Counter.DATA_LOCAL_MAPS, 1);\r\n    counters.findCounter(\"FileSystemCounters\", \"FILE_BYTES_READ\").increment(1);\r\n    checkLegacyNames(counters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "checkLegacyNames",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void checkLegacyNames(Counters counters)\n{\r\n    assertEquals(\"New name\", 1, counters.findCounter(TaskCounter.class.getName(), \"MAP_INPUT_RECORDS\").getValue());\r\n    assertEquals(\"Legacy name\", 1, counters.findCounter(\"org.apache.hadoop.mapred.Task$Counter\", \"MAP_INPUT_RECORDS\").getValue());\r\n    assertEquals(\"Legacy enum\", 1, counters.findCounter(Task.Counter.MAP_INPUT_RECORDS).getValue());\r\n    assertEquals(\"New name\", 1, counters.findCounter(JobCounter.class.getName(), \"DATA_LOCAL_MAPS\").getValue());\r\n    assertEquals(\"Legacy name\", 1, counters.findCounter(\"org.apache.hadoop.mapred.JobInProgress$Counter\", \"DATA_LOCAL_MAPS\").getValue());\r\n    assertEquals(\"Legacy enum\", 1, counters.findCounter(JobInProgress.Counter.DATA_LOCAL_MAPS).getValue());\r\n    assertEquals(\"New name\", 1, counters.findCounter(FileSystemCounter.class.getName(), \"FILE_BYTES_READ\").getValue());\r\n    assertEquals(\"New name and method\", 1, counters.findCounter(\"file\", FileSystemCounter.BYTES_READ).getValue());\r\n    assertEquals(\"Legacy name\", 1, counters.findCounter(\"FileSystemCounters\", \"FILE_BYTES_READ\").getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCounterIteratorConcurrency",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCounterIteratorConcurrency()\n{\r\n    Counters counters = new Counters();\r\n    counters.incrCounter(\"group1\", \"counter1\", 1);\r\n    Iterator<Group> iterator = counters.iterator();\r\n    counters.incrCounter(\"group2\", \"counter2\", 1);\r\n    iterator.next();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGroupIteratorConcurrency",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testGroupIteratorConcurrency()\n{\r\n    Counters counters = new Counters();\r\n    counters.incrCounter(\"group1\", \"counter1\", 1);\r\n    Group group = counters.getGroup(\"group1\");\r\n    Iterator<Counter> iterator = group.iterator();\r\n    counters.incrCounter(\"group1\", \"counter2\", 1);\r\n    iterator.next();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFileSystemGroupIteratorConcurrency",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testFileSystemGroupIteratorConcurrency()\n{\r\n    Counters counters = new Counters();\r\n    counters.findCounter(\"fs1\", FileSystemCounter.BYTES_READ).increment(1);\r\n    counters.findCounter(\"fs2\", FileSystemCounter.BYTES_READ).increment(1);\r\n    Group group = counters.getGroup(FileSystemCounter.class.getName());\r\n    Iterator<Counter> iterator = group.iterator();\r\n    counters.findCounter(\"fs3\", FileSystemCounter.BYTES_READ).increment(1);\r\n    assertTrue(iterator.hasNext());\r\n    iterator.next();\r\n    counters.findCounter(\"fs3\", FileSystemCounter.BYTES_READ).increment(1);\r\n    assertTrue(iterator.hasNext());\r\n    iterator.next();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLegacyGetGroupNames",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testLegacyGetGroupNames()\n{\r\n    Counters counters = new Counters();\r\n    counters.findCounter(\"fs1\", FileSystemCounter.BYTES_READ).increment(1);\r\n    counters.findCounter(\"fs2\", FileSystemCounter.BYTES_READ).increment(1);\r\n    counters.incrCounter(\"group1\", \"counter1\", 1);\r\n    HashSet<String> groups = new HashSet<String>(counters.getGroupNames());\r\n    HashSet<String> expectedGroups = new HashSet<String>();\r\n    expectedGroups.add(\"group1\");\r\n    expectedGroups.add(\"FileSystemCounters\");\r\n    expectedGroups.add(\"org.apache.hadoop.mapreduce.FileSystemCounter\");\r\n    assertEquals(expectedGroups, groups);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMakeCompactString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMakeCompactString()\n{\r\n    final String GC1 = \"group1.counter1:1\";\r\n    final String GC2 = \"group2.counter2:3\";\r\n    Counters counters = new Counters();\r\n    counters.incrCounter(\"group1\", \"counter1\", 1);\r\n    assertEquals(\"group1.counter1:1\", counters.makeCompactString());\r\n    counters.incrCounter(\"group2\", \"counter2\", 3);\r\n    String cs = counters.makeCompactString();\r\n    assertTrue(\"Bad compact string\", cs.equals(GC1 + ',' + GC2) || cs.equals(GC2 + ',' + GC1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCounterLimits",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCounterLimits()\n{\r\n    testMaxCountersLimits(new Counters());\r\n    testMaxGroupsLimits(new Counters());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMaxCountersLimits",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testMaxCountersLimits(final Counters counters)\n{\r\n    for (int i = 0; i < org.apache.hadoop.mapred.Counters.MAX_COUNTER_LIMIT; ++i) {\r\n        counters.findCounter(\"test\", \"test\" + i);\r\n    }\r\n    setExpected(counters);\r\n    shouldThrow(CountersExceededException.class, new Runnable() {\r\n\r\n        public void run() {\r\n            counters.findCounter(\"test\", \"bad\");\r\n        }\r\n    });\r\n    checkExpected(counters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMaxGroupsLimits",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testMaxGroupsLimits(final Counters counters)\n{\r\n    for (int i = 0; i < org.apache.hadoop.mapred.Counters.MAX_GROUP_LIMIT; ++i) {\r\n        counters.findCounter(\"test\" + i, \"test\");\r\n    }\r\n    setExpected(counters);\r\n    shouldThrow(CountersExceededException.class, new Runnable() {\r\n\r\n        public void run() {\r\n            counters.findCounter(\"bad\", \"test\");\r\n        }\r\n    });\r\n    checkExpected(counters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setExpected",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setExpected(Counters counters)\n{\r\n    counters.findCounter(FRAMEWORK_COUNTER).setValue(FRAMEWORK_COUNTER_VALUE);\r\n    counters.findCounter(FS_SCHEME, FS_COUNTER).setValue(FS_COUNTER_VALUE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "checkExpected",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkExpected(Counters counters)\n{\r\n    assertEquals(FRAMEWORK_COUNTER_VALUE, counters.findCounter(FRAMEWORK_COUNTER).getValue());\r\n    assertEquals(FS_COUNTER_VALUE, counters.findCounter(FS_SCHEME, FS_COUNTER).getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "shouldThrow",
  "errType" : [ "CountersExceededException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void shouldThrow(Class<? extends Exception> ecls, Runnable runnable)\n{\r\n    try {\r\n        runnable.run();\r\n    } catch (CountersExceededException e) {\r\n        return;\r\n    }\r\n    Assert.fail(\"Should've thrown \" + ecls.getSimpleName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    new TestCounters().testCounters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFrameworkCounter",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testFrameworkCounter()\n{\r\n    GroupFactory groupFactory = new GroupFactoryForTest();\r\n    FrameworkGroupFactory frameworkGroupFactory = groupFactory.newFrameworkGroupFactory(JobCounter.class);\r\n    Group group = (Group) frameworkGroupFactory.newGroup(\"JobCounter\");\r\n    FrameworkCounterGroup counterGroup = (FrameworkCounterGroup) group.getUnderlyingGroup();\r\n    org.apache.hadoop.mapreduce.Counter count1 = counterGroup.findCounter(JobCounter.NUM_FAILED_MAPS.toString());\r\n    Assert.assertNotNull(count1);\r\n    org.apache.hadoop.mapreduce.Counter count2 = counterGroup.findCounter(\"Unknown\");\r\n    Assert.assertNull(count2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testTaskCounter",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testTaskCounter()\n{\r\n    GroupFactory groupFactory = new GroupFactoryForTest();\r\n    FrameworkGroupFactory frameworkGroupFactory = groupFactory.newFrameworkGroupFactory(TaskCounter.class);\r\n    Group group = (Group) frameworkGroupFactory.newGroup(\"TaskCounter\");\r\n    FrameworkCounterGroup counterGroup = (FrameworkCounterGroup) group.getUnderlyingGroup();\r\n    org.apache.hadoop.mapreduce.Counter count1 = counterGroup.findCounter(TaskCounter.PHYSICAL_MEMORY_BYTES.toString());\r\n    Assert.assertNotNull(count1);\r\n    count1.increment(10);\r\n    count1.increment(10);\r\n    Assert.assertEquals(20, count1.getValue());\r\n    org.apache.hadoop.mapreduce.Counter count2 = counterGroup.findCounter(TaskCounter.MAP_PHYSICAL_MEMORY_BYTES_MAX.toString());\r\n    Assert.assertNotNull(count2);\r\n    count2.increment(5);\r\n    count2.increment(10);\r\n    Assert.assertEquals(10, count2.getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFilesystemCounter",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testFilesystemCounter()\n{\r\n    GroupFactory groupFactory = new GroupFactoryForTest();\r\n    Group fsGroup = groupFactory.newFileSystemGroup();\r\n    org.apache.hadoop.mapreduce.Counter count1 = fsGroup.findCounter(\"ANY_BYTES_READ\");\r\n    Assert.assertNotNull(count1);\r\n    org.apache.hadoop.mapreduce.Counter count2 = fsGroup.findCounter(\"Unknown\");\r\n    Assert.assertNull(count2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "newFrameworkGroupFactory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FrameworkGroupFactory<Group> newFrameworkGroupFactory(final Class<T> cls)\n{\r\n    return super.newFrameworkGroupFactory(cls);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "newFileSystemGroup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Group newFileSystemGroup()\n{\r\n    return super.newFileSystemGroup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup() throws Exception\n{\r\n    remoteFs.delete(remoteStagingDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    localFs = FileSystem.getLocal(conf);\r\n    testRootDir = new Path(\"target\", TestJobResourceUploaderWithSharedCache.class.getName() + \"-tmpDir\").makeQualified(localFs.getUri(), localFs.getWorkingDirectory());\r\n    dfs = new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();\r\n    remoteFs = dfs.getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "tearDown",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void tearDown()\n{\r\n    try {\r\n        if (localFs != null) {\r\n            localFs.close();\r\n        }\r\n        if (remoteFs != null) {\r\n            remoteFs.close();\r\n        }\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n    } catch (IOException ioe) {\r\n        LOG.info(\"IO exception in closing file system\");\r\n        ioe.printStackTrace();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testSharedCacheDisabled",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSharedCacheDisabled() throws Exception\n{\r\n    JobConf jobConf = createJobConf();\r\n    Job job = new Job(jobConf);\r\n    job.setJobID(new JobID(\"567789\", 1));\r\n    uploadFilesToRemoteFS(job, jobConf, 0, 0, 0, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testSharedCacheEnabled",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSharedCacheEnabled() throws Exception\n{\r\n    JobConf jobConf = createJobConf();\r\n    jobConf.set(MRJobConfig.SHARED_CACHE_MODE, \"enabled\");\r\n    Job job = new Job(jobConf);\r\n    job.setJobID(new JobID(\"567789\", 1));\r\n    uploadFilesToRemoteFS(job, jobConf, 8, 3, 2, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testSharedCacheEnabledWithJobJarInSharedCache",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSharedCacheEnabledWithJobJarInSharedCache() throws Exception\n{\r\n    JobConf jobConf = createJobConf();\r\n    jobConf.set(MRJobConfig.SHARED_CACHE_MODE, \"enabled\");\r\n    Job job = new Job(jobConf);\r\n    job.setJobID(new JobID(\"567789\", 1));\r\n    uploadFilesToRemoteFS(job, jobConf, 8, 3, 2, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testSharedCacheArchivesAndLibjarsEnabled",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSharedCacheArchivesAndLibjarsEnabled() throws Exception\n{\r\n    JobConf jobConf = createJobConf();\r\n    jobConf.set(MRJobConfig.SHARED_CACHE_MODE, \"archives,libjars\");\r\n    Job job = new Job(jobConf);\r\n    job.setJobID(new JobID(\"567789\", 1));\r\n    uploadFilesToRemoteFS(job, jobConf, 5, 1, 2, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createJobConf",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobConf createJobConf()\n{\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n    jobConf.setBoolean(YarnConfiguration.SHARED_CACHE_ENABLED, true);\r\n    jobConf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, remoteFs.getUri().toString());\r\n    return jobConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "copyToRemote",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path copyToRemote(Path jar) throws IOException\n{\r\n    Path remoteFile = new Path(\"/tmp\", jar.getName());\r\n    remoteFs.copyFromLocalFile(jar, remoteFile);\r\n    return remoteFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "makeJarAvailableInSharedCache",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void makeJarAvailableInSharedCache(Path jar, MyFileUploader fileUploader) throws YarnException, IOException\n{\r\n    Path remoteFile = copyToRemote(jar);\r\n    fileUploader.mockFileInSharedCache(jar, URL.fromPath(remoteFile));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "uploadFilesToRemoteFS",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "void uploadFilesToRemoteFS(Job job, JobConf jobConf, int useCallCountExpected, int numOfFilesShouldBeUploadedToSharedCacheExpected, int numOfArchivesShouldBeUploadedToSharedCacheExpected, boolean jobJarInSharedCacheBeforeUpload) throws Exception\n{\r\n    MyFileUploader fileUploader = new MyFileUploader(remoteFs, jobConf);\r\n    SharedCacheConfig sharedCacheConfig = new SharedCacheConfig();\r\n    sharedCacheConfig.init(jobConf);\r\n    Path firstFile = createTempFile(\"first-input-file\", \"x\");\r\n    Path secondFile = createTempFile(\"second-input-file\", \"xx\");\r\n    boolean fileAdded = Job.addFileToSharedCache(firstFile.toUri(), jobConf);\r\n    assertEquals(sharedCacheConfig.isSharedCacheFilesEnabled(), fileAdded);\r\n    if (!fileAdded) {\r\n        Path remoteFile = copyToRemote(firstFile);\r\n        job.addCacheFile(remoteFile.toUri());\r\n    }\r\n    jobConf.set(\"tmpfiles\", secondFile.toString());\r\n    Path firstJar = makeJar(new Path(testRootDir, \"distributed.first.jar\"), 1);\r\n    Path secondJar = makeJar(new Path(testRootDir, \"distributed.second.jar\"), 2);\r\n    Path thirdJar = new Path(testRootDir, \"distributed.third.jar\");\r\n    localFs.copyFromLocalFile(secondJar, thirdJar);\r\n    makeJarAvailableInSharedCache(secondJar, fileUploader);\r\n    boolean libjarAdded = Job.addFileToSharedCacheAndClasspath(firstJar.toUri(), jobConf);\r\n    assertEquals(sharedCacheConfig.isSharedCacheLibjarsEnabled(), libjarAdded);\r\n    if (!libjarAdded) {\r\n        Path remoteJar = copyToRemote(firstJar);\r\n        job.addFileToClassPath(remoteJar);\r\n    }\r\n    jobConf.set(\"tmpjars\", secondJar.toString() + \",\" + thirdJar.toString());\r\n    Path firstArchive = makeArchive(\"first-archive.zip\", \"first-file\");\r\n    Path secondArchive = makeArchive(\"second-archive.zip\", \"second-file\");\r\n    boolean archiveAdded = Job.addArchiveToSharedCache(firstArchive.toUri(), jobConf);\r\n    assertEquals(sharedCacheConfig.isSharedCacheArchivesEnabled(), archiveAdded);\r\n    if (!archiveAdded) {\r\n        Path remoteArchive = copyToRemote(firstArchive);\r\n        job.addCacheArchive(remoteArchive.toUri());\r\n    }\r\n    jobConf.set(\"tmparchives\", secondArchive.toString());\r\n    Path jobJar = makeJar(new Path(testRootDir, \"test-job.jar\"), 4);\r\n    if (jobJarInSharedCacheBeforeUpload) {\r\n        makeJarAvailableInSharedCache(jobJar, fileUploader);\r\n    }\r\n    jobConf.setJar(jobJar.toString());\r\n    fileUploader.uploadResources(job, remoteStagingDir);\r\n    verify(fileUploader.mockscClient, times(useCallCountExpected)).use(any(ApplicationId.class), anyString());\r\n    int numOfFilesShouldBeUploadedToSharedCache = 0;\r\n    Map<String, Boolean> filesSharedCacheUploadPolicies = Job.getFileSharedCacheUploadPolicies(jobConf);\r\n    for (Boolean policy : filesSharedCacheUploadPolicies.values()) {\r\n        if (policy) {\r\n            numOfFilesShouldBeUploadedToSharedCache++;\r\n        }\r\n    }\r\n    assertEquals(numOfFilesShouldBeUploadedToSharedCacheExpected, numOfFilesShouldBeUploadedToSharedCache);\r\n    int numOfArchivesShouldBeUploadedToSharedCache = 0;\r\n    Map<String, Boolean> archivesSharedCacheUploadPolicies = Job.getArchiveSharedCacheUploadPolicies(jobConf);\r\n    for (Boolean policy : archivesSharedCacheUploadPolicies.values()) {\r\n        if (policy) {\r\n            numOfArchivesShouldBeUploadedToSharedCache++;\r\n        }\r\n    }\r\n    assertEquals(numOfArchivesShouldBeUploadedToSharedCacheExpected, numOfArchivesShouldBeUploadedToSharedCache);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createTempFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path createTempFile(String filename, String contents) throws IOException\n{\r\n    Path path = new Path(testRootDir, filename);\r\n    FSDataOutputStream os = localFs.create(path);\r\n    os.writeBytes(contents);\r\n    os.close();\r\n    localFs.setPermission(path, new FsPermission(\"700\"));\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "makeJar",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Path makeJar(Path p, int index) throws FileNotFoundException, IOException\n{\r\n    FileOutputStream fos = new FileOutputStream(new File(p.toUri().getPath()));\r\n    JarOutputStream jos = new JarOutputStream(fos);\r\n    ZipEntry ze = new ZipEntry(\"distributed.jar.inside\" + index);\r\n    jos.putNextEntry(ze);\r\n    jos.write((\"inside the jar!\" + index).getBytes());\r\n    jos.closeEntry();\r\n    jos.close();\r\n    localFs.setPermission(p, new FsPermission(\"700\"));\r\n    return p;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "makeArchive",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Path makeArchive(String archiveFile, String filename) throws Exception\n{\r\n    Path archive = new Path(testRootDir, archiveFile);\r\n    Path file = new Path(testRootDir, filename);\r\n    DataOutputStream out = localFs.create(archive);\r\n    ZipOutputStream zos = new ZipOutputStream(out);\r\n    ZipEntry ze = new ZipEntry(file.toString());\r\n    zos.putNextEntry(ze);\r\n    zos.write(input.getBytes(\"UTF-8\"));\r\n    zos.closeEntry();\r\n    zos.close();\r\n    return archive;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    conf = new Configuration(false);\r\n    conf.set(\"fs.file.impl\", MockFileSystem.class.getName());\r\n    fileSys = FileSystem.getLocal(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "after",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void after()\n{\r\n    if (mkdirs) {\r\n        FileUtil.fullyDelete(dir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testExecutorsShutDown",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testExecutorsShutDown() throws Exception\n{\r\n    Path scanPath = new Path(dir.getAbsolutePath());\r\n    mkdirs = fileSys.mkdirs(scanPath);\r\n    Path[] dirs = new Path[] { scanPath };\r\n    final LocatedFileStatusFetcher fetcher = new LocatedFileStatusFetcher(conf, dirs, true, new PathFilter() {\r\n\r\n        @Override\r\n        public boolean accept(Path path) {\r\n            return true;\r\n        }\r\n    }, true);\r\n    Thread t = new Thread() {\r\n\r\n        @Override\r\n        public void run() {\r\n            try {\r\n                fetcher.getFileStatuses();\r\n            } catch (Exception e) {\r\n                Assert.assertTrue(e instanceof InterruptedException);\r\n            }\r\n        }\r\n    };\r\n    t.start();\r\n    LATCH.await();\r\n    t.interrupt();\r\n    t.join();\r\n    Assert.assertTrue(\"The executor service should have been shut down\", fetcher.getListeningExecutorService().isShutdown());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testPluginAbility",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testPluginAbility()\n{\r\n    try {\r\n        JobConf jobConf = new JobConf();\r\n        jobConf.setClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, TestShufflePlugin.TestShuffleConsumerPlugin.class, ShuffleConsumerPlugin.class);\r\n        ShuffleConsumerPlugin shuffleConsumerPlugin = null;\r\n        Class<? extends ShuffleConsumerPlugin> clazz = jobConf.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, Shuffle.class, ShuffleConsumerPlugin.class);\r\n        assertNotNull(\"Unable to get \" + MRConfig.SHUFFLE_CONSUMER_PLUGIN, clazz);\r\n        shuffleConsumerPlugin = ReflectionUtils.newInstance(clazz, jobConf);\r\n        assertNotNull(\"Unable to load \" + MRConfig.SHUFFLE_CONSUMER_PLUGIN, shuffleConsumerPlugin);\r\n    } catch (Exception e) {\r\n        assertTrue(\"Threw exception:\" + e, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testConsumerApi",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testConsumerApi()\n{\r\n    JobConf jobConf = new JobConf();\r\n    ShuffleConsumerPlugin<K, V> shuffleConsumerPlugin = new TestShuffleConsumerPlugin<K, V>();\r\n    ReduceTask mockReduceTask = mock(ReduceTask.class);\r\n    TaskUmbilicalProtocol mockUmbilical = mock(TaskUmbilicalProtocol.class);\r\n    Reporter mockReporter = mock(Reporter.class);\r\n    FileSystem mockFileSystem = mock(FileSystem.class);\r\n    Class<? extends org.apache.hadoop.mapred.Reducer> combinerClass = jobConf.getCombinerClass();\r\n    @SuppressWarnings(\"unchecked\")\r\n    CombineOutputCollector<K, V> mockCombineOutputCollector = (CombineOutputCollector<K, V>) mock(CombineOutputCollector.class);\r\n    org.apache.hadoop.mapreduce.TaskAttemptID mockTaskAttemptID = mock(org.apache.hadoop.mapreduce.TaskAttemptID.class);\r\n    LocalDirAllocator mockLocalDirAllocator = mock(LocalDirAllocator.class);\r\n    CompressionCodec mockCompressionCodec = mock(CompressionCodec.class);\r\n    Counter mockCounter = mock(Counter.class);\r\n    TaskStatus mockTaskStatus = mock(TaskStatus.class);\r\n    Progress mockProgress = mock(Progress.class);\r\n    MapOutputFile mockMapOutputFile = mock(MapOutputFile.class);\r\n    Task mockTask = mock(Task.class);\r\n    try {\r\n        String[] dirs = jobConf.getLocalDirs();\r\n        ShuffleConsumerPlugin.Context<K, V> context = new ShuffleConsumerPlugin.Context<K, V>(mockTaskAttemptID, jobConf, mockFileSystem, mockUmbilical, mockLocalDirAllocator, mockReporter, mockCompressionCodec, combinerClass, mockCombineOutputCollector, mockCounter, mockCounter, mockCounter, mockCounter, mockCounter, mockCounter, mockTaskStatus, mockProgress, mockProgress, mockTask, mockMapOutputFile, null);\r\n        shuffleConsumerPlugin.init(context);\r\n        shuffleConsumerPlugin.run();\r\n        shuffleConsumerPlugin.close();\r\n    } catch (Exception e) {\r\n        assertTrue(\"Threw exception:\" + e, false);\r\n    }\r\n    mockReduceTask.getTaskID();\r\n    mockReduceTask.getJobID();\r\n    mockReduceTask.getNumMaps();\r\n    mockReduceTask.getPartition();\r\n    mockReporter.progress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testProviderApi",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testProviderApi()\n{\r\n    LocalDirAllocator mockLocalDirAllocator = mock(LocalDirAllocator.class);\r\n    JobConf mockJobConf = mock(JobConf.class);\r\n    try {\r\n        mockLocalDirAllocator.getLocalPathToRead(\"\", mockJobConf);\r\n    } catch (Exception e) {\r\n        assertTrue(\"Threw exception:\" + e, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testGetJobID",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetJobID()\n{\r\n    JobID jobId = new JobID(\"1234\", 0);\r\n    TaskID taskId = new TaskID(jobId, TaskType.MAP, 0);\r\n    assertSame(\"TaskID did not store the JobID correctly\", jobId, taskId.getJobID());\r\n    taskId = new TaskID();\r\n    assertEquals(\"Job ID was set unexpectedly in default contsructor\", \"\", taskId.getJobID().getJtIdentifier());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testIsMap",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testIsMap()\n{\r\n    JobID jobId = new JobID(\"1234\", 0);\r\n    for (TaskType type : TaskType.values()) {\r\n        TaskID taskId = new TaskID(jobId, type, 0);\r\n        if (type == TaskType.MAP) {\r\n            assertTrue(\"TaskID for map task did not correctly identify itself \" + \"as a map task\", taskId.isMap());\r\n        } else {\r\n            assertFalse(\"TaskID for \" + type + \" task incorrectly identified \" + \"itself as a map task\", taskId.isMap());\r\n        }\r\n    }\r\n    TaskID taskId = new TaskID();\r\n    assertFalse(\"TaskID of default type incorrectly identified itself as a \" + \"map task\", taskId.isMap());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testGetTaskType0args",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGetTaskType0args()\n{\r\n    JobID jobId = new JobID(\"1234\", 0);\r\n    for (TaskType type : TaskType.values()) {\r\n        TaskID taskId = new TaskID(jobId, type, 0);\r\n        assertEquals(\"TaskID incorrectly reported its type\", type, taskId.getTaskType());\r\n    }\r\n    TaskID taskId = new TaskID();\r\n    assertEquals(\"TaskID of default type incorrectly reported its type\", TaskType.REDUCE, taskId.getTaskType());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testEquals",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testEquals()\n{\r\n    JobID jobId1 = new JobID(\"1234\", 1);\r\n    JobID jobId2 = new JobID(\"2345\", 2);\r\n    TaskID taskId1 = new TaskID(jobId1, TaskType.MAP, 0);\r\n    TaskID taskId2 = new TaskID(jobId1, TaskType.MAP, 0);\r\n    assertTrue(\"The equals() method reported two equal task IDs were not equal\", taskId1.equals(taskId2));\r\n    taskId2 = new TaskID(jobId2, TaskType.MAP, 0);\r\n    assertFalse(\"The equals() method reported two task IDs with different \" + \"job IDs were equal\", taskId1.equals(taskId2));\r\n    taskId2 = new TaskID(jobId1, TaskType.MAP, 1);\r\n    assertFalse(\"The equals() method reported two task IDs with different IDs \" + \"were equal\", taskId1.equals(taskId2));\r\n    TaskType[] types = TaskType.values();\r\n    for (int i = 0; i < types.length; i++) {\r\n        for (int j = 0; j < types.length; j++) {\r\n            taskId1 = new TaskID(jobId1, types[i], 0);\r\n            taskId2 = new TaskID(jobId1, types[j], 0);\r\n            if (i == j) {\r\n                assertTrue(\"The equals() method reported two equal task IDs were not \" + \"equal\", taskId1.equals(taskId2));\r\n            } else {\r\n                assertFalse(\"The equals() method reported two task IDs with \" + \"different types were equal\", taskId1.equals(taskId2));\r\n            }\r\n        }\r\n    }\r\n    assertFalse(\"The equals() method matched against a JobID object\", taskId1.equals(jobId1));\r\n    assertFalse(\"The equals() method matched against a null object\", taskId1.equals(null));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testCompareTo",
  "errType" : [ "ClassCastException", "NullPointerException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testCompareTo()\n{\r\n    JobID jobId = new JobID(\"1234\", 1);\r\n    TaskID taskId1 = new TaskID(jobId, TaskType.REDUCE, 0);\r\n    TaskID taskId2 = new TaskID(jobId, TaskType.REDUCE, 0);\r\n    assertEquals(\"The compareTo() method returned non-zero for two equal \" + \"task IDs\", 0, taskId1.compareTo(taskId2));\r\n    taskId2 = new TaskID(jobId, TaskType.MAP, 1);\r\n    assertTrue(\"The compareTo() method did not weigh task type more than task \" + \"ID\", taskId1.compareTo(taskId2) > 0);\r\n    TaskType[] types = TaskType.values();\r\n    for (int i = 0; i < types.length; i++) {\r\n        for (int j = 0; j < types.length; j++) {\r\n            taskId1 = new TaskID(jobId, types[i], 0);\r\n            taskId2 = new TaskID(jobId, types[j], 0);\r\n            if (i == j) {\r\n                assertEquals(\"The compareTo() method returned non-zero for two equal \" + \"task IDs\", 0, taskId1.compareTo(taskId2));\r\n            } else if (i < j) {\r\n                assertTrue(\"The compareTo() method did not order \" + types[i] + \" before \" + types[j], taskId1.compareTo(taskId2) < 0);\r\n            } else {\r\n                assertTrue(\"The compareTo() method did not order \" + types[i] + \" after \" + types[j], taskId1.compareTo(taskId2) > 0);\r\n            }\r\n        }\r\n    }\r\n    try {\r\n        taskId1.compareTo(jobId);\r\n        fail(\"The compareTo() method allowed comparison to a JobID object\");\r\n    } catch (ClassCastException ex) {\r\n    }\r\n    try {\r\n        taskId1.compareTo(null);\r\n        fail(\"The compareTo() method allowed comparison to a null object\");\r\n    } catch (NullPointerException ex) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testToString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testToString()\n{\r\n    JobID jobId = new JobID(\"1234\", 1);\r\n    for (TaskType type : TaskType.values()) {\r\n        TaskID taskId = new TaskID(jobId, type, 0);\r\n        String str = String.format(\"task_1234_0001_%c_000000\", TaskID.getRepresentingCharacter(type));\r\n        assertEquals(\"The toString() method returned the wrong value\", str, taskId.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testAppendTo",
  "errType" : [ "NullPointerException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAppendTo()\n{\r\n    JobID jobId = new JobID(\"1234\", 1);\r\n    StringBuilder builder = new StringBuilder();\r\n    for (TaskType type : TaskType.values()) {\r\n        builder.setLength(0);\r\n        TaskID taskId = new TaskID(jobId, type, 0);\r\n        String str = String.format(\"_1234_0001_%c_000000\", TaskID.getRepresentingCharacter(type));\r\n        assertEquals(\"The appendTo() method appended the wrong value\", str, taskId.appendTo(builder).toString());\r\n    }\r\n    try {\r\n        new TaskID().appendTo(null);\r\n        fail(\"The appendTo() method allowed a null builder\");\r\n    } catch (NullPointerException ex) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testHashCode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testHashCode()\n{\r\n    TaskType[] types = TaskType.values();\r\n    for (int i = 0; i < types.length; i++) {\r\n        JobID jobId = new JobID(\"1234\" + i, i);\r\n        TaskID taskId1 = new TaskID(jobId, types[i], i);\r\n        TaskID taskId2 = new TaskID(jobId, types[i], i);\r\n        assertTrue(\"The hashcode() method gave unequal hash codes for two equal \" + \"task IDs\", taskId1.hashCode() == taskId2.hashCode());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testReadFields",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testReadFields() throws Exception\n{\r\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\r\n    DataOutputStream out = new DataOutputStream(baos);\r\n    out.writeInt(0);\r\n    out.writeInt(1);\r\n    WritableUtils.writeVInt(out, 4);\r\n    out.write(new byte[] { 0x31, 0x32, 0x33, 0x34 });\r\n    WritableUtils.writeEnum(out, TaskType.REDUCE);\r\n    DataInputByteBuffer in = new DataInputByteBuffer();\r\n    in.reset(ByteBuffer.wrap(baos.toByteArray()));\r\n    TaskID instance = new TaskID();\r\n    instance.readFields(in);\r\n    assertEquals(\"The readFields() method did not produce the expected task ID\", \"task_1234_0001_r_000000\", instance.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testWrite",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testWrite() throws Exception\n{\r\n    JobID jobId = new JobID(\"1234\", 1);\r\n    TaskID taskId = new TaskID(jobId, TaskType.JOB_SETUP, 0);\r\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\r\n    DataOutputStream out = new DataOutputStream(baos);\r\n    taskId.write(out);\r\n    DataInputByteBuffer in = new DataInputByteBuffer();\r\n    byte[] buffer = new byte[4];\r\n    in.reset(ByteBuffer.wrap(baos.toByteArray()));\r\n    assertEquals(\"The write() method did not write the expected task ID\", 0, in.readInt());\r\n    assertEquals(\"The write() method did not write the expected job ID\", 1, in.readInt());\r\n    assertEquals(\"The write() method did not write the expected job \" + \"identifier length\", 4, WritableUtils.readVInt(in));\r\n    in.readFully(buffer, 0, 4);\r\n    assertEquals(\"The write() method did not write the expected job \" + \"identifier length\", \"1234\", new String(buffer));\r\n    assertEquals(\"The write() method did not write the expected task type\", TaskType.JOB_SETUP, WritableUtils.readEnum(in, TaskType.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testForName",
  "errType" : [ "IllegalArgumentException", "IllegalArgumentException", "IllegalArgumentException", "IllegalArgumentException", "IllegalArgumentException", "IllegalArgumentException", "IllegalArgumentException", "IllegalArgumentException", "IllegalArgumentException", "IllegalArgumentException" ],
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testForName()\n{\r\n    assertEquals(\"The forName() method did not parse the task ID string \" + \"correctly\", \"task_1_0001_m_000000\", TaskID.forName(\"task_1_0001_m_000\").toString());\r\n    assertEquals(\"The forName() method did not parse the task ID string \" + \"correctly\", \"task_23_0002_r_000001\", TaskID.forName(\"task_23_0002_r_0001\").toString());\r\n    assertEquals(\"The forName() method did not parse the task ID string \" + \"correctly\", \"task_345_0003_s_000002\", TaskID.forName(\"task_345_0003_s_00002\").toString());\r\n    assertEquals(\"The forName() method did not parse the task ID string \" + \"correctly\", \"task_6789_0004_c_000003\", TaskID.forName(\"task_6789_0004_c_000003\").toString());\r\n    assertEquals(\"The forName() method did not parse the task ID string \" + \"correctly\", \"task_12345_0005_t_4000000\", TaskID.forName(\"task_12345_0005_t_4000000\").toString());\r\n    try {\r\n        TaskID.forName(\"tisk_12345_0005_t_4000000\");\r\n        fail(\"The forName() method parsed an invalid job ID: \" + \"tisk_12345_0005_t_4000000\");\r\n    } catch (IllegalArgumentException ex) {\r\n    }\r\n    try {\r\n        TaskID.forName(\"tisk_12345_0005_t_4000000\");\r\n        fail(\"The forName() method parsed an invalid job ID: \" + \"tisk_12345_0005_t_4000000\");\r\n    } catch (IllegalArgumentException ex) {\r\n    }\r\n    try {\r\n        TaskID.forName(\"task_abc_0005_t_4000000\");\r\n        fail(\"The forName() method parsed an invalid job ID: \" + \"task_abc_0005_t_4000000\");\r\n    } catch (IllegalArgumentException ex) {\r\n    }\r\n    try {\r\n        TaskID.forName(\"task_12345_xyz_t_4000000\");\r\n        fail(\"The forName() method parsed an invalid job ID: \" + \"task_12345_xyz_t_4000000\");\r\n    } catch (IllegalArgumentException ex) {\r\n    }\r\n    try {\r\n        TaskID.forName(\"task_12345_0005_x_4000000\");\r\n        fail(\"The forName() method parsed an invalid job ID: \" + \"task_12345_0005_x_4000000\");\r\n    } catch (IllegalArgumentException ex) {\r\n    }\r\n    try {\r\n        TaskID.forName(\"task_12345_0005_t_jkl\");\r\n        fail(\"The forName() method parsed an invalid job ID: \" + \"task_12345_0005_t_jkl\");\r\n    } catch (IllegalArgumentException ex) {\r\n    }\r\n    try {\r\n        TaskID.forName(\"task_12345_0005_t\");\r\n        fail(\"The forName() method parsed an invalid job ID: \" + \"task_12345_0005_t\");\r\n    } catch (IllegalArgumentException ex) {\r\n    }\r\n    try {\r\n        TaskID.forName(\"task_12345_0005_4000000\");\r\n        fail(\"The forName() method parsed an invalid job ID: \" + \"task_12345_0005_4000000\");\r\n    } catch (IllegalArgumentException ex) {\r\n    }\r\n    try {\r\n        TaskID.forName(\"task_12345_t_4000000\");\r\n        fail(\"The forName() method parsed an invalid job ID: \" + \"task_12345_t_4000000\");\r\n    } catch (IllegalArgumentException ex) {\r\n    }\r\n    try {\r\n        TaskID.forName(\"12345_0005_t_4000000\");\r\n        fail(\"The forName() method parsed an invalid job ID: \" + \"12345_0005_t_4000000\");\r\n    } catch (IllegalArgumentException ex) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 10,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testGetRepresentingCharacter",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testGetRepresentingCharacter()\n{\r\n    assertEquals(\"The getRepresentingCharacter() method did not return the \" + \"expected character\", 'm', TaskID.getRepresentingCharacter(TaskType.MAP));\r\n    assertEquals(\"The getRepresentingCharacter() method did not return the \" + \"expected character\", 'r', TaskID.getRepresentingCharacter(TaskType.REDUCE));\r\n    assertEquals(\"The getRepresentingCharacter() method did not return the \" + \"expected character\", 's', TaskID.getRepresentingCharacter(TaskType.JOB_SETUP));\r\n    assertEquals(\"The getRepresentingCharacter() method did not return the \" + \"expected character\", 'c', TaskID.getRepresentingCharacter(TaskType.JOB_CLEANUP));\r\n    assertEquals(\"The getRepresentingCharacter() method did not return the \" + \"expected character\", 't', TaskID.getRepresentingCharacter(TaskType.TASK_CLEANUP));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testGetTaskTypeChar",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testGetTaskTypeChar()\n{\r\n    assertEquals(\"The getTaskType() method did not return the expected type\", TaskType.MAP, TaskID.getTaskType('m'));\r\n    assertEquals(\"The getTaskType() method did not return the expected type\", TaskType.REDUCE, TaskID.getTaskType('r'));\r\n    assertEquals(\"The getTaskType() method did not return the expected type\", TaskType.JOB_SETUP, TaskID.getTaskType('s'));\r\n    assertEquals(\"The getTaskType() method did not return the expected type\", TaskType.JOB_CLEANUP, TaskID.getTaskType('c'));\r\n    assertEquals(\"The getTaskType() method did not return the expected type\", TaskType.TASK_CLEANUP, TaskID.getTaskType('t'));\r\n    assertNull(\"The getTaskType() method did not return null for an unknown \" + \"type\", TaskID.getTaskType('x'));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testGetAllTaskTypes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetAllTaskTypes()\n{\r\n    assertEquals(\"The getAllTaskTypes method did not return the expected \" + \"string\", \"(m|r|s|c|t)\", TaskID.getAllTaskTypes());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGetMasterAddress",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetMasterAddress()\n{\r\n    YarnConfiguration conf = new YarnConfiguration();\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.CLASSIC_FRAMEWORK_NAME);\r\n    conf.set(MRConfig.MASTER_ADDRESS, \"local:invalid\");\r\n    try {\r\n        Master.getMasterAddress(conf);\r\n        fail(\"Should not reach here as there is a bad master address\");\r\n    } catch (Exception e) {\r\n    }\r\n    conf.set(MRConfig.MASTER_ADDRESS, \"bar.com:8042\");\r\n    String masterHostname = Master.getMasterAddress(conf);\r\n    assertThat(masterHostname).isEqualTo(\"bar.com\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testSplitRecords",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSplitRecords(String testFileName, long firstSplitLength) throws IOException\n{\r\n    URL testFileUrl = getClass().getClassLoader().getResource(testFileName);\r\n    assertNotNull(\"Cannot find \" + testFileName, testFileUrl);\r\n    File testFile = new File(testFileUrl.getFile());\r\n    long testFileSize = testFile.length();\r\n    Path testFilePath = new Path(testFile.getAbsolutePath());\r\n    Configuration conf = new Configuration();\r\n    testSplitRecordsForFile(conf, firstSplitLength, testFileSize, testFilePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testSplitRecordsForFile",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testSplitRecordsForFile(Configuration conf, long firstSplitLength, long testFileSize, Path testFilePath) throws IOException\n{\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH, Integer.MAX_VALUE);\r\n    assertTrue(\"unexpected test data at \" + testFilePath, testFileSize > firstSplitLength);\r\n    String delimiter = conf.get(\"textinputformat.record.delimiter\");\r\n    byte[] recordDelimiterBytes = null;\r\n    if (null != delimiter) {\r\n        recordDelimiterBytes = delimiter.getBytes(StandardCharsets.UTF_8);\r\n    }\r\n    TaskAttemptContext context = new TaskAttemptContextImpl(conf, new TaskAttemptID());\r\n    FileSplit split = new FileSplit(testFilePath, 0, testFileSize, (String[]) null);\r\n    LineRecordReader reader = new LineRecordReader(recordDelimiterBytes);\r\n    reader.initialize(split, context);\r\n    int numRecordsNoSplits = 0;\r\n    while (reader.nextKeyValue()) {\r\n        ++numRecordsNoSplits;\r\n    }\r\n    reader.close();\r\n    split = new FileSplit(testFilePath, 0, firstSplitLength, (String[]) null);\r\n    reader = new LineRecordReader(recordDelimiterBytes);\r\n    reader.initialize(split, context);\r\n    int numRecordsFirstSplit = 0;\r\n    while (reader.nextKeyValue()) {\r\n        ++numRecordsFirstSplit;\r\n    }\r\n    reader.close();\r\n    split = new FileSplit(testFilePath, firstSplitLength, testFileSize - firstSplitLength, (String[]) null);\r\n    reader = new LineRecordReader(recordDelimiterBytes);\r\n    reader.initialize(split, context);\r\n    int numRecordsRemainingSplits = 0;\r\n    while (reader.nextKeyValue()) {\r\n        ++numRecordsRemainingSplits;\r\n    }\r\n    reader.close();\r\n    assertEquals(\"Unexpected number of records in split \", numRecordsNoSplits, numRecordsFirstSplit + numRecordsRemainingSplits);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testBzip2SplitEndsAtCR",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testBzip2SplitEndsAtCR() throws IOException\n{\r\n    testSplitRecords(\"blockEndingInCR.txt.bz2\", 136498);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testBzip2SplitEndsAtCRThenLF",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testBzip2SplitEndsAtCRThenLF() throws IOException\n{\r\n    testSplitRecords(\"blockEndingInCRThenLF.txt.bz2\", 136498);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testSafeguardSplittingUnsplittableFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSafeguardSplittingUnsplittableFiles() throws IOException\n{\r\n    testSplitRecords(\"TestSafeguardSplittingUnsplittableFiles.txt.gz\", 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "readRecords",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "ArrayList<String> readRecords(URL testFileUrl, int splitSize) throws IOException\n{\r\n    File testFile = new File(testFileUrl.getFile());\r\n    long testFileSize = testFile.length();\r\n    Path testFilePath = new Path(testFile.getAbsolutePath());\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(\"io.file.buffer.size\", 1);\r\n    TaskAttemptContext context = new TaskAttemptContextImpl(conf, new TaskAttemptID());\r\n    ArrayList<String> records = new ArrayList<String>();\r\n    long offset = 0;\r\n    while (offset < testFileSize) {\r\n        FileSplit split = new FileSplit(testFilePath, offset, splitSize, null);\r\n        LineRecordReader reader = new LineRecordReader();\r\n        reader.initialize(split, context);\r\n        while (reader.nextKeyValue()) {\r\n            records.add(reader.getCurrentValue().toString());\r\n        }\r\n        offset += splitSize;\r\n    }\r\n    return records;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "readRecordsDirectly",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String[] readRecordsDirectly(URL testFileUrl, boolean bzip) throws IOException\n{\r\n    int MAX_DATA_SIZE = 1024 * 1024;\r\n    byte[] data = new byte[MAX_DATA_SIZE];\r\n    FileInputStream fis = new FileInputStream(testFileUrl.getFile());\r\n    int count;\r\n    if (bzip) {\r\n        BZip2CompressorInputStream bzIn = new BZip2CompressorInputStream(fis);\r\n        count = bzIn.read(data);\r\n        bzIn.close();\r\n    } else {\r\n        count = fis.read(data);\r\n    }\r\n    fis.close();\r\n    assertTrue(\"Test file data too big for buffer\", count < data.length);\r\n    return new String(data, 0, count, \"UTF-8\").split(\"\\n\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "checkRecordSpanningMultipleSplits",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void checkRecordSpanningMultipleSplits(String testFile, int splitSize, boolean bzip) throws IOException\n{\r\n    URL testFileUrl = getClass().getClassLoader().getResource(testFile);\r\n    ArrayList<String> records = readRecords(testFileUrl, splitSize);\r\n    String[] actuals = readRecordsDirectly(testFileUrl, bzip);\r\n    assertEquals(\"Wrong number of records\", actuals.length, records.size());\r\n    boolean hasLargeRecord = false;\r\n    for (int i = 0; i < actuals.length; ++i) {\r\n        assertEquals(actuals[i], records.get(i));\r\n        if (actuals[i].length() > 2 * splitSize) {\r\n            hasLargeRecord = true;\r\n        }\r\n    }\r\n    assertTrue(\"Invalid test data. Doesn't have a large enough record\", hasLargeRecord);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testRecordSpanningMultipleSplits",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRecordSpanningMultipleSplits() throws IOException\n{\r\n    checkRecordSpanningMultipleSplits(\"recordSpanningMultipleSplits.txt\", 10, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testRecordSpanningMultipleSplitsCompressed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRecordSpanningMultipleSplitsCompressed() throws IOException\n{\r\n    checkRecordSpanningMultipleSplits(\"recordSpanningMultipleSplits.txt.bz2\", 200 * 1000, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testStripBOM",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testStripBOM() throws IOException\n{\r\n    String UTF8_BOM = \"\\uFEFF\";\r\n    URL testFileUrl = getClass().getClassLoader().getResource(\"testBOM.txt\");\r\n    assertNotNull(\"Cannot find testBOM.txt\", testFileUrl);\r\n    File testFile = new File(testFileUrl.getFile());\r\n    Path testFilePath = new Path(testFile.getAbsolutePath());\r\n    long testFileSize = testFile.length();\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH, Integer.MAX_VALUE);\r\n    TaskAttemptContext context = new TaskAttemptContextImpl(conf, new TaskAttemptID());\r\n    FileSplit split = new FileSplit(testFilePath, 0, testFileSize, (String[]) null);\r\n    LineRecordReader reader = new LineRecordReader();\r\n    reader.initialize(split, context);\r\n    int numRecords = 0;\r\n    boolean firstLine = true;\r\n    boolean skipBOM = true;\r\n    while (reader.nextKeyValue()) {\r\n        if (firstLine) {\r\n            firstLine = false;\r\n            if (reader.getCurrentValue().toString().startsWith(UTF8_BOM)) {\r\n                skipBOM = false;\r\n            }\r\n        }\r\n        ++numRecords;\r\n    }\r\n    reader.close();\r\n    assertTrue(\"BOM is not skipped\", skipBOM);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testMultipleClose",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testMultipleClose() throws IOException\n{\r\n    URL testFileUrl = getClass().getClassLoader().getResource(\"recordSpanningMultipleSplits.txt.bz2\");\r\n    assertNotNull(\"Cannot find recordSpanningMultipleSplits.txt.bz2\", testFileUrl);\r\n    File testFile = new File(testFileUrl.getFile());\r\n    Path testFilePath = new Path(testFile.getAbsolutePath());\r\n    long testFileSize = testFile.length();\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH, Integer.MAX_VALUE);\r\n    TaskAttemptContext context = new TaskAttemptContextImpl(conf, new TaskAttemptID());\r\n    FileSplit split = new FileSplit(testFilePath, 0, testFileSize, null);\r\n    LineRecordReader reader = new LineRecordReader();\r\n    reader.initialize(split, context);\r\n    while (reader.nextKeyValue()) ;\r\n    reader.close();\r\n    reader.close();\r\n    BZip2Codec codec = new BZip2Codec();\r\n    codec.setConf(conf);\r\n    Set<Decompressor> decompressors = new HashSet<Decompressor>();\r\n    for (int i = 0; i < 10; ++i) {\r\n        decompressors.add(CodecPool.getDecompressor(codec));\r\n    }\r\n    assertEquals(10, decompressors.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createInputFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path createInputFile(Configuration conf, String data) throws IOException\n{\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    Path file = new Path(inputDir, \"test.txt\");\r\n    Writer writer = new OutputStreamWriter(localFs.create(file));\r\n    try {\r\n        writer.write(data);\r\n    } finally {\r\n        writer.close();\r\n    }\r\n    return file;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testUncompressedInput",
  "errType" : null,
  "containingMethodsNum" : 42,
  "sourceCodeText" : "void testUncompressedInput() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    String inputData = \"abc+def+ghi+jkl+mno+pqr+stu+vw +xyz\";\r\n    Path inputFile = createInputFile(conf, inputData);\r\n    conf.set(\"textinputformat.record.delimiter\", \"+\");\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            testSplitRecordsForFile(conf, splitSize, inputData.length(), inputFile);\r\n        }\r\n    }\r\n    inputData = \"abc|+|def|+|ghi|+|jkl|+|mno|+|pqr|+|stu|+|vw |+|xyz\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    conf.set(\"textinputformat.record.delimiter\", \"|+|\");\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            testSplitRecordsForFile(conf, splitSize, inputData.length(), inputFile);\r\n        }\r\n    }\r\n    inputData = \"abc+def++ghi+jkl++mno+pqr++stu+vw ++xyz\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    conf.set(\"textinputformat.record.delimiter\", \"+\");\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            testSplitRecordsForFile(conf, splitSize, inputData.length(), inputFile);\r\n        }\r\n    }\r\n    inputData = \"abc|+||+|defghi|+|jkl|+||+|mno|+|pqr|+||+|stu|+|vw |+||+|xyz\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    conf.set(\"textinputformat.record.delimiter\", \"|+|\");\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            testSplitRecordsForFile(conf, splitSize, inputData.length(), inputFile);\r\n        }\r\n    }\r\n    inputData = \"abc+def+-ghi+jkl+-mno+pqr+-stu+vw +-xyz\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    conf.set(\"textinputformat.record.delimiter\", \"+-\");\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            testSplitRecordsForFile(conf, splitSize, inputData.length(), inputFile);\r\n        }\r\n    }\r\n    inputData = \"abc\\n+def\\n+ghi\\n+jkl\\n+mno\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    conf.set(\"textinputformat.record.delimiter\", \"\\n+\");\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            testSplitRecordsForFile(conf, splitSize, inputData.length(), inputFile);\r\n        }\r\n    }\r\n    inputData = \"abc\\ndef+\\nghi+\\njkl\\nmno\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    conf.set(\"textinputformat.record.delimiter\", \"+\\n\");\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            testSplitRecordsForFile(conf, splitSize, inputData.length(), inputFile);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testUncompressedInputContainingCRLF",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testUncompressedInputContainingCRLF() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    String inputData = \"a\\r\\nb\\rc\\nd\\r\\n\";\r\n    Path inputFile = createInputFile(conf, inputData);\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            testSplitRecordsForFile(conf, splitSize, inputData.length(), inputFile);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testUncompressedInputCustomDelimiterPosValue",
  "errType" : null,
  "containingMethodsNum" : 75,
  "sourceCodeText" : "void testUncompressedInputCustomDelimiterPosValue() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(\"io.file.buffer.size\", 10);\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH, Integer.MAX_VALUE);\r\n    String inputData = \"abcdefghij++kl++mno\";\r\n    Path inputFile = createInputFile(conf, inputData);\r\n    String delimiter = \"++\";\r\n    byte[] recordDelimiterBytes = delimiter.getBytes(StandardCharsets.UTF_8);\r\n    int splitLength = 15;\r\n    FileSplit split = new FileSplit(inputFile, 0, splitLength, (String[]) null);\r\n    TaskAttemptContext context = new TaskAttemptContextImpl(conf, new TaskAttemptID());\r\n    LineRecordReader reader = new LineRecordReader(recordDelimiterBytes);\r\n    reader.initialize(split, context);\r\n    assertTrue(\"Expected record got nothing\", reader.nextKeyValue());\r\n    LongWritable key = reader.getCurrentKey();\r\n    Text value = reader.getCurrentValue();\r\n    assertEquals(\"Wrong length for record value\", 10, value.getLength());\r\n    assertEquals(\"Wrong position after record read\", 0, key.get());\r\n    assertTrue(\"Expected record got nothing\", reader.nextKeyValue());\r\n    assertEquals(\"Wrong length for record value\", 2, value.getLength());\r\n    assertEquals(\"Wrong position after record read\", 12, key.get());\r\n    assertTrue(\"Expected record got nothing\", reader.nextKeyValue());\r\n    assertEquals(\"Wrong length for record value\", 3, value.getLength());\r\n    assertEquals(\"Wrong position after record read\", 16, key.get());\r\n    assertFalse(reader.nextKeyValue());\r\n    assertEquals(\"Wrong position after record read\", 19, key.get());\r\n    key = reader.getCurrentKey();\r\n    assertNull(\"Unexpected key returned\", key);\r\n    reader.close();\r\n    split = new FileSplit(inputFile, splitLength, inputData.length() - splitLength, (String[]) null);\r\n    reader = new LineRecordReader(recordDelimiterBytes);\r\n    reader.initialize(split, context);\r\n    assertFalse(\"Unexpected record returned\", reader.nextKeyValue());\r\n    key = reader.getCurrentKey();\r\n    assertNull(\"Unexpected key returned\", key);\r\n    reader.close();\r\n    inputData = \"abcd+efgh++ijk++mno\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    splitLength = 5;\r\n    split = new FileSplit(inputFile, 0, splitLength, (String[]) null);\r\n    reader = new LineRecordReader(recordDelimiterBytes);\r\n    reader.initialize(split, context);\r\n    assertTrue(\"Expected record got nothing\", reader.nextKeyValue());\r\n    key = reader.getCurrentKey();\r\n    value = reader.getCurrentValue();\r\n    assertEquals(\"Wrong position after record read\", 0, key.get());\r\n    assertEquals(\"Wrong length for record value\", 9, value.getLength());\r\n    assertFalse(reader.nextKeyValue());\r\n    assertEquals(\"Wrong position after record read\", 11, key.get());\r\n    key = reader.getCurrentKey();\r\n    assertNull(\"Unexpected key returned\", key);\r\n    reader.close();\r\n    split = new FileSplit(inputFile, splitLength, inputData.length() - splitLength, (String[]) null);\r\n    reader = new LineRecordReader(recordDelimiterBytes);\r\n    reader.initialize(split, context);\r\n    assertTrue(\"Expected record got nothing\", reader.nextKeyValue());\r\n    key = reader.getCurrentKey();\r\n    value = reader.getCurrentValue();\r\n    assertEquals(\"Wrong position after record read\", 11, key.get());\r\n    assertEquals(\"Wrong length for record value\", 3, value.getLength());\r\n    assertTrue(\"Expected record got nothing\", reader.nextKeyValue());\r\n    assertEquals(\"Wrong position after record read\", 16, key.get());\r\n    assertEquals(\"Wrong length for record value\", 3, value.getLength());\r\n    assertFalse(reader.nextKeyValue());\r\n    assertEquals(\"Wrong position after record read\", 19, key.get());\r\n    reader.close();\r\n    inputData = \"abcd|efgh|+|ij|kl|+|mno|pqr\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    delimiter = \"|+|\";\r\n    recordDelimiterBytes = delimiter.getBytes(StandardCharsets.UTF_8);\r\n    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {\r\n        for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {\r\n            int keyPosition = 0;\r\n            conf.setInt(\"io.file.buffer.size\", bufferSize);\r\n            split = new FileSplit(inputFile, 0, bufferSize, (String[]) null);\r\n            reader = new LineRecordReader(recordDelimiterBytes);\r\n            reader.initialize(split, context);\r\n            assertTrue(\"Expected record got nothing\", reader.nextKeyValue());\r\n            key = reader.getCurrentKey();\r\n            value = reader.getCurrentValue();\r\n            assertTrue(\"abcd|efgh\".equals(value.toString()));\r\n            assertEquals(\"Wrong position after record read\", keyPosition, key.get());\r\n            keyPosition = 12;\r\n            if (reader.nextKeyValue()) {\r\n                assertTrue(\"ij|kl\".equals(value.toString()));\r\n                assertEquals(\"Wrong position after record read\", keyPosition, key.get());\r\n                keyPosition = 20;\r\n            }\r\n            if (reader.nextKeyValue()) {\r\n                assertTrue(\"mno|pqr\".equals(value.toString()));\r\n                assertEquals(\"Wrong position after record read\", keyPosition, key.get());\r\n                keyPosition = inputData.length();\r\n            }\r\n            assertFalse(\"Unexpected record returned\", reader.nextKeyValue());\r\n            assertEquals(\"Wrong position after record read\", keyPosition, key.get());\r\n            key = reader.getCurrentKey();\r\n            assertNull(\"Unexpected key returned\", key);\r\n            reader.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testUncompressedInputDefaultDelimiterPosValue",
  "errType" : null,
  "containingMethodsNum" : 34,
  "sourceCodeText" : "void testUncompressedInputDefaultDelimiterPosValue() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    String inputData = \"1234567890\\r\\n12\\r\\n345\";\r\n    Path inputFile = createInputFile(conf, inputData);\r\n    conf.setInt(\"io.file.buffer.size\", 10);\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH, Integer.MAX_VALUE);\r\n    FileSplit split = new FileSplit(inputFile, 0, 15, (String[]) null);\r\n    TaskAttemptContext context = new TaskAttemptContextImpl(conf, new TaskAttemptID());\r\n    LineRecordReader reader = new LineRecordReader(null);\r\n    reader.initialize(split, context);\r\n    LongWritable key;\r\n    Text value;\r\n    reader.nextKeyValue();\r\n    key = reader.getCurrentKey();\r\n    value = reader.getCurrentValue();\r\n    assertEquals(10, value.getLength());\r\n    assertEquals(0, key.get());\r\n    reader.nextKeyValue();\r\n    assertEquals(2, value.getLength());\r\n    assertEquals(12, key.get());\r\n    assertFalse(reader.nextKeyValue());\r\n    assertEquals(16, key.get());\r\n    split = new FileSplit(inputFile, 15, 4, (String[]) null);\r\n    reader = new LineRecordReader(null);\r\n    reader.initialize(split, context);\r\n    reader.nextKeyValue();\r\n    key = reader.getCurrentKey();\r\n    value = reader.getCurrentValue();\r\n    assertEquals(3, value.getLength());\r\n    assertEquals(16, key.get());\r\n    assertFalse(reader.nextKeyValue());\r\n    assertEquals(19, key.get());\r\n    inputData = \"123456789\\r\\r\\n\";\r\n    inputFile = createInputFile(conf, inputData);\r\n    split = new FileSplit(inputFile, 0, 12, (String[]) null);\r\n    reader = new LineRecordReader(null);\r\n    reader.initialize(split, context);\r\n    reader.nextKeyValue();\r\n    key = reader.getCurrentKey();\r\n    value = reader.getCurrentValue();\r\n    assertEquals(9, value.getLength());\r\n    assertEquals(0, key.get());\r\n    reader.nextKeyValue();\r\n    assertEquals(0, value.getLength());\r\n    assertEquals(10, key.get());\r\n    assertFalse(reader.nextKeyValue());\r\n    assertEquals(12, key.get());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testBzipWithMultibyteDelimiter",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testBzipWithMultibyteDelimiter() throws IOException\n{\r\n    String testFileName = \"compressedMultibyteDelimiter.txt.bz2\";\r\n    int firstSplitLength = 100;\r\n    URL testFileUrl = getClass().getClassLoader().getResource(testFileName);\r\n    assertNotNull(\"Cannot find \" + testFileName, testFileUrl);\r\n    File testFile = new File(testFileUrl.getFile());\r\n    long testFileSize = testFile.length();\r\n    Path testFilePath = new Path(testFile.getAbsolutePath());\r\n    assertTrue(\"Split size is smaller than header length\", firstSplitLength > 9);\r\n    assertTrue(\"Split size is larger than compressed file size \" + testFilePath, testFileSize > firstSplitLength);\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH, Integer.MAX_VALUE);\r\n    String delimiter = \"<E-LINE>\\r\\r\\n\";\r\n    conf.set(\"textinputformat.record.delimiter\", delimiter);\r\n    testSplitRecordsForFile(conf, firstSplitLength, testFileSize, testFilePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testConsecutiveFetch",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testConsecutiveFetch() throws IOException, InterruptedException\n{\r\n    final int MAX_EVENTS_TO_FETCH = 100;\r\n    TaskAttemptID tid = new TaskAttemptID(\"12345\", 1, TaskType.REDUCE, 1, 1);\r\n    TaskUmbilicalProtocol umbilical = mock(TaskUmbilicalProtocol.class);\r\n    when(umbilical.getMapCompletionEvents(any(JobID.class), anyInt(), anyInt(), any(TaskAttemptID.class))).thenReturn(getMockedCompletionEventsUpdate(0, 0));\r\n    when(umbilical.getMapCompletionEvents(any(JobID.class), eq(0), eq(MAX_EVENTS_TO_FETCH), eq(tid))).thenReturn(getMockedCompletionEventsUpdate(0, MAX_EVENTS_TO_FETCH));\r\n    when(umbilical.getMapCompletionEvents(any(JobID.class), eq(MAX_EVENTS_TO_FETCH), eq(MAX_EVENTS_TO_FETCH), eq(tid))).thenReturn(getMockedCompletionEventsUpdate(MAX_EVENTS_TO_FETCH, MAX_EVENTS_TO_FETCH));\r\n    when(umbilical.getMapCompletionEvents(any(JobID.class), eq(MAX_EVENTS_TO_FETCH * 2), eq(MAX_EVENTS_TO_FETCH), eq(tid))).thenReturn(getMockedCompletionEventsUpdate(MAX_EVENTS_TO_FETCH * 2, 3));\r\n    @SuppressWarnings(\"unchecked\")\r\n    ShuffleScheduler<String, String> scheduler = mock(ShuffleScheduler.class);\r\n    ExceptionReporter reporter = mock(ExceptionReporter.class);\r\n    EventFetcherForTest<String, String> ef = new EventFetcherForTest<String, String>(tid, umbilical, scheduler, reporter, MAX_EVENTS_TO_FETCH);\r\n    ef.getMapCompletionEvents();\r\n    verify(reporter, never()).reportException(any(Throwable.class));\r\n    InOrder inOrder = inOrder(umbilical);\r\n    inOrder.verify(umbilical).getMapCompletionEvents(any(JobID.class), eq(0), eq(MAX_EVENTS_TO_FETCH), eq(tid));\r\n    inOrder.verify(umbilical).getMapCompletionEvents(any(JobID.class), eq(MAX_EVENTS_TO_FETCH), eq(MAX_EVENTS_TO_FETCH), eq(tid));\r\n    inOrder.verify(umbilical).getMapCompletionEvents(any(JobID.class), eq(MAX_EVENTS_TO_FETCH * 2), eq(MAX_EVENTS_TO_FETCH), eq(tid));\r\n    verify(scheduler, times(MAX_EVENTS_TO_FETCH * 2 + 3)).resolve(any(TaskCompletionEvent.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getMockedCompletionEventsUpdate",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MapTaskCompletionEventsUpdate getMockedCompletionEventsUpdate(int startIdx, int numEvents)\n{\r\n    ArrayList<TaskCompletionEvent> tceList = new ArrayList<TaskCompletionEvent>(numEvents);\r\n    for (int i = 0; i < numEvents; ++i) {\r\n        int eventIdx = startIdx + i;\r\n        TaskCompletionEvent tce = new TaskCompletionEvent(eventIdx, new TaskAttemptID(\"12345\", 1, TaskType.MAP, eventIdx, 0), eventIdx, true, TaskCompletionEvent.Status.SUCCEEDED, \"http://somehost:8888\");\r\n        tceList.add(tce);\r\n    }\r\n    TaskCompletionEvent[] events = {};\r\n    return new MapTaskCompletionEventsUpdate(tceList.toArray(events), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    conf = new JobConf();\r\n    fs = FileSystem.getLocal(conf).getRaw();\r\n    p = new Path(System.getProperty(\"test.build.data\", \"/tmp\"), \"cache\").makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLRCPolicy",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testLRCPolicy() throws Exception\n{\r\n    Random r = new Random();\r\n    long seed = r.nextLong();\r\n    r.setSeed(seed);\r\n    System.out.println(\"seed: \" + seed);\r\n    fs.delete(p, true);\r\n    conf.setInt(TTConfig.TT_INDEX_CACHE, 1);\r\n    final int partsPerMap = 1000;\r\n    final int bytesPerFile = partsPerMap * 24;\r\n    IndexCache cache = new IndexCache(conf);\r\n    int totalsize = bytesPerFile;\r\n    for (; totalsize < 1024 * 1024; totalsize += bytesPerFile) {\r\n        Path f = new Path(p, Integer.toString(totalsize, 36));\r\n        writeFile(fs, f, totalsize, partsPerMap);\r\n        IndexRecord rec = cache.getIndexInformation(Integer.toString(totalsize, 36), r.nextInt(partsPerMap), f, UserGroupInformation.getCurrentUser().getShortUserName());\r\n        checkRecord(rec, totalsize);\r\n    }\r\n    for (FileStatus stat : fs.listStatus(p)) {\r\n        fs.delete(stat.getPath(), true);\r\n    }\r\n    for (int i = bytesPerFile; i < 1024 * 1024; i += bytesPerFile) {\r\n        Path f = new Path(p, Integer.toString(i, 36));\r\n        IndexRecord rec = cache.getIndexInformation(Integer.toString(i, 36), r.nextInt(partsPerMap), f, UserGroupInformation.getCurrentUser().getShortUserName());\r\n        checkRecord(rec, i);\r\n    }\r\n    Path f = new Path(p, Integer.toString(totalsize, 36));\r\n    writeFile(fs, f, totalsize, partsPerMap);\r\n    cache.getIndexInformation(Integer.toString(totalsize, 36), r.nextInt(partsPerMap), f, UserGroupInformation.getCurrentUser().getShortUserName());\r\n    fs.delete(f, false);\r\n    boolean fnf = false;\r\n    try {\r\n        cache.getIndexInformation(Integer.toString(bytesPerFile, 36), r.nextInt(partsPerMap), new Path(p, Integer.toString(bytesPerFile)), UserGroupInformation.getCurrentUser().getShortUserName());\r\n    } catch (IOException e) {\r\n        if (e.getCause() == null || !(e.getCause() instanceof FileNotFoundException)) {\r\n            throw e;\r\n        } else {\r\n            fnf = true;\r\n        }\r\n    }\r\n    if (!fnf)\r\n        fail(\"Failed to push out last entry\");\r\n    for (int i = bytesPerFile << 1; i < 1024 * 1024; i += bytesPerFile) {\r\n        IndexRecord rec = cache.getIndexInformation(Integer.toString(i, 36), r.nextInt(partsPerMap), new Path(p, Integer.toString(i, 36)), UserGroupInformation.getCurrentUser().getShortUserName());\r\n        checkRecord(rec, i);\r\n    }\r\n    IndexRecord rec = cache.getIndexInformation(Integer.toString(totalsize, 36), r.nextInt(partsPerMap), f, UserGroupInformation.getCurrentUser().getShortUserName());\r\n    checkRecord(rec, totalsize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBadIndex",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testBadIndex() throws Exception\n{\r\n    final int parts = 30;\r\n    fs.delete(p, true);\r\n    conf.setInt(TTConfig.TT_INDEX_CACHE, 1);\r\n    IndexCache cache = new IndexCache(conf);\r\n    Path f = new Path(p, \"badindex\");\r\n    FSDataOutputStream out = fs.create(f, false);\r\n    CheckedOutputStream iout = new CheckedOutputStream(out, new CRC32());\r\n    DataOutputStream dout = new DataOutputStream(iout);\r\n    for (int i = 0; i < parts; ++i) {\r\n        for (int j = 0; j < MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8; ++j) {\r\n            if (0 == (i % 3)) {\r\n                dout.writeLong(i);\r\n            } else {\r\n                out.writeLong(i);\r\n            }\r\n        }\r\n    }\r\n    out.writeLong(iout.getChecksum().getValue());\r\n    dout.close();\r\n    try {\r\n        cache.getIndexInformation(\"badindex\", 7, f, UserGroupInformation.getCurrentUser().getShortUserName());\r\n        fail(\"Did not detect bad checksum\");\r\n    } catch (IOException e) {\r\n        if (!(e.getCause() instanceof ChecksumException)) {\r\n            throw e;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testInvalidReduceNumberOrLength",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testInvalidReduceNumberOrLength() throws Exception\n{\r\n    fs.delete(p, true);\r\n    conf.setInt(TTConfig.TT_INDEX_CACHE, 1);\r\n    final int partsPerMap = 1000;\r\n    final int bytesPerFile = partsPerMap * 24;\r\n    IndexCache cache = new IndexCache(conf);\r\n    Path feq = new Path(p, \"invalidReduceOrPartsPerMap\");\r\n    writeFile(fs, feq, bytesPerFile, partsPerMap);\r\n    try {\r\n        cache.getIndexInformation(\"reduceEqualPartsPerMap\", partsPerMap, feq, UserGroupInformation.getCurrentUser().getShortUserName());\r\n        fail(\"Number of reducers equal to partsPerMap did not fail\");\r\n    } catch (Exception e) {\r\n        if (!(e instanceof IOException)) {\r\n            throw e;\r\n        }\r\n    }\r\n    try {\r\n        cache.getIndexInformation(\"reduceMorePartsPerMap\", partsPerMap + 1, feq, UserGroupInformation.getCurrentUser().getShortUserName());\r\n        fail(\"Number of reducers more than partsPerMap did not fail\");\r\n    } catch (Exception e) {\r\n        if (!(e instanceof IOException)) {\r\n            throw e;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRemoveMap",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testRemoveMap() throws Exception\n{\r\n    fs.delete(p, true);\r\n    conf.setInt(TTConfig.TT_INDEX_CACHE, 10);\r\n    final int partsPerMap = 100000;\r\n    final int bytesPerFile = partsPerMap * 24;\r\n    final IndexCache cache = new IndexCache(conf);\r\n    final Path big = new Path(p, \"bigIndex\");\r\n    final String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    writeFile(fs, big, bytesPerFile, partsPerMap);\r\n    for (int i = 0; i < 20; ++i) {\r\n        Thread getInfoThread = new Thread() {\r\n\r\n            @Override\r\n            public void run() {\r\n                try {\r\n                    cache.getIndexInformation(\"bigIndex\", partsPerMap, big, user);\r\n                } catch (Exception e) {\r\n                }\r\n            }\r\n        };\r\n        Thread removeMapThread = new Thread() {\r\n\r\n            @Override\r\n            public void run() {\r\n                cache.removeMap(\"bigIndex\");\r\n            }\r\n        };\r\n        if (i % 2 == 0) {\r\n            getInfoThread.start();\r\n            removeMapThread.start();\r\n        } else {\r\n            removeMapThread.start();\r\n            getInfoThread.start();\r\n        }\r\n        getInfoThread.join();\r\n        removeMapThread.join();\r\n        assertTrue(cache.checkTotalMemoryUsed());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCreateRace",
  "errType" : [ "Exception", "InterruptedException", "InterruptedException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testCreateRace() throws Exception\n{\r\n    fs.delete(p, true);\r\n    conf.setInt(TTConfig.TT_INDEX_CACHE, 1);\r\n    final int partsPerMap = 1000;\r\n    final int bytesPerFile = partsPerMap * 24;\r\n    final IndexCache cache = new IndexCache(conf);\r\n    final Path racy = new Path(p, \"racyIndex\");\r\n    final String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    writeFile(fs, racy, bytesPerFile, partsPerMap);\r\n    Thread[] getInfoThreads = new Thread[50];\r\n    for (int i = 0; i < 50; i++) {\r\n        getInfoThreads[i] = new Thread() {\r\n\r\n            @Override\r\n            public void run() {\r\n                try {\r\n                    cache.getIndexInformation(\"racyIndex\", partsPerMap, racy, user);\r\n                    cache.removeMap(\"racyIndex\");\r\n                } catch (Exception e) {\r\n                }\r\n            }\r\n        };\r\n    }\r\n    for (int i = 0; i < 50; i++) {\r\n        getInfoThreads[i].start();\r\n    }\r\n    final Thread mainTestThread = Thread.currentThread();\r\n    Thread timeoutThread = new Thread() {\r\n\r\n        @Override\r\n        public void run() {\r\n            try {\r\n                Thread.sleep(15000);\r\n                mainTestThread.interrupt();\r\n            } catch (InterruptedException ie) {\r\n            }\r\n        }\r\n    };\r\n    for (int i = 0; i < 50; i++) {\r\n        try {\r\n            getInfoThreads[i].join();\r\n        } catch (InterruptedException ie) {\r\n            fail(\"Unexpectedly long delay during concurrent cache entry creations\");\r\n        }\r\n    }\r\n    timeoutThread.interrupt();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "checkRecord",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkRecord(IndexRecord rec, long fill)\n{\r\n    assertEquals(fill, rec.startOffset);\r\n    assertEquals(fill, rec.rawLength);\r\n    assertEquals(fill, rec.partLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeFile(FileSystem fs, Path f, long fill, int parts) throws IOException\n{\r\n    FSDataOutputStream out = fs.create(f, false);\r\n    CheckedOutputStream iout = new CheckedOutputStream(out, new CRC32());\r\n    DataOutputStream dout = new DataOutputStream(iout);\r\n    for (int i = 0; i < parts; ++i) {\r\n        for (int j = 0; j < MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8; ++j) {\r\n            dout.writeLong(fill);\r\n        }\r\n    }\r\n    out.writeLong(iout.getChecksum().getValue());\r\n    dout.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJobConf",
  "errType" : null,
  "containingMethodsNum" : 73,
  "sourceCodeText" : "void testJobConf()\n{\r\n    JobConf conf = new JobConf();\r\n    Pattern pattern = conf.getJarUnpackPattern();\r\n    assertEquals(Pattern.compile(\"(?:classes/|lib/).*\").toString(), pattern.toString());\r\n    assertFalse(conf.getKeepFailedTaskFiles());\r\n    conf.setKeepFailedTaskFiles(true);\r\n    assertTrue(conf.getKeepFailedTaskFiles());\r\n    assertNull(conf.getKeepTaskFilesPattern());\r\n    conf.setKeepTaskFilesPattern(\"123454\");\r\n    assertEquals(\"123454\", conf.getKeepTaskFilesPattern());\r\n    assertNotNull(conf.getWorkingDirectory());\r\n    conf.setWorkingDirectory(new Path(\"test\"));\r\n    assertTrue(conf.getWorkingDirectory().toString().endsWith(\"test\"));\r\n    assertEquals(1, conf.getNumTasksToExecutePerJvm());\r\n    assertNull(conf.getKeyFieldComparatorOption());\r\n    conf.setKeyFieldComparatorOptions(\"keySpec\");\r\n    assertEquals(\"keySpec\", conf.getKeyFieldComparatorOption());\r\n    assertFalse(conf.getUseNewReducer());\r\n    conf.setUseNewReducer(true);\r\n    assertTrue(conf.getUseNewReducer());\r\n    assertTrue(conf.getMapSpeculativeExecution());\r\n    assertTrue(conf.getReduceSpeculativeExecution());\r\n    assertTrue(conf.getSpeculativeExecution());\r\n    conf.setReduceSpeculativeExecution(false);\r\n    assertTrue(conf.getSpeculativeExecution());\r\n    conf.setMapSpeculativeExecution(false);\r\n    assertFalse(conf.getSpeculativeExecution());\r\n    assertFalse(conf.getMapSpeculativeExecution());\r\n    assertFalse(conf.getReduceSpeculativeExecution());\r\n    conf.setSessionId(\"ses\");\r\n    assertEquals(\"ses\", conf.getSessionId());\r\n    assertEquals(3, conf.getMaxTaskFailuresPerTracker());\r\n    conf.setMaxTaskFailuresPerTracker(2);\r\n    assertEquals(2, conf.getMaxTaskFailuresPerTracker());\r\n    assertEquals(0, conf.getMaxMapTaskFailuresPercent());\r\n    conf.setMaxMapTaskFailuresPercent(50);\r\n    assertEquals(50, conf.getMaxMapTaskFailuresPercent());\r\n    assertEquals(0, conf.getMaxReduceTaskFailuresPercent());\r\n    conf.setMaxReduceTaskFailuresPercent(70);\r\n    assertEquals(70, conf.getMaxReduceTaskFailuresPercent());\r\n    assertThat(conf.getJobPriority()).isEqualTo(JobPriority.DEFAULT);\r\n    conf.setJobPriority(JobPriority.HIGH);\r\n    assertThat(conf.getJobPriority()).isEqualTo(JobPriority.HIGH);\r\n    assertNull(conf.getJobSubmitHostName());\r\n    conf.setJobSubmitHostName(\"hostname\");\r\n    assertEquals(\"hostname\", conf.getJobSubmitHostName());\r\n    assertNull(conf.getJobSubmitHostAddress());\r\n    conf.setJobSubmitHostAddress(\"ww\");\r\n    assertEquals(\"ww\", conf.getJobSubmitHostAddress());\r\n    assertFalse(conf.getProfileEnabled());\r\n    conf.setProfileEnabled(true);\r\n    assertTrue(conf.getProfileEnabled());\r\n    assertEquals(conf.getProfileTaskRange(true).toString(), \"0-2\");\r\n    assertEquals(conf.getProfileTaskRange(false).toString(), \"0-2\");\r\n    conf.setProfileTaskRange(true, \"0-3\");\r\n    assertEquals(conf.getProfileTaskRange(false).toString(), \"0-2\");\r\n    assertEquals(conf.getProfileTaskRange(true).toString(), \"0-3\");\r\n    assertNull(conf.getMapDebugScript());\r\n    conf.setMapDebugScript(\"mDbgScript\");\r\n    assertEquals(\"mDbgScript\", conf.getMapDebugScript());\r\n    assertNull(conf.getReduceDebugScript());\r\n    conf.setReduceDebugScript(\"rDbgScript\");\r\n    assertEquals(\"rDbgScript\", conf.getReduceDebugScript());\r\n    assertNull(conf.getJobLocalDir());\r\n    assertEquals(\"default\", conf.getQueueName());\r\n    conf.setQueueName(\"qname\");\r\n    assertEquals(\"qname\", conf.getQueueName());\r\n    conf.setMemoryForMapTask(100 * 1000);\r\n    assertEquals(100 * 1000, conf.getMemoryForMapTask());\r\n    conf.setMemoryForReduceTask(1000 * 1000);\r\n    assertEquals(1000 * 1000, conf.getMemoryForReduceTask());\r\n    assertEquals(-1, conf.getMaxPhysicalMemoryForTask());\r\n    assertEquals(\"The variable key is no longer used.\", JobConf.deprecatedString(\"key\"));\r\n    assertNull(\"mapreduce.map.java.opts should not be set by default\", conf.get(JobConf.MAPRED_MAP_TASK_JAVA_OPTS));\r\n    assertNull(\"mapreduce.reduce.java.opts should not be set by default\", conf.get(JobConf.MAPRED_REDUCE_TASK_JAVA_OPTS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testDeprecatedPropertyNameForTaskVmem",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testDeprecatedPropertyNameForTaskVmem()\n{\r\n    JobConf configuration = new JobConf();\r\n    configuration.setLong(JobConf.MAPRED_JOB_MAP_MEMORY_MB_PROPERTY, 1024);\r\n    configuration.setLong(JobConf.MAPRED_JOB_REDUCE_MEMORY_MB_PROPERTY, 1024);\r\n    Assert.assertEquals(1024, configuration.getMemoryForMapTask());\r\n    Assert.assertEquals(1024, configuration.getMemoryForReduceTask());\r\n    configuration.setLong(JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY, 1025);\r\n    configuration.setLong(JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY, 1025);\r\n    Assert.assertEquals(1025, configuration.getMemoryForMapTask());\r\n    Assert.assertEquals(1025, configuration.getMemoryForReduceTask());\r\n    configuration.setMemoryForMapTask(2048);\r\n    configuration.setMemoryForReduceTask(2048);\r\n    Assert.assertEquals(2048, configuration.getLong(JobConf.MAPRED_JOB_MAP_MEMORY_MB_PROPERTY, -1));\r\n    Assert.assertEquals(2048, configuration.getLong(JobConf.MAPRED_JOB_REDUCE_MEMORY_MB_PROPERTY, -1));\r\n    Assert.assertEquals(2048, configuration.getLong(JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY, -1));\r\n    Assert.assertEquals(2048, configuration.getLong(JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY, -1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testProfileParamsDefaults",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testProfileParamsDefaults()\n{\r\n    JobConf configuration = new JobConf();\r\n    String result = configuration.getProfileParams();\r\n    Assert.assertNotNull(result);\r\n    Assert.assertTrue(result.contains(\"file=%s\"));\r\n    Assert.assertTrue(result.startsWith(\"-agentlib:hprof\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testProfileParamsSetter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testProfileParamsSetter()\n{\r\n    JobConf configuration = new JobConf();\r\n    configuration.setProfileParams(\"test\");\r\n    Assert.assertEquals(\"test\", configuration.get(MRJobConfig.TASK_PROFILE_PARAMS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testProfileParamsGetter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testProfileParamsGetter()\n{\r\n    JobConf configuration = new JobConf();\r\n    configuration.set(MRJobConfig.TASK_PROFILE_PARAMS, \"test\");\r\n    Assert.assertEquals(\"test\", configuration.getProfileParams());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMemoryConfigForMapOrReduceTask",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testMemoryConfigForMapOrReduceTask()\n{\r\n    JobConf configuration = new JobConf();\r\n    configuration.set(MRJobConfig.MAP_MEMORY_MB, String.valueOf(300));\r\n    configuration.set(MRJobConfig.REDUCE_MEMORY_MB, String.valueOf(300));\r\n    assertThat(configuration.getMemoryForMapTask()).isEqualTo(300);\r\n    assertThat(configuration.getMemoryForReduceTask()).isEqualTo(300);\r\n    configuration.set(\"mapred.task.maxvmem\", String.valueOf(2 * 1024 * 1024));\r\n    configuration.set(MRJobConfig.MAP_MEMORY_MB, String.valueOf(300));\r\n    configuration.set(MRJobConfig.REDUCE_MEMORY_MB, String.valueOf(300));\r\n    assertThat(configuration.getMemoryForMapTask()).isEqualTo(2);\r\n    assertThat(configuration.getMemoryForReduceTask()).isEqualTo(2);\r\n    configuration = new JobConf();\r\n    configuration.set(\"mapred.task.maxvmem\", \"-1\");\r\n    configuration.set(MRJobConfig.MAP_MEMORY_MB, String.valueOf(300));\r\n    configuration.set(MRJobConfig.REDUCE_MEMORY_MB, String.valueOf(400));\r\n    assertThat(configuration.getMemoryForMapTask()).isEqualTo(300);\r\n    assertThat(configuration.getMemoryForReduceTask()).isEqualTo(400);\r\n    configuration = new JobConf();\r\n    configuration.set(\"mapred.task.maxvmem\", String.valueOf(2 * 1024 * 1024));\r\n    configuration.set(MRJobConfig.MAP_MEMORY_MB, \"-1\");\r\n    configuration.set(MRJobConfig.REDUCE_MEMORY_MB, \"-1\");\r\n    assertThat(configuration.getMemoryForMapTask()).isEqualTo(2);\r\n    assertThat(configuration.getMemoryForReduceTask()).isEqualTo(2);\r\n    configuration = new JobConf();\r\n    configuration.set(\"mapred.task.maxvmem\", String.valueOf(-1));\r\n    configuration.set(MRJobConfig.MAP_MEMORY_MB, \"-1\");\r\n    configuration.set(MRJobConfig.REDUCE_MEMORY_MB, \"-1\");\r\n    assertThat(configuration.getMemoryForMapTask()).isEqualTo(MRJobConfig.DEFAULT_MAP_MEMORY_MB);\r\n    assertThat(configuration.getMemoryForReduceTask()).isEqualTo(MRJobConfig.DEFAULT_REDUCE_MEMORY_MB);\r\n    configuration = new JobConf();\r\n    configuration.set(\"mapred.task.maxvmem\", String.valueOf(2 * 1024 * 1024));\r\n    configuration.set(MRJobConfig.MAP_MEMORY_MB, \"3\");\r\n    configuration.set(MRJobConfig.REDUCE_MEMORY_MB, \"3\");\r\n    assertThat(configuration.getMemoryForMapTask()).isEqualTo(2);\r\n    assertThat(configuration.getMemoryForReduceTask()).isEqualTo(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNegativeValueForTaskVmem",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testNegativeValueForTaskVmem()\n{\r\n    JobConf configuration = new JobConf();\r\n    configuration.set(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY, \"-3\");\r\n    Assert.assertEquals(MRJobConfig.DEFAULT_MAP_MEMORY_MB, configuration.getMemoryForMapTask());\r\n    Assert.assertEquals(MRJobConfig.DEFAULT_REDUCE_MEMORY_MB, configuration.getMemoryForReduceTask());\r\n    configuration.set(MRJobConfig.MAP_MEMORY_MB, \"4\");\r\n    configuration.set(MRJobConfig.REDUCE_MEMORY_MB, \"5\");\r\n    Assert.assertEquals(4, configuration.getMemoryForMapTask());\r\n    Assert.assertEquals(5, configuration.getMemoryForReduceTask());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNegativeValuesForMemoryParams",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testNegativeValuesForMemoryParams()\n{\r\n    JobConf configuration = new JobConf();\r\n    configuration.set(MRJobConfig.MAP_MEMORY_MB, \"-5\");\r\n    configuration.set(MRJobConfig.REDUCE_MEMORY_MB, \"-6\");\r\n    Assert.assertEquals(MRJobConfig.DEFAULT_MAP_MEMORY_MB, configuration.getMemoryForMapTask());\r\n    Assert.assertEquals(MRJobConfig.DEFAULT_REDUCE_MEMORY_MB, configuration.getMemoryForReduceTask());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMaxVirtualMemoryForTask",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testMaxVirtualMemoryForTask()\n{\r\n    JobConf configuration = new JobConf();\r\n    configuration.set(MRJobConfig.MAP_MEMORY_MB, String.valueOf(300));\r\n    configuration.set(MRJobConfig.REDUCE_MEMORY_MB, String.valueOf(-1));\r\n    assertThat(configuration.getMaxVirtualMemoryForTask()).isEqualTo(1024 * 1024 * 1024);\r\n    configuration = new JobConf();\r\n    configuration.set(MRJobConfig.MAP_MEMORY_MB, String.valueOf(-1));\r\n    configuration.set(MRJobConfig.REDUCE_MEMORY_MB, String.valueOf(200));\r\n    assertThat(configuration.getMaxVirtualMemoryForTask()).isEqualTo(1024 * 1024 * 1024);\r\n    configuration = new JobConf();\r\n    configuration.set(MRJobConfig.MAP_MEMORY_MB, String.valueOf(-1));\r\n    configuration.set(MRJobConfig.REDUCE_MEMORY_MB, String.valueOf(-1));\r\n    configuration.set(\"mapred.task.maxvmem\", String.valueOf(1 * 1024 * 1024));\r\n    assertThat(configuration.getMaxVirtualMemoryForTask()).isEqualTo(1 * 1024 * 1024);\r\n    configuration = new JobConf();\r\n    configuration.set(\"mapred.task.maxvmem\", String.valueOf(1 * 1024 * 1024));\r\n    assertThat(configuration.getMaxVirtualMemoryForTask()).isEqualTo(1 * 1024 * 1024);\r\n    configuration = new JobConf();\r\n    configuration.setMaxVirtualMemoryForTask(2 * 1024 * 1024);\r\n    assertThat(configuration.getMemoryForMapTask()).isEqualTo(2);\r\n    assertThat(configuration.getMemoryForReduceTask()).isEqualTo(2);\r\n    configuration = new JobConf();\r\n    configuration.set(MRJobConfig.MAP_MEMORY_MB, String.valueOf(300));\r\n    configuration.set(MRJobConfig.REDUCE_MEMORY_MB, String.valueOf(400));\r\n    configuration.setMaxVirtualMemoryForTask(2 * 1024 * 1024);\r\n    assertThat(configuration.getMemoryForMapTask()).isEqualTo(2);\r\n    assertThat(configuration.getMemoryForReduceTask()).isEqualTo(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMaxTaskFailuresPerTracker",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMaxTaskFailuresPerTracker()\n{\r\n    JobConf jobConf = new JobConf(true);\r\n    Assert.assertTrue(\"By default JobContext.MAX_TASK_FAILURES_PER_TRACKER was \" + \"not less than JobContext.MAP_MAX_ATTEMPTS and REDUCE_MAX_ATTEMPTS\", jobConf.getMaxTaskFailuresPerTracker() < jobConf.getMaxMapAttempts() && jobConf.getMaxTaskFailuresPerTracker() < jobConf.getMaxReduceAttempts());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testParseMaximumHeapSizeMB",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testParseMaximumHeapSizeMB()\n{\r\n    Assert.assertEquals(4096, JobConf.parseMaximumHeapSizeMB(\"-Xmx4294967296\"));\r\n    Assert.assertEquals(4096, JobConf.parseMaximumHeapSizeMB(\"-Xmx4194304k\"));\r\n    Assert.assertEquals(4096, JobConf.parseMaximumHeapSizeMB(\"-Xmx4096m\"));\r\n    Assert.assertEquals(4096, JobConf.parseMaximumHeapSizeMB(\"-Xmx4g\"));\r\n    Assert.assertEquals(-1, JobConf.parseMaximumHeapSizeMB(\"-Xmx4?\"));\r\n    Assert.assertEquals(-1, JobConf.parseMaximumHeapSizeMB(\"\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJobPriorityConf",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testJobPriorityConf()\n{\r\n    JobConf conf = new JobConf();\r\n    assertThat(conf.getJobPriority()).isEqualTo(JobPriority.DEFAULT);\r\n    assertEquals(0, conf.getJobPriorityAsInteger());\r\n    conf.setJobPriority(JobPriority.LOW);\r\n    assertThat(conf.getJobPriority()).isEqualTo(JobPriority.LOW);\r\n    assertEquals(2, conf.getJobPriorityAsInteger());\r\n    conf.setJobPriority(JobPriority.VERY_HIGH);\r\n    assertThat(conf.getJobPriority()).isEqualTo(JobPriority.VERY_HIGH);\r\n    assertEquals(5, conf.getJobPriorityAsInteger());\r\n    conf.setJobPriorityAsInteger(3);\r\n    assertThat(conf.getJobPriority()).isEqualTo(JobPriority.NORMAL);\r\n    assertEquals(3, conf.getJobPriorityAsInteger());\r\n    conf.setJobPriorityAsInteger(4);\r\n    assertThat(conf.getJobPriority()).isEqualTo(JobPriority.HIGH);\r\n    assertEquals(4, conf.getJobPriorityAsInteger());\r\n    conf.setJobPriorityAsInteger(57);\r\n    assertThat(conf.getJobPriority()).isEqualTo(JobPriority.UNDEFINED_PRIORITY);\r\n    assertEquals(57, conf.getJobPriorityAsInteger());\r\n    conf.setJobPriority(JobPriority.UNDEFINED_PRIORITY);\r\n    assertThat(conf.getJobPriority()).isEqualTo(JobPriority.UNDEFINED_PRIORITY);\r\n    assertEquals(0, conf.getJobPriorityAsInteger());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testGetStagingDirWhenFullFileOwnerNameAndFullUserName",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testGetStagingDirWhenFullFileOwnerNameAndFullUserName() throws IOException, InterruptedException\n{\r\n    Cluster cluster = mock(Cluster.class);\r\n    Configuration conf = new Configuration();\r\n    Path stagingPath = mock(Path.class);\r\n    UserGroupInformation user = UserGroupInformation.createUserForTesting(USER_1, GROUP_NAMES);\r\n    assertEquals(USER_1, user.getUserName());\r\n    FileSystem fs = new FileSystemTestHelper.MockFileSystem();\r\n    when(cluster.getStagingAreaDir()).thenReturn(stagingPath);\r\n    when(stagingPath.getFileSystem(conf)).thenReturn(fs);\r\n    String stagingDirOwner = USER_1.toLowerCase();\r\n    FileStatus fileStatus = new FileStatus(1, true, 1, 1, 100L, 100L, FsPermission.getDefault(), stagingDirOwner, stagingDirOwner, stagingPath);\r\n    when(fs.getFileStatus(stagingPath)).thenReturn(fileStatus);\r\n    assertEquals(stagingPath, JobSubmissionFiles.getStagingDir(cluster, conf, user));\r\n    stagingDirOwner = USER_1;\r\n    fileStatus = new FileStatus(1, true, 1, 1, 100L, 100L, FsPermission.getDefault(), stagingDirOwner, stagingDirOwner, stagingPath);\r\n    when(fs.getFileStatus(stagingPath)).thenReturn(fileStatus);\r\n    assertEquals(stagingPath, JobSubmissionFiles.getStagingDir(cluster, conf, user));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testGetStagingWhenFileOwnerNameAndCurrentUserNameDoesNotMatch",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGetStagingWhenFileOwnerNameAndCurrentUserNameDoesNotMatch() throws IOException, InterruptedException\n{\r\n    Cluster cluster = mock(Cluster.class);\r\n    Configuration conf = new Configuration();\r\n    String stagingDirOwner = \"someuser\";\r\n    Path stagingPath = mock(Path.class);\r\n    UserGroupInformation user = UserGroupInformation.createUserForTesting(USER_1, GROUP_NAMES);\r\n    assertEquals(USER_1, user.getUserName());\r\n    FileSystem fs = new FileSystemTestHelper.MockFileSystem();\r\n    FileStatus fileStatus = new FileStatus(1, true, 1, 1, 100L, 100L, FsPermission.getDefault(), stagingDirOwner, stagingDirOwner, stagingPath);\r\n    when(stagingPath.getFileSystem(conf)).thenReturn(fs);\r\n    when(fs.getFileStatus(stagingPath)).thenReturn(fileStatus);\r\n    when(cluster.getStagingAreaDir()).thenReturn(stagingPath);\r\n    assertEquals(stagingPath, JobSubmissionFiles.getStagingDir(cluster, conf, user));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testGetStagingDirWhenShortFileOwnerNameAndFullUserName",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGetStagingDirWhenShortFileOwnerNameAndFullUserName() throws IOException, InterruptedException\n{\r\n    Cluster cluster = mock(Cluster.class);\r\n    Configuration conf = new Configuration();\r\n    String stagingDirOwner = USER_1_SHORT_NAME;\r\n    Path stagingPath = mock(Path.class);\r\n    UserGroupInformation user = UserGroupInformation.createUserForTesting(USER_1, GROUP_NAMES);\r\n    assertEquals(USER_1, user.getUserName());\r\n    FileSystem fs = new FileSystemTestHelper.MockFileSystem();\r\n    FileStatus fileStatus = new FileStatus(1, true, 1, 1, 100L, 100L, FsPermission.getDefault(), stagingDirOwner, stagingDirOwner, stagingPath);\r\n    when(stagingPath.getFileSystem(conf)).thenReturn(fs);\r\n    when(fs.getFileStatus(stagingPath)).thenReturn(fileStatus);\r\n    when(cluster.getStagingAreaDir()).thenReturn(stagingPath);\r\n    assertEquals(stagingPath, JobSubmissionFiles.getStagingDir(cluster, conf, user));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testGetStagingDirWhenShortFileOwnerNameAndShortUserName",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGetStagingDirWhenShortFileOwnerNameAndShortUserName() throws IOException, InterruptedException\n{\r\n    Cluster cluster = mock(Cluster.class);\r\n    Configuration conf = new Configuration();\r\n    String stagingDirOwner = USER_1_SHORT_NAME;\r\n    Path stagingPath = mock(Path.class);\r\n    UserGroupInformation user = UserGroupInformation.createUserForTesting(USER_1_SHORT_NAME, GROUP_NAMES);\r\n    assertEquals(USER_1_SHORT_NAME, user.getUserName());\r\n    FileSystem fs = new FileSystemTestHelper.MockFileSystem();\r\n    FileStatus fileStatus = new FileStatus(1, true, 1, 1, 100L, 100L, FsPermission.getDefault(), stagingDirOwner, stagingDirOwner, stagingPath);\r\n    when(stagingPath.getFileSystem(conf)).thenReturn(fs);\r\n    when(fs.getFileStatus(stagingPath)).thenReturn(fileStatus);\r\n    when(cluster.getStagingAreaDir()).thenReturn(stagingPath);\r\n    assertEquals(stagingPath, JobSubmissionFiles.getStagingDir(cluster, conf, user));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testJobToString",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testJobToString() throws IOException, InterruptedException\n{\r\n    Cluster cluster = mock(Cluster.class);\r\n    ClientProtocol client = mock(ClientProtocol.class);\r\n    when(cluster.getClient()).thenReturn(client);\r\n    JobID jobid = new JobID(\"1014873536921\", 6);\r\n    JobStatus status = new JobStatus(jobid, 0.0f, 0.0f, 0.0f, 0.0f, State.FAILED, JobPriority.DEFAULT, \"root\", \"TestJobToString\", \"job file\", \"tracking url\");\r\n    when(client.getJobStatus(jobid)).thenReturn(status);\r\n    when(client.getTaskReports(jobid, TaskType.MAP)).thenReturn(new TaskReport[0]);\r\n    when(client.getTaskReports(jobid, TaskType.REDUCE)).thenReturn(new TaskReport[0]);\r\n    when(client.getTaskCompletionEvents(jobid, 0, 10)).thenReturn(TaskCompletionEvent.EMPTY_ARRAY);\r\n    Job job = Job.getInstance(cluster, status, new JobConf());\r\n    Assert.assertNotNull(job.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testUnexpectedJobStatus",
  "errType" : [ "IOException", "NullPointerException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testUnexpectedJobStatus() throws Exception\n{\r\n    Cluster cluster = mock(Cluster.class);\r\n    JobID jobid = new JobID(\"1014873536921\", 6);\r\n    ClientProtocol clientProtocol = mock(ClientProtocol.class);\r\n    when(cluster.getClient()).thenReturn(clientProtocol);\r\n    JobStatus status = new JobStatus(jobid, 0f, 0f, 0f, 0f, State.RUNNING, JobPriority.DEFAULT, \"root\", \"testUnexpectedJobStatus\", \"job file\", \"tracking URL\");\r\n    when(clientProtocol.getJobStatus(jobid)).thenReturn(status);\r\n    Job job = Job.getInstance(cluster, status, new JobConf());\r\n    Assert.assertNotNull(job.getStatus());\r\n    Assert.assertTrue(job.getStatus().getState() == State.RUNNING);\r\n    when(clientProtocol.getJobStatus(jobid)).thenReturn(null);\r\n    try {\r\n        job.updateStatus();\r\n    } catch (IOException e) {\r\n        Assert.assertTrue(e != null && e.getMessage().contains(\"Job status not available\"));\r\n    }\r\n    try {\r\n        ControlledJob cj = new ControlledJob(job, null);\r\n        Assert.assertNotNull(cj.toString());\r\n    } catch (NullPointerException e) {\r\n        Assert.fail(\"job API fails with NPE\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testUGICredentialsPropogation",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testUGICredentialsPropogation() throws Exception\n{\r\n    Credentials creds = new Credentials();\r\n    Token<?> token = mock(Token.class);\r\n    Text tokenService = new Text(\"service\");\r\n    Text secretName = new Text(\"secret\");\r\n    byte[] secret = new byte[] {};\r\n    creds.addToken(tokenService, token);\r\n    creds.addSecretKey(secretName, secret);\r\n    UserGroupInformation.getLoginUser().addCredentials(creds);\r\n    JobConf jobConf = new JobConf();\r\n    Job job = new Job(jobConf);\r\n    assertSame(token, job.getCredentials().getToken(tokenService));\r\n    assertSame(secret, job.getCredentials().getSecretKey(secretName));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup()\n{\r\n    FileSystem.clearStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testScratchDirSize",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testScratchDirSize() throws Exception\n{\r\n    String tmpPath = TEST_DIR + \"/testBytesWrittenLimit-tmpFile-\" + new Random(System.currentTimeMillis()).nextInt();\r\n    File data = new File(tmpPath + \"/out\");\r\n    File testDir = new File(tmpPath);\r\n    testDir.mkdirs();\r\n    testDir.deleteOnExit();\r\n    JobConf conf = new JobConf();\r\n    conf.setStrings(MRConfig.LOCAL_DIR, \"file://\" + tmpPath);\r\n    conf.setLong(MRJobConfig.JOB_SINGLE_DISK_LIMIT_BYTES, 1024L);\r\n    conf.setBoolean(MRJobConfig.JOB_SINGLE_DISK_LIMIT_KILL_LIMIT_EXCEED, true);\r\n    getBaseConfAndWriteToFile(-1, data);\r\n    testScratchDirLimit(false, conf);\r\n    data.delete();\r\n    getBaseConfAndWriteToFile(100, data);\r\n    testScratchDirLimit(false, conf);\r\n    data.delete();\r\n    getBaseConfAndWriteToFile(1536, data);\r\n    testScratchDirLimit(true, conf);\r\n    conf.setBoolean(MRJobConfig.JOB_SINGLE_DISK_LIMIT_KILL_LIMIT_EXCEED, false);\r\n    testScratchDirLimit(false, conf);\r\n    conf.setBoolean(MRJobConfig.JOB_SINGLE_DISK_LIMIT_KILL_LIMIT_EXCEED, true);\r\n    conf.setLong(MRJobConfig.JOB_SINGLE_DISK_LIMIT_BYTES, -1L);\r\n    testScratchDirLimit(false, conf);\r\n    data.delete();\r\n    FileUtil.fullyDelete(testDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getBaseConfAndWriteToFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void getBaseConfAndWriteToFile(int size, File data) throws IOException\n{\r\n    if (size > 0) {\r\n        byte[] b = new byte[size];\r\n        for (int i = 0; i < size; i++) {\r\n            b[i] = 1;\r\n        }\r\n        FileUtils.writeByteArrayToFile(data, b);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testScratchDirLimit",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testScratchDirLimit(boolean fastFail, JobConf conf) throws Exception\n{\r\n    ExitUtil.disableSystemExit();\r\n    threadExited = false;\r\n    Thread.UncaughtExceptionHandler h = new Thread.UncaughtExceptionHandler() {\r\n\r\n        public void uncaughtException(Thread th, Throwable ex) {\r\n            if (ex instanceof ExitUtil.ExitException) {\r\n                threadExited = true;\r\n                th.interrupt();\r\n            }\r\n        }\r\n    };\r\n    Task task = new DummyTask();\r\n    task.setConf(conf);\r\n    DummyTaskReporter reporter = new DummyTaskReporter(task);\r\n    reporter.startDiskLimitCheckerThreadIfNeeded();\r\n    Thread t = new Thread(reporter);\r\n    t.setUncaughtExceptionHandler(h);\r\n    reporter.setProgressFlag();\r\n    t.start();\r\n    while (!reporter.taskLimitIsChecked) {\r\n        Thread.yield();\r\n    }\r\n    task.done(fakeUmbilical, reporter);\r\n    reporter.resetDoneFlag();\r\n    t.join(1000L);\r\n    Assert.assertEquals(fastFail, threadExited);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testTaskProgress",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testTaskProgress() throws Exception\n{\r\n    JobConf job = new JobConf();\r\n    job.setLong(MRJobConfig.TASK_PROGRESS_REPORT_INTERVAL, 1000);\r\n    Task task = new DummyTask();\r\n    task.setConf(job);\r\n    DummyTaskReporter reporter = new DummyTaskReporter(task);\r\n    Thread t = new Thread(reporter);\r\n    t.start();\r\n    Thread.sleep(2100);\r\n    task.setTaskDone();\r\n    reporter.resetDoneFlag();\r\n    t.join();\r\n    assertThat(statusUpdateTimes).isEqualTo(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBytesWrittenRespectingLimit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testBytesWrittenRespectingLimit() throws Exception\n{\r\n    testBytesWrittenLimit(LOCAL_BYTES_WRITTEN + 1024, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBytesWrittenExceedingLimit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testBytesWrittenExceedingLimit() throws Exception\n{\r\n    testBytesWrittenLimit(LOCAL_BYTES_WRITTEN - 1, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBytesWrittenLimit",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testBytesWrittenLimit(long limit, boolean failFast) throws Exception\n{\r\n    ExitUtil.disableSystemExit();\r\n    threadExited = false;\r\n    Thread.UncaughtExceptionHandler h = new Thread.UncaughtExceptionHandler() {\r\n\r\n        public void uncaughtException(Thread th, Throwable ex) {\r\n            System.out.println(\"Uncaught exception: \" + ex);\r\n            if (ex instanceof ExitUtil.ExitException) {\r\n                threadExited = true;\r\n            }\r\n        }\r\n    };\r\n    JobConf conf = new JobConf();\r\n    conf.getLong(MRJobConfig.TASK_PROGRESS_REPORT_INTERVAL, 0);\r\n    conf.setLong(MRJobConfig.TASK_LOCAL_WRITE_LIMIT_BYTES, limit);\r\n    LocalFileSystem localFS = FileSystem.getLocal(conf);\r\n    Path tmpPath = new Path(TEST_DIR + \"/testBytesWrittenLimit-tmpFile-\" + new Random(System.currentTimeMillis()).nextInt());\r\n    FSDataOutputStream out = localFS.create(tmpPath, true);\r\n    out.write(new byte[LOCAL_BYTES_WRITTEN]);\r\n    out.close();\r\n    Task task = new DummyTask();\r\n    task.setConf(conf);\r\n    DummyTaskReporter reporter = new DummyTaskReporter(task);\r\n    Thread t = new Thread(reporter);\r\n    t.setUncaughtExceptionHandler(h);\r\n    reporter.setProgressFlag();\r\n    t.start();\r\n    while (!reporter.taskLimitIsChecked) {\r\n        Thread.yield();\r\n    }\r\n    task.setTaskDone();\r\n    reporter.resetDoneFlag();\r\n    t.join();\r\n    Assert.assertEquals(failFast, threadExited);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testFileOutputCommitterOverrride",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testFileOutputCommitterOverrride() throws Throwable\n{\r\n    TaskContext context = new TaskContext();\r\n    Path workPath = new Path(\"file:///work\");\r\n    context.setOutputCommitter(new SimpleCommitter(new Path(\"/\"), context, workPath));\r\n    assertEquals(workPath, FileOutputFormat.getWorkOutputPath(context));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testFileOutputCommitterNullWorkPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testFileOutputCommitterNullWorkPath() throws Throwable\n{\r\n    TaskContext context = new TaskContext();\r\n    context.setOutputCommitter(new SimpleCommitter(new Path(\"/\"), context, null));\r\n    assertNull(FileOutputFormat.getWorkOutputPath(context));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testDepricatedMethods",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testDepricatedMethods() throws IOException\n{\r\n    JobID jid = new JobID();\r\n    TaskID test = new TaskID(jid, true, 1);\r\n    assertThat(test.getTaskType()).isEqualTo(TaskType.MAP);\r\n    test = new TaskID(jid, false, 1);\r\n    assertThat(test.getTaskType()).isEqualTo(TaskType.REDUCE);\r\n    test = new TaskID(\"001\", 1, false, 1);\r\n    assertThat(test.getTaskType()).isEqualTo(TaskType.REDUCE);\r\n    test = new TaskID(\"001\", 1, true, 1);\r\n    assertThat(test.getTaskType()).isEqualTo(TaskType.MAP);\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    test.write(new DataOutputStream(out));\r\n    TaskID ti = TaskID.read(new DataInputStream(new ByteArrayInputStream(out.toByteArray())));\r\n    assertEquals(ti.toString(), test.toString());\r\n    assertEquals(\"task_001_0001_m_000002\", TaskID.getTaskIDsPattern(\"001\", 1, true, 2));\r\n    assertEquals(\"task_003_0001_m_000004\", TaskID.getTaskIDsPattern(\"003\", 1, TaskType.MAP, 4));\r\n    assertEquals(\"003_0001_m_000004\", TaskID.getTaskIDsPatternWOPrefix(\"003\", 1, TaskType.MAP, 4).toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJobID",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testJobID() throws IOException\n{\r\n    JobID jid = new JobID(\"001\", 2);\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    jid.write(new DataOutputStream(out));\r\n    assertEquals(jid, JobID.read(new DataInputStream(new ByteArrayInputStream(out.toByteArray()))));\r\n    assertEquals(\"job_001_0001\", JobID.getJobIDsPattern(\"001\", 1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testTaskCompletionEvent",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testTaskCompletionEvent()\n{\r\n    TaskAttemptID taid = new TaskAttemptID(\"001\", 1, TaskType.REDUCE, 2, 3);\r\n    TaskCompletionEvent template = new TaskCompletionEvent(12, taid, 13, true, Status.SUCCEEDED, \"httptracker\");\r\n    TaskCompletionEvent testEl = TaskCompletionEvent.downgrade(template);\r\n    testEl.setTaskAttemptId(taid);\r\n    testEl.setTaskTrackerHttp(\"httpTracker\");\r\n    testEl.setTaskId(\"attempt_001_0001_m_000002_04\");\r\n    assertEquals(\"attempt_001_0001_m_000002_4\", testEl.getTaskId());\r\n    testEl.setTaskStatus(Status.OBSOLETE);\r\n    assertEquals(Status.OBSOLETE.toString(), testEl.getStatus().toString());\r\n    testEl.setTaskRunTime(20);\r\n    assertThat(testEl.getTaskRunTime()).isEqualTo(20);\r\n    testEl.setEventId(16);\r\n    assertThat(testEl.getEventId()).isEqualTo(16);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJobProfile",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testJobProfile() throws IOException\n{\r\n    JobProfile profile = new JobProfile(\"user\", \"job_001_03\", \"jobFile\", \"uri\", \"name\");\r\n    assertEquals(\"job_001_0003\", profile.getJobId());\r\n    assertEquals(\"default\", profile.getQueueName());\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    profile.write(new DataOutputStream(out));\r\n    JobProfile profile2 = new JobProfile();\r\n    profile2.readFields(new DataInputStream(new ByteArrayInputStream(out.toByteArray())));\r\n    assertEquals(profile2.name, profile.name);\r\n    assertEquals(profile2.jobFile, profile.jobFile);\r\n    assertEquals(profile2.queueName, profile.queueName);\r\n    assertEquals(profile2.url, profile.url);\r\n    assertEquals(profile2.user, profile.user);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testTaskAttemptID",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testTaskAttemptID()\n{\r\n    TaskAttemptID task = new TaskAttemptID(\"001\", 2, true, 3, 4);\r\n    assertEquals(\"attempt_001_0002_m_000003_4\", TaskAttemptID.getTaskAttemptIDsPattern(\"001\", 2, true, 3, 4));\r\n    assertEquals(\"task_001_0002_m_000003\", task.getTaskID().toString());\r\n    assertEquals(\"attempt_001_0001_r_000002_3\", TaskAttemptID.getTaskAttemptIDsPattern(\"001\", 1, TaskType.REDUCE, 2, 3));\r\n    assertEquals(\"001_0001_m_000001_2\", TaskAttemptID.getTaskAttemptIDsPatternWOPrefix(\"001\", 1, TaskType.MAP, 1, 2).toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReporter",
  "errType" : [ "UnsupportedOperationException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testReporter()\n{\r\n    Reporter nullReporter = Reporter.NULL;\r\n    assertNull(nullReporter.getCounter(null));\r\n    assertNull(nullReporter.getCounter(\"group\", \"name\"));\r\n    try {\r\n        assertNull(nullReporter.getInputSplit());\r\n    } catch (UnsupportedOperationException e) {\r\n        assertEquals(\"NULL reporter has no input\", e.getMessage());\r\n    }\r\n    assertEquals(0, nullReporter.getProgress(), 0.01);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    failures = new UnreliableManifestStoreOperations(createManifestStoreOperations());\r\n    setStoreOperations(failures);\r\n    Path destDir = methodPath();\r\n    StageConfig stageConfig = createStageConfigForJob(JOB1, destDir);\r\n    setJobStageConfig(stageConfig);\r\n    new SetupJobStage(stageConfig).apply(true);\r\n    manifests = executeTaskAttempts(TASK_ATTEMPT_COUNT, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testCleanupInParallelHealthy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCleanupInParallelHealthy() throws Throwable\n{\r\n    describe(\"parallel cleanup of TA dirs.\");\r\n    cleanup(true, true, false, CleanupJobStage.Outcome.PARALLEL_DELETE, PARALLEL_DELETE_COUNT);\r\n    verifyJobDirsCleanedUp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testCleanupSingletonHealthy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCleanupSingletonHealthy() throws Throwable\n{\r\n    describe(\"Cleanup with a single delete. Not the default; would be best on HDFS\");\r\n    cleanup(true, false, false, CleanupJobStage.Outcome.DELETED, ROOT_DELETE_COUNT);\r\n    verifyJobDirsCleanedUp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testCleanupNoDir",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCleanupNoDir() throws Throwable\n{\r\n    describe(\"parallel cleanup MUST not fail if there's no dir\");\r\n    cleanup(true, true, false, CleanupJobStage.Outcome.PARALLEL_DELETE, PARALLEL_DELETE_COUNT);\r\n    cleanup(true, false, false, CleanupJobStage.Outcome.NOTHING_TO_CLEAN_UP, 0);\r\n    cleanup(false, true, false, CleanupJobStage.Outcome.DISABLED, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testFailureInParallelDelete",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testFailureInParallelDelete() throws Throwable\n{\r\n    describe(\"A parallel delete fails, but the base delete works\");\r\n    TaskManifest manifest = manifests.get(4);\r\n    Path taPath = new Path(manifest.getTaskAttemptDir());\r\n    failures.addDeletePathToFail(taPath);\r\n    cleanup(true, true, false, CleanupJobStage.Outcome.DELETED, PARALLEL_DELETE_COUNT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testParallelDeleteNoTaskAttemptDir",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testParallelDeleteNoTaskAttemptDir() throws Throwable\n{\r\n    describe(\"Execute parallel delete where\" + \" the job task directory does not exist\");\r\n    StageConfig stageConfig = getJobStageConfig();\r\n    failures.addPathNotFound(stageConfig.getJobAttemptTaskSubDir());\r\n    cleanup(true, true, false, CleanupJobStage.Outcome.DELETED, ROOT_DELETE_COUNT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    Path destDir = methodPath();\r\n    StageConfig stageConfig = createStageConfigForJob(JOB1, destDir);\r\n    setJobStageConfig(stageConfig);\r\n    new SetupJobStage(stageConfig).apply(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testCommitMissingDirectory",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCommitMissingDirectory() throws Throwable\n{\r\n    String tid = String.format(\"task_%03d\", 1);\r\n    String taskAttemptId = String.format(\"%s_%02d\", tid, 1);\r\n    StageConfig taskStageConfig = createTaskStageConfig(JOB1, tid, taskAttemptId);\r\n    Path taDir = taskStageConfig.getTaskAttemptDir();\r\n    assertPathDoesNotExist(\"task attempt path\", taDir);\r\n    intercept(FileNotFoundException.class, () -> new CommitTaskStage(taskStageConfig).apply(null));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testCommitEmptyDirectory",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testCommitEmptyDirectory() throws Throwable\n{\r\n    describe(\"Commit an empty directory as task then job\");\r\n    String tid = String.format(\"task_%03d\", 2);\r\n    String taskAttemptId = String.format(\"%s_%02d\", tid, 1);\r\n    StageConfig taskStageConfig = createTaskStageConfig(JOB1, tid, taskAttemptId);\r\n    new SetupTaskStage(taskStageConfig).apply(\"setup\");\r\n    CommitTaskStage.Result result = new CommitTaskStage(taskStageConfig).apply(null);\r\n    final TaskManifest manifest = result.getTaskManifest();\r\n    Assertions.assertThat(manifest.getDestDirectories()).as(\"directories to create\").isEmpty();\r\n    Assertions.assertThat(manifest.getFilesToCommit()).as(\"files to commit\").isEmpty();\r\n    final Path path = result.getPath();\r\n    final String manifestBody = readText(path);\r\n    LOG.info(\"manifest at {} of length {}:\\n{}\", path, manifestBody.length(), manifestBody);\r\n    final CommitJobStage.Result outcome = new CommitJobStage(getJobStageConfig()).apply(new CommitJobStage.Arguments(true, true, null, new CleanupJobStage.Arguments(OP_STAGE_JOB_CLEANUP, true, true, false)));\r\n    final Path successPath = outcome.getSuccessPath();\r\n    String successBody = readText(successPath);\r\n    LOG.info(\"successBody at {} of length {}:\\n{}\", successPath, successBody.length(), successBody);\r\n    final ManifestSuccessData successData = outcome.getJobSuccessData();\r\n    Assertions.assertThat(successData.getFilenames()).as(\"Filenames in _SUCCESS\").isEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "data",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> data()\n{\r\n    Object[][] data = new Object[][] { { 1 }, { 5 } };\r\n    return Arrays.asList(data);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    LOG.info(\"Using Test Dir: \" + TEST_ROOT_DIR);\r\n    localFs = FileSystem.getLocal(new Configuration());\r\n    localFs.delete(TEST_ROOT_DIR, true);\r\n    localFs.mkdirs(TEST_ROOT_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup() throws IOException\n{\r\n    localFs.delete(TEST_ROOT_DIR, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testListLocatedStatus",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testListLocatedStatus() throws Exception\n{\r\n    Configuration conf = getConfiguration();\r\n    conf.setBoolean(\"fs.test.impl.disable.cache\", false);\r\n    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);\r\n    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, \"test:///a1/a2\");\r\n    MockFileSystem mockFs = (MockFileSystem) new Path(\"test:///\").getFileSystem(conf);\r\n    Assert.assertEquals(\"listLocatedStatus already called\", 0, mockFs.numListLocatedStatusCalls);\r\n    JobConf job = new JobConf(conf);\r\n    TextInputFormat fileInputFormat = new TextInputFormat();\r\n    fileInputFormat.configure(job);\r\n    InputSplit[] splits = fileInputFormat.getSplits(job, 1);\r\n    Assert.assertEquals(\"Input splits are not correct\", 2, splits.length);\r\n    Assert.assertEquals(\"listLocatedStatuss calls\", 1, mockFs.numListLocatedStatusCalls);\r\n    FileSystem.closeAll();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testIgnoreDirs",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testIgnoreDirs() throws Exception\n{\r\n    Configuration conf = getConfiguration();\r\n    conf.setBoolean(FileInputFormat.INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, true);\r\n    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);\r\n    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, \"test:///a1\");\r\n    MockFileSystem mockFs = (MockFileSystem) new Path(\"test:///\").getFileSystem(conf);\r\n    JobConf job = new JobConf(conf);\r\n    TextInputFormat fileInputFormat = new TextInputFormat();\r\n    fileInputFormat.configure(job);\r\n    InputSplit[] splits = fileInputFormat.getSplits(job, 1);\r\n    Assert.assertEquals(\"Input splits are not correct\", 1, splits.length);\r\n    FileSystem.closeAll();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testSplitLocationInfo",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testSplitLocationInfo() throws Exception\n{\r\n    Configuration conf = getConfiguration();\r\n    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, \"test:///a1/a2\");\r\n    JobConf job = new JobConf(conf);\r\n    TextInputFormat fileInputFormat = new TextInputFormat();\r\n    fileInputFormat.configure(job);\r\n    FileSplit[] splits = (FileSplit[]) fileInputFormat.getSplits(job, 1);\r\n    String[] locations = splits[0].getLocations();\r\n    Assert.assertEquals(2, locations.length);\r\n    SplitLocationInfo[] locationInfo = splits[0].getLocationInfo();\r\n    Assert.assertEquals(2, locationInfo.length);\r\n    SplitLocationInfo localhostInfo = locations[0].equals(\"localhost\") ? locationInfo[0] : locationInfo[1];\r\n    SplitLocationInfo otherhostInfo = locations[0].equals(\"otherhost\") ? locationInfo[0] : locationInfo[1];\r\n    Assert.assertTrue(localhostInfo.isOnDisk());\r\n    Assert.assertTrue(localhostInfo.isInMemory());\r\n    Assert.assertTrue(otherhostInfo.isOnDisk());\r\n    Assert.assertFalse(otherhostInfo.isInMemory());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testListStatusSimple",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testListStatusSimple() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);\r\n    List<Path> expectedPaths = org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat.configureTestSimple(conf, localFs);\r\n    JobConf jobConf = new JobConf(conf);\r\n    TextInputFormat fif = new TextInputFormat();\r\n    fif.configure(jobConf);\r\n    FileStatus[] statuses = fif.listStatus(jobConf);\r\n    org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat.verifyFileStatuses(expectedPaths, Lists.newArrayList(statuses), localFs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testListStatusNestedRecursive",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testListStatusNestedRecursive() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);\r\n    List<Path> expectedPaths = org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat.configureTestNestedRecursive(conf, localFs);\r\n    JobConf jobConf = new JobConf(conf);\r\n    TextInputFormat fif = new TextInputFormat();\r\n    fif.configure(jobConf);\r\n    FileStatus[] statuses = fif.listStatus(jobConf);\r\n    org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat.verifyFileStatuses(expectedPaths, Lists.newArrayList(statuses), localFs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testListStatusNestedNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testListStatusNestedNonRecursive() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);\r\n    List<Path> expectedPaths = org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat.configureTestNestedNonRecursive(conf, localFs);\r\n    JobConf jobConf = new JobConf(conf);\r\n    TextInputFormat fif = new TextInputFormat();\r\n    fif.configure(jobConf);\r\n    FileStatus[] statuses = fif.listStatus(jobConf);\r\n    org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat.verifyFileStatuses(expectedPaths, Lists.newArrayList(statuses), localFs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testListStatusErrorOnNonExistantDir",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testListStatusErrorOnNonExistantDir() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);\r\n    org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat.configureTestErrorOnNonExistantDir(conf, localFs);\r\n    JobConf jobConf = new JobConf(conf);\r\n    TextInputFormat fif = new TextInputFormat();\r\n    fif.configure(jobConf);\r\n    try {\r\n        fif.listStatus(jobConf);\r\n        Assert.fail(\"Expecting an IOException for a missing Input path\");\r\n    } catch (IOException e) {\r\n        Path expectedExceptionPath = new Path(TEST_ROOT_DIR, \"input2\");\r\n        expectedExceptionPath = localFs.makeQualified(expectedExceptionPath);\r\n        Assert.assertTrue(e instanceof InvalidInputException);\r\n        Assert.assertEquals(\"Input path does not exist: \" + expectedExceptionPath.toString(), e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(\"fs.test.impl.disable.cache\", \"true\");\r\n    conf.setClass(\"fs.test.impl\", MockFileSystem.class, FileSystem.class);\r\n    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, \"test:///a1\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileStatus getFileStatus(final Path path) throws IOException\n{\r\n    return new TaggedFileStatus(0, false, 1, 1024, 0, path, path.getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean delete(final Path path, final boolean recursive) throws IOException\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean mkdirs(final Path path) throws IOException\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "renameFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean renameFile(final Path source, final Path dest) throws IOException\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "listStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RemoteIterator<FileStatus> listStatusIterator(final Path path) throws IOException\n{\r\n    return new EmptyRemoteIterator<>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "loadTaskManifest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskManifest loadTaskManifest(JsonSerialization<TaskManifest> serializer, final FileStatus st) throws IOException\n{\r\n    return new TaskManifest();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "save",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void save(T manifestData, final Path path, final boolean overwrite) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "storePreservesEtagsThroughRenames",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean storePreservesEtagsThroughRenames(final Path path)\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    conf = new Configuration();\r\n    clientProtocol = mock(ClientProtocol.class);\r\n    Cluster cluster = mock(Cluster.class);\r\n    when(cluster.getConf()).thenReturn(conf);\r\n    when(cluster.getClient()).thenReturn(clientProtocol);\r\n    JobStatus jobStatus = new JobStatus(new JobID(\"job_000\", 1), 0f, 0f, 0f, 0f, State.RUNNING, JobPriority.HIGH, \"tmp-user\", \"tmp-jobname\", \"tmp-jobfile\", \"tmp-url\");\r\n    job = Job.getInstance(cluster, jobStatus, conf);\r\n    job = spy(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testJobMonitorAndPrint",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testJobMonitorAndPrint() throws Exception\n{\r\n    JobStatus jobStatus_1 = new JobStatus(new JobID(\"job_000\", 1), 1f, 0.1f, 0.1f, 0f, State.RUNNING, JobPriority.HIGH, \"tmp-user\", \"tmp-jobname\", \"tmp-queue\", \"tmp-jobfile\", \"tmp-url\", true);\r\n    JobStatus jobStatus_2 = new JobStatus(new JobID(\"job_000\", 1), 1f, 1f, 1f, 1f, State.SUCCEEDED, JobPriority.HIGH, \"tmp-user\", \"tmp-jobname\", \"tmp-queue\", \"tmp-jobfile\", \"tmp-url\", true);\r\n    doAnswer((Answer<TaskCompletionEvent[]>) invocation -> TaskCompletionEvent.EMPTY_ARRAY).when(job).getTaskCompletionEvents(anyInt(), anyInt());\r\n    doReturn(new TaskReport[5]).when(job).getTaskReports(isA(TaskType.class));\r\n    when(clientProtocol.getJobStatus(any(JobID.class))).thenReturn(jobStatus_1, jobStatus_2);\r\n    Layout layout = Logger.getRootLogger().getAppender(\"stdout\").getLayout();\r\n    ByteArrayOutputStream os = new ByteArrayOutputStream();\r\n    WriterAppender appender = new WriterAppender(layout, os);\r\n    appender.setThreshold(Level.ALL);\r\n    Logger qlogger = Logger.getLogger(Job.class);\r\n    qlogger.addAppender(appender);\r\n    job.monitorAndPrintJob();\r\n    qlogger.removeAppender(appender);\r\n    LineNumberReader r = new LineNumberReader(new StringReader(os.toString()));\r\n    String line;\r\n    boolean foundHundred = false;\r\n    boolean foundComplete = false;\r\n    boolean foundUber = false;\r\n    String uberModeMatch = \"uber mode : true\";\r\n    String progressMatch = \"map 100% reduce 100%\";\r\n    String completionMatch = \"completed successfully\";\r\n    while ((line = r.readLine()) != null) {\r\n        if (line.contains(uberModeMatch)) {\r\n            foundUber = true;\r\n        }\r\n        foundHundred = line.contains(progressMatch);\r\n        if (foundHundred)\r\n            break;\r\n    }\r\n    line = r.readLine();\r\n    foundComplete = line.contains(completionMatch);\r\n    assertTrue(foundUber);\r\n    assertTrue(foundHundred);\r\n    assertTrue(foundComplete);\r\n    System.out.println(\"The output of job.toString() is : \\n\" + job.toString());\r\n    assertTrue(job.toString().contains(\"Number of maps: 5\\n\"));\r\n    assertTrue(job.toString().contains(\"Number of reduces: 5\\n\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "setupClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupClass() throws Exception\n{\r\n    testRootDir = GenericTestUtils.setupTestRootDir(TestMerger.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    unitTestDir = new File(testRootDir, unitTestName.getMethodName());\r\n    unitTestDir.mkdirs();\r\n    jobConf = new JobConf();\r\n    MRJobConfUtil.setLocalDirectoriesConfigForTesting(jobConf, unitTestDir);\r\n    jobConf.set(MRConfig.FRAMEWORK_NAME, \"local\");\r\n    fs = FileSystem.getLocal(jobConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testEncryptedMerger",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testEncryptedMerger() throws Throwable\n{\r\n    MRJobConfUtil.initEncryptedIntermediateConfigsForTesting(jobConf);\r\n    Credentials credentials = UserGroupInformation.getCurrentUser().getCredentials();\r\n    TokenCache.setEncryptedSpillKey(new byte[16], credentials);\r\n    UserGroupInformation.getCurrentUser().addCredentials(credentials);\r\n    testInMemoryAndOnDiskMerger();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testInMemoryAndOnDiskMerger",
  "errType" : null,
  "containingMethodsNum" : 42,
  "sourceCodeText" : "void testInMemoryAndOnDiskMerger() throws Throwable\n{\r\n    JobID jobId = new JobID(\"a\", 0);\r\n    TaskAttemptID reduceId1 = new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 0), 0);\r\n    TaskAttemptID mapId1 = new TaskAttemptID(new TaskID(jobId, TaskType.MAP, 1), 0);\r\n    TaskAttemptID mapId2 = new TaskAttemptID(new TaskID(jobId, TaskType.MAP, 2), 0);\r\n    LocalDirAllocator lda = new LocalDirAllocator(MRConfig.LOCAL_DIR);\r\n    MergeManagerImpl<Text, Text> mergeManager = new MergeManagerImpl<Text, Text>(reduceId1, jobConf, fs, lda, Reporter.NULL, null, null, null, null, null, null, null, new Progress(), new MROutputFiles());\r\n    Map<String, String> map1 = new TreeMap<String, String>();\r\n    map1.put(\"apple\", \"disgusting\");\r\n    map1.put(\"carrot\", \"delicious\");\r\n    Map<String, String> map2 = new TreeMap<String, String>();\r\n    map1.put(\"banana\", \"pretty good\");\r\n    byte[] mapOutputBytes1 = writeMapOutput(jobConf, map1);\r\n    byte[] mapOutputBytes2 = writeMapOutput(jobConf, map2);\r\n    InMemoryMapOutput<Text, Text> mapOutput1 = new InMemoryMapOutput<Text, Text>(jobConf, mapId1, mergeManager, mapOutputBytes1.length, null, true);\r\n    InMemoryMapOutput<Text, Text> mapOutput2 = new InMemoryMapOutput<Text, Text>(jobConf, mapId2, mergeManager, mapOutputBytes2.length, null, true);\r\n    System.arraycopy(mapOutputBytes1, 0, mapOutput1.getMemory(), 0, mapOutputBytes1.length);\r\n    System.arraycopy(mapOutputBytes2, 0, mapOutput2.getMemory(), 0, mapOutputBytes2.length);\r\n    MergeThread<InMemoryMapOutput<Text, Text>, Text, Text> inMemoryMerger = mergeManager.createInMemoryMerger();\r\n    List<InMemoryMapOutput<Text, Text>> mapOutputs1 = new ArrayList<InMemoryMapOutput<Text, Text>>();\r\n    mapOutputs1.add(mapOutput1);\r\n    mapOutputs1.add(mapOutput2);\r\n    inMemoryMerger.merge(mapOutputs1);\r\n    Assert.assertEquals(1, mergeManager.onDiskMapOutputs.size());\r\n    TaskAttemptID reduceId2 = new TaskAttemptID(new TaskID(jobId, TaskType.REDUCE, 3), 0);\r\n    TaskAttemptID mapId3 = new TaskAttemptID(new TaskID(jobId, TaskType.MAP, 4), 0);\r\n    TaskAttemptID mapId4 = new TaskAttemptID(new TaskID(jobId, TaskType.MAP, 5), 0);\r\n    Map<String, String> map3 = new TreeMap<String, String>();\r\n    map3.put(\"apple\", \"awesome\");\r\n    map3.put(\"carrot\", \"amazing\");\r\n    Map<String, String> map4 = new TreeMap<String, String>();\r\n    map4.put(\"banana\", \"bla\");\r\n    byte[] mapOutputBytes3 = writeMapOutput(jobConf, map3);\r\n    byte[] mapOutputBytes4 = writeMapOutput(jobConf, map4);\r\n    InMemoryMapOutput<Text, Text> mapOutput3 = new InMemoryMapOutput<Text, Text>(jobConf, mapId3, mergeManager, mapOutputBytes3.length, null, true);\r\n    InMemoryMapOutput<Text, Text> mapOutput4 = new InMemoryMapOutput<Text, Text>(jobConf, mapId4, mergeManager, mapOutputBytes4.length, null, true);\r\n    System.arraycopy(mapOutputBytes3, 0, mapOutput3.getMemory(), 0, mapOutputBytes3.length);\r\n    System.arraycopy(mapOutputBytes4, 0, mapOutput4.getMemory(), 0, mapOutputBytes4.length);\r\n    MergeThread<InMemoryMapOutput<Text, Text>, Text, Text> inMemoryMerger2 = mergeManager.createInMemoryMerger();\r\n    List<InMemoryMapOutput<Text, Text>> mapOutputs2 = new ArrayList<InMemoryMapOutput<Text, Text>>();\r\n    mapOutputs2.add(mapOutput3);\r\n    mapOutputs2.add(mapOutput4);\r\n    inMemoryMerger2.merge(mapOutputs2);\r\n    Assert.assertEquals(2, mergeManager.onDiskMapOutputs.size());\r\n    List<CompressAwarePath> paths = new ArrayList<CompressAwarePath>();\r\n    Iterator<CompressAwarePath> iterator = mergeManager.onDiskMapOutputs.iterator();\r\n    List<String> keys = new ArrayList<String>();\r\n    List<String> values = new ArrayList<String>();\r\n    while (iterator.hasNext()) {\r\n        CompressAwarePath next = iterator.next();\r\n        readOnDiskMapOutput(jobConf, fs, next, keys, values);\r\n        paths.add(next);\r\n    }\r\n    assertThat(keys).isEqualTo(Arrays.asList(\"apple\", \"banana\", \"carrot\", \"apple\", \"banana\", \"carrot\"));\r\n    assertThat(values).isEqualTo(Arrays.asList(\"awesome\", \"bla\", \"amazing\", \"disgusting\", \"pretty good\", \"delicious\"));\r\n    mergeManager.close();\r\n    mergeManager = new MergeManagerImpl<Text, Text>(reduceId2, jobConf, fs, lda, Reporter.NULL, null, null, null, null, null, null, null, new Progress(), new MROutputFiles());\r\n    MergeThread<CompressAwarePath, Text, Text> onDiskMerger = mergeManager.createOnDiskMerger();\r\n    onDiskMerger.merge(paths);\r\n    Assert.assertEquals(1, mergeManager.onDiskMapOutputs.size());\r\n    keys = new ArrayList<String>();\r\n    values = new ArrayList<String>();\r\n    readOnDiskMapOutput(jobConf, fs, mergeManager.onDiskMapOutputs.iterator().next(), keys, values);\r\n    assertThat(keys).isEqualTo(Arrays.asList(\"apple\", \"apple\", \"banana\", \"banana\", \"carrot\", \"carrot\"));\r\n    assertThat(values).isEqualTo(Arrays.asList(\"awesome\", \"disgusting\", \"pretty good\", \"bla\", \"amazing\", \"delicious\"));\r\n    mergeManager.close();\r\n    Assert.assertEquals(0, mergeManager.inMemoryMapOutputs.size());\r\n    Assert.assertEquals(0, mergeManager.inMemoryMergedMapOutputs.size());\r\n    Assert.assertEquals(0, mergeManager.onDiskMapOutputs.size());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "writeMapOutput",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "byte[] writeMapOutput(Configuration conf, Map<String, String> keysToValues) throws IOException\n{\r\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\r\n    FSDataOutputStream fsdos = new FSDataOutputStream(baos, null);\r\n    IFile.Writer<Text, Text> writer = new IFile.Writer<Text, Text>(conf, fsdos, Text.class, Text.class, null, null);\r\n    for (String key : keysToValues.keySet()) {\r\n        String value = keysToValues.get(key);\r\n        writer.append(new Text(key), new Text(value));\r\n    }\r\n    writer.close();\r\n    return baos.toByteArray();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "readOnDiskMapOutput",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void readOnDiskMapOutput(Configuration conf, FileSystem fs, Path path, List<String> keys, List<String> values) throws IOException\n{\r\n    FSDataInputStream in = IntermediateEncryptedStream.wrapIfNecessary(conf, fs.open(path), path);\r\n    IFile.Reader<Text, Text> reader = new IFile.Reader<Text, Text>(conf, in, fs.getFileStatus(path).getLen(), null, null);\r\n    DataInputBuffer keyBuff = new DataInputBuffer();\r\n    DataInputBuffer valueBuff = new DataInputBuffer();\r\n    Text key = new Text();\r\n    Text value = new Text();\r\n    while (reader.nextRawKey(keyBuff)) {\r\n        key.readFields(keyBuff);\r\n        keys.add(key.toString());\r\n        reader.nextRawValue(valueBuff);\r\n        value.readFields(valueBuff);\r\n        values.add(value.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testCompressed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCompressed() throws IOException\n{\r\n    testMergeShouldReturnProperProgress(getCompressedSegments());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testUncompressed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testUncompressed() throws IOException\n{\r\n    testMergeShouldReturnProperProgress(getUncompressedSegments());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testMergeShouldReturnProperProgress",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testMergeShouldReturnProperProgress(List<Segment<Text, Text>> segments) throws IOException\n{\r\n    Path tmpDir = new Path(jobConf.get(\"mapreduce.cluster.temp.dir\"), \"localpath\");\r\n    Class<Text> keyClass = (Class<Text>) jobConf.getMapOutputKeyClass();\r\n    Class<Text> valueClass = (Class<Text>) jobConf.getMapOutputValueClass();\r\n    RawComparator<Text> comparator = jobConf.getOutputKeyComparator();\r\n    Counter readsCounter = new Counter();\r\n    Counter writesCounter = new Counter();\r\n    Progress mergePhase = new Progress();\r\n    RawKeyValueIterator mergeQueue = Merger.merge(jobConf, fs, keyClass, valueClass, segments, 2, tmpDir, comparator, getReporter(), readsCounter, writesCounter, mergePhase);\r\n    final float epsilon = 0.00001f;\r\n    Assert.assertEquals(2 / 6.0f, mergeQueue.getProgress().get(), epsilon);\r\n    Assert.assertTrue(mergeQueue.next());\r\n    Assert.assertEquals(2 / 6.0f, mergeQueue.getProgress().get(), epsilon);\r\n    Assert.assertTrue(mergeQueue.next());\r\n    Assert.assertEquals(3 / 6.0f, mergeQueue.getProgress().get(), epsilon);\r\n    Assert.assertTrue(mergeQueue.next());\r\n    Assert.assertEquals(4 / 6.0f, mergeQueue.getProgress().get(), epsilon);\r\n    Assert.assertTrue(mergeQueue.next());\r\n    Assert.assertEquals(4 / 6.0f, mergeQueue.getProgress().get(), epsilon);\r\n    Assert.assertTrue(mergeQueue.next());\r\n    Assert.assertEquals(5 / 6.0f, mergeQueue.getProgress().get(), epsilon);\r\n    Assert.assertTrue(mergeQueue.next());\r\n    Assert.assertEquals(1.0f, mergeQueue.getProgress().get(), epsilon);\r\n    Assert.assertFalse(mergeQueue.next());\r\n    Assert.assertEquals(1.0f, mergeQueue.getProgress().get(), epsilon);\r\n    Assert.assertTrue(mergeQueue.getKey() == null);\r\n    Assert.assertEquals(0, mergeQueue.getValue().getData().length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getReporter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Progressable getReporter()\n{\r\n    Progressable reporter = new Progressable() {\r\n\r\n        @Override\r\n        public void progress() {\r\n        }\r\n    };\r\n    return reporter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getUncompressedSegments",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<Segment<Text, Text>> getUncompressedSegments() throws IOException\n{\r\n    List<Segment<Text, Text>> segments = new ArrayList<Segment<Text, Text>>();\r\n    for (int i = 0; i < 2; i++) {\r\n        segments.add(getUncompressedSegment(i));\r\n    }\r\n    return segments;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getCompressedSegments",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<Segment<Text, Text>> getCompressedSegments() throws IOException\n{\r\n    List<Segment<Text, Text>> segments = new ArrayList<Segment<Text, Text>>();\r\n    for (int i = 0; i < 2; i++) {\r\n        segments.add(getCompressedSegment(i));\r\n    }\r\n    return segments;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getUncompressedSegment",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Segment<Text, Text> getUncompressedSegment(int i) throws IOException\n{\r\n    return new Segment<Text, Text>(getReader(i, false), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getCompressedSegment",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Segment<Text, Text> getCompressedSegment(int i) throws IOException\n{\r\n    return new Segment<Text, Text>(getReader(i, true), false, 3000l);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getReader",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Reader<Text, Text> getReader(int i, boolean isCompressedInput) throws IOException\n{\r\n    Reader<Text, Text> readerMock = mock(Reader.class);\r\n    when(readerMock.getLength()).thenReturn(30l);\r\n    when(readerMock.getPosition()).thenReturn(0l).thenReturn(10l).thenReturn(20l);\r\n    when(readerMock.nextRawKey(any(DataInputBuffer.class))).thenAnswer(getKeyAnswer(\"Segment\" + i, isCompressedInput));\r\n    doAnswer(getValueAnswer(\"Segment\" + i)).when(readerMock).nextRawValue(any(DataInputBuffer.class));\r\n    return readerMock;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getKeyAnswer",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Answer<?> getKeyAnswer(final String segmentName, final boolean isCompressedInput)\n{\r\n    return new Answer<Object>() {\r\n\r\n        int i = 0;\r\n\r\n        @SuppressWarnings(\"unchecked\")\r\n        public Boolean answer(InvocationOnMock invocation) {\r\n            if (i++ == 3) {\r\n                return false;\r\n            }\r\n            Reader<Text, Text> mock = (Reader<Text, Text>) invocation.getMock();\r\n            int multiplier = isCompressedInput ? 100 : 1;\r\n            mock.bytesRead += 10 * multiplier;\r\n            Object[] args = invocation.getArguments();\r\n            DataInputBuffer key = (DataInputBuffer) args[0];\r\n            key.reset((\"Segment Key \" + segmentName + i).getBytes(), 20);\r\n            return true;\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getValueAnswer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Answer<?> getValueAnswer(final String segmentName)\n{\r\n    return new Answer<Void>() {\r\n\r\n        int i = 0;\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            Object[] args = invocation.getArguments();\r\n            DataInputBuffer key = (DataInputBuffer) args[0];\r\n            key.reset((\"Segment Value \" + segmentName + i).getBytes(), 20);\r\n            return null;\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "testListAttemptIdsWithValidInput",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testListAttemptIdsWithValidInput() throws Exception\n{\r\n    JobID jobId = JobID.forName(jobIdStr);\r\n    Cluster mockCluster = mock(Cluster.class);\r\n    Job job = mock(Job.class);\r\n    CLI cli = spy(new CLI(new Configuration()));\r\n    doReturn(mockCluster).when(cli).createCluster();\r\n    when(job.getTaskReports(TaskType.MAP)).thenReturn(getTaskReports(jobId, TaskType.MAP));\r\n    when(job.getTaskReports(TaskType.REDUCE)).thenReturn(getTaskReports(jobId, TaskType.REDUCE));\r\n    when(mockCluster.getJob(jobId)).thenReturn(job);\r\n    int retCode_MAP = cli.run(new String[] { \"-list-attempt-ids\", jobIdStr, \"MAP\", \"running\" });\r\n    int retCode_map = cli.run(new String[] { \"-list-attempt-ids\", jobIdStr, \"map\", \"running\" });\r\n    int retCode_REDUCE = cli.run(new String[] { \"-list-attempt-ids\", jobIdStr, \"REDUCE\", \"running\" });\r\n    int retCode_completed = cli.run(new String[] { \"-list-attempt-ids\", jobIdStr, \"REDUCE\", \"completed\" });\r\n    assertEquals(\"MAP is a valid input,exit code should be 0\", 0, retCode_MAP);\r\n    assertEquals(\"map is a valid input,exit code should be 0\", 0, retCode_map);\r\n    assertEquals(\"REDUCE is a valid input,exit code should be 0\", 0, retCode_REDUCE);\r\n    assertEquals(\"REDUCE and completed are a valid inputs to -list-attempt-ids,exit code should be 0\", 0, retCode_completed);\r\n    verify(job, times(2)).getTaskReports(TaskType.MAP);\r\n    verify(job, times(2)).getTaskReports(TaskType.REDUCE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "testListAttemptIdsWithInvalidInputs",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testListAttemptIdsWithInvalidInputs() throws Exception\n{\r\n    JobID jobId = JobID.forName(jobIdStr);\r\n    Cluster mockCluster = mock(Cluster.class);\r\n    Job job = mock(Job.class);\r\n    CLI cli = spy(new CLI(new Configuration()));\r\n    doReturn(mockCluster).when(cli).createCluster();\r\n    when(mockCluster.getJob(jobId)).thenReturn(job);\r\n    int retCode_JOB_SETUP = cli.run(new String[] { \"-list-attempt-ids\", jobIdStr, \"JOB_SETUP\", \"running\" });\r\n    int retCode_JOB_CLEANUP = cli.run(new String[] { \"-list-attempt-ids\", jobIdStr, \"JOB_CLEANUP\", \"running\" });\r\n    int retCode_invalidTaskState = cli.run(new String[] { \"-list-attempt-ids\", jobIdStr, \"REDUCE\", \"complete\" });\r\n    String jobIdStr2 = \"job_1015298225799_0016\";\r\n    int retCode_invalidJobId = cli.run(new String[] { \"-list-attempt-ids\", jobIdStr2, \"MAP\", \"running\" });\r\n    assertEquals(\"JOB_SETUP is an invalid input,exit code should be -1\", -1, retCode_JOB_SETUP);\r\n    assertEquals(\"JOB_CLEANUP is an invalid input,exit code should be -1\", -1, retCode_JOB_CLEANUP);\r\n    assertEquals(\"complete is an invalid input,exit code should be -1\", -1, retCode_invalidTaskState);\r\n    assertEquals(\"Non existing job id should be skippted with -1\", -1, retCode_invalidJobId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "getTaskReports",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskReport[] getTaskReports(JobID jobId, TaskType type)\n{\r\n    return new TaskReport[] { new TaskReport(), new TaskReport() };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "testJobKIll",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testJobKIll() throws Exception\n{\r\n    Cluster mockCluster = mock(Cluster.class);\r\n    CLI cli = spy(new CLI(new Configuration()));\r\n    doReturn(mockCluster).when(cli).createCluster();\r\n    String jobId1 = \"job_1234654654_001\";\r\n    String jobId2 = \"job_1234654654_002\";\r\n    String jobId3 = \"job_1234654654_003\";\r\n    String jobId4 = \"job_1234654654_004\";\r\n    Job mockJob1 = mockJob(mockCluster, jobId1, State.RUNNING);\r\n    Job mockJob2 = mockJob(mockCluster, jobId2, State.KILLED);\r\n    Job mockJob3 = mockJob(mockCluster, jobId3, State.FAILED);\r\n    Job mockJob4 = mockJob(mockCluster, jobId4, State.PREP);\r\n    int exitCode1 = cli.run(new String[] { \"-kill\", jobId1 });\r\n    assertEquals(0, exitCode1);\r\n    verify(mockJob1, times(1)).killJob();\r\n    int exitCode2 = cli.run(new String[] { \"-kill\", jobId2 });\r\n    assertEquals(-1, exitCode2);\r\n    verify(mockJob2, times(0)).killJob();\r\n    int exitCode3 = cli.run(new String[] { \"-kill\", jobId3 });\r\n    assertEquals(-1, exitCode3);\r\n    verify(mockJob3, times(0)).killJob();\r\n    int exitCode4 = cli.run(new String[] { \"-kill\", jobId4 });\r\n    assertEquals(0, exitCode4);\r\n    verify(mockJob4, times(1)).killJob();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "mockJob",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Job mockJob(Cluster mockCluster, String jobId, State jobState) throws IOException, InterruptedException\n{\r\n    Job mockJob = mock(Job.class);\r\n    when(mockCluster.getJob(JobID.forName(jobId))).thenReturn(mockJob);\r\n    JobStatus status = new JobStatus(null, 0, 0, 0, 0, jobState, JobPriority.HIGH, null, null, null, null);\r\n    when(mockJob.getStatus()).thenReturn(status);\r\n    return mockJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "testGetJobWithoutRetry",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testGetJobWithoutRetry() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.MR_CLIENT_JOB_MAX_RETRIES, 0);\r\n    final Cluster mockCluster = mock(Cluster.class);\r\n    when(mockCluster.getJob(any(JobID.class))).thenReturn(null);\r\n    CLI cli = new CLI(conf);\r\n    cli.cluster = mockCluster;\r\n    Job job = cli.getJob(JobID.forName(\"job_1234654654_001\"));\r\n    Assert.assertTrue(\"job is not null\", job == null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "testGetJobWithRetry",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testGetJobWithRetry() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.MR_CLIENT_JOB_MAX_RETRIES, 1);\r\n    final Cluster mockCluster = mock(Cluster.class);\r\n    final Job mockJob = Job.getInstance(conf);\r\n    when(mockCluster.getJob(any(JobID.class))).thenReturn(null).thenReturn(mockJob);\r\n    CLI cli = new CLI(conf);\r\n    cli.cluster = mockCluster;\r\n    Job job = cli.getJob(JobID.forName(\"job_1234654654_001\"));\r\n    Assert.assertTrue(\"job is null\", job != null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "testListEvents",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testListEvents() throws Exception\n{\r\n    Cluster mockCluster = mock(Cluster.class);\r\n    CLI cli = spy(new CLI(new Configuration()));\r\n    doReturn(mockCluster).when(cli).createCluster();\r\n    String jobId1 = \"job_1234654654_001\";\r\n    String jobId2 = \"job_1234654656_002\";\r\n    Job mockJob1 = mockJob(mockCluster, jobId1, State.RUNNING);\r\n    int exitCode = cli.run(new String[] { \"-events\", jobId2, \"0\", \"10\" });\r\n    assertEquals(-1, exitCode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "testLogs",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testLogs() throws Exception\n{\r\n    Cluster mockCluster = mock(Cluster.class);\r\n    CLI cli = spy(new CLI(new Configuration()));\r\n    doReturn(mockCluster).when(cli).createCluster();\r\n    String jobId1 = \"job_1234654654_001\";\r\n    String jobId2 = \"job_1234654656_002\";\r\n    Job mockJob1 = mockJob(mockCluster, jobId1, State.SUCCEEDED);\r\n    int exitCode = cli.run(new String[] { \"-logs\", jobId2 });\r\n    assertEquals(-1, exitCode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testPatterns",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testPatterns()\n{\r\n    int[] results = new int[PARTITIONS];\r\n    RehashPartitioner<IntWritable, NullWritable> p = new RehashPartitioner<IntWritable, NullWritable>();\r\n    for (int i = 0; i < END; i += STEP) {\r\n        results[p.getPartition(new IntWritable(i), null, PARTITIONS)]++;\r\n    }\r\n    int badbuckets = 0;\r\n    Integer min = Collections.min(Arrays.asList(ArrayUtils.toObject(results)));\r\n    Integer max = Collections.max(Arrays.asList(ArrayUtils.toObject(results)));\r\n    Integer avg = (int) Math.round((max + min) / 2.0);\r\n    System.out.println(\"Dumping buckets distribution: min=\" + min + \" avg=\" + avg + \" max=\" + max);\r\n    for (int i = 0; i < PARTITIONS; i++) {\r\n        double var = (results[i] - avg) / (double) (avg);\r\n        System.out.println(\"bucket \" + i + \" \" + results[i] + \" items, variance \" + var);\r\n        if (Math.abs(var) > MAX_ERROR)\r\n            badbuckets++;\r\n    }\r\n    System.out.println(badbuckets + \" of \" + PARTITIONS + \" are too small or large buckets\");\r\n    assertTrue(\"too many overflow buckets\", badbuckets < PARTITIONS * MAX_BADBUCKETS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setup()\n{\r\n    configuration = new Configuration();\r\n    configuration.setInt(MRJobConfig.NUM_MAPS, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testBooleanSplitter",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testBooleanSplitter() throws Exception\n{\r\n    BooleanSplitter splitter = new BooleanSplitter();\r\n    ResultSet result = mock(ResultSet.class);\r\n    when(result.getString(1)).thenReturn(\"result1\");\r\n    List<InputSplit> splits = splitter.split(configuration, result, \"column\");\r\n    assertSplits(new String[] { \"column = FALSE column = FALSE\", \"column IS NULL column IS NULL\" }, splits);\r\n    when(result.getString(1)).thenReturn(\"result1\");\r\n    when(result.getString(2)).thenReturn(\"result2\");\r\n    when(result.getBoolean(1)).thenReturn(true);\r\n    when(result.getBoolean(2)).thenReturn(false);\r\n    splits = splitter.split(configuration, result, \"column\");\r\n    assertEquals(0, splits.size());\r\n    when(result.getString(1)).thenReturn(\"result1\");\r\n    when(result.getString(2)).thenReturn(\"result2\");\r\n    when(result.getBoolean(1)).thenReturn(false);\r\n    when(result.getBoolean(2)).thenReturn(true);\r\n    splits = splitter.split(configuration, result, \"column\");\r\n    assertSplits(new String[] { \"column = FALSE column = FALSE\", \".*column = TRUE\" }, splits);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testFloatSplitter",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testFloatSplitter() throws Exception\n{\r\n    FloatSplitter splitter = new FloatSplitter();\r\n    ResultSet results = mock(ResultSet.class);\r\n    List<InputSplit> splits = splitter.split(configuration, results, \"column\");\r\n    assertSplits(new String[] { \".*column IS NULL\" }, splits);\r\n    when(results.getString(1)).thenReturn(\"result1\");\r\n    when(results.getString(2)).thenReturn(\"result2\");\r\n    when(results.getDouble(1)).thenReturn(5.0);\r\n    when(results.getDouble(2)).thenReturn(7.0);\r\n    splits = splitter.split(configuration, results, \"column1\");\r\n    assertSplits(new String[] { \"column1 >= 5.0 column1 < 6.0\", \"column1 >= 6.0 column1 <= 7.0\" }, splits);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testBigDecimalSplitter",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testBigDecimalSplitter() throws Exception\n{\r\n    BigDecimalSplitter splitter = new BigDecimalSplitter();\r\n    ResultSet result = mock(ResultSet.class);\r\n    List<InputSplit> splits = splitter.split(configuration, result, \"column\");\r\n    assertSplits(new String[] { \".*column IS NULL\" }, splits);\r\n    when(result.getString(1)).thenReturn(\"result1\");\r\n    when(result.getString(2)).thenReturn(\"result2\");\r\n    when(result.getBigDecimal(1)).thenReturn(new BigDecimal(10));\r\n    when(result.getBigDecimal(2)).thenReturn(new BigDecimal(12));\r\n    splits = splitter.split(configuration, result, \"column1\");\r\n    assertSplits(new String[] { \"column1 >= 10 column1 < 11\", \"column1 >= 11 column1 <= 12\" }, splits);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testIntegerSplitter",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testIntegerSplitter() throws Exception\n{\r\n    IntegerSplitter splitter = new IntegerSplitter();\r\n    ResultSet result = mock(ResultSet.class);\r\n    List<InputSplit> splits = splitter.split(configuration, result, \"column\");\r\n    assertSplits(new String[] { \".*column IS NULL\" }, splits);\r\n    when(result.getString(1)).thenReturn(\"result1\");\r\n    when(result.getString(2)).thenReturn(\"result2\");\r\n    when(result.getLong(1)).thenReturn(8L);\r\n    when(result.getLong(2)).thenReturn(19L);\r\n    splits = splitter.split(configuration, result, \"column1\");\r\n    assertSplits(new String[] { \"column1 >= 8 column1 < 13\", \"column1 >= 13 column1 < 18\", \"column1 >= 18 column1 <= 19\" }, splits);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testTextSplitter",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testTextSplitter() throws Exception\n{\r\n    TextSplitter splitter = new TextSplitter();\r\n    ResultSet result = mock(ResultSet.class);\r\n    List<InputSplit> splits = splitter.split(configuration, result, \"column\");\r\n    assertSplits(new String[] { \"column IS NULL column IS NULL\" }, splits);\r\n    when(result.getString(1)).thenReturn(\"result1\");\r\n    when(result.getString(2)).thenReturn(\"result2\");\r\n    splits = splitter.split(configuration, result, \"column1\");\r\n    assertSplits(new String[] { \"column1 >= 'result1' column1 < 'result1.'\", \"column1 >= 'result1' column1 <= 'result2'\" }, splits);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "assertSplits",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void assertSplits(String[] expectedSplitRE, List<InputSplit> splits) throws IOException\n{\r\n    assertEquals(expectedSplitRE.length, splits.size());\r\n    for (int i = 0; i < expectedSplitRE.length; i++) {\r\n        DataDrivenDBInputSplit split = (DataDrivenDBInputSplit) splits.get(i);\r\n        String actualExpr = split.getLowerClause() + \" \" + split.getUpperClause();\r\n        assertTrue(\"Split #\" + (i + 1) + \" expression is wrong.\" + \" Expected \" + expectedSplitRE[i] + \" Actual \" + actualExpr, Pattern.matches(expectedSplitRE[i], actualExpr));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void reset()\n{\r\n    deletePathsToFail.clear();\r\n    deletePathsToTimeOut.clear();\r\n    pathNotFound.clear();\r\n    renameSourceFilesToFail.clear();\r\n    renameDestDirsToFail.clear();\r\n    timeoutSleepTimeMillis = 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "getTimeoutSleepTimeMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTimeoutSleepTimeMillis()\n{\r\n    return timeoutSleepTimeMillis;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "setTimeoutSleepTimeMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTimeoutSleepTimeMillis(final int timeoutSleepTimeMillis)\n{\r\n    this.timeoutSleepTimeMillis = timeoutSleepTimeMillis;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "getRenameToFailWithException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getRenameToFailWithException()\n{\r\n    return renameToFailWithException;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "setRenameToFailWithException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRenameToFailWithException(final boolean renameToFailWithException)\n{\r\n    this.renameToFailWithException = renameToFailWithException;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "addDeletePathToFail",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addDeletePathToFail(Path path)\n{\r\n    deletePathsToFail.add(requireNonNull(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "addDeletePathToTimeOut",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addDeletePathToTimeOut(Path path)\n{\r\n    deletePathsToTimeOut.add(requireNonNull(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "addListToFail",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addListToFail(Path path)\n{\r\n    listToFail.add(requireNonNull(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "addMkdirsToFail",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addMkdirsToFail(Path path)\n{\r\n    mkdirsToFail.add(requireNonNull(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "addPathNotFound",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addPathNotFound(Path path)\n{\r\n    pathNotFound.add(requireNonNull(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "addRenameSourceFilesToFail",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addRenameSourceFilesToFail(Path path)\n{\r\n    renameSourceFilesToFail.add(requireNonNull(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "addRenameDestDirsFail",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addRenameDestDirsFail(Path path)\n{\r\n    renameDestDirsToFail.add(requireNonNull(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "addSaveToFail",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addSaveToFail(Path path)\n{\r\n    saveToFail.add(requireNonNull(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "maybeRaiseIOE",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void maybeRaiseIOE(String operation, Path path, Set<Path> paths) throws IOException\n{\r\n    if (paths.contains(path)) {\r\n        LOG.info(\"Simulating failure of {} with {}\", operation, path);\r\n        throw new PathIOException(path.toString(), SIMULATED_FAILURE + \" of \" + operation);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "verifyExists",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyExists(Path path) throws FileNotFoundException\n{\r\n    if (pathNotFound.contains(path)) {\r\n        throw new FileNotFoundException(path.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "maybeTimeout",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void maybeTimeout(String operation, Path path, Set<Path> paths) throws IOException\n{\r\n    if (paths.contains(path)) {\r\n        LOG.info(\"Simulating timeout of {} with {}\", operation, path);\r\n        try {\r\n            if (timeoutSleepTimeMillis > 0) {\r\n                Thread.sleep(timeoutSleepTimeMillis);\r\n            }\r\n        } catch (InterruptedException e) {\r\n            throw new InterruptedIOException(e.toString());\r\n        }\r\n        throw new PathIOException(path.toString(), \"ErrorCode=\" + OPERATION_TIMED_OUT + \" ErrorMessage=\" + E_TIMEOUT);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileStatus getFileStatus(final Path path) throws IOException\n{\r\n    verifyExists(path);\r\n    return wrappedOperations.getFileStatus(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean delete(final Path path, final boolean recursive) throws IOException\n{\r\n    String op = \"delete\";\r\n    maybeTimeout(op, path, deletePathsToTimeOut);\r\n    maybeRaiseIOE(op, path, deletePathsToFail);\r\n    return wrappedOperations.delete(path, recursive);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean mkdirs(final Path path) throws IOException\n{\r\n    maybeRaiseIOE(\"mkdirs\", path, mkdirsToFail);\r\n    return wrappedOperations.mkdirs(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "renameFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean renameFile(final Path source, final Path dest) throws IOException\n{\r\n    String op = \"rename\";\r\n    if (renameToFailWithException) {\r\n        maybeRaiseIOE(op, source, renameSourceFilesToFail);\r\n        maybeRaiseIOE(op, dest.getParent(), renameDestDirsToFail);\r\n    } else {\r\n        if (renameSourceFilesToFail.contains(source) || renameDestDirsToFail.contains(dest.getParent())) {\r\n            LOG.info(\"Failing rename({}, {})\", source, dest);\r\n            return false;\r\n        }\r\n    }\r\n    return wrappedOperations.renameFile(source, dest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "listStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RemoteIterator<FileStatus> listStatusIterator(final Path path) throws IOException\n{\r\n    verifyExists(path);\r\n    maybeRaiseIOE(\"listStatus\", path, listToFail);\r\n    return wrappedOperations.listStatusIterator(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "loadTaskManifest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskManifest loadTaskManifest(JsonSerialization<TaskManifest> serializer, final FileStatus st) throws IOException\n{\r\n    verifyExists(st.getPath());\r\n    return wrappedOperations.loadTaskManifest(serializer, st);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "save",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void save(T manifestData, final Path path, final boolean overwrite) throws IOException\n{\r\n    maybeRaiseIOE(\"save\", path, saveToFail);\r\n    wrappedOperations.save(manifestData, path, overwrite);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "msync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void msync(Path path) throws IOException\n{\r\n    wrappedOperations.msync(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "getEtag",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getEtag(FileStatus status)\n{\r\n    return wrappedOperations.getEtag(status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "storeSupportsResilientCommit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean storeSupportsResilientCommit()\n{\r\n    return wrappedOperations.storeSupportsResilientCommit();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "commitFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "CommitFileResult commitFile(final FileEntry entry) throws IOException\n{\r\n    if (renameToFailWithException) {\r\n        maybeRaiseIOE(\"commitFile\", entry.getSourcePath(), renameSourceFilesToFail);\r\n        maybeRaiseIOE(\"commitFile\", entry.getDestPath().getParent(), renameDestDirsToFail);\r\n    }\r\n    return wrappedOperations.commitFile(entry);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "storePreservesEtagsThroughRenames",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean storePreservesEtagsThroughRenames(Path path)\n{\r\n    return wrappedOperations.storePreservesEtagsThroughRenames(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    wrappedOperations.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testTipFailed",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testTipFailed() throws Exception\n{\r\n    JobConf job = new JobConf();\r\n    job.setNumMapTasks(2);\r\n    TaskStatus status = new TaskStatus() {\r\n\r\n        @Override\r\n        public boolean getIsMap() {\r\n            return false;\r\n        }\r\n\r\n        @Override\r\n        public void addFetchFailedMap(TaskAttemptID mapTaskId) {\r\n        }\r\n    };\r\n    Progress progress = new Progress();\r\n    TaskAttemptID reduceId = new TaskAttemptID(\"314159\", 0, TaskType.REDUCE, 0, 0);\r\n    ShuffleSchedulerImpl scheduler = new ShuffleSchedulerImpl(job, status, reduceId, null, progress, null, null, null);\r\n    JobID jobId = new JobID();\r\n    TaskID taskId1 = new TaskID(jobId, TaskType.REDUCE, 1);\r\n    scheduler.tipFailed(taskId1);\r\n    Assert.assertEquals(\"Progress should be 0.5\", 0.5f, progress.getProgress(), 0.0f);\r\n    Assert.assertFalse(scheduler.waitUntilDone(1));\r\n    TaskID taskId0 = new TaskID(jobId, TaskType.REDUCE, 0);\r\n    scheduler.tipFailed(taskId0);\r\n    Assert.assertEquals(\"Progress should be 1.0\", 1.0f, progress.getProgress(), 0.0f);\r\n    Assert.assertTrue(scheduler.waitUntilDone(1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "TestAggregatedTransferRate",
  "errType" : null,
  "containingMethodsNum" : 38,
  "sourceCodeText" : "void TestAggregatedTransferRate() throws Exception\n{\r\n    JobConf job = new JobConf();\r\n    job.setNumMapTasks(10);\r\n    TaskUmbilicalProtocol mockUmbilical = mock(TaskUmbilicalProtocol.class);\r\n    Reporter mockReporter = mock(Reporter.class);\r\n    FileSystem mockFileSystem = mock(FileSystem.class);\r\n    Class<? extends org.apache.hadoop.mapred.Reducer> combinerClass = job.getCombinerClass();\r\n    @SuppressWarnings(\"unchecked\")\r\n    CombineOutputCollector<K, V> mockCombineOutputCollector = (CombineOutputCollector<K, V>) mock(CombineOutputCollector.class);\r\n    org.apache.hadoop.mapreduce.TaskAttemptID mockTaskAttemptID = mock(org.apache.hadoop.mapreduce.TaskAttemptID.class);\r\n    LocalDirAllocator mockLocalDirAllocator = mock(LocalDirAllocator.class);\r\n    CompressionCodec mockCompressionCodec = mock(CompressionCodec.class);\r\n    Counter mockCounter = mock(Counter.class);\r\n    TaskStatus mockTaskStatus = mock(TaskStatus.class);\r\n    Progress mockProgress = mock(Progress.class);\r\n    MapOutputFile mockMapOutputFile = mock(MapOutputFile.class);\r\n    Task mockTask = mock(Task.class);\r\n    @SuppressWarnings(\"unchecked\")\r\n    MapOutput<K, V> output = mock(MapOutput.class);\r\n    ShuffleConsumerPlugin.Context<K, V> context = new ShuffleConsumerPlugin.Context<K, V>(mockTaskAttemptID, job, mockFileSystem, mockUmbilical, mockLocalDirAllocator, mockReporter, mockCompressionCodec, combinerClass, mockCombineOutputCollector, mockCounter, mockCounter, mockCounter, mockCounter, mockCounter, mockCounter, mockTaskStatus, mockProgress, mockProgress, mockTask, mockMapOutputFile, null);\r\n    TaskStatus status = new TaskStatus() {\r\n\r\n        @Override\r\n        public boolean getIsMap() {\r\n            return false;\r\n        }\r\n\r\n        @Override\r\n        public void addFetchFailedMap(TaskAttemptID mapTaskId) {\r\n        }\r\n    };\r\n    Progress progress = new Progress();\r\n    ShuffleSchedulerImpl<K, V> scheduler = new ShuffleSchedulerImpl<K, V>(job, status, null, null, progress, context.getShuffledMapsCounter(), context.getReduceShuffleBytes(), context.getFailedShuffleCounter());\r\n    TaskAttemptID attemptID0 = new TaskAttemptID(new org.apache.hadoop.mapred.TaskID(new JobID(\"test\", 0), TaskType.MAP, 0), 0);\r\n    long bytes = (long) 40 * 1024 * 1024;\r\n    scheduler.copySucceeded(attemptID0, new MapHost(null, null), bytes, 60000, 100000, output);\r\n    Assert.assertEquals(copyMessage(1, 1, 1), progress.toString());\r\n    TaskAttemptID attemptID1 = new TaskAttemptID(new org.apache.hadoop.mapred.TaskID(new JobID(\"test\", 0), TaskType.MAP, 1), 1);\r\n    bytes = (long) 50 * 1024 * 1024;\r\n    scheduler.copySucceeded(attemptID1, new MapHost(null, null), bytes, 0, 50000, output);\r\n    Assert.assertEquals(copyMessage(2, 1, 1), progress.toString());\r\n    TaskAttemptID attemptID2 = new TaskAttemptID(new org.apache.hadoop.mapred.TaskID(new JobID(\"test\", 0), TaskType.MAP, 2), 2);\r\n    bytes = (long) 110 * 1024 * 1024;\r\n    scheduler.copySucceeded(attemptID2, new MapHost(null, null), bytes, 25000, 80000, output);\r\n    Assert.assertEquals(copyMessage(3, 2, 2), progress.toString());\r\n    TaskAttemptID attemptID3 = new TaskAttemptID(new org.apache.hadoop.mapred.TaskID(new JobID(\"test\", 0), TaskType.MAP, 3), 3);\r\n    bytes = (long) 100 * 1024 * 1024;\r\n    scheduler.copySucceeded(attemptID3, new MapHost(null, null), bytes, 100000, 300000, output);\r\n    Assert.assertEquals(copyMessage(4, 0.5, 1), progress.toString());\r\n    TaskAttemptID attemptID4 = new TaskAttemptID(new org.apache.hadoop.mapred.TaskID(new JobID(\"test\", 0), TaskType.MAP, 4), 4);\r\n    bytes = (long) 50 * 1024 * 1024;\r\n    scheduler.copySucceeded(attemptID4, new MapHost(null, null), bytes, 350000, 400000, output);\r\n    Assert.assertEquals(copyMessage(5, 1, 1), progress.toString());\r\n    TaskAttemptID attemptID5 = new TaskAttemptID(new org.apache.hadoop.mapred.TaskID(new JobID(\"test\", 0), TaskType.MAP, 5), 5);\r\n    bytes = (long) 50 * 1024 * 1024;\r\n    scheduler.copySucceeded(attemptID5, new MapHost(null, null), bytes, 450000, 500000, output);\r\n    Assert.assertEquals(copyMessage(6, 1, 1), progress.toString());\r\n    TaskAttemptID attemptID6 = new TaskAttemptID(new org.apache.hadoop.mapred.TaskID(new JobID(\"test\", 0), TaskType.MAP, 6), 6);\r\n    bytes = (long) 20 * 1024 * 1024;\r\n    scheduler.copySucceeded(attemptID6, new MapHost(null, null), bytes, 320000, 340000, output);\r\n    Assert.assertEquals(copyMessage(7, 1, 1), progress.toString());\r\n    TaskAttemptID attemptID7 = new TaskAttemptID(new org.apache.hadoop.mapred.TaskID(new JobID(\"test\", 0), TaskType.MAP, 7), 7);\r\n    bytes = (long) 30 * 1024 * 1024;\r\n    scheduler.copySucceeded(attemptID7, new MapHost(null, null), bytes, 290000, 350000, output);\r\n    Assert.assertEquals(copyMessage(8, 0.5, 1), progress.toString());\r\n    TaskAttemptID attemptID8 = new TaskAttemptID(new org.apache.hadoop.mapred.TaskID(new JobID(\"test\", 0), TaskType.MAP, 8), 8);\r\n    bytes = (long) 50 * 1024 * 1024;\r\n    scheduler.copySucceeded(attemptID8, new MapHost(null, null), bytes, 400000, 450000, output);\r\n    Assert.assertEquals(copyMessage(9, 1, 1), progress.toString());\r\n    TaskAttemptID attemptID9 = new TaskAttemptID(new org.apache.hadoop.mapred.TaskID(new JobID(\"test\", 0), TaskType.MAP, 9), 9);\r\n    bytes = (long) 500 * 1024 * 1024;\r\n    scheduler.copySucceeded(attemptID9, new MapHost(null, null), bytes, 0, 500000, output);\r\n    Assert.assertEquals(copyMessage(10, 1, 2), progress.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "TestSucceedAndFailedCopyMap",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void TestSucceedAndFailedCopyMap() throws Exception\n{\r\n    JobConf job = new JobConf();\r\n    job.setNumMapTasks(2);\r\n    TaskUmbilicalProtocol mockUmbilical = mock(TaskUmbilicalProtocol.class);\r\n    Reporter mockReporter = mock(Reporter.class);\r\n    FileSystem mockFileSystem = mock(FileSystem.class);\r\n    Class<? extends org.apache.hadoop.mapred.Reducer> combinerClass = job.getCombinerClass();\r\n    @SuppressWarnings(\"unchecked\")\r\n    CombineOutputCollector<K, V> mockCombineOutputCollector = (CombineOutputCollector<K, V>) mock(CombineOutputCollector.class);\r\n    org.apache.hadoop.mapreduce.TaskAttemptID mockTaskAttemptID = mock(org.apache.hadoop.mapreduce.TaskAttemptID.class);\r\n    LocalDirAllocator mockLocalDirAllocator = mock(LocalDirAllocator.class);\r\n    CompressionCodec mockCompressionCodec = mock(CompressionCodec.class);\r\n    Counter mockCounter = mock(Counter.class);\r\n    TaskStatus mockTaskStatus = mock(TaskStatus.class);\r\n    Progress mockProgress = mock(Progress.class);\r\n    MapOutputFile mockMapOutputFile = mock(MapOutputFile.class);\r\n    Task mockTask = mock(Task.class);\r\n    @SuppressWarnings(\"unchecked\")\r\n    MapOutput<K, V> output = mock(MapOutput.class);\r\n    ShuffleConsumerPlugin.Context<K, V> context = new ShuffleConsumerPlugin.Context<K, V>(mockTaskAttemptID, job, mockFileSystem, mockUmbilical, mockLocalDirAllocator, mockReporter, mockCompressionCodec, combinerClass, mockCombineOutputCollector, mockCounter, mockCounter, mockCounter, mockCounter, mockCounter, mockCounter, mockTaskStatus, mockProgress, mockProgress, mockTask, mockMapOutputFile, null);\r\n    TaskStatus status = new TaskStatus() {\r\n\r\n        @Override\r\n        public boolean getIsMap() {\r\n            return false;\r\n        }\r\n\r\n        @Override\r\n        public void addFetchFailedMap(TaskAttemptID mapTaskId) {\r\n        }\r\n    };\r\n    Progress progress = new Progress();\r\n    ShuffleSchedulerImpl<K, V> scheduler = new ShuffleSchedulerImpl<K, V>(job, status, null, null, progress, context.getShuffledMapsCounter(), context.getReduceShuffleBytes(), context.getFailedShuffleCounter());\r\n    MapHost host1 = new MapHost(\"host1\", null);\r\n    TaskAttemptID failedAttemptID = new TaskAttemptID(new org.apache.hadoop.mapred.TaskID(new JobID(\"test\", 0), TaskType.MAP, 0), 0);\r\n    TaskAttemptID succeedAttemptID = new TaskAttemptID(new org.apache.hadoop.mapred.TaskID(new JobID(\"test\", 0), TaskType.MAP, 1), 1);\r\n    scheduler.hostFailed(host1.getHostName());\r\n    long bytes = (long) 500 * 1024 * 1024;\r\n    scheduler.copySucceeded(succeedAttemptID, host1, bytes, 0, 500000, output);\r\n    scheduler.copyFailed(failedAttemptID, host1, true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "testDuplicateCopySucceeded",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testDuplicateCopySucceeded() throws Exception\n{\r\n    JobConf job = new JobConf();\r\n    job.setNumMapTasks(2);\r\n    TaskUmbilicalProtocol mockUmbilical = mock(TaskUmbilicalProtocol.class);\r\n    Reporter mockReporter = mock(Reporter.class);\r\n    FileSystem mockFileSystem = mock(FileSystem.class);\r\n    Class<? extends org.apache.hadoop.mapred.Reducer> combinerClass = job.getCombinerClass();\r\n    @SuppressWarnings(\"unchecked\")\r\n    CombineOutputCollector<K, V> mockCombineOutputCollector = (CombineOutputCollector<K, V>) mock(CombineOutputCollector.class);\r\n    org.apache.hadoop.mapreduce.TaskAttemptID mockTaskAttemptID = mock(org.apache.hadoop.mapreduce.TaskAttemptID.class);\r\n    LocalDirAllocator mockLocalDirAllocator = mock(LocalDirAllocator.class);\r\n    CompressionCodec mockCompressionCodec = mock(CompressionCodec.class);\r\n    Counter mockCounter = mock(Counter.class);\r\n    TaskStatus mockTaskStatus = mock(TaskStatus.class);\r\n    Progress mockProgress = mock(Progress.class);\r\n    MapOutputFile mockMapOutputFile = mock(MapOutputFile.class);\r\n    Task mockTask = mock(Task.class);\r\n    @SuppressWarnings(\"unchecked\")\r\n    MapOutput<K, V> output1 = mock(MapOutput.class);\r\n    @SuppressWarnings(\"unchecked\")\r\n    MapOutput<K, V> output2 = mock(MapOutput.class);\r\n    @SuppressWarnings(\"unchecked\")\r\n    MapOutput<K, V> output3 = mock(MapOutput.class);\r\n    ShuffleConsumerPlugin.Context<K, V> context = new ShuffleConsumerPlugin.Context<K, V>(mockTaskAttemptID, job, mockFileSystem, mockUmbilical, mockLocalDirAllocator, mockReporter, mockCompressionCodec, combinerClass, mockCombineOutputCollector, mockCounter, mockCounter, mockCounter, mockCounter, mockCounter, mockCounter, mockTaskStatus, mockProgress, mockProgress, mockTask, mockMapOutputFile, null);\r\n    TaskStatus status = new TaskStatus() {\r\n\r\n        @Override\r\n        public boolean getIsMap() {\r\n            return false;\r\n        }\r\n\r\n        @Override\r\n        public void addFetchFailedMap(TaskAttemptID mapTaskId) {\r\n        }\r\n    };\r\n    Progress progress = new Progress();\r\n    ShuffleSchedulerImpl<K, V> scheduler = new ShuffleSchedulerImpl<K, V>(job, status, null, null, progress, context.getShuffledMapsCounter(), context.getReduceShuffleBytes(), context.getFailedShuffleCounter());\r\n    MapHost host1 = new MapHost(\"host1\", null);\r\n    TaskAttemptID succeedAttempt1ID = new TaskAttemptID(new org.apache.hadoop.mapred.TaskID(new JobID(\"test\", 0), TaskType.MAP, 0), 0);\r\n    TaskAttemptID succeedAttempt2ID = new TaskAttemptID(new org.apache.hadoop.mapred.TaskID(new JobID(\"test\", 0), TaskType.MAP, 0), 1);\r\n    TaskAttemptID succeedAttempt3ID = new TaskAttemptID(new org.apache.hadoop.mapred.TaskID(new JobID(\"test\", 0), TaskType.MAP, 1), 0);\r\n    long bytes = (long) 500 * 1024 * 1024;\r\n    scheduler.copySucceeded(succeedAttempt1ID, host1, bytes, 0, 1, output1);\r\n    verify(output1).commit();\r\n    scheduler.copySucceeded(succeedAttempt2ID, host1, bytes, 0, 1, output2);\r\n    verify(output2).abort();\r\n    scheduler.copySucceeded(succeedAttempt3ID, host1, bytes, 0, 1, output3);\r\n    verify(output3).commit();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "copyMessage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String copyMessage(int attemptNo, double rate1, double rate2)\n{\r\n    int attemptZero = attemptNo - 1;\r\n    return String.format(\"copy task(attempt_test_0000_m_%06d_%d succeeded at %1.2f MB/s)\" + \" Aggregated copy rate(%d of 10 at %1.2f MB/s)\", attemptZero, attemptZero, rate1, attemptNo, rate2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeOutput",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void writeOutput(RecordWriter theRecordWriter, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    NullWritable nullWritable = NullWritable.get();\r\n    try {\r\n        theRecordWriter.write(key1, val1);\r\n        theRecordWriter.write(null, nullWritable);\r\n        theRecordWriter.write(null, val1);\r\n        theRecordWriter.write(nullWritable, val2);\r\n        theRecordWriter.write(key2, nullWritable);\r\n        theRecordWriter.write(key1, null);\r\n        theRecordWriter.write(null, null);\r\n        theRecordWriter.write(key2, val2);\r\n    } finally {\r\n        theRecordWriter.close(null);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeMapFileOutput",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writeMapFileOutput(RecordWriter theRecordWriter, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    try {\r\n        int key = 0;\r\n        for (int i = 0; i < 10; ++i) {\r\n            key = i;\r\n            Text val = (i % 2 == 1) ? val1 : val2;\r\n            theRecordWriter.write(new LongWritable(key), val);\r\n        }\r\n    } finally {\r\n        theRecordWriter.close(null);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRecoveryInternal",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testRecoveryInternal(int commitVersion, int recoveryVersion) throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.set(JobContext.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(MRConstants.APPLICATION_ATTEMPT_ID, 1);\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, commitVersion);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter();\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(null, conf, partFile, null);\r\n    writeOutput(theRecordWriter, tContext);\r\n    if (committer.needsTaskCommit(tContext)) {\r\n        committer.commitTask(tContext);\r\n    }\r\n    Path jobTempDir1 = committer.getCommittedTaskPath(tContext);\r\n    File jtd1 = new File(jobTempDir1.toUri().getPath());\r\n    if (commitVersion == 1) {\r\n        assertTrue(\"Version 1 commits to temporary dir \" + jtd1, jtd1.exists());\r\n        validateContent(jobTempDir1);\r\n    } else {\r\n        assertFalse(\"Version 2 commits to output dir \" + jtd1, jtd1.exists());\r\n    }\r\n    JobConf conf2 = new JobConf(conf);\r\n    conf2.set(JobContext.TASK_ATTEMPT_ID, attempt);\r\n    conf2.setInt(MRConstants.APPLICATION_ATTEMPT_ID, 2);\r\n    conf2.setInt(org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, recoveryVersion);\r\n    JobContext jContext2 = new JobContextImpl(conf2, taskID.getJobID());\r\n    TaskAttemptContext tContext2 = new TaskAttemptContextImpl(conf2, taskID);\r\n    FileOutputCommitter committer2 = new FileOutputCommitter();\r\n    committer2.setupJob(jContext2);\r\n    committer2.recoverTask(tContext2);\r\n    Path jobTempDir2 = committer2.getCommittedTaskPath(tContext2);\r\n    File jtd2 = new File(jobTempDir2.toUri().getPath());\r\n    if (recoveryVersion == 1) {\r\n        assertTrue(\"Version 1 recovers to \" + jtd2, jtd2.exists());\r\n        validateContent(jobTempDir2);\r\n    } else {\r\n        assertFalse(\"Version 2 commits to output dir \" + jtd2, jtd2.exists());\r\n        if (commitVersion == 1) {\r\n            assertTrue(\"Version 2  recovery moves to output dir from \" + jtd1, jtd1.list().length == 0);\r\n        }\r\n    }\r\n    committer2.commitJob(jContext2);\r\n    validateContent(outDir);\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRecoveryV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRecoveryV1() throws Exception\n{\r\n    testRecoveryInternal(1, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRecoveryV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRecoveryV2() throws Exception\n{\r\n    testRecoveryInternal(2, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRecoveryUpgradeV1V2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRecoveryUpgradeV1V2() throws Exception\n{\r\n    testRecoveryInternal(1, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "validateContent",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void validateContent(Path dir) throws IOException\n{\r\n    File fdir = new File(dir.toUri().getPath());\r\n    File expectedFile = new File(fdir, partFile);\r\n    StringBuffer expectedOutput = new StringBuffer();\r\n    expectedOutput.append(key1).append('\\t').append(val1).append(\"\\n\");\r\n    expectedOutput.append(val1).append(\"\\n\");\r\n    expectedOutput.append(val2).append(\"\\n\");\r\n    expectedOutput.append(key2).append(\"\\n\");\r\n    expectedOutput.append(key1).append(\"\\n\");\r\n    expectedOutput.append(key2).append('\\t').append(val2).append(\"\\n\");\r\n    String output = slurp(expectedFile);\r\n    assertThat(output).isEqualTo(expectedOutput.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "validateMapFileOutputContent",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void validateMapFileOutputContent(FileSystem fs, Path dir) throws IOException\n{\r\n    Path expectedMapDir = new Path(dir, partFile);\r\n    assert (fs.getFileStatus(expectedMapDir).isDirectory());\r\n    FileStatus[] files = fs.listStatus(expectedMapDir);\r\n    int fileCount = 0;\r\n    boolean dataFileFound = false;\r\n    boolean indexFileFound = false;\r\n    for (FileStatus f : files) {\r\n        if (f.isFile()) {\r\n            ++fileCount;\r\n            if (f.getPath().getName().equals(MapFile.INDEX_FILE_NAME)) {\r\n                indexFileFound = true;\r\n            } else if (f.getPath().getName().equals(MapFile.DATA_FILE_NAME)) {\r\n                dataFileFound = true;\r\n            }\r\n        }\r\n    }\r\n    assert (fileCount > 0);\r\n    assert (dataFileFound && indexFileFound);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitterWithFailureV1",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCommitterWithFailureV1() throws Exception\n{\r\n    testCommitterWithFailureInternal(1, 1);\r\n    testCommitterWithFailureInternal(1, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitterWithFailureV2",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCommitterWithFailureV2() throws Exception\n{\r\n    testCommitterWithFailureInternal(2, 1);\r\n    testCommitterWithFailureInternal(2, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitterWithFailureInternal",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCommitterWithFailureInternal(int version, int maxAttempts) throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.set(JobContext.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, version);\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.FILEOUTPUTCOMMITTER_FAILURE_ATTEMPTS, maxAttempts);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new CommitterWithFailedThenSucceed();\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(null, conf, partFile, null);\r\n    writeOutput(theRecordWriter, tContext);\r\n    if (committer.needsTaskCommit(tContext)) {\r\n        committer.commitTask(tContext);\r\n    }\r\n    try {\r\n        committer.commitJob(jContext);\r\n        if (version == 1 || maxAttempts <= 1) {\r\n            Assert.fail(\"Commit successful: wrong behavior for version 1.\");\r\n        }\r\n    } catch (IOException e) {\r\n        if (version == 2 && maxAttempts > 2) {\r\n            Assert.fail(\"Commit failed: wrong behavior for version 2.\");\r\n        }\r\n    }\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitterWithDuplicatedCommitV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitterWithDuplicatedCommitV1() throws Exception\n{\r\n    testCommitterWithDuplicatedCommitInternal(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitterWithDuplicatedCommitV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitterWithDuplicatedCommitV2() throws Exception\n{\r\n    testCommitterWithDuplicatedCommitInternal(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitterWithDuplicatedCommitInternal",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testCommitterWithDuplicatedCommitInternal(int version) throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.set(JobContext.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, version);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter();\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(null, conf, partFile, null);\r\n    writeOutput(theRecordWriter, tContext);\r\n    if (committer.needsTaskCommit(tContext)) {\r\n        committer.commitTask(tContext);\r\n    }\r\n    committer.commitJob(jContext);\r\n    validateContent(outDir);\r\n    try {\r\n        committer.commitJob(jContext);\r\n        if (version == 1) {\r\n            Assert.fail(\"Duplicate commit successful: wrong behavior \" + \"for version 1.\");\r\n        }\r\n    } catch (IOException e) {\r\n        if (version == 2) {\r\n            Assert.fail(\"Duplicate commit failed: wrong behavior for version 2.\");\r\n        }\r\n    }\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitterInternal",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testCommitterInternal(int version) throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.set(JobContext.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, version);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter();\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(null, conf, partFile, null);\r\n    writeOutput(theRecordWriter, tContext);\r\n    if (committer.needsTaskCommit(tContext)) {\r\n        committer.commitTask(tContext);\r\n    }\r\n    committer.commitJob(jContext);\r\n    validateContent(outDir);\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitterV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitterV1() throws Exception\n{\r\n    testCommitterInternal(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitterV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitterV2() throws Exception\n{\r\n    testCommitterInternal(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMapFileOutputCommitterInternal",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testMapFileOutputCommitterInternal(int version) throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.set(JobContext.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, version);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter();\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    MapFileOutputFormat theOutputFormat = new MapFileOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(null, conf, partFile, null);\r\n    writeMapFileOutput(theRecordWriter, tContext);\r\n    if (committer.needsTaskCommit(tContext)) {\r\n        committer.commitTask(tContext);\r\n    }\r\n    committer.commitJob(jContext);\r\n    validateMapFileOutputContent(FileSystem.get(conf), outDir);\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMapFileOutputCommitterV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMapFileOutputCommitterV1() throws Exception\n{\r\n    testMapFileOutputCommitterInternal(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMapFileOutputCommitterV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMapFileOutputCommitterV2() throws Exception\n{\r\n    testMapFileOutputCommitterInternal(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMapOnlyNoOutputV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMapOnlyNoOutputV1() throws Exception\n{\r\n    testMapOnlyNoOutputInternal(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMapOnlyNoOutputV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMapOnlyNoOutputV2() throws Exception\n{\r\n    testMapOnlyNoOutputInternal(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMapOnlyNoOutputInternal",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testMapOnlyNoOutputInternal(int version) throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    conf.set(JobContext.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, version);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter();\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    if (committer.needsTaskCommit(tContext)) {\r\n        committer.commitTask(tContext);\r\n    }\r\n    committer.commitJob(jContext);\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAbortInternal",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testAbortInternal(int version) throws IOException, InterruptedException\n{\r\n    JobConf conf = new JobConf();\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.set(JobContext.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, version);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter();\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(null, conf, partFile, null);\r\n    writeOutput(theRecordWriter, tContext);\r\n    committer.abortTask(tContext);\r\n    File out = new File(outDir.toUri().getPath());\r\n    Path workPath = committer.getWorkPath(tContext, outDir);\r\n    File wp = new File(workPath.toUri().getPath());\r\n    File expectedFile = new File(wp, partFile);\r\n    assertFalse(\"task temp dir still exists\", expectedFile.exists());\r\n    committer.abortJob(jContext, JobStatus.State.FAILED);\r\n    expectedFile = new File(out, FileOutputCommitter.TEMP_DIR_NAME);\r\n    assertFalse(\"job temp dir still exists\", expectedFile.exists());\r\n    assertEquals(\"Output directory not empty\", 0, out.listFiles().length);\r\n    FileUtil.fullyDelete(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAbortV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAbortV1() throws Exception\n{\r\n    testAbortInternal(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAbortV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAbortV2() throws Exception\n{\r\n    testAbortInternal(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFailAbortInternal",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testFailAbortInternal(int version) throws IOException, InterruptedException\n{\r\n    JobConf conf = new JobConf();\r\n    conf.set(FileSystem.FS_DEFAULT_NAME_KEY, \"faildel:///\");\r\n    conf.setClass(\"fs.faildel.impl\", FakeFileSystem.class, FileSystem.class);\r\n    conf.set(JobContext.TASK_ATTEMPT_ID, attempt);\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, version);\r\n    conf.setInt(MRConstants.APPLICATION_ATTEMPT_ID, 1);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter();\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    File jobTmpDir = new File(new Path(outDir, FileOutputCommitter.TEMP_DIR_NAME + Path.SEPARATOR + conf.getInt(MRConstants.APPLICATION_ATTEMPT_ID, 0) + Path.SEPARATOR + FileOutputCommitter.TEMP_DIR_NAME).toString());\r\n    File taskTmpDir = new File(jobTmpDir, \"_\" + taskID);\r\n    File expectedFile = new File(taskTmpDir, partFile);\r\n    TextOutputFormat<?, ?> theOutputFormat = new TextOutputFormat();\r\n    RecordWriter<?, ?> theRecordWriter = theOutputFormat.getRecordWriter(null, conf, expectedFile.getAbsolutePath(), null);\r\n    writeOutput(theRecordWriter, tContext);\r\n    Throwable th = null;\r\n    try {\r\n        committer.abortTask(tContext);\r\n    } catch (IOException ie) {\r\n        th = ie;\r\n    }\r\n    assertNotNull(th);\r\n    assertTrue(th instanceof IOException);\r\n    assertTrue(th.getMessage().contains(\"fake delete failed\"));\r\n    assertTrue(expectedFile + \" does not exists\", expectedFile.exists());\r\n    th = null;\r\n    try {\r\n        committer.abortJob(jContext, JobStatus.State.FAILED);\r\n    } catch (IOException ie) {\r\n        th = ie;\r\n    }\r\n    assertNotNull(th);\r\n    assertTrue(th instanceof IOException);\r\n    assertTrue(th.getMessage().contains(\"fake delete failed\"));\r\n    assertTrue(\"job temp dir does not exists\", jobTmpDir.exists());\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFailAbortV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFailAbortV1() throws Exception\n{\r\n    testFailAbortInternal(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFailAbortV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFailAbortV2() throws Exception\n{\r\n    testFailAbortInternal(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "slurp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String slurp(File f) throws IOException\n{\r\n    int len = (int) f.length();\r\n    byte[] buf = new byte[len];\r\n    FileInputStream in = new FileInputStream(f);\r\n    String contents = null;\r\n    try {\r\n        in.read(buf, 0, len);\r\n        contents = new String(buf, \"UTF-8\");\r\n    } finally {\r\n        in.close();\r\n    }\r\n    return contents;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterFactoryForSchema",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitterFactoryForSchema() throws Throwable\n{\r\n    createCommitterFactory(SimpleCommitterFactory.class, HTTP_PATH, newBondedConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterFactoryFallbackDefault",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitterFactoryFallbackDefault() throws Throwable\n{\r\n    createCommitterFactory(FileOutputCommitterFactory.class, HDFS_PATH, newBondedConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterFallbackDefault",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitterFallbackDefault() throws Throwable\n{\r\n    createCommitter(FileOutputCommitter.class, HDFS_PATH, taskAttempt(newBondedConfiguration()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterFactoryOverride",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCommitterFactoryOverride() throws Throwable\n{\r\n    Configuration conf = newBondedConfiguration();\r\n    conf.set(COMMITTER_FACTORY_CLASS, OtherFactory.class.getName());\r\n    createCommitterFactory(OtherFactory.class, HDFS_PATH, conf);\r\n    createCommitterFactory(OtherFactory.class, HTTP_PATH, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterFactoryEmptyOption",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCommitterFactoryEmptyOption() throws Throwable\n{\r\n    Configuration conf = newBondedConfiguration();\r\n    conf.set(COMMITTER_FACTORY_CLASS, \"\");\r\n    createCommitterFactory(SimpleCommitterFactory.class, HTTP_PATH, conf);\r\n    createCommitterFactory(FileOutputCommitterFactory.class, HDFS_PATH, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterFactoryUnknown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCommitterFactoryUnknown() throws Throwable\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(COMMITTER_FACTORY_CLASS, \"unknown\");\r\n    intercept(RuntimeException.class, () -> getCommitterFactory(HDFS_PATH, conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitterNullOutputPath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCommitterNullOutputPath() throws Throwable\n{\r\n    Configuration conf = newBondedConfiguration();\r\n    FileOutputCommitter committer = createCommitter(FileOutputCommitterFactory.class, FileOutputCommitter.class, null, conf);\r\n    assertNull(committer.getOutputPath());\r\n    assertNull(committer.getWorkPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testNamedCommitterFactory",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testNamedCommitterFactory() throws Throwable\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(COMMITTER_FACTORY_CLASS, NAMED_COMMITTER_FACTORY);\r\n    conf.set(NAMED_COMMITTER_CLASS, SimpleCommitter.class.getName());\r\n    SimpleCommitter sc = createCommitter(NamedCommitterFactory.class, SimpleCommitter.class, HDFS_PATH, conf);\r\n    assertEquals(\"Wrong output path from \" + sc, HDFS_PATH, sc.getOutputPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testNamedCommitterFactoryNullPath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testNamedCommitterFactoryNullPath() throws Throwable\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(COMMITTER_FACTORY_CLASS, NAMED_COMMITTER_FACTORY);\r\n    conf.set(NAMED_COMMITTER_CLASS, SimpleCommitter.class.getName());\r\n    SimpleCommitter sc = createCommitter(NamedCommitterFactory.class, SimpleCommitter.class, null, conf);\r\n    assertNull(sc.getOutputPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testNamedCommitterNullPath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testNamedCommitterNullPath() throws Throwable\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(COMMITTER_FACTORY_CLASS, NAMED_COMMITTER_FACTORY);\r\n    conf.set(NAMED_COMMITTER_CLASS, SimpleCommitter.class.getName());\r\n    SimpleCommitter sc = createCommitter(SimpleCommitter.class, null, taskAttempt(conf));\r\n    assertNull(sc.getOutputPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "createCommitter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "U createCommitter(Class<T> factoryClass, Class<U> committerClass, Path path, Configuration conf) throws IOException\n{\r\n    T f = createCommitterFactory(factoryClass, path, conf);\r\n    PathOutputCommitter committer = f.createOutputCommitter(path, taskAttempt(conf));\r\n    assertEquals(\" Wrong committer for path \" + path + \" from factory \" + f, committerClass, committer.getClass());\r\n    return (U) committer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "createCommitter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "U createCommitter(Class<U> committerClass, Path path, TaskAttemptContext context) throws IOException\n{\r\n    PathOutputCommitter committer = PathOutputCommitterFactory.createCommitter(path, context);\r\n    assertEquals(\" Wrong committer for path \" + path, committerClass, committer.getClass());\r\n    return (U) committer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "createCommitterFactory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T createCommitterFactory(Class<T> factoryClass, Path path, Configuration conf)\n{\r\n    PathOutputCommitterFactory factory = getCommitterFactory(path, conf);\r\n    assertEquals(\" Wrong factory for path \" + path, factoryClass, factory.getClass());\r\n    return (T) factory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "taskAttempt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptContext taskAttempt(Configuration conf)\n{\r\n    return new TaskAttemptContextImpl(conf, taskAttemptID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testFileOutputCommitterFactory",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testFileOutputCommitterFactory() throws Throwable\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(COMMITTER_FACTORY_CLASS, FILE_COMMITTER_FACTORY);\r\n    conf.set(NAMED_COMMITTER_CLASS, SimpleCommitter.class.getName());\r\n    getCommitterFactory(HDFS_PATH, conf);\r\n    createCommitter(FileOutputCommitterFactory.class, FileOutputCommitter.class, null, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testFileOutputFormatBinding",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testFileOutputFormatBinding() throws Throwable\n{\r\n    Configuration conf = newBondedConfiguration();\r\n    conf.set(FileOutputFormat.OUTDIR, HTTP_PATH.toUri().toString());\r\n    TextOutputFormat<String, String> off = new TextOutputFormat<>();\r\n    SimpleCommitter committer = (SimpleCommitter) off.getOutputCommitter(taskAttempt(conf));\r\n    assertEquals(\"Wrong output path from \" + committer, HTTP_PATH, committer.getOutputPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testFileOutputFormatBindingNoPath",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testFileOutputFormatBindingNoPath() throws Throwable\n{\r\n    Configuration conf = new Configuration();\r\n    conf.unset(FileOutputFormat.OUTDIR);\r\n    conf.set(COMMITTER_FACTORY_CLASS, NAMED_COMMITTER_FACTORY);\r\n    conf.set(NAMED_COMMITTER_CLASS, SimpleCommitter.class.getName());\r\n    httpToSimpleFactory(conf);\r\n    TextOutputFormat<String, String> off = new TextOutputFormat<>();\r\n    SimpleCommitter committer = (SimpleCommitter) off.getOutputCommitter(taskAttempt(conf));\r\n    assertNull(\"Output path from \" + committer, committer.getOutputPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "httpToSimpleFactory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration httpToSimpleFactory(Configuration conf)\n{\r\n    conf.set(HTTP_COMMITTER_FACTORY, SimpleCommitterFactory.class.getName());\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "newBondedConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration newBondedConfiguration()\n{\r\n    return httpToSimpleFactory(new Configuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "verifyCauseClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "E verifyCauseClass(Throwable ex, Class<E> clazz) throws AssertionError\n{\r\n    Throwable cause = ex.getCause();\r\n    if (cause == null) {\r\n        throw new AssertionError(\"No cause\", ex);\r\n    }\r\n    if (!cause.getClass().equals(clazz)) {\r\n        throw new AssertionError(\"Wrong cause class\", cause);\r\n    }\r\n    return (E) cause;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testBadCommitterFactory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testBadCommitterFactory() throws Throwable\n{\r\n    expectFactoryConstructionFailure(HTTP_COMMITTER_FACTORY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testBoundCommitterWithSchema",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBoundCommitterWithSchema() throws Throwable\n{\r\n    Configuration conf = newBondedConfiguration();\r\n    TestPathOutputCommitter.TaskContext tac = new TestPathOutputCommitter.TaskContext(conf);\r\n    BindingPathOutputCommitter committer = new BindingPathOutputCommitter(HTTP_PATH, tac);\r\n    intercept(IOException.class, \"setupJob\", () -> committer.setupJob(tac));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testBoundCommitterWithDefault",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBoundCommitterWithDefault() throws Throwable\n{\r\n    Configuration conf = newBondedConfiguration();\r\n    TestPathOutputCommitter.TaskContext tac = new TestPathOutputCommitter.TaskContext(conf);\r\n    BindingPathOutputCommitter committer = new BindingPathOutputCommitter(HDFS_PATH, tac);\r\n    assertEquals(FileOutputCommitter.class, committer.getCommitter().getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "expectFactoryConstructionFailure",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void expectFactoryConstructionFailure(String key) throws Throwable\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(key, \"Not a factory\");\r\n    RuntimeException ex = intercept(RuntimeException.class, () -> getCommitterFactory(HTTP_PATH, conf));\r\n    verifyCauseClass(verifyCauseClass(ex, RuntimeException.class), ClassNotFoundException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createCommitterConfig",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ManifestCommitterConfig createCommitterConfig(Path outputPath)\n{\r\n    return new ManifestCommitterConfig(outputPath, TASK_COMMITTER, taskAttemptContext, iostatistics, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createStageConfig",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig createStageConfig(Path outputPath)\n{\r\n    return createCommitterConfig(outputPath).createStageConfig().withProgressable(new ProgressCallback());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsStore getIOStatistics()\n{\r\n    return iostatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testPartitionerShouldNotBeCalledWhenOneReducerIsPresent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testPartitionerShouldNotBeCalledWhenOneReducerIsPresent() throws Exception\n{\r\n    MapFileOutputFormat outputFormat = new MapFileOutputFormat();\r\n    Reader reader = Mockito.mock(Reader.class);\r\n    Reader[] readers = new Reader[] { reader };\r\n    outputFormat.getEntry(readers, new MyPartitioner(), new Text(), new Text());\r\n    assertTrue(!MyPartitioner.isGetPartitionCalled());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    MyPartitioner.setGetPartitionCalled(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "assertNoThreadLeakage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertNoThreadLeakage()\n{\r\n    Assertions.assertThat(getCurrentThreadNames()).describedAs(\"The threads at the end of the test run\").isSubsetOf(trackedThreads);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getTrackedThreads",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<String> getTrackedThreads()\n{\r\n    return trackedThreads;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getCurrentThreadNames",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<String> getCurrentThreadNames()\n{\r\n    TreeSet<String> threads = Thread.getAllStackTraces().keySet().stream().map(Thread::getName).filter(n -> n.startsWith(\"JUnit\")).filter(n -> n.startsWith(\"surefire\")).collect(Collectors.toCollection(TreeSet::new));\r\n    return threads;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "listInitialThreadsForLifecycleChecks",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Set<String> listInitialThreadsForLifecycleChecks()\n{\r\n    Set<String> threadSet = getCurrentThreadNames();\r\n    threadSet.add(\"org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner\");\r\n    threadSet.add(\"process reaper\");\r\n    threadSet.add(\"MutableQuantiles-0\");\r\n    threadSet.add(\"Attach Listener\");\r\n    return threadSet;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testPartialOutputCleanup",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testPartialOutputCleanup() throws FileNotFoundException, IllegalArgumentException, IOException\n{\r\n    Configuration conf = new Configuration(false);\r\n    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1);\r\n    TaskAttemptID tid0 = new TaskAttemptID(\"1363718006656\", 1, TaskType.REDUCE, 14, 3);\r\n    Path p = spy(new Path(\"/user/hadoop/out\"));\r\n    Path a = new Path(\"hdfs://user/hadoop/out\");\r\n    Path p0 = new Path(a, \"_temporary/1/attempt_1363718006656_0001_r_000014_0\");\r\n    Path p1 = new Path(a, \"_temporary/1/attempt_1363718006656_0001_r_000014_1\");\r\n    Path p2 = new Path(a, \"_temporary/1/attempt_1363718006656_0001_r_000013_0\");\r\n    Path p3 = new Path(a, \"_temporary/1/attempt_1363718006656_0001_r_000014_2\");\r\n    FileStatus[] fsa = new FileStatus[3];\r\n    fsa[0] = new FileStatus();\r\n    fsa[0].setPath(p0);\r\n    fsa[1] = new FileStatus();\r\n    fsa[1].setPath(p1);\r\n    fsa[2] = new FileStatus();\r\n    fsa[2].setPath(p2);\r\n    final FileSystem fs = mock(FileSystem.class);\r\n    when(fs.exists(eq(p0))).thenReturn(true);\r\n    when(fs.exists(eq(p1))).thenReturn(true);\r\n    when(fs.exists(eq(p2))).thenReturn(true);\r\n    when(fs.exists(eq(p3))).thenReturn(false);\r\n    when(fs.delete(eq(p0), eq(true))).thenReturn(true);\r\n    when(fs.delete(eq(p1), eq(true))).thenReturn(true);\r\n    doReturn(fs).when(p).getFileSystem(any(Configuration.class));\r\n    when(fs.makeQualified(eq(p))).thenReturn(a);\r\n    TaskAttemptContext context = mock(TaskAttemptContext.class);\r\n    when(context.getTaskAttemptID()).thenReturn(tid0);\r\n    when(context.getConfiguration()).thenReturn(conf);\r\n    PartialFileOutputCommitter foc = new TestPFOC(p, context, fs);\r\n    foc.cleanUpPartialOutputForTask(context);\r\n    verify(fs).delete(eq(p0), eq(true));\r\n    verify(fs).delete(eq(p1), eq(true));\r\n    verify(fs, times(1)).delete(eq(p3), eq(true));\r\n    verify(fs, never()).delete(eq(p2), eq(true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    Locale.setDefault(Locale.ENGLISH);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    Locale.setDefault(DEFAULT_LOCALE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testHumanPrinter",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testHumanPrinter() throws Exception\n{\r\n    JobHistoryParser.JobInfo job = createJobInfo();\r\n    HumanReadableHistoryViewerPrinter printer = new HumanReadableHistoryViewerPrinter(job, false, \"http://\", TimeZone.getTimeZone(\"GMT\"));\r\n    String outStr = run(printer);\r\n    Assert.assertEquals(\"\\n\" + \"Hadoop job: job_1317928501754_0001\\n\" + \"=====================================\\n\" + \"User: rkanter\\n\" + \"JobName: my job\\n\" + \"JobConf: /tmp/job.xml\\n\" + \"Submitted At: 6-Oct-2011 19:15:01\\n\" + \"Launched At: 6-Oct-2011 19:15:02 (1sec)\\n\" + \"Finished At: 6-Oct-2011 19:15:16 (14sec)\\n\" + \"Status: SUCCEEDED\\n\" + \"Counters: \\n\" + \"\\n\" + \"|Group Name                    |Counter name                  |Map Value |Reduce Value|Total Value|\\n\" + \"---------------------------------------------------------------------------------------\" + LINE_SEPARATOR + \"|group1                        |counter1                      |5         |5         |5         \" + LINE_SEPARATOR + \"|group1                        |counter2                      |10        |10        |10        \" + LINE_SEPARATOR + \"|group2                        |counter1                      |15        |15        |15        \" + \"\\n\\n\" + \"=====================================\" + LINE_SEPARATOR + \"\\n\" + \"Task Summary\\n\" + \"============================\\n\" + \"Kind\\tTotal\\tSuccessful\\tFailed\\tKilled\\tStartTime\\tFinishTime\\n\" + \"\\n\" + \"Setup\\t1\\t1\\t\\t0\\t0\\t6-Oct-2011 19:15:03\\t6-Oct-2011 19:15:04 (1sec)\\n\" + \"Map\\t6\\t5\\t\\t1\\t0\\t6-Oct-2011 19:15:04\\t6-Oct-2011 19:15:16 (12sec)\\n\" + \"Reduce\\t1\\t1\\t\\t0\\t0\\t6-Oct-2011 19:15:10\\t6-Oct-2011 19:15:18 (8sec)\\n\" + \"Cleanup\\t1\\t1\\t\\t0\\t0\\t6-Oct-2011 19:15:11\\t6-Oct-2011 19:15:20 (9sec)\\n\" + \"============================\\n\" + LINE_SEPARATOR + \"\\n\" + \"Analysis\" + LINE_SEPARATOR + \"=========\" + LINE_SEPARATOR + \"\\n\" + \"Time taken by best performing map task task_1317928501754_0001_m_000003: 3sec\\n\" + \"Average time taken by map tasks: 5sec\\n\" + \"Worse performing map tasks: \\n\" + \"TaskId\\t\\tTimetaken\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000007 7sec\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000006 6sec\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000005 5sec\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000004 4sec\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000003 3sec\" + LINE_SEPARATOR + \"The last map task task_1317928501754_0001_m_000007 finished at (relative to the Job launch time): 6-Oct-2011 19:15:16 (14sec)\" + LINE_SEPARATOR + \"\\n\" + \"Time taken by best performing shuffle task task_1317928501754_0001_r_000008: 8sec\\n\" + \"Average time taken by shuffle tasks: 8sec\\n\" + \"Worse performing shuffle tasks: \\n\" + \"TaskId\\t\\tTimetaken\" + LINE_SEPARATOR + \"task_1317928501754_0001_r_000008 8sec\" + LINE_SEPARATOR + \"The last shuffle task task_1317928501754_0001_r_000008 finished at (relative to the Job launch time): 6-Oct-2011 19:15:18 (16sec)\" + LINE_SEPARATOR + \"\\n\" + \"Time taken by best performing reduce task task_1317928501754_0001_r_000008: 0sec\\n\" + \"Average time taken by reduce tasks: 0sec\\n\" + \"Worse performing reduce tasks: \\n\" + \"TaskId\\t\\tTimetaken\" + LINE_SEPARATOR + \"task_1317928501754_0001_r_000008 0sec\" + LINE_SEPARATOR + \"The last reduce task task_1317928501754_0001_r_000008 finished at (relative to the Job launch time): 6-Oct-2011 19:15:18 (16sec)\" + LINE_SEPARATOR + \"=========\" + LINE_SEPARATOR + \"\\n\" + \"FAILED MAP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\tInputSplits\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000002\\t6-Oct-2011 19:15:04\\t6-Oct-2011 19:15:06 (2sec)\\t\\t\" + LINE_SEPARATOR + \"\\n\" + \"FAILED task attempts by nodes\\n\" + \"Hostname\\tFailedTasks\\n\" + \"===============================\" + LINE_SEPARATOR + \"localhost\\ttask_1317928501754_0001_m_000002, \" + LINE_SEPARATOR, outStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testHumanPrinterAll",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testHumanPrinterAll() throws Exception\n{\r\n    JobHistoryParser.JobInfo job = createJobInfo();\r\n    HumanReadableHistoryViewerPrinter printer = new HumanReadableHistoryViewerPrinter(job, true, \"http://\", TimeZone.getTimeZone(\"GMT\"));\r\n    String outStr = run(printer);\r\n    if (System.getProperty(\"java.version\").startsWith(\"1.7\")) {\r\n        Assert.assertEquals(\"\\n\" + \"Hadoop job: job_1317928501754_0001\\n\" + \"=====================================\\n\" + \"User: rkanter\\n\" + \"JobName: my job\\n\" + \"JobConf: /tmp/job.xml\\n\" + \"Submitted At: 6-Oct-2011 19:15:01\\n\" + \"Launched At: 6-Oct-2011 19:15:02 (1sec)\\n\" + \"Finished At: 6-Oct-2011 19:15:16 (14sec)\\n\" + \"Status: SUCCEEDED\\n\" + \"Counters: \\n\" + \"\\n\" + \"|Group Name                    |Counter name                  |Map Value |Reduce Value|Total Value|\\n\" + \"---------------------------------------------------------------------------------------\" + LINE_SEPARATOR + \"|group1                        |counter1                      |5         |5         |5         \" + LINE_SEPARATOR + \"|group1                        |counter2                      |10        |10        |10        \" + LINE_SEPARATOR + \"|group2                        |counter1                      |15        |15        |15        \\n\" + \"\\n\" + \"=====================================\" + LINE_SEPARATOR + \"\\n\" + \"Task Summary\\n\" + \"============================\\n\" + \"Kind\\tTotal\\tSuccessful\\tFailed\\tKilled\\tStartTime\\tFinishTime\\n\" + \"\\n\" + \"Setup\\t1\\t1\\t\\t0\\t0\\t6-Oct-2011 19:15:03\\t6-Oct-2011 19:15:04 (1sec)\\n\" + \"Map\\t6\\t5\\t\\t1\\t0\\t6-Oct-2011 19:15:04\\t6-Oct-2011 19:15:16 (12sec)\\n\" + \"Reduce\\t1\\t1\\t\\t0\\t0\\t6-Oct-2011 19:15:10\\t6-Oct-2011 19:15:18 (8sec)\\n\" + \"Cleanup\\t1\\t1\\t\\t0\\t0\\t6-Oct-2011 19:15:11\\t6-Oct-2011 19:15:20 (9sec)\\n\" + \"============================\\n\" + LINE_SEPARATOR + \"\\n\" + \"Analysis\" + LINE_SEPARATOR + \"=========\" + LINE_SEPARATOR + \"\\n\" + \"Time taken by best performing map task task_1317928501754_0001_m_000003: 3sec\\n\" + \"Average time taken by map tasks: 5sec\\n\" + \"Worse performing map tasks: \\n\" + \"TaskId\\t\\tTimetaken\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000007 7sec\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000006 6sec\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000005 5sec\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000004 4sec\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000003 3sec\" + LINE_SEPARATOR + \"The last map task task_1317928501754_0001_m_000007 finished at (relative to the Job launch time): 6-Oct-2011 19:15:16 (14sec)\" + LINE_SEPARATOR + \"\\n\" + \"Time taken by best performing shuffle task task_1317928501754_0001_r_000008: 8sec\\n\" + \"Average time taken by shuffle tasks: 8sec\\n\" + \"Worse performing shuffle tasks: \\n\" + \"TaskId\\t\\tTimetaken\" + LINE_SEPARATOR + \"task_1317928501754_0001_r_000008 8sec\" + LINE_SEPARATOR + \"The last shuffle task task_1317928501754_0001_r_000008 finished at (relative to the Job launch time): 6-Oct-2011 19:15:18 (16sec)\" + LINE_SEPARATOR + \"\\n\" + \"Time taken by best performing reduce task task_1317928501754_0001_r_000008: 0sec\\n\" + \"Average time taken by reduce tasks: 0sec\\n\" + \"Worse performing reduce tasks: \\n\" + \"TaskId\\t\\tTimetaken\" + LINE_SEPARATOR + \"task_1317928501754_0001_r_000008 0sec\" + LINE_SEPARATOR + \"The last reduce task task_1317928501754_0001_r_000008 finished at (relative to the Job launch time): 6-Oct-2011 19:15:18 (16sec)\" + LINE_SEPARATOR + \"=========\" + LINE_SEPARATOR + \"\\n\" + \"FAILED MAP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\tInputSplits\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000002\\t6-Oct-2011 19:15:04\\t6-Oct-2011 19:15:06 (2sec)\\t\\t\" + LINE_SEPARATOR + \"\\n\" + \"SUCCEEDED JOB_SETUP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_s_000001\\t6-Oct-2011 19:15:03\\t6-Oct-2011 19:15:04 (1sec)\\t\" + LINE_SEPARATOR + \"\\n\" + \"SUCCEEDED MAP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\tInputSplits\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000006\\t6-Oct-2011 19:15:08\\t6-Oct-2011 19:15:14 (6sec)\\t\\t\\n\" + LINE_SEPARATOR + \"\\n\" + \"SUCCEEDED MAP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\tInputSplits\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000005\\t6-Oct-2011 19:15:07\\t6-Oct-2011 19:15:12 (5sec)\\t\\t\\n\" + LINE_SEPARATOR + \"\\n\" + \"SUCCEEDED MAP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\tInputSplits\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000004\\t6-Oct-2011 19:15:06\\t6-Oct-2011 19:15:10 (4sec)\\t\\t\\n\" + LINE_SEPARATOR + \"\\n\" + \"SUCCEEDED MAP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\tInputSplits\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000003\\t6-Oct-2011 19:15:05\\t6-Oct-2011 19:15:08 (3sec)\\t\\t\\n\" + LINE_SEPARATOR + \"\\n\" + \"SUCCEEDED MAP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\tInputSplits\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000007\\t6-Oct-2011 19:15:09\\t6-Oct-2011 19:15:16 (7sec)\\t\\t\\n\" + LINE_SEPARATOR + \"\\n\" + \"SUCCEEDED REDUCE task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_r_000008\\t6-Oct-2011 19:15:10\\t6-Oct-2011 19:15:18 (8sec)\\t\" + LINE_SEPARATOR + \"\\n\" + \"SUCCEEDED JOB_CLEANUP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_c_000009\\t6-Oct-2011 19:15:11\\t6-Oct-2011 19:15:20 (9sec)\\t\" + LINE_SEPARATOR + \"\\n\" + \"JOB_SETUP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tHostName\\tError\\tTaskLogs\\n\" + \"====================================================\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_s_000001_1\\t6-Oct-2011 19:15:03\\t6-Oct-2011 19:15:04 (1sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_s_000001_1\" + LINE_SEPARATOR + \"\\n\" + \"MAP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tHostName\\tError\\tTaskLogs\\n\" + \"====================================================\\n\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_m_000002_1\\t6-Oct-2011 19:15:04\\t6-Oct-2011 19:15:06 (2sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000002_1\\n\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_m_000006_1\\t6-Oct-2011 19:15:08\\t6-Oct-2011 19:15:14 (6sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000006_1\\n\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_m_000005_1\\t6-Oct-2011 19:15:07\\t6-Oct-2011 19:15:12 (5sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000005_1\\n\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_m_000004_1\\t6-Oct-2011 19:15:06\\t6-Oct-2011 19:15:10 (4sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000004_1\\n\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_m_000003_1\\t6-Oct-2011 19:15:05\\t6-Oct-2011 19:15:08 (3sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000003_1\\n\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_m_000007_1\\t6-Oct-2011 19:15:09\\t6-Oct-2011 19:15:16 (7sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000007_1\\n\" + LINE_SEPARATOR + \"\\n\" + \"REDUCE task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tShuffleFinished\\tSortFinished\\tFinishTime\\tHostName\\tError\\tTaskLogs\\n\" + \"====================================================\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_r_000008_1\\t6-Oct-2011 19:15:10\\t6-Oct-2011 19:15:18 (8sec)\\t6-Oct-2011 19:15:18 (0sec)6-Oct-2011 19:15:18 (8sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_r_000008_1\" + LINE_SEPARATOR + \"\\n\" + \"JOB_CLEANUP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tHostName\\tError\\tTaskLogs\\n\" + \"====================================================\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_c_000009_1\\t6-Oct-2011 19:15:11\\t6-Oct-2011 19:15:20 (9sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_c_000009_1\" + LINE_SEPARATOR + \"\\n\" + \"FAILED task attempts by nodes\\n\" + \"Hostname\\tFailedTasks\\n\" + \"===============================\" + LINE_SEPARATOR + \"localhost\\ttask_1317928501754_0001_m_000002, \" + LINE_SEPARATOR, outStr);\r\n    } else {\r\n        Assert.assertEquals(\"\\n\" + \"Hadoop job: job_1317928501754_0001\\n\" + \"=====================================\\n\" + \"User: rkanter\\n\" + \"JobName: my job\\n\" + \"JobConf: /tmp/job.xml\\n\" + \"Submitted At: 6-Oct-2011 19:15:01\\n\" + \"Launched At: 6-Oct-2011 19:15:02 (1sec)\\n\" + \"Finished At: 6-Oct-2011 19:15:16 (14sec)\\n\" + \"Status: SUCCEEDED\\n\" + \"Counters: \\n\" + \"\\n\" + \"|Group Name                    |Counter name                  |Map Value |Reduce Value|Total Value|\\n\" + \"---------------------------------------------------------------------------------------\" + LINE_SEPARATOR + \"|group1                        |counter1                      |5         |5         |5         \" + LINE_SEPARATOR + \"|group1                        |counter2                      |10        |10        |10        \" + LINE_SEPARATOR + \"|group2                        |counter1                      |15        |15        |15        \\n\" + \"\\n\" + \"=====================================\" + LINE_SEPARATOR + \"\\n\" + \"Task Summary\\n\" + \"============================\\n\" + \"Kind\\tTotal\\tSuccessful\\tFailed\\tKilled\\tStartTime\\tFinishTime\\n\" + \"\\n\" + \"Setup\\t1\\t1\\t\\t0\\t0\\t6-Oct-2011 19:15:03\\t6-Oct-2011 19:15:04 (1sec)\\n\" + \"Map\\t6\\t5\\t\\t1\\t0\\t6-Oct-2011 19:15:04\\t6-Oct-2011 19:15:16 (12sec)\\n\" + \"Reduce\\t1\\t1\\t\\t0\\t0\\t6-Oct-2011 19:15:10\\t6-Oct-2011 19:15:18 (8sec)\\n\" + \"Cleanup\\t1\\t1\\t\\t0\\t0\\t6-Oct-2011 19:15:11\\t6-Oct-2011 19:15:20 (9sec)\\n\" + \"============================\\n\" + LINE_SEPARATOR + \"\\n\" + \"Analysis\" + LINE_SEPARATOR + \"=========\" + LINE_SEPARATOR + \"\\n\" + \"Time taken by best performing map task task_1317928501754_0001_m_000003: 3sec\\n\" + \"Average time taken by map tasks: 5sec\\n\" + \"Worse performing map tasks: \\n\" + \"TaskId\\t\\tTimetaken\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000007 7sec\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000006 6sec\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000005 5sec\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000004 4sec\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000003 3sec\" + LINE_SEPARATOR + \"The last map task task_1317928501754_0001_m_000007 finished at (relative to the Job launch time): 6-Oct-2011 19:15:16 (14sec)\" + LINE_SEPARATOR + \"\\n\" + \"Time taken by best performing shuffle task task_1317928501754_0001_r_000008: 8sec\\n\" + \"Average time taken by shuffle tasks: 8sec\\n\" + \"Worse performing shuffle tasks: \\n\" + \"TaskId\\t\\tTimetaken\" + LINE_SEPARATOR + \"task_1317928501754_0001_r_000008 8sec\" + LINE_SEPARATOR + \"The last shuffle task task_1317928501754_0001_r_000008 finished at (relative to the Job launch time): 6-Oct-2011 19:15:18 (16sec)\" + LINE_SEPARATOR + \"\\n\" + \"Time taken by best performing reduce task task_1317928501754_0001_r_000008: 0sec\\n\" + \"Average time taken by reduce tasks: 0sec\\n\" + \"Worse performing reduce tasks: \\n\" + \"TaskId\\t\\tTimetaken\" + LINE_SEPARATOR + \"task_1317928501754_0001_r_000008 0sec\" + LINE_SEPARATOR + \"The last reduce task task_1317928501754_0001_r_000008 finished at (relative to the Job launch time): 6-Oct-2011 19:15:18 (16sec)\" + LINE_SEPARATOR + \"=========\" + LINE_SEPARATOR + \"\\n\" + \"FAILED MAP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\tInputSplits\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000002\\t6-Oct-2011 19:15:04\\t6-Oct-2011 19:15:06 (2sec)\\t\\t\" + LINE_SEPARATOR + \"\\n\" + \"SUCCEEDED JOB_SETUP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_s_000001\\t6-Oct-2011 19:15:03\\t6-Oct-2011 19:15:04 (1sec)\\t\" + LINE_SEPARATOR + \"\\n\" + \"SUCCEEDED MAP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\tInputSplits\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000007\\t6-Oct-2011 19:15:09\\t6-Oct-2011 19:15:16 (7sec)\\t\\t\" + LINE_SEPARATOR + \"\\n\" + \"SUCCEEDED MAP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\tInputSplits\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000006\\t6-Oct-2011 19:15:08\\t6-Oct-2011 19:15:14 (6sec)\\t\\t\" + LINE_SEPARATOR + \"\\n\" + \"SUCCEEDED MAP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\tInputSplits\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000005\\t6-Oct-2011 19:15:07\\t6-Oct-2011 19:15:12 (5sec)\\t\\t\" + LINE_SEPARATOR + \"\\n\" + \"SUCCEEDED MAP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\tInputSplits\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000004\\t6-Oct-2011 19:15:06\\t6-Oct-2011 19:15:10 (4sec)\\t\\t\" + LINE_SEPARATOR + \"\\n\" + \"SUCCEEDED MAP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\tInputSplits\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_m_000003\\t6-Oct-2011 19:15:05\\t6-Oct-2011 19:15:08 (3sec)\\t\\t\" + LINE_SEPARATOR + \"\\n\" + \"SUCCEEDED REDUCE task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_r_000008\\t6-Oct-2011 19:15:10\\t6-Oct-2011 19:15:18 (8sec)\\t\" + LINE_SEPARATOR + \"\\n\" + \"SUCCEEDED JOB_CLEANUP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tError\\n\" + \"====================================================\" + LINE_SEPARATOR + \"task_1317928501754_0001_c_000009\\t6-Oct-2011 19:15:11\\t6-Oct-2011 19:15:20 (9sec)\\t\" + LINE_SEPARATOR + \"\\n\" + \"JOB_SETUP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tHostName\\tError\\tTaskLogs\\n\" + \"====================================================\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_s_000001_1\\t6-Oct-2011 19:15:03\\t6-Oct-2011 19:15:04 (1sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_s_000001_1\" + LINE_SEPARATOR + \"\\n\" + \"MAP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tHostName\\tError\\tTaskLogs\\n\" + \"====================================================\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_m_000007_1\\t6-Oct-2011 19:15:09\\t6-Oct-2011 19:15:16 (7sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000007_1\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_m_000002_1\\t6-Oct-2011 19:15:04\\t6-Oct-2011 19:15:06 (2sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000002_1\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_m_000006_1\\t6-Oct-2011 19:15:08\\t6-Oct-2011 19:15:14 (6sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000006_1\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_m_000005_1\\t6-Oct-2011 19:15:07\\t6-Oct-2011 19:15:12 (5sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000005_1\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_m_000004_1\\t6-Oct-2011 19:15:06\\t6-Oct-2011 19:15:10 (4sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000004_1\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_m_000003_1\\t6-Oct-2011 19:15:05\\t6-Oct-2011 19:15:08 (3sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000003_1\" + LINE_SEPARATOR + \"\\n\" + \"REDUCE task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tShuffleFinished\\tSortFinished\\tFinishTime\\tHostName\\tError\\tTaskLogs\\n\" + \"====================================================\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_r_000008_1\\t6-Oct-2011 19:15:10\\t6-Oct-2011 19:15:18 (8sec)\\t6-Oct-2011 19:15:18 (0sec)6-Oct-2011 19:15:18 (8sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_r_000008_1\" + LINE_SEPARATOR + \"\\n\" + \"JOB_CLEANUP task list for job_1317928501754_0001\\n\" + \"TaskId\\t\\tStartTime\\tFinishTime\\tHostName\\tError\\tTaskLogs\\n\" + \"====================================================\" + LINE_SEPARATOR + \"attempt_1317928501754_0001_c_000009_1\\t6-Oct-2011 19:15:11\\t6-Oct-2011 19:15:20 (9sec)\\tlocalhost\\thttp://t:1234/tasklog?attemptid=attempt_1317928501754_0001_c_000009_1\" + LINE_SEPARATOR + \"\\n\" + \"FAILED task attempts by nodes\\n\" + \"Hostname\\tFailedTasks\\n\" + \"===============================\" + LINE_SEPARATOR + \"localhost\\ttask_1317928501754_0001_m_000002, \" + LINE_SEPARATOR, outStr);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testJSONPrinter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testJSONPrinter() throws Exception\n{\r\n    JobHistoryParser.JobInfo job = createJobInfo();\r\n    JSONHistoryViewerPrinter printer = new JSONHistoryViewerPrinter(job, false, \"http://\");\r\n    String outStr = run(printer);\r\n    JSONAssert.assertEquals(\"{\\n\" + \"    \\\"counters\\\": {\\n\" + \"        \\\"group1\\\": [\\n\" + \"            {\\n\" + \"                \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                \\\"mapValue\\\": 5,\\n\" + \"                \\\"reduceValue\\\": 5,\\n\" + \"                \\\"totalValue\\\": 5\\n\" + \"            },\\n\" + \"            {\\n\" + \"                \\\"counterName\\\": \\\"counter2\\\",\\n\" + \"                \\\"mapValue\\\": 10,\\n\" + \"                \\\"reduceValue\\\": 10,\\n\" + \"                \\\"totalValue\\\": 10\\n\" + \"            }\\n\" + \"        ],\\n\" + \"        \\\"group2\\\": [\\n\" + \"            {\\n\" + \"                \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                \\\"mapValue\\\": 15,\\n\" + \"                \\\"reduceValue\\\": 15,\\n\" + \"                \\\"totalValue\\\": 15\\n\" + \"            }\\n\" + \"        ]\\n\" + \"    },\\n\" + \"    \\\"finishedAt\\\": 1317928516754,\\n\" + \"    \\\"hadoopJob\\\": \\\"job_1317928501754_0001\\\",\\n\" + \"    \\\"jobConf\\\": \\\"/tmp/job.xml\\\",\\n\" + \"    \\\"jobName\\\": \\\"my job\\\",\\n\" + \"    \\\"launchedAt\\\": 1317928502754,\\n\" + \"    \\\"status\\\": \\\"SUCCEEDED\\\",\\n\" + \"    \\\"submittedAt\\\": 1317928501754,\\n\" + \"    \\\"taskSummary\\\": {\\n\" + \"        \\\"cleanup\\\": {\\n\" + \"            \\\"failed\\\": 0,\\n\" + \"            \\\"finishTime\\\": 1317928520754,\\n\" + \"            \\\"killed\\\": 0,\\n\" + \"            \\\"startTime\\\": 1317928511754,\\n\" + \"            \\\"successful\\\": 1,\\n\" + \"            \\\"total\\\": 1\\n\" + \"        },\\n\" + \"        \\\"map\\\": {\\n\" + \"            \\\"failed\\\": 1,\\n\" + \"            \\\"finishTime\\\": 1317928516754,\\n\" + \"            \\\"killed\\\": 0,\\n\" + \"            \\\"startTime\\\": 1317928504754,\\n\" + \"            \\\"successful\\\": 5,\\n\" + \"            \\\"total\\\": 6\\n\" + \"        },\\n\" + \"        \\\"reduce\\\": {\\n\" + \"            \\\"failed\\\": 0,\\n\" + \"            \\\"finishTime\\\": 1317928518754,\\n\" + \"            \\\"killed\\\": 0,\\n\" + \"            \\\"startTime\\\": 1317928510754,\\n\" + \"            \\\"successful\\\": 1,\\n\" + \"            \\\"total\\\": 1\\n\" + \"        },\\n\" + \"        \\\"setup\\\": {\\n\" + \"            \\\"failed\\\": 0,\\n\" + \"            \\\"finishTime\\\": 1317928504754,\\n\" + \"            \\\"killed\\\": 0,\\n\" + \"            \\\"startTime\\\": 1317928503754,\\n\" + \"            \\\"successful\\\": 1,\\n\" + \"            \\\"total\\\": 1\\n\" + \"        }\\n\" + \"    },\\n\" + \"    \\\"tasks\\\": [\\n\" + \"        {\\n\" + \"            \\\"finishTime\\\": 1317928506754,\\n\" + \"            \\\"inputSplits\\\": \\\"\\\",\\n\" + \"            \\\"startTime\\\": 1317928504754,\\n\" + \"            \\\"status\\\": \\\"FAILED\\\",\\n\" + \"            \\\"taskId\\\": \\\"task_1317928501754_0001_m_000002\\\",\\n\" + \"            \\\"type\\\": \\\"MAP\\\"\\n\" + \"        }\\n\" + \"    ],\\n\" + \"    \\\"user\\\": \\\"rkanter\\\"\\n\" + \"}\\n\", outStr, JSONCompareMode.NON_EXTENSIBLE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testJSONPrinterAll",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testJSONPrinterAll() throws Exception\n{\r\n    JobHistoryParser.JobInfo job = createJobInfo();\r\n    JSONHistoryViewerPrinter printer = new JSONHistoryViewerPrinter(job, true, \"http://\");\r\n    String outStr = run(printer);\r\n    JSONAssert.assertEquals(\"{\\n\" + \"    \\\"counters\\\": {\\n\" + \"        \\\"group1\\\": [\\n\" + \"            {\\n\" + \"                \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                \\\"mapValue\\\": 5,\\n\" + \"                \\\"reduceValue\\\": 5,\\n\" + \"                \\\"totalValue\\\": 5\\n\" + \"            },\\n\" + \"            {\\n\" + \"                \\\"counterName\\\": \\\"counter2\\\",\\n\" + \"                \\\"mapValue\\\": 10,\\n\" + \"                \\\"reduceValue\\\": 10,\\n\" + \"                \\\"totalValue\\\": 10\\n\" + \"            }\\n\" + \"        ],\\n\" + \"        \\\"group2\\\": [\\n\" + \"            {\\n\" + \"                \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                \\\"mapValue\\\": 15,\\n\" + \"                \\\"reduceValue\\\": 15,\\n\" + \"                \\\"totalValue\\\": 15\\n\" + \"            }\\n\" + \"        ]\\n\" + \"    },\\n\" + \"    \\\"finishedAt\\\": 1317928516754,\\n\" + \"    \\\"hadoopJob\\\": \\\"job_1317928501754_0001\\\",\\n\" + \"    \\\"jobConf\\\": \\\"/tmp/job.xml\\\",\\n\" + \"    \\\"jobName\\\": \\\"my job\\\",\\n\" + \"    \\\"launchedAt\\\": 1317928502754,\\n\" + \"    \\\"status\\\": \\\"SUCCEEDED\\\",\\n\" + \"    \\\"submittedAt\\\": 1317928501754,\\n\" + \"    \\\"taskSummary\\\": {\\n\" + \"        \\\"cleanup\\\": {\\n\" + \"            \\\"failed\\\": 0,\\n\" + \"            \\\"finishTime\\\": 1317928520754,\\n\" + \"            \\\"killed\\\": 0,\\n\" + \"            \\\"startTime\\\": 1317928511754,\\n\" + \"            \\\"successful\\\": 1,\\n\" + \"            \\\"total\\\": 1\\n\" + \"        },\\n\" + \"        \\\"map\\\": {\\n\" + \"            \\\"failed\\\": 1,\\n\" + \"            \\\"finishTime\\\": 1317928516754,\\n\" + \"            \\\"killed\\\": 0,\\n\" + \"            \\\"startTime\\\": 1317928504754,\\n\" + \"            \\\"successful\\\": 5,\\n\" + \"            \\\"total\\\": 6\\n\" + \"        },\\n\" + \"        \\\"reduce\\\": {\\n\" + \"            \\\"failed\\\": 0,\\n\" + \"            \\\"finishTime\\\": 1317928518754,\\n\" + \"            \\\"killed\\\": 0,\\n\" + \"            \\\"startTime\\\": 1317928510754,\\n\" + \"            \\\"successful\\\": 1,\\n\" + \"            \\\"total\\\": 1\\n\" + \"        },\\n\" + \"        \\\"setup\\\": {\\n\" + \"            \\\"failed\\\": 0,\\n\" + \"            \\\"finishTime\\\": 1317928504754,\\n\" + \"            \\\"killed\\\": 0,\\n\" + \"            \\\"startTime\\\": 1317928503754,\\n\" + \"            \\\"successful\\\": 1,\\n\" + \"            \\\"total\\\": 1\\n\" + \"        }\\n\" + \"    },\\n\" + \"    \\\"tasks\\\": [\\n\" + \"        {\\n\" + \"            \\\"attempts\\\": {\\n\" + \"                \\\"attemptId\\\": \\\"attempt_1317928501754_0001_m_000002_1\\\",\\n\" + \"                \\\"finishTime\\\": 1317928506754,\\n\" + \"                \\\"hostName\\\": \\\"localhost\\\",\\n\" + \"                \\\"startTime\\\": 1317928504754,\\n\" + \"                \\\"taskLogs\\\": \\\"http://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000002_1\\\"\\n\" + \"            },\\n\" + \"            \\\"counters\\\": {\\n\" + \"                \\\"group1\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 5\\n\" + \"                    },\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter2\\\",\\n\" + \"                        \\\"value\\\": 10\\n\" + \"                    }\\n\" + \"                ],\\n\" + \"                \\\"group2\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 15\\n\" + \"                    }\\n\" + \"                ]\\n\" + \"            },\\n\" + \"            \\\"finishTime\\\": 1317928506754,\\n\" + \"            \\\"inputSplits\\\": \\\"\\\",\\n\" + \"            \\\"startTime\\\": 1317928504754,\\n\" + \"            \\\"status\\\": \\\"FAILED\\\",\\n\" + \"            \\\"taskId\\\": \\\"task_1317928501754_0001_m_000002\\\",\\n\" + \"            \\\"type\\\": \\\"MAP\\\"\\n\" + \"        },\\n\" + \"        {\\n\" + \"            \\\"attempts\\\": {\\n\" + \"                \\\"attemptId\\\": \\\"attempt_1317928501754_0001_s_000001_1\\\",\\n\" + \"                \\\"finishTime\\\": 1317928504754,\\n\" + \"                \\\"hostName\\\": \\\"localhost\\\",\\n\" + \"                \\\"startTime\\\": 1317928503754,\\n\" + \"                \\\"taskLogs\\\": \\\"http://t:1234/tasklog?attemptid=attempt_1317928501754_0001_s_000001_1\\\"\\n\" + \"            },\\n\" + \"            \\\"counters\\\": {\\n\" + \"                \\\"group1\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 5\\n\" + \"                    },\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter2\\\",\\n\" + \"                        \\\"value\\\": 10\\n\" + \"                    }\\n\" + \"                ],\\n\" + \"                \\\"group2\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 15\\n\" + \"                    }\\n\" + \"                ]\\n\" + \"            },\\n\" + \"            \\\"finishTime\\\": 1317928504754,\\n\" + \"            \\\"startTime\\\": 1317928503754,\\n\" + \"            \\\"status\\\": \\\"SUCCEEDED\\\",\\n\" + \"            \\\"taskId\\\": \\\"task_1317928501754_0001_s_000001\\\",\\n\" + \"            \\\"type\\\": \\\"JOB_SETUP\\\"\\n\" + \"        },\\n\" + \"        {\\n\" + \"            \\\"attempts\\\": {\\n\" + \"                \\\"attemptId\\\": \\\"attempt_1317928501754_0001_m_000006_1\\\",\\n\" + \"                \\\"finishTime\\\": 1317928514754,\\n\" + \"                \\\"hostName\\\": \\\"localhost\\\",\\n\" + \"                \\\"startTime\\\": 1317928508754,\\n\" + \"                \\\"taskLogs\\\": \\\"http://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000006_1\\\"\\n\" + \"            },\\n\" + \"            \\\"counters\\\": {\\n\" + \"                \\\"group1\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 5\\n\" + \"                    },\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter2\\\",\\n\" + \"                        \\\"value\\\": 10\\n\" + \"                    }\\n\" + \"                ],\\n\" + \"                \\\"group2\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 15\\n\" + \"                    }\\n\" + \"                ]\\n\" + \"            },\\n\" + \"            \\\"finishTime\\\": 1317928514754,\\n\" + \"            \\\"inputSplits\\\": \\\"\\\",\\n\" + \"            \\\"startTime\\\": 1317928508754,\\n\" + \"            \\\"status\\\": \\\"SUCCEEDED\\\",\\n\" + \"            \\\"taskId\\\": \\\"task_1317928501754_0001_m_000006\\\",\\n\" + \"            \\\"type\\\": \\\"MAP\\\"\\n\" + \"        },\\n\" + \"        {\\n\" + \"            \\\"attempts\\\": {\\n\" + \"                \\\"attemptId\\\": \\\"attempt_1317928501754_0001_m_000005_1\\\",\\n\" + \"                \\\"finishTime\\\": 1317928512754,\\n\" + \"                \\\"hostName\\\": \\\"localhost\\\",\\n\" + \"                \\\"startTime\\\": 1317928507754,\\n\" + \"                \\\"taskLogs\\\": \\\"http://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000005_1\\\"\\n\" + \"            },\\n\" + \"            \\\"counters\\\": {\\n\" + \"                \\\"group1\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 5\\n\" + \"                    },\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter2\\\",\\n\" + \"                        \\\"value\\\": 10\\n\" + \"                    }\\n\" + \"                ],\\n\" + \"                \\\"group2\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 15\\n\" + \"                    }\\n\" + \"                ]\\n\" + \"            },\\n\" + \"            \\\"finishTime\\\": 1317928512754,\\n\" + \"            \\\"inputSplits\\\": \\\"\\\",\\n\" + \"            \\\"startTime\\\": 1317928507754,\\n\" + \"            \\\"status\\\": \\\"SUCCEEDED\\\",\\n\" + \"            \\\"taskId\\\": \\\"task_1317928501754_0001_m_000005\\\",\\n\" + \"            \\\"type\\\": \\\"MAP\\\"\\n\" + \"        },\\n\" + \"        {\\n\" + \"            \\\"attempts\\\": {\\n\" + \"                \\\"attemptId\\\": \\\"attempt_1317928501754_0001_m_000004_1\\\",\\n\" + \"                \\\"finishTime\\\": 1317928510754,\\n\" + \"                \\\"hostName\\\": \\\"localhost\\\",\\n\" + \"                \\\"startTime\\\": 1317928506754,\\n\" + \"                \\\"taskLogs\\\": \\\"http://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000004_1\\\"\\n\" + \"            },\\n\" + \"            \\\"counters\\\": {\\n\" + \"                \\\"group1\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 5\\n\" + \"                    },\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter2\\\",\\n\" + \"                        \\\"value\\\": 10\\n\" + \"                    }\\n\" + \"                ],\\n\" + \"                \\\"group2\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 15\\n\" + \"                    }\\n\" + \"                ]\\n\" + \"            },\\n\" + \"            \\\"finishTime\\\": 1317928510754,\\n\" + \"            \\\"inputSplits\\\": \\\"\\\",\\n\" + \"            \\\"startTime\\\": 1317928506754,\\n\" + \"            \\\"status\\\": \\\"SUCCEEDED\\\",\\n\" + \"            \\\"taskId\\\": \\\"task_1317928501754_0001_m_000004\\\",\\n\" + \"            \\\"type\\\": \\\"MAP\\\"\\n\" + \"        },\\n\" + \"        {\\n\" + \"            \\\"attempts\\\": {\\n\" + \"                \\\"attemptId\\\": \\\"attempt_1317928501754_0001_m_000003_1\\\",\\n\" + \"                \\\"finishTime\\\": 1317928508754,\\n\" + \"                \\\"hostName\\\": \\\"localhost\\\",\\n\" + \"                \\\"startTime\\\": 1317928505754,\\n\" + \"                \\\"taskLogs\\\": \\\"http://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000003_1\\\"\\n\" + \"            },\\n\" + \"            \\\"counters\\\": {\\n\" + \"                \\\"group1\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 5\\n\" + \"                    },\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter2\\\",\\n\" + \"                        \\\"value\\\": 10\\n\" + \"                    }\\n\" + \"                ],\\n\" + \"                \\\"group2\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 15\\n\" + \"                    }\\n\" + \"                ]\\n\" + \"            },\\n\" + \"            \\\"finishTime\\\": 1317928508754,\\n\" + \"            \\\"inputSplits\\\": \\\"\\\",\\n\" + \"            \\\"startTime\\\": 1317928505754,\\n\" + \"            \\\"status\\\": \\\"SUCCEEDED\\\",\\n\" + \"            \\\"taskId\\\": \\\"task_1317928501754_0001_m_000003\\\",\\n\" + \"            \\\"type\\\": \\\"MAP\\\"\\n\" + \"        },\\n\" + \"        {\\n\" + \"            \\\"attempts\\\": {\\n\" + \"                \\\"attemptId\\\": \\\"attempt_1317928501754_0001_c_000009_1\\\",\\n\" + \"                \\\"finishTime\\\": 1317928520754,\\n\" + \"                \\\"hostName\\\": \\\"localhost\\\",\\n\" + \"                \\\"startTime\\\": 1317928511754,\\n\" + \"                \\\"taskLogs\\\": \\\"http://t:1234/tasklog?attemptid=attempt_1317928501754_0001_c_000009_1\\\"\\n\" + \"            },\\n\" + \"            \\\"counters\\\": {\\n\" + \"                \\\"group1\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 5\\n\" + \"                    },\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter2\\\",\\n\" + \"                        \\\"value\\\": 10\\n\" + \"                    }\\n\" + \"                ],\\n\" + \"                \\\"group2\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 15\\n\" + \"                    }\\n\" + \"                ]\\n\" + \"            },\\n\" + \"            \\\"finishTime\\\": 1317928520754,\\n\" + \"            \\\"startTime\\\": 1317928511754,\\n\" + \"            \\\"status\\\": \\\"SUCCEEDED\\\",\\n\" + \"            \\\"taskId\\\": \\\"task_1317928501754_0001_c_000009\\\",\\n\" + \"            \\\"type\\\": \\\"JOB_CLEANUP\\\"\\n\" + \"        },\\n\" + \"        {\\n\" + \"            \\\"attempts\\\": {\\n\" + \"                \\\"attemptId\\\": \\\"attempt_1317928501754_0001_m_000007_1\\\",\\n\" + \"                \\\"finishTime\\\": 1317928516754,\\n\" + \"                \\\"hostName\\\": \\\"localhost\\\",\\n\" + \"                \\\"startTime\\\": 1317928509754,\\n\" + \"                \\\"taskLogs\\\": \\\"http://t:1234/tasklog?attemptid=attempt_1317928501754_0001_m_000007_1\\\"\\n\" + \"            },\\n\" + \"            \\\"counters\\\": {\\n\" + \"                \\\"group1\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 5\\n\" + \"                    },\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter2\\\",\\n\" + \"                        \\\"value\\\": 10\\n\" + \"                    }\\n\" + \"                ],\\n\" + \"                \\\"group2\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 15\\n\" + \"                    }\\n\" + \"                ]\\n\" + \"            },\\n\" + \"            \\\"finishTime\\\": 1317928516754,\\n\" + \"            \\\"inputSplits\\\": \\\"\\\",\\n\" + \"            \\\"startTime\\\": 1317928509754,\\n\" + \"            \\\"status\\\": \\\"SUCCEEDED\\\",\\n\" + \"            \\\"taskId\\\": \\\"task_1317928501754_0001_m_000007\\\",\\n\" + \"            \\\"type\\\": \\\"MAP\\\"\\n\" + \"        },\\n\" + \"        {\\n\" + \"            \\\"attempts\\\": {\\n\" + \"                \\\"attemptId\\\": \\\"attempt_1317928501754_0001_r_000008_1\\\",\\n\" + \"                \\\"finishTime\\\": 1317928518754,\\n\" + \"                \\\"hostName\\\": \\\"localhost\\\",\\n\" + \"                \\\"shuffleFinished\\\": 1317928518754,\\n\" + \"                \\\"sortFinished\\\": 1317928518754,\\n\" + \"                \\\"startTime\\\": 1317928510754,\\n\" + \"                \\\"taskLogs\\\": \\\"http://t:1234/tasklog?attemptid=attempt_1317928501754_0001_r_000008_1\\\"\\n\" + \"            },\\n\" + \"            \\\"counters\\\": {\\n\" + \"                \\\"group1\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 5\\n\" + \"                    },\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter2\\\",\\n\" + \"                        \\\"value\\\": 10\\n\" + \"                    }\\n\" + \"                ],\\n\" + \"                \\\"group2\\\": [\\n\" + \"                    {\\n\" + \"                        \\\"counterName\\\": \\\"counter1\\\",\\n\" + \"                        \\\"value\\\": 15\\n\" + \"                    }\\n\" + \"                ]\\n\" + \"            },\\n\" + \"            \\\"finishTime\\\": 1317928518754,\\n\" + \"            \\\"startTime\\\": 1317928510754,\\n\" + \"            \\\"status\\\": \\\"SUCCEEDED\\\",\\n\" + \"            \\\"taskId\\\": \\\"task_1317928501754_0001_r_000008\\\",\\n\" + \"            \\\"type\\\": \\\"REDUCE\\\"\\n\" + \"        }\\n\" + \"    ],\\n\" + \"    \\\"user\\\": \\\"rkanter\\\"\\n\" + \"}\\n\", outStr, JSONCompareMode.NON_EXTENSIBLE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testHumanDupePrinter",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testHumanDupePrinter() throws Exception\n{\r\n    JobHistoryParser.JobInfo job = createJobInfo2();\r\n    HumanReadableHistoryViewerPrinter printer = new HumanReadableHistoryViewerPrinter(job, false, \"http://\", TimeZone.getTimeZone(\"GMT\"));\r\n    String outStr = run(printer);\r\n    int count1 = outStr.indexOf(\"|Map-Reduce Framework          |Map input records             |\");\r\n    Assert.assertNotEquals(\"First counter occurrence not found\", -1, count1);\r\n    int count2 = outStr.indexOf(\"|Map-Reduce Framework          |Map input records             |\", count1 + 1);\r\n    Assert.assertEquals(\"Duplicate counter found at: \" + count1 + \" and \" + count2, -1, count2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testJSONDupePrinter",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testJSONDupePrinter() throws Exception\n{\r\n    JobHistoryParser.JobInfo job = createJobInfo2();\r\n    JSONHistoryViewerPrinter printer = new JSONHistoryViewerPrinter(job, false, \"http://\");\r\n    String outStr = run(printer);\r\n    int count1 = outStr.indexOf(\"\\\"counterName\\\":\\\"MAP_INPUT_RECORDS\\\"\");\r\n    Assert.assertNotEquals(\"First counter occurrence not found\", -1, count1);\r\n    int count2 = outStr.indexOf(\"\\\"counterName\\\":\\\"MAP_INPUT_RECORDS\\\"\", count1 + 1);\r\n    Assert.assertEquals(\"Duplicate counter found at: \" + count1 + \" and \" + count2, -1, count2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String run(HistoryViewerPrinter printer) throws Exception\n{\r\n    ByteArrayOutputStream boas = new ByteArrayOutputStream();\r\n    PrintStream out = new PrintStream(boas, true);\r\n    printer.print(out);\r\n    out.close();\r\n    String outStr = boas.toString(\"UTF-8\");\r\n    LOG.info(\"out = \" + outStr);\r\n    return outStr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "createJobInfo",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "JobHistoryParser.JobInfo createJobInfo()\n{\r\n    JobHistoryParser.JobInfo job = new JobHistoryParser.JobInfo();\r\n    job.submitTime = 1317928501754L;\r\n    job.finishTime = job.submitTime + 15000;\r\n    job.jobid = JobID.forName(\"job_1317928501754_0001\");\r\n    job.username = \"rkanter\";\r\n    job.jobname = \"my job\";\r\n    job.jobQueueName = \"my queue\";\r\n    job.jobConfPath = \"/tmp/job.xml\";\r\n    job.launchTime = job.submitTime + 1000;\r\n    job.totalMaps = 5;\r\n    job.totalReduces = 1;\r\n    job.failedMaps = 1;\r\n    job.failedReduces = 0;\r\n    job.succeededMaps = 5;\r\n    job.succeededReduces = 1;\r\n    job.jobStatus = JobStatus.State.SUCCEEDED.name();\r\n    job.totalCounters = createCounters();\r\n    job.mapCounters = createCounters();\r\n    job.reduceCounters = createCounters();\r\n    job.tasksMap = new HashMap<>();\r\n    addTaskInfo(job, TaskType.JOB_SETUP, 1, TaskStatus.State.SUCCEEDED);\r\n    addTaskInfo(job, TaskType.MAP, 2, TaskStatus.State.FAILED);\r\n    addTaskInfo(job, TaskType.MAP, 3, TaskStatus.State.SUCCEEDED);\r\n    addTaskInfo(job, TaskType.MAP, 4, TaskStatus.State.SUCCEEDED);\r\n    addTaskInfo(job, TaskType.MAP, 5, TaskStatus.State.SUCCEEDED);\r\n    addTaskInfo(job, TaskType.MAP, 6, TaskStatus.State.SUCCEEDED);\r\n    addTaskInfo(job, TaskType.MAP, 7, TaskStatus.State.SUCCEEDED);\r\n    addTaskInfo(job, TaskType.REDUCE, 8, TaskStatus.State.SUCCEEDED);\r\n    addTaskInfo(job, TaskType.JOB_CLEANUP, 9, TaskStatus.State.SUCCEEDED);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "createJobInfo2",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "JobHistoryParser.JobInfo createJobInfo2()\n{\r\n    JobHistoryParser.JobInfo job = new JobHistoryParser.JobInfo();\r\n    job.submitTime = 1317928501754L;\r\n    job.finishTime = job.submitTime + 15000;\r\n    job.jobid = JobID.forName(\"job_1317928501754_0001\");\r\n    job.username = \"test\";\r\n    job.jobname = \"Dupe counter output\";\r\n    job.jobQueueName = \"root.test\";\r\n    job.jobConfPath = \"/tmp/job.xml\";\r\n    job.launchTime = job.submitTime + 1000;\r\n    job.totalMaps = 1;\r\n    job.totalReduces = 0;\r\n    job.failedMaps = 0;\r\n    job.failedReduces = 0;\r\n    job.succeededMaps = 1;\r\n    job.succeededReduces = 0;\r\n    job.jobStatus = JobStatus.State.SUCCEEDED.name();\r\n    job.totalCounters = createDeprecatedCounters();\r\n    job.mapCounters = createDeprecatedCounters();\r\n    job.reduceCounters = createDeprecatedCounters();\r\n    job.tasksMap = new HashMap<>();\r\n    addTaskInfo(job, TaskType.JOB_SETUP, 1, TaskStatus.State.SUCCEEDED);\r\n    addTaskInfo(job, TaskType.MAP, 2, TaskStatus.State.SUCCEEDED);\r\n    addTaskInfo(job, TaskType.JOB_CLEANUP, 3, TaskStatus.State.SUCCEEDED);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "createCounters",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Counters createCounters()\n{\r\n    Counters counters = new Counters();\r\n    counters.findCounter(\"group1\", \"counter1\").setValue(5);\r\n    counters.findCounter(\"group1\", \"counter2\").setValue(10);\r\n    counters.findCounter(\"group2\", \"counter1\").setValue(15);\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "createDeprecatedCounters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Counters createDeprecatedCounters()\n{\r\n    Counters counters = new Counters();\r\n    counters.findCounter(\"org.apache.hadoop.mapred.Task$Counter\", \"MAP_INPUT_RECORDS\").setValue(1);\r\n    counters.findCounter(\"File System Counters\", \"FILE: Number of bytes read\").setValue(1);\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "addTaskInfo",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void addTaskInfo(JobHistoryParser.JobInfo job, TaskType type, int id, TaskStatus.State status)\n{\r\n    JobHistoryParser.TaskInfo task = new JobHistoryParser.TaskInfo();\r\n    task.taskId = new TaskID(job.getJobId(), type, id);\r\n    task.startTime = job.getLaunchTime() + id * 1000;\r\n    task.finishTime = task.startTime + id * 1000;\r\n    task.taskType = type;\r\n    task.counters = createCounters();\r\n    task.status = status.name();\r\n    task.attemptsMap = new HashMap<>();\r\n    addTaskAttemptInfo(task, 1);\r\n    job.tasksMap.put(task.getTaskId(), task);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "addTaskAttemptInfo",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void addTaskAttemptInfo(JobHistoryParser.TaskInfo task, int id)\n{\r\n    JobHistoryParser.TaskAttemptInfo attempt = new JobHistoryParser.TaskAttemptInfo();\r\n    attempt.attemptId = new TaskAttemptID(TaskID.downgrade(task.getTaskId()), id);\r\n    attempt.startTime = task.getStartTime();\r\n    attempt.finishTime = task.getFinishTime();\r\n    attempt.shuffleFinishTime = task.getFinishTime();\r\n    attempt.sortFinishTime = task.getFinishTime();\r\n    attempt.mapFinishTime = task.getFinishTime();\r\n    attempt.status = task.getTaskStatus();\r\n    attempt.taskType = task.getTaskType();\r\n    attempt.trackerName = \"localhost\";\r\n    attempt.httpPort = 1234;\r\n    attempt.hostname = \"localhost\";\r\n    task.attemptsMap.put(attempt.getAttemptId(), attempt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testClusterAdmins",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testClusterAdmins()\n{\r\n    Map<JobACL, AccessControlList> tmpJobACLs = new HashMap<JobACL, AccessControlList>();\r\n    Configuration conf = new Configuration();\r\n    String jobOwner = \"testuser\";\r\n    conf.set(JobACL.VIEW_JOB.getAclName(), jobOwner);\r\n    conf.set(JobACL.MODIFY_JOB.getAclName(), jobOwner);\r\n    conf.setBoolean(MRConfig.MR_ACLS_ENABLED, true);\r\n    String clusterAdmin = \"testuser2\";\r\n    conf.set(MRConfig.MR_ADMINS, clusterAdmin);\r\n    JobACLsManager aclsManager = new JobACLsManager(conf);\r\n    tmpJobACLs = aclsManager.constructJobACLs(conf);\r\n    final Map<JobACL, AccessControlList> jobACLs = tmpJobACLs;\r\n    UserGroupInformation callerUGI = UserGroupInformation.createUserForTesting(clusterAdmin, new String[] {});\r\n    boolean val = aclsManager.checkAccess(callerUGI, JobACL.VIEW_JOB, jobOwner, jobACLs.get(JobACL.VIEW_JOB));\r\n    assertTrue(\"cluster admin should have view access\", val);\r\n    val = aclsManager.checkAccess(callerUGI, JobACL.MODIFY_JOB, jobOwner, jobACLs.get(JobACL.MODIFY_JOB));\r\n    assertTrue(\"cluster admin should have modify access\", val);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testClusterNoAdmins",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testClusterNoAdmins()\n{\r\n    Map<JobACL, AccessControlList> tmpJobACLs = new HashMap<JobACL, AccessControlList>();\r\n    Configuration conf = new Configuration();\r\n    String jobOwner = \"testuser\";\r\n    conf.set(JobACL.VIEW_JOB.getAclName(), \"\");\r\n    conf.setBoolean(MRConfig.MR_ACLS_ENABLED, true);\r\n    String noAdminUser = \"testuser2\";\r\n    JobACLsManager aclsManager = new JobACLsManager(conf);\r\n    tmpJobACLs = aclsManager.constructJobACLs(conf);\r\n    final Map<JobACL, AccessControlList> jobACLs = tmpJobACLs;\r\n    UserGroupInformation callerUGI = UserGroupInformation.createUserForTesting(noAdminUser, new String[] {});\r\n    boolean val = aclsManager.checkAccess(callerUGI, JobACL.VIEW_JOB, jobOwner, jobACLs.get(JobACL.VIEW_JOB));\r\n    assertFalse(\"random user should not have view access\", val);\r\n    val = aclsManager.checkAccess(callerUGI, JobACL.MODIFY_JOB, jobOwner, jobACLs.get(JobACL.MODIFY_JOB));\r\n    assertFalse(\"random user should not have modify access\", val);\r\n    callerUGI = UserGroupInformation.createUserForTesting(jobOwner, new String[] {});\r\n    val = aclsManager.checkAccess(callerUGI, JobACL.VIEW_JOB, jobOwner, jobACLs.get(JobACL.VIEW_JOB));\r\n    assertTrue(\"owner should have view access\", val);\r\n    val = aclsManager.checkAccess(callerUGI, JobACL.MODIFY_JOB, jobOwner, jobACLs.get(JobACL.MODIFY_JOB));\r\n    assertTrue(\"owner should have modify access\", val);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAclsOff",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAclsOff()\n{\r\n    Map<JobACL, AccessControlList> tmpJobACLs = new HashMap<JobACL, AccessControlList>();\r\n    Configuration conf = new Configuration();\r\n    String jobOwner = \"testuser\";\r\n    conf.set(JobACL.VIEW_JOB.getAclName(), jobOwner);\r\n    conf.setBoolean(MRConfig.MR_ACLS_ENABLED, false);\r\n    String noAdminUser = \"testuser2\";\r\n    JobACLsManager aclsManager = new JobACLsManager(conf);\r\n    tmpJobACLs = aclsManager.constructJobACLs(conf);\r\n    final Map<JobACL, AccessControlList> jobACLs = tmpJobACLs;\r\n    UserGroupInformation callerUGI = UserGroupInformation.createUserForTesting(noAdminUser, new String[] {});\r\n    boolean val = aclsManager.checkAccess(callerUGI, JobACL.VIEW_JOB, jobOwner, jobACLs.get(JobACL.VIEW_JOB));\r\n    assertTrue(\"acls off so anyone should have access\", val);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGroups",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGroups()\n{\r\n    Map<JobACL, AccessControlList> tmpJobACLs = new HashMap<JobACL, AccessControlList>();\r\n    Configuration conf = new Configuration();\r\n    String jobOwner = \"testuser\";\r\n    conf.set(JobACL.VIEW_JOB.getAclName(), jobOwner);\r\n    conf.setBoolean(MRConfig.MR_ACLS_ENABLED, true);\r\n    String user = \"testuser2\";\r\n    String adminGroup = \"adminGroup\";\r\n    conf.set(MRConfig.MR_ADMINS, \" \" + adminGroup);\r\n    JobACLsManager aclsManager = new JobACLsManager(conf);\r\n    tmpJobACLs = aclsManager.constructJobACLs(conf);\r\n    final Map<JobACL, AccessControlList> jobACLs = tmpJobACLs;\r\n    UserGroupInformation callerUGI = UserGroupInformation.createUserForTesting(user, new String[] { adminGroup });\r\n    boolean val = aclsManager.checkAccess(callerUGI, JobACL.VIEW_JOB, jobOwner, jobACLs.get(JobACL.VIEW_JOB));\r\n    assertTrue(\"user in admin group should have access\", val);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "numberOfTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int numberOfTaskAttempts()\n{\r\n    return ManifestCommitterTestSupport.NUMBER_OF_TASK_ATTEMPTS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    taskAttemptCount = numberOfTaskAttempts();\r\n    Assertions.assertThat(taskAttemptCount).describedAs(\"Task attempt count\").isGreaterThan(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "testSaveThenLoadManyManifests",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testSaveThenLoadManyManifests() throws Throwable\n{\r\n    describe(\"Creating many manifests with fake file/dir entries,\" + \" load them and prepare the output dirs.\");\r\n    int filesPerTaskAttempt = 10;\r\n    LOG.info(\"Number of task attempts: {}, files per task attempt {}\", taskAttemptCount, filesPerTaskAttempt);\r\n    setJobStageConfig(createStageConfigForJob(JOB1, getDestDir()));\r\n    new SetupJobStage(getJobStageConfig()).apply(false);\r\n    LOG.info(\"Creating manifest files for {}\", taskAttemptCount);\r\n    executeTaskAttempts(taskAttemptCount, filesPerTaskAttempt);\r\n    LOG.info(\"Loading in the manifests\");\r\n    LoadManifestsStage stage = new LoadManifestsStage(getJobStageConfig());\r\n    LoadManifestsStage.Result result = stage.apply(true);\r\n    LoadManifestsStage.SummaryInfo summary = result.getSummary();\r\n    List<TaskManifest> loadedManifests = result.getManifests();\r\n    Assertions.assertThat(summary.getManifestCount()).describedAs(\"Manifest count of  %s\", summary).isEqualTo(taskAttemptCount);\r\n    Assertions.assertThat(summary.getFileCount()).describedAs(\"File count of  %s\", summary).isEqualTo(taskAttemptCount * (long) filesPerTaskAttempt);\r\n    Assertions.assertThat(summary.getTotalFileSize()).describedAs(\"File Size of  %s\", summary).isEqualTo(getTotalDataSize());\r\n    List<String> manifestTaskIds = loadedManifests.stream().map(TaskManifest::getTaskID).collect(Collectors.toList());\r\n    Assertions.assertThat(getTaskIds()).describedAs(\"Task IDs of all tasks\").containsExactlyInAnyOrderElementsOf(manifestTaskIds);\r\n    Set<Path> createdDirectories = new CreateOutputDirectoriesStage(getJobStageConfig()).apply(loadedManifests).getCreatedDirectories();\r\n    Assertions.assertThat(createdDirectories).describedAs(\"Directories created\").hasSize(filesPerTaskAttempt);\r\n    new CleanupJobStage(getJobStageConfig()).apply(new CleanupJobStage.Arguments(\"\", true, true, false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getEtag",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getEtag()\n{\r\n    return etag;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
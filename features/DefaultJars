[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\main\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration configuration)\n{\r\n    conf = configuration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\main\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "printHelp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void printHelp(Options options)\n{\r\n    HelpFormatter formatter = new HelpFormatter();\r\n    formatter.printHelp(\"mapred frameworkuploader\", options);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\main\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "run",
  "errType" : [ "UploaderException|IOException|InterruptedException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void run()\n{\r\n    try {\r\n        collectPackages();\r\n        buildPackage();\r\n        LOG.info(\"Uploaded \" + target);\r\n        System.out.println(\"Suggested mapreduce.application.framework.path \" + target);\r\n        LOG.info(\"Suggested mapreduce.application.classpath $PWD/\" + alias + \"/*\");\r\n        System.out.println(\"Suggested classpath $PWD/\" + alias + \"/*\");\r\n    } catch (UploaderException | IOException | InterruptedException e) {\r\n        LOG.error(\"Error in execution \" + e.getMessage());\r\n        e.printStackTrace();\r\n        throw new RuntimeException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\main\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "collectPackages",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void collectPackages() throws UploaderException\n{\r\n    parseLists();\r\n    String[] list = StringUtils.split(input, File.pathSeparatorChar);\r\n    for (String item : list) {\r\n        LOG.info(\"Original source \" + item);\r\n        String expanded = expandEnvironmentVariables(item, System.getenv());\r\n        LOG.info(\"Expanded source \" + expanded);\r\n        if (expanded.endsWith(\"*\")) {\r\n            File path = new File(expanded.substring(0, expanded.length() - 1));\r\n            if (path.isDirectory()) {\r\n                File[] files = path.listFiles();\r\n                if (files != null) {\r\n                    for (File jar : files) {\r\n                        if (!jar.isDirectory()) {\r\n                            addJar(jar);\r\n                        } else {\r\n                            LOG.info(\"Ignored \" + jar + \" because it is a directory\");\r\n                        }\r\n                    }\r\n                } else {\r\n                    LOG.warn(\"Could not list directory \" + path);\r\n                }\r\n            } else {\r\n                LOG.warn(\"Ignored \" + expanded + \". It is not a directory\");\r\n            }\r\n        } else if (expanded.endsWith(\".jar\")) {\r\n            File jarFile = new File(expanded);\r\n            addJar(jarFile);\r\n        } else if (!expanded.isEmpty()) {\r\n            LOG.warn(\"Ignored \" + expanded + \" only jars are supported\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\main\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "beginUpload",
  "errType" : [ "AccessControlException" ],
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void beginUpload() throws IOException, UploaderException\n{\r\n    if (targetStream == null) {\r\n        int lastIndex = target.indexOf('#');\r\n        targetPath = new Path(target.substring(0, lastIndex == -1 ? target.length() : lastIndex));\r\n        alias = lastIndex != -1 ? target.substring(lastIndex + 1) : targetPath.getName();\r\n        LOG.info(\"Target \" + targetPath);\r\n        FileSystem fileSystem = targetPath.getFileSystem(conf);\r\n        targetStream = null;\r\n        if (fileSystem instanceof DistributedFileSystem) {\r\n            LOG.info(\"Set replication to \" + initialReplication + \" for path: \" + targetPath);\r\n            LOG.info(\"Disabling Erasure Coding for path: \" + targetPath);\r\n            DistributedFileSystem dfs = (DistributedFileSystem) fileSystem;\r\n            DistributedFileSystem.HdfsDataOutputStreamBuilder builder = dfs.createFile(targetPath).overwrite(true).ecPolicyName(SystemErasureCodingPolicies.getReplicationPolicy().getName());\r\n            if (initialReplication > 0) {\r\n                builder.replication(initialReplication);\r\n            }\r\n            targetStream = builder.build();\r\n        } else {\r\n            LOG.warn(\"Cannot set replication to \" + initialReplication + \" for path: \" + targetPath + \" on a non-distributed fileystem \" + fileSystem.getClass().getName());\r\n        }\r\n        if (targetStream == null) {\r\n            targetStream = fileSystem.create(targetPath, true);\r\n        }\r\n        if (!FRAMEWORK_PERMISSION.equals(FRAMEWORK_PERMISSION.applyUMask(FsPermission.getUMask(conf)))) {\r\n            LOG.info(\"Modifying permissions to \" + FRAMEWORK_PERMISSION);\r\n            fileSystem.setPermission(targetPath, FRAMEWORK_PERMISSION);\r\n        }\r\n        fsDataStream = (FSDataOutputStream) targetStream;\r\n        if (targetPath.getName().endsWith(\"gz\") || targetPath.getName().endsWith(\"tgz\")) {\r\n            LOG.info(\"Creating GZip\");\r\n            targetStream = new GZIPOutputStream(targetStream);\r\n        }\r\n        Path current = targetPath.getParent();\r\n        while (current != null) {\r\n            try {\r\n                FileStatus fstat = fileSystem.getFileStatus(current);\r\n                FsPermission perm = fstat.getPermission();\r\n                boolean userCanEnter = perm.getUserAction().implies(FsAction.EXECUTE);\r\n                boolean groupCanEnter = perm.getGroupAction().implies(FsAction.EXECUTE);\r\n                boolean othersCanEnter = perm.getOtherAction().implies(FsAction.EXECUTE);\r\n                if (!userCanEnter || !groupCanEnter || !othersCanEnter) {\r\n                    LOG.warn(\"Path \" + current + \" is not accessible\" + \" for all users. Current permissions are: \" + perm);\r\n                    LOG.warn(\"Please set EXECUTE permissions on this directory\");\r\n                }\r\n                current = current.getParent();\r\n            } catch (AccessControlException e) {\r\n                LOG.warn(\"Path \" + current + \" is not accessible,\" + \" cannot retrieve permissions\");\r\n                LOG.warn(\"Please set EXECUTE permissions on this directory\");\r\n                LOG.debug(\"Stack trace\", e);\r\n                break;\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\main\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "getSmallestReplicatedBlockCount",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "long getSmallestReplicatedBlockCount() throws IOException\n{\r\n    FileSystem fileSystem = targetPath.getFileSystem(conf);\r\n    FileStatus status = fileSystem.getFileStatus(targetPath);\r\n    long length = status.getLen();\r\n    HashMap<Long, Integer> blockCount = new HashMap<>();\r\n    for (long offset = 0; offset < length; offset += status.getBlockSize()) {\r\n        blockCount.put(offset, 0);\r\n    }\r\n    BlockLocation[] locations = fileSystem.getFileBlockLocations(targetPath, 0, length);\r\n    for (BlockLocation location : locations) {\r\n        final int replicas = location.getHosts().length;\r\n        blockCount.compute(location.getOffset(), (key, value) -> value == null ? 0 : value + replicas);\r\n    }\r\n    for (long offset = 0; offset < length; offset += status.getBlockSize()) {\r\n        LOG.info(String.format(\"Replication counts offset:%d blocks:%d\", offset, blockCount.get(offset)));\r\n    }\r\n    return Collections.min(blockCount.values());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\main\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "endUpload",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void endUpload() throws IOException, InterruptedException\n{\r\n    FileSystem fileSystem = targetPath.getFileSystem(conf);\r\n    if (fileSystem instanceof DistributedFileSystem) {\r\n        fileSystem.setReplication(targetPath, finalReplication);\r\n        LOG.info(\"Set replication to \" + finalReplication + \" for path: \" + targetPath);\r\n        if (timeout == 0) {\r\n            LOG.info(\"Timeout is set to 0. Skipping replication check.\");\r\n        } else {\r\n            long startTime = System.currentTimeMillis();\r\n            long endTime = startTime;\r\n            long currentReplication = 0;\r\n            while (endTime - startTime < timeout * 1000 && currentReplication < acceptableReplication) {\r\n                Thread.sleep(1000);\r\n                endTime = System.currentTimeMillis();\r\n                currentReplication = getSmallestReplicatedBlockCount();\r\n            }\r\n            if (endTime - startTime >= timeout * 1000) {\r\n                LOG.error(String.format(\"Timed out after %d seconds while waiting for acceptable\" + \" replication of %d (current replication is %d)\", timeout, acceptableReplication, currentReplication));\r\n            }\r\n        }\r\n    } else {\r\n        LOG.info(\"Cannot set replication to \" + finalReplication + \" for path: \" + targetPath + \" on a non-distributed fileystem \" + fileSystem.getClass().getName());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\main\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "buildPackage",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void buildPackage() throws IOException, UploaderException, InterruptedException\n{\r\n    beginUpload();\r\n    LOG.info(\"Compressing tarball\");\r\n    try (TarArchiveOutputStream out = new TarArchiveOutputStream(targetStream)) {\r\n        for (String fullPath : filteredInputFiles) {\r\n            LOG.info(\"Adding \" + fullPath);\r\n            File file = new File(fullPath);\r\n            try (FileInputStream inputStream = new FileInputStream(file)) {\r\n                ArchiveEntry entry = out.createArchiveEntry(file, file.getName());\r\n                out.putArchiveEntry(entry);\r\n                IOUtils.copyBytes(inputStream, out, 1024 * 1024);\r\n                out.closeArchiveEntry();\r\n            }\r\n        }\r\n        fsDataStream.hflush();\r\n        endUpload();\r\n    } finally {\r\n        if (targetStream != null) {\r\n            targetStream.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\main\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "parseLists",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void parseLists() throws UploaderException\n{\r\n    Map<String, String> env = System.getenv();\r\n    for (Map.Entry<String, String> item : env.entrySet()) {\r\n        LOG.info(\"Environment \" + item.getKey() + \" \" + item.getValue());\r\n    }\r\n    String[] whiteListItems = StringUtils.split(whitelist);\r\n    for (String pattern : whiteListItems) {\r\n        String expandedPattern = expandEnvironmentVariables(pattern, env);\r\n        Pattern compiledPattern = Pattern.compile(\"^\" + expandedPattern + \"$\");\r\n        LOG.info(\"Whitelisted \" + compiledPattern.toString());\r\n        whitelistedFiles.add(compiledPattern);\r\n    }\r\n    String[] blacklistItems = StringUtils.split(blacklist);\r\n    for (String pattern : blacklistItems) {\r\n        String expandedPattern = expandEnvironmentVariables(pattern, env);\r\n        Pattern compiledPattern = Pattern.compile(\"^\" + expandedPattern + \"$\");\r\n        LOG.info(\"Blacklisted \" + compiledPattern.toString());\r\n        blacklistedFiles.add(compiledPattern);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\main\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "expandEnvironmentVariables",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String expandEnvironmentVariables(String innerInput, Map<String, String> env) throws UploaderException\n{\r\n    boolean found;\r\n    do {\r\n        found = false;\r\n        Matcher matcher = VAR_SUBBER.matcher(innerInput);\r\n        StringBuffer stringBuffer = new StringBuffer();\r\n        while (matcher.find()) {\r\n            found = true;\r\n            String var = matcher.group(1);\r\n            String replace = env.get(var);\r\n            if (replace == null) {\r\n                throw new UploaderException(\"Environment variable does not exist \" + var);\r\n            }\r\n            matcher.appendReplacement(stringBuffer, Matcher.quoteReplacement(replace));\r\n        }\r\n        matcher.appendTail(stringBuffer);\r\n        innerInput = stringBuffer.toString();\r\n    } while (found);\r\n    return innerInput;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\main\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "addJar",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void addJar(File jar) throws UploaderException\n{\r\n    boolean found = false;\r\n    if (!jar.getName().endsWith(\".jar\")) {\r\n        LOG.info(\"Ignored non-jar \" + jar.getAbsolutePath());\r\n    }\r\n    for (Pattern pattern : whitelistedFiles) {\r\n        Matcher matcher = pattern.matcher(jar.getAbsolutePath());\r\n        if (matcher.matches()) {\r\n            LOG.info(\"Whitelisted \" + jar.getAbsolutePath());\r\n            found = true;\r\n            break;\r\n        }\r\n    }\r\n    boolean excluded = false;\r\n    for (Pattern pattern : blacklistedFiles) {\r\n        Matcher matcher = pattern.matcher(jar.getAbsolutePath());\r\n        if (matcher.matches()) {\r\n            LOG.info(\"Blacklisted \" + jar.getAbsolutePath());\r\n            excluded = true;\r\n            break;\r\n        }\r\n    }\r\n    if (ignoreSymlink && !excluded) {\r\n        excluded = checkSymlink(jar);\r\n    }\r\n    if (found && !excluded) {\r\n        LOG.info(\"Whitelisted \" + jar.getAbsolutePath());\r\n        if (!filteredInputFiles.add(jar.getAbsolutePath())) {\r\n            throw new UploaderException(\"Duplicate jar\" + jar.getAbsolutePath());\r\n        }\r\n    }\r\n    if (!found) {\r\n        LOG.info(\"Ignored \" + jar.getAbsolutePath() + \" because it is missing \" + \"from the whitelist\");\r\n    } else if (excluded) {\r\n        LOG.info(\"Ignored \" + jar.getAbsolutePath() + \" because it is on \" + \"the the blacklist\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\main\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "checkSymlink",
  "errType" : [ "NotLinkException", "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "boolean checkSymlink(File jar)\n{\r\n    if (Files.isSymbolicLink(jar.toPath())) {\r\n        try {\r\n            java.nio.file.Path link = Files.readSymbolicLink(jar.toPath());\r\n            java.nio.file.Path jarPath = Paths.get(jar.getAbsolutePath());\r\n            String linkString = link.toString();\r\n            java.nio.file.Path jarParent = jarPath.getParent();\r\n            java.nio.file.Path linkPath = jarParent == null ? null : jarParent.resolve(linkString);\r\n            java.nio.file.Path linkPathParent = linkPath == null ? null : linkPath.getParent();\r\n            java.nio.file.Path normalizedLinkPath = linkPathParent == null ? null : linkPathParent.normalize();\r\n            if (normalizedLinkPath != null && jarParent.normalize().equals(normalizedLinkPath)) {\r\n                LOG.info(String.format(\"Ignoring same directory link %s to %s\", jarPath.toString(), link.toString()));\r\n                return true;\r\n            }\r\n        } catch (NotLinkException ex) {\r\n            LOG.debug(\"Not a link\", jar);\r\n        } catch (IOException ex) {\r\n            LOG.warn(\"Cannot read symbolic link on\", jar);\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\main\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "parseArguments",
  "errType" : null,
  "containingMethodsNum" : 41,
  "sourceCodeText" : "boolean parseArguments(String[] args) throws IOException\n{\r\n    Options opts = new Options();\r\n    opts.addOption(OptionBuilder.create(\"h\"));\r\n    opts.addOption(OptionBuilder.create(\"help\"));\r\n    opts.addOption(OptionBuilder.withDescription(\"Input class path. Defaults to the default classpath.\").hasArg().create(\"input\"));\r\n    opts.addOption(OptionBuilder.withDescription(\"Regex specifying the full path of jars to include in the\" + \" framework tarball. Default is a hardcoded set of jars\" + \" considered necessary to include\").hasArg().create(\"whitelist\"));\r\n    opts.addOption(OptionBuilder.withDescription(\"Regex specifying the full path of jars to exclude in the\" + \" framework tarball. Default is a hardcoded set of jars\" + \" considered unnecessary to include\").hasArg().create(\"blacklist\"));\r\n    opts.addOption(OptionBuilder.withDescription(\"Target file system to upload to.\" + \" Example: hdfs://foo.com:8020\").hasArg().create(\"fs\"));\r\n    opts.addOption(OptionBuilder.withDescription(\"Target file to upload to with a reference name.\" + \" Example: /usr/mr-framework.tar.gz#mr-framework\").hasArg().create(\"target\"));\r\n    opts.addOption(OptionBuilder.withDescription(\"Desired initial replication count. Default 3.\").hasArg().create(\"initialReplication\"));\r\n    opts.addOption(OptionBuilder.withDescription(\"Desired final replication count. Default 10.\").hasArg().create(\"finalReplication\"));\r\n    opts.addOption(OptionBuilder.withDescription(\"Desired acceptable replication count. Default 9.\").hasArg().create(\"acceptableReplication\"));\r\n    opts.addOption(OptionBuilder.withDescription(\"Desired timeout for the acceptable\" + \" replication in seconds. Default 10\").hasArg().create(\"timeout\"));\r\n    opts.addOption(OptionBuilder.withDescription(\"Ignore symlinks into the same directory\").create(\"nosymlink\"));\r\n    GenericOptionsParser parser = new GenericOptionsParser(opts, args);\r\n    if (parser.getCommandLine().hasOption(\"help\") || parser.getCommandLine().hasOption(\"h\")) {\r\n        printHelp(opts);\r\n        return false;\r\n    }\r\n    input = parser.getCommandLine().getOptionValue(\"input\", System.getProperty(\"java.class.path\"));\r\n    whitelist = parser.getCommandLine().getOptionValue(\"whitelist\", DefaultJars.DEFAULT_MR_JARS);\r\n    blacklist = parser.getCommandLine().getOptionValue(\"blacklist\", DefaultJars.DEFAULT_EXCLUDED_MR_JARS);\r\n    initialReplication = Short.parseShort(parser.getCommandLine().getOptionValue(\"initialReplication\", \"3\"));\r\n    finalReplication = Short.parseShort(parser.getCommandLine().getOptionValue(\"finalReplication\", \"10\"));\r\n    acceptableReplication = Short.parseShort(parser.getCommandLine().getOptionValue(\"acceptableReplication\", \"9\"));\r\n    timeout = Integer.parseInt(parser.getCommandLine().getOptionValue(\"timeout\", \"10\"));\r\n    if (parser.getCommandLine().hasOption(\"nosymlink\")) {\r\n        ignoreSymlink = true;\r\n    }\r\n    String fs = parser.getCommandLine().getOptionValue(\"fs\", null);\r\n    String path = parser.getCommandLine().getOptionValue(\"target\", \"/usr/lib/mr-framework.tar.gz#mr-framework\");\r\n    boolean isFullPath = path.startsWith(\"hdfs://\") || path.startsWith(\"file://\");\r\n    if (fs == null) {\r\n        fs = conf.getTrimmed(FS_DEFAULT_NAME_KEY);\r\n        if (fs == null && !isFullPath) {\r\n            LOG.error(\"No filesystem specified in either fs or target.\");\r\n            printHelp(opts);\r\n            return false;\r\n        } else {\r\n            LOG.info(String.format(\"Target file system not specified. Using default %s\", fs));\r\n        }\r\n    }\r\n    if (path.isEmpty()) {\r\n        LOG.error(\"Target directory not specified\");\r\n        printHelp(opts);\r\n        return false;\r\n    }\r\n    StringBuilder absolutePath = new StringBuilder();\r\n    if (!isFullPath) {\r\n        absolutePath.append(fs);\r\n        absolutePath.append(path.startsWith(\"/\") ? \"\" : \"/\");\r\n    }\r\n    absolutePath.append(path);\r\n    target = absolutePath.toString();\r\n    if (parser.getRemainingArgs().length > 0) {\r\n        LOG.warn(\"Unexpected parameters\");\r\n        printHelp(opts);\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\main\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    FrameworkUploader uploader = new FrameworkUploader();\r\n    if (uploader.parseArguments(args)) {\r\n        uploader.run();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]